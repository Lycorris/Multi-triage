{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T14:15:13.862036Z",
     "start_time": "2024-04-04T14:15:11.399342Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6364a1518c0e7be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T14:15:14.377836Z",
     "start_time": "2024-04-04T14:15:14.277260Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_map(labels):\n",
    "    l_set = set()\n",
    "    for label in labels:\n",
    "        l_set.update(label)\n",
    "    ids2token = list(l_set)\n",
    "    token2ids = {ids2token[i] : i for i in range(len(ids2token))}\n",
    "    return ids2token, token2ids\n",
    "\n",
    "def onehot(labels, token2ids):\n",
    "    vec = [0 for i in token2ids]\n",
    "    for label in labels.split('| '):\n",
    "        vec[token2ids[label]] = 1\n",
    "    return vec\n",
    "\n",
    "def lab(labels, token2ids):\n",
    "    return [token2ids[label] for label in labels.split('| ')]\n",
    "\n",
    "def label_vectorize(data):\n",
    "    data = data.rename(columns={'Title_Description' : 'Context', 'AST' : 'AST', 'FixedByID' : 'Dev', 'Name' : 'Btype'})\n",
    "    data = data[['Context', 'AST', 'Dev', 'Btype']]\n",
    "    # avoid NaN in dataset\n",
    "    data['Context'].fillna('[UNK]', inplace=True)\n",
    "    data['AST'].fillna('[UNK]', inplace=True)\n",
    "    data['Dev'].fillna('unknown', inplace=True)\n",
    "    data['Btype'].fillna('unknown', inplace=True)\n",
    "    \n",
    "    D_labels = [label.split('| ') for label in data['Dev']]\n",
    "    _D_ids2token, D_token2ids = get_map(D_labels)\n",
    "    data['Dev_l'] = data['Dev'].map(partial(lab, token2ids = D_token2ids))\n",
    "    data['Dev_vec'] = data['Dev'].map(partial(onehot, token2ids = D_token2ids))\n",
    "    \n",
    "    B_labels = [label.split('| ') for label in data['Btype']]\n",
    "    _B_ids2token, B_token2ids = get_map(B_labels)\n",
    "    data['Btype_l'] = data['Btype'].map(partial(lab, token2ids = B_token2ids))\n",
    "    data['Btype_vec'] = data['Btype'].map(partial(onehot, token2ids = B_token2ids))\n",
    "    \n",
    "    return data, _D_ids2token, _B_ids2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db4e3511da0459d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T14:15:40.349664Z",
     "start_time": "2024-04-04T14:15:17.920526Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 可能AST被截断太多\n",
    "def tokenize_function(_tokenizer, example, max_seq_len = 512):\n",
    "    example = example if type(example) == str else _tokenizer.unk_token\n",
    "    return _tokenizer(example, padding='max_length',\n",
    "                                truncation=True, max_length=max_seq_len, return_tensors=\"pt\")\n",
    "\n",
    "def tensor_func(example):\n",
    "    return torch.tensor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfec14fda1e80d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T14:15:40.354460Z",
     "start_time": "2024-04-04T14:15:40.347251Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return self.data['Input'][item], self.data['Output'][item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d46b7a-9374-4201-ada9-792baef0bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, n_classes: list, seq_len = 256, vocab_size: list = [30700, 30700], emb_dim = 300, filter: list = [64, 64], linear_concat = 60):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: list, the size of C/A vocab\n",
    "            emb_dim: : int, the dim of C&A emb layer\n",
    "            seq_len:  MAX_SEQ_LEN\n",
    "            n_classes: list, the output size of D/B\n",
    "        \"\"\"\n",
    "        super(MetaModel, self).__init__()\n",
    "        # 1. Embedding\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_C = nn.Embedding(self.vocab_size[0], self.emb_dim)\n",
    "        self.emb_A = nn.Embedding(self.vocab_size[1], self.emb_dim)\n",
    "\n",
    "        # 2. Feature Extracting separately\n",
    "        self.filter_C, self.filter_A = filter\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_C = nn.Sequential(\n",
    "            # (Batch_sz, emb_dim, seq_len)\n",
    "            nn.Conv1d(self.emb_dim, self.filter_C, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            # (Batch_sz, filter_c, seq_len)\n",
    "            nn.MaxPool1d(self.seq_len, 1),\n",
    "            # (Batch_sz, filter_c, 1)\n",
    "            nn.Flatten(),\n",
    "            # (Batch_sz, filter_c)\n",
    "        )\n",
    "        self.feature_A = nn.Sequential(\n",
    "            nn.Conv1d(self.emb_dim, self.filter_A, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.seq_len, 1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # 3. Joint Linear\n",
    "        self.linear_concat = linear_concat\n",
    "        self.fc = nn.Sequential(\n",
    "            # (Batch_sz, filter_C + filter_A)\n",
    "            nn.BatchNorm1d(self.filter_C + self.filter_A, affine=False),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.filter_C + self.filter_A, self.linear_concat),\n",
    "            # (Batch_sz, linear_concat)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 4. Respective CLS\n",
    "        self.n_classes_D, self.n_classes_B = n_classes\n",
    "        self.fc_D = nn.Linear(self.linear_concat, self.n_classes_D)\n",
    "        self.fc_B = nn.Linear(self.linear_concat, self.n_classes_B)\n",
    "\n",
    "    def forward(self, x_C, x_A):\n",
    "        # 1. Embedding\n",
    "        # (Batch_sz, seq_len)\n",
    "        x_C = self.emb_C(x_C)\n",
    "        x_A = self.emb_A(x_A)\n",
    "        # (Batch_sz, seq_len, emb_dim)\n",
    "        # 2. Feature Extracting separately\n",
    "        x_C = x_C.permute(0, 2, 1)\n",
    "        x_A = x_A.permute(0, 2, 1)\n",
    "        # (Batch_sz, emb_dim, seq_len)\n",
    "        x_C = self.feature_C(x_C)\n",
    "        x_A = self.feature_A(x_A)\n",
    "        # (Batch_sz, filter)\n",
    "        # 3. Joint Linear\n",
    "        x = torch.concat((x_C, x_A), 1)\n",
    "        # (Batch_sz, filter_C + filter_A)\n",
    "        x = self.fc(x)\n",
    "        # (Batch_sz, linear_concat)\n",
    "        # 4. Respective CLS\n",
    "        y_D = self.fc_D(x)\n",
    "        # (Batch_sz, n_classes_D)\n",
    "        y_B = self.fc_B(x)\n",
    "        # (Batch_sz, n_classes_B)\n",
    "        y = torch.concat((y_D, y_B), dim = 1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe02fce-275e-4ca0-80f2-f25e72398747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    a flexible version of BCE,\n",
    "    which enable the loss to focus more on the performance of positive samples' prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_pos=0.8, weight_neg=0.2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight_pos = weight_pos\n",
    "        self.weight_neg = weight_neg\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = nn.Sigmoid()(x)\n",
    "        loss_pos = y * torch.log(x)\n",
    "        loss_neg = (1 - y) * torch.log(1 - x)\n",
    "        # loss = 0.8*loss_pos + 0.2*loss_neg\n",
    "        loss = self.weight_pos * loss_pos + self.weight_neg * loss_neg\n",
    "        return -torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95c873dd3b1393b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T14:15:40.380211Z",
     "start_time": "2024-04-04T14:15:40.359063Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def metrics(y: torch.Tensor, pred: torch.Tensor, split_pos: list, threshold: float = 0.5, from_logits=True):\n",
    "    if from_logits:\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "    pred = torch.where(pred > threshold, 1, 0)\n",
    "\n",
    "    y_d, y_b = torch.split(y, split_pos, dim=1)\n",
    "    pred_d, pred_b = torch.split(pred, split_pos, dim=1)\n",
    "\n",
    "    TPd, TPb = torch.sum(y_d * pred_d, dim=1), torch.sum(y_b * pred_b, dim=1)\n",
    "    TNd, TNb = torch.sum((1 - y_d) * (1 - pred_d), dim=1), torch.sum((1 - y_b) * (1 - pred_b), dim=1)\n",
    "    FPd, FPb = torch.sum((1 - y_d) * pred_d, dim=1), torch.sum((1 - y_b) * pred_b, dim=1)\n",
    "    FNd, FNb = torch.sum(y_d * (1 - pred_d), dim=1), torch.sum(y_b * (1 - pred_b), dim=1)\n",
    "\n",
    "    acc = torch.mean((TPd + TNd) / (TPd + TNd + FPd + FNd + 1e-6)).item(), torch.mean(\n",
    "        (TPb + TNb) / (TPb + TNb + FPb + FNb + 1e-6)).item()\n",
    "    recall = torch.mean(TPd / (TPd + FNd + 1e-6)).item(), torch.mean(TPb / (TPb + FNb + 1e-6)).item()\n",
    "    precision = torch.mean(TPd / (TPd + FPd + 1e-6)).item(), torch.mean(TPb / (TPb + FPb + 1e-6)).item()\n",
    "    F1 = 2 * recall[0] * precision[0] / (recall[0] + precision[0] + 1e-6), 2 * recall[1] * precision[1] / (\n",
    "            recall[1] + precision[1] + 1e-6)\n",
    "\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': F1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8710ecaf-fa9b-4e52-b973-b544e363f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imm(_path, _logname, _loss_fn, _is_textcnn = False, _num_epochs = 20, _lr = 3e-5, _ckpt = 'bert-base-uncased', device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    logname = '../res_log/' + _logname + '.txt'\n",
    "    logstr = _logname + '\\n' + '-'*60 + '\\n'\n",
    "    \n",
    "    # dataset label vectorize\n",
    "    dataset = pd.read_csv(_path)\n",
    "    logstr += 'dataset shape:{}\\n'.format(dataset.shape)\n",
    "    print('dataset shape:{}'.format(dataset.shape))\n",
    "    dataset, D_ids2token, B_ids2token = label_vectorize(dataset)\n",
    "    n_classes = [len(D_ids2token), len(B_ids2token)]\n",
    "    logstr += 'n_classes:{}\\n'.format(n_classes) + '-'*60 + '\\n'\n",
    "    print('n_classes: ', n_classes)\n",
    "\n",
    "    check_point = _ckpt\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "    # datset tensorize\n",
    "    dataset['Input'] = dataset['Context'] + dataset['AST']\n",
    "    dataset['Input'] = dataset['Input'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['Output'] = dataset['Dev_vec'] + dataset['Btype_vec']\n",
    "    dataset['Output'] = dataset['Output'].map(tensor_func)\n",
    "\n",
    "    # split datset\n",
    "    train_dataset = dataset[:int(0.64*len(dataset))].reset_index(drop=True)\n",
    "    # TODO: val random -- scikit\n",
    "    val_dataset = dataset[int(0.64*len(dataset)) : int(0.8*len(dataset))].reset_index(drop=True)\n",
    "    test_dataset = dataset[int(0.8*len(dataset)):].reset_index(drop=True)\n",
    "\n",
    "    # wrap dataset & dataloader\n",
    "    train_dataset = TextCodeDataset(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "    val_dataset = TextCodeDataset(val_dataset)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=16)\n",
    "    test_dataset = TextCodeDataset(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "    # load model\n",
    "    if _is_textcnn:\n",
    "        model = MetaModel(n_classes = n_classes)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            check_point, num_labels=n_classes[0] + n_classes[1], problem_type=\"multi_label_classification\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss\n",
    "    loss_fn = _loss_fn.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr)\n",
    "\n",
    "    # lr_scheduler\n",
    "    num_epochs = _num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # train process\n",
    "    for epoch in trange(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = {k: v.to(device) for k, v in x.items()}, y.to(device)\n",
    "            if _is_textcnn:\n",
    "                outputs = model(x['input_ids'].squeeze(dim=1)[:, :256], x['input_ids'].squeeze(dim=1)[:, 256:])\n",
    "            else:\n",
    "                outputs = model(x['input_ids'].squeeze(dim=1), attention_mask=x['attention_mask'].squeeze(dim=1))[0]\n",
    "\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            train_loss += loss.item()/len(train_dataloader)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\n",
    "        # print('{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss))\n",
    "    \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "        for x, y in val_dataloader:\n",
    "            # 这里有没有现成的metrics函数可以用啊 - 库里面的\n",
    "            x, y = {k: v.to(device) for k, v in x.items()}, y.to(device)\n",
    "            if _is_textcnn:\n",
    "                outputs = model(x['input_ids'].squeeze(dim=1)[:, :256], x['input_ids'].squeeze(dim=1)[:, 256:])\n",
    "            else:\n",
    "                outputs = model(x['input_ids'].squeeze(dim=1), attention_mask=x['attention_mask'].squeeze(dim=1))[0]\n",
    "        \n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            val_loss += loss.item()/len(val_dataloader)\n",
    "            metric = metrics(y, outputs, split_pos = n_classes)\n",
    "            val_acc[0] += metric['acc'][0]/len(val_dataloader)\n",
    "            val_acc[1] += metric['acc'][1]/len(val_dataloader)\n",
    "            val_f1[0] += metric['F1'][0]/len(val_dataloader)\n",
    "            val_f1[1] += metric['F1'][1]/len(val_dataloader)\n",
    "        logstr += '{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(epoch, val_loss, val_acc, val_f1)\n",
    "        # print('{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}'.format(epoch, val_loss, val_acc, val_f1))\n",
    "\n",
    "    # test\n",
    "    model.eval()\n",
    "    test_loss, test_acc, test_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "    for x, y in tqdm(test_dataloader):\n",
    "        x, y = {k: v.to(device) for k, v in x.items()}, y.to(device)\n",
    "        if _is_textcnn:\n",
    "            outputs = model(x['input_ids'].squeeze(dim=1)[:, :256], x['input_ids'].squeeze(dim=1)[:, 256:])\n",
    "        else:\n",
    "            outputs = model(x['input_ids'].squeeze(dim=1), attention_mask=x['attention_mask'].squeeze(dim=1))[0]\n",
    "                \n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        test_loss += loss.item()/len(test_dataloader)\n",
    "        metric = metrics(y, outputs, split_pos = n_classes)\n",
    "        test_acc[0] += metric['acc'][0]/len(test_dataloader)\n",
    "        test_acc[1] += metric['acc'][1]/len(test_dataloader)\n",
    "        test_f1[0] += metric['F1'][0]/len(test_dataloader)\n",
    "        test_f1[1] += metric['F1'][1]/len(test_dataloader)\n",
    "    logstr += '-' * 60 + '\\ntest_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1)\n",
    "    print('test_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1))\n",
    "\n",
    "    with open(logname, 'w') as f:\n",
    "        f.write(logstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06eddbeb-50fe-4beb-bcfc-07638df32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 dataset\n",
    "pathlist = [\n",
    "    ('../Data/efcore/IssueefcoreWebScrap.csv', ' efcore'),\n",
    "    ('../Data/elasticSearch/IssueelasticsearchWebScrap.csv', 'elasticSearch'),\n",
    "    ('../Data/mixedRealityToolUnity/IssuemixedrealitytoolkitunityWebScrap.csv', 'mixedRealityToolkitUnity'),\n",
    "    ('../Data/monoGame/IssuemonogameWebScrap.csv', 'monogame'),\n",
    "    ('../Data/powershell/Issueazure-powershellWebScrap.csv', ' powershell'),\n",
    "    ('../Data/realmJava/IssuerealmjavaWebScrap.csv', 'realmJava'),\n",
    "    ('../Data/roslyn/IssueroslynWebScrap.csv', 'roslyn'),\n",
    "]\n",
    "losslist = [\n",
    "    # (nn.BCEWithLogitsLoss(), ' BCE'),\n",
    "    (CustomizedBCELoss(), ' CBCE'),\n",
    "]\n",
    "\n",
    "ckptlist = [\n",
    "    ('bert-base-uncased', 'TextCnn'),  # just for tokenize\n",
    "    ('bert-base-uncased', ' Bert'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9f5bcf-58f3-4100-b88f-f1f52b620be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      " efcore TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.21s/it]\n",
      "100%|██████████| 83/83 [00:00<00:00, 398.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 58.37801600077067\n",
      " test_acc:[0.9704024289027755, 0.9486275803611939]\n",
      " test_f1: [0.6326659284055287, 0.33103401637796614]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " efcore  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [19:16<00:00, 57.83s/it]\n",
      "100%|██████████| 83/83 [00:05<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 58.198204638010054\n",
      " test_acc:[0.9668810741010915, 0.9689105181808929]\n",
      " test_f1: [0.627194250389262, 0.44487236337918135]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "elasticSearch TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(5190, 7)\n",
      "\n",
      "n_classes:  [105, 239]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "100%|██████████| 65/65 [00:00<00:00, 387.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 117.51949533315802\n",
      " test_acc:[0.9921009852336002, 0.9888115479395936]\n",
      " test_f1: [0.5973890403059429, 0.28411149366010224]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "elasticSearch  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(5190, 7)\n",
      "\n",
      "n_classes:  [105, 239]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [15:10<00:00, 45.55s/it]\n",
      "100%|██████████| 65/65 [00:04<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 98.26378373366134\n",
      " test_acc:[0.992620342511397, 0.9912691299731913]\n",
      " test_f1: [0.5778835647298459, 0.3939297324276889]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mixedRealityToolkitUnity TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(2294, 7)\n",
      "\n",
      "n_classes:  [56, 125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.35it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 518.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 229.9492040173761\n",
      " test_acc:[0.9723431262476689, 0.9670125533794537]\n",
      " test_f1: [0.5035057354130569, 0.2679069333614074]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mixedRealityToolkitUnity  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(2294, 7)\n",
      "\n",
      "n_classes:  [56, 125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [06:38<00:00, 19.94s/it]\n",
      "100%|██████████| 29/29 [00:02<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 72.7936168539113\n",
      " test_acc:[0.9846654020506759, 0.9868150558964961]\n",
      " test_f1: [0.6103399669761274, 0.48731759486398823]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "monogame TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1008, 7)\n",
      "\n",
      "n_classes:  [5, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.23it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 444.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 68.82819131704478\n",
      " test_acc:[0.9865382955624509, 0.8708222325031574]\n",
      " test_f1: [0.9780504947949146, 0.3289915489306137]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "monogame  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1008, 7)\n",
      "\n",
      "n_classes:  [5, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [02:55<00:00,  8.79s/it]\n",
      "100%|██████████| 13/13 [00:00<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 26.67033298198993\n",
      " test_acc:[0.9961536618379446, 0.9666113440807048]\n",
      " test_f1: [0.9903831708804972, 0.6462369696195541]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " powershell TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(2540, 7)\n",
      "\n",
      "n_classes:  [334, 150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.08it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 519.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 341.51186752319336\n",
      " test_acc:[0.9891018532216549, 0.9786154739558697]\n",
      " test_f1: [0.1766735567725918, 0.353310928710809]\n",
      "----------------------------------------------------------------------------------------------------\n",
      " powershell  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(2540, 7)\n",
      "\n",
      "n_classes:  [334, 150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [07:22<00:00, 22.12s/it]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 107.40738582611084\n",
      " test_acc:[0.9958598706871271, 0.9842578209936619]\n",
      " test_f1: [0.35546791099945385, 0.4141248890460474]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "realmJava TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1160, 7)\n",
      "\n",
      "n_classes:  [16, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.56it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 455.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 72.78055267333983\n",
      " test_acc:[0.937239464124044, 0.8477429747581482]\n",
      " test_f1: [0.7352861962009034, 0.30120411463555324]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "realmJava  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1160, 7)\n",
      "\n",
      "n_classes:  [16, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [03:22<00:00, 10.14s/it]\n",
      "100%|██████████| 15/15 [00:01<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 36.91586329142253\n",
      " test_acc:[0.9799477974573771, 0.9350693782170612]\n",
      " test_f1: [0.8478021549570639, 0.43160216262663004]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "roslyn TextCnn  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(5062, 7)\n",
      "\n",
      "n_classes:  [81, 124]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.06it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 514.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 113.55053472518921\n",
      " test_acc:[0.9873432740569115, 0.9708952959626913]\n",
      " test_f1: [0.4917959059864993, 0.379168203168294]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "roslyn  Bert  CBCE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(5062, 7)\n",
      "\n",
      "n_classes:  [81, 124]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [14:42<00:00, 44.10s/it]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 89.99552261829376\n",
      " test_acc:[0.9873794391751289, 0.9790212530642748]\n",
      " test_f1: [0.46915675930611195, 0.5004688569873694]\n"
     ]
    }
   ],
   "source": [
    "for path in pathlist:\n",
    "    for ckpt in ckptlist:\n",
    "        for loss in losslist:\n",
    "            is_t = (ckpt[1] == 'TextCnn')\n",
    "            logname = ' '.join([path[1], ckpt[1], loss[1]])\n",
    "            print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            train_imm(_path = path[0], _logname = logname, _loss_fn = loss[0], _is_textcnn = is_t, _ckpt = ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b5586-3e8e-44dd-8ac1-559ea1457121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
