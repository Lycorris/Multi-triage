{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b392f11-4942-47e5-947e-ce499b721456",
   "metadata": {},
   "source": [
    "1. early-stop --> epoch\n",
    "2. hyper-prarm with list\n",
    "- datasets * 8\n",
    "- model * 3\n",
    "- loss * 3\n",
    "- use_ast\n",
    "  \n",
    "3. other hyper-prarm\n",
    "- lr = 3e-5\n",
    "- bsz = 8 (24g4090 + 2Bert)\n",
    "\n",
    "4. TBC\n",
    "- customized loss\n",
    "- ast ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.466477Z",
     "start_time": "2024-04-06T07:03:20.823265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6364a1518c0e7be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.472339Z",
     "start_time": "2024-04-06T07:03:27.466750Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset utils\n",
    "def get_map(labels):\n",
    "    l_set = set()\n",
    "    for label in labels:\n",
    "        l_set.update(label)\n",
    "    ids2token = list(l_set)\n",
    "    token2ids = {ids2token[i] : i for i in range(len(ids2token))}\n",
    "    return ids2token, token2ids\n",
    "\n",
    "def onehot(labels, token2ids):\n",
    "    vec = [0 for i in token2ids]\n",
    "    for label in labels.split('| '):\n",
    "        vec[token2ids[label]] = 1\n",
    "    return vec\n",
    "\n",
    "def lab(labels, token2ids):\n",
    "    return [token2ids[label] for label in labels.split('| ')]\n",
    "\n",
    "def label_vectorize(data):\n",
    "    data = data.rename(columns={'Title_Description' : 'Context', 'AST' : 'AST', 'FixedByID' : 'Dev', 'Name' : 'Btype'})\n",
    "    data = data[['Context', 'AST', 'Dev', 'Btype']]\n",
    "    # avoid NaN in dataset\n",
    "    data['Context'].fillna('[UNK]', inplace=True)\n",
    "    data['AST'].fillna('[UNK]', inplace=True)\n",
    "    data['Dev'].fillna('unknown', inplace=True)\n",
    "    data['Btype'].fillna('unknown', inplace=True)\n",
    "    \n",
    "    D_labels = [label.split('| ') for label in data['Dev']]\n",
    "    _D_ids2token, D_token2ids = get_map(D_labels)\n",
    "    data['Dev_l'] = data['Dev'].map(partial(lab, token2ids = D_token2ids))\n",
    "    data['Dev_vec'] = data['Dev'].map(partial(onehot, token2ids = D_token2ids))\n",
    "    \n",
    "    B_labels = [label.split('| ') for label in data['Btype']]\n",
    "    _B_ids2token, B_token2ids = get_map(B_labels)\n",
    "    data['Btype_l'] = data['Btype'].map(partial(lab, token2ids = B_token2ids))\n",
    "    data['Btype_vec'] = data['Btype'].map(partial(onehot, token2ids = B_token2ids))\n",
    "    \n",
    "    return data, _D_ids2token, _B_ids2token\n",
    "\n",
    "def tokenize_function(_tokenizer, example, max_seq_len = 512):\n",
    "    example = example if type(example) == str else _tokenizer.unk_token\n",
    "    return _tokenizer(example, padding='max_length',\n",
    "                                truncation=True, max_length=max_seq_len, return_tensors=\"pt\")\n",
    "\n",
    "def tensor_func(example):\n",
    "    return torch.tensor(example)\n",
    "\n",
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return (self.data['x_C'][item], self.data['x_A'][item]), self.data['y'][item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe02fce-275e-4ca0-80f2-f25e72398747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T06:23:54.167637Z",
     "start_time": "2024-04-06T06:23:54.158896Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss & metrics\n",
    "class CustomizedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    a flexible version of BCE,\n",
    "    which enable the loss to focus more on the performance of positive samples' prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_pos=0.8, weight_neg=0.2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight_pos = weight_pos\n",
    "        self.weight_neg = weight_neg\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = nn.Sigmoid()(x)\n",
    "        loss_pos = y * torch.log(x)\n",
    "        loss_neg = (1 - y) * torch.log(1 - x)\n",
    "        # loss = 0.8*loss_pos + 0.2*loss_neg\n",
    "        loss = self.weight_pos * loss_pos + self.weight_neg * loss_neg\n",
    "        return -torch.sum(loss)\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    \"\"\"\n",
    "    AsymmetricLoss from https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "\n",
    "    Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()\n",
    "\n",
    "def metrics(y: torch.Tensor, pred: torch.Tensor, split_pos: list, threshold: float = 0.5, from_logits=True):\n",
    "    if from_logits:\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "    pred = torch.where(pred > threshold, 1, 0)\n",
    "\n",
    "    y_d, y_b = torch.split(y, split_pos, dim=1)\n",
    "    pred_d, pred_b = torch.split(pred, split_pos, dim=1)\n",
    "\n",
    "    TPd, TPb = torch.sum(y_d * pred_d, dim=1), torch.sum(y_b * pred_b, dim=1)\n",
    "    TNd, TNb = torch.sum((1 - y_d) * (1 - pred_d), dim=1), torch.sum((1 - y_b) * (1 - pred_b), dim=1)\n",
    "    FPd, FPb = torch.sum((1 - y_d) * pred_d, dim=1), torch.sum((1 - y_b) * pred_b, dim=1)\n",
    "    FNd, FNb = torch.sum(y_d * (1 - pred_d), dim=1), torch.sum(y_b * (1 - pred_b), dim=1)\n",
    "\n",
    "    acc = torch.mean((TPd + TNd) / (TPd + TNd + FPd + FNd + 1e-6)).item(), torch.mean(\n",
    "        (TPb + TNb) / (TPb + TNb + FPb + FNb + 1e-6)).item()\n",
    "    recall = torch.mean(TPd / (TPd + FNd + 1e-6)).item(), torch.mean(TPb / (TPb + FNb + 1e-6)).item()\n",
    "    precision = torch.mean(TPd / (TPd + FPd + 1e-6)).item(), torch.mean(TPb / (TPb + FPb + 1e-6)).item()\n",
    "    F1 = 2 * recall[0] * precision[0] / (recall[0] + precision[0] + 1e-6), 2 * recall[1] * precision[1] / (\n",
    "            recall[1] + precision[1] + 1e-6)\n",
    "\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': F1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710ecaf-fa9b-4e52-b973-b544e363f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imm(_path, _logname, _loss_fn, _use_ast = True, _is_textcnn = False, _num_epochs = 20, _bsz = 8,\n",
    "              _lr = 3e-5, _ckpt = 'bert-base-uncased', device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    logname = '../res_log/' + _logname + '.txt'\n",
    "    logstr = _logname + '\\n' + '-'*60 + '\\n'\n",
    "    \n",
    "    # dataset label vectorize\n",
    "    dataset = pd.read_csv(_path)\n",
    "    logstr += 'dataset shape:{}\\n'.format(dataset.shape)\n",
    "    print('dataset shape:{}'.format(dataset.shape))\n",
    "    dataset, D_ids2token, B_ids2token = label_vectorize(dataset)\n",
    "    n_classes = [len(D_ids2token), len(B_ids2token)]\n",
    "    logstr += 'n_classes:{}\\n'.format(n_classes) + '-'*60 + '\\n'\n",
    "    print('n_classes: ', n_classes)\n",
    "\n",
    "    check_point = _ckpt\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "    # datset tensorize\n",
    "    dataset['x_C'] = dataset['Context'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['x_A'] = dataset['AST'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['y'] = dataset['Dev_vec'] + dataset['Btype_vec']\n",
    "    dataset['y'] = dataset['y'].map(tensor_func)\n",
    "\n",
    "    # split datset\n",
    "    t_dataset = dataset[:int(0.8*len(dataset))].reset_index(drop=True)\n",
    "    train_dataset = t_dataset.sample(frac=0.8,random_state=0,axis=0).reset_index(drop=True)\n",
    "    val_dataset = t_dataset[~t_dataset.index.isin(train_dataset.index)].reset_index(drop=True)\n",
    "    test_dataset = dataset[int(0.8*len(dataset)):].reset_index(drop=True)\n",
    "\n",
    "    # wrap dataset & dataloader\n",
    "    train_dataset = TextCodeDataset(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "    val_dataset = TextCodeDataset(val_dataset)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "    test_dataset = TextCodeDataset(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "\n",
    "    # load model\n",
    "    if _is_textcnn:\n",
    "        model = MetaModel(n_classes = n_classes, use_AST=_use_ast)\n",
    "    else:\n",
    "        # TODO: seperate ckpt\n",
    "        model = PretrainModel(text_ckpt=_ckpt, code_ckpt=_ckpt, n_classes=n_classes, use_AST=_use_ast)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss\n",
    "    loss_fn = _loss_fn.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr)\n",
    "\n",
    "    # lr_scheduler\n",
    "    num_epochs = _num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # train process\n",
    "    val_loss_min, val_down = 100000.0, [1, 1, 1, 1, 1]\n",
    "    for epoch in trange(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "            x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(x_C, x_A)\n",
    "\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            train_loss += loss.item()/len(train_dataloader)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\n",
    "        # print('{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss))\n",
    "    \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "        for x, y in val_dataloader:\n",
    "            x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "            x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x_C, x_A)\n",
    "        \n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            val_loss += loss.item()/len(val_dataloader)\n",
    "            metric = metrics(y, outputs, split_pos = n_classes)\n",
    "            val_acc[0] += metric['acc'][0]/len(val_dataloader)\n",
    "            val_acc[1] += metric['acc'][1]/len(val_dataloader)\n",
    "            val_f1[0] += metric['F1'][0]/len(val_dataloader)\n",
    "            val_f1[1] += metric['F1'][1]/len(val_dataloader)\n",
    "        logstr += '{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(epoch, val_loss, val_acc, val_f1)\n",
    "        # print('{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}'.format(epoch, val_loss, val_acc, val_f1))\n",
    "\n",
    "        val_down.append(1 if val_loss_min - val_loss > 1e-10 else 0)\n",
    "        val_loss_min = min(val_loss_min, val_loss)\n",
    "        if val_down[-1] + val_down[-2] + val_down[-3] + val_down[-4] + val_down[-5] == 0:\n",
    "            break\n",
    "\n",
    "    # test\n",
    "    model.eval()\n",
    "    test_loss, test_acc, test_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "    for x, y in tqdm(test_dataloader):\n",
    "        x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "        x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "        y = y.to(device)\n",
    "            \n",
    "        outputs = model(x_C, x_A)\n",
    "                \n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        test_loss += loss.item()/len(test_dataloader)\n",
    "        metric = metrics(y, outputs, split_pos = n_classes)\n",
    "        test_acc[0] += metric['acc'][0]/len(test_dataloader)\n",
    "        test_acc[1] += metric['acc'][1]/len(test_dataloader)\n",
    "        test_f1[0] += metric['F1'][0]/len(test_dataloader)\n",
    "        test_f1[1] += metric['F1'][1]/len(test_dataloader)\n",
    "    logstr += '-' * 60 + '\\ntest_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1)\n",
    "    print('test_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1))\n",
    "\n",
    "    with open(logname, 'w') as f:\n",
    "        f.write(logstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06eddbeb-50fe-4beb-bcfc-07638df32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 datasets\n",
    "# path, name, dataset size\n",
    "pathlist = [\n",
    "    ('../Data/aspnet/aspnet_1.csv', 'aspnet', 7151),\n",
    "    ('../Data/efcore/efcore_1.csv', 'efcore', ),\n",
    "    ('../Data/elasticSearch/elasticSearch_1.csv', 'elasticSearch'),\n",
    "    ('../Data/mixedRealityToolUnity/mixedRealityToolUnity_1.csv', 'mixedRealityToolUnity'),\n",
    "    ('../Data/monoGame/monoGame_1.csv', 'monoGame'),\n",
    "    ('../Data/powershell/powerShell_1.csv', 'powerShell'),\n",
    "    ('../Data/realmJava/realmJava_1.csv', 'realmJava'),\n",
    "    ('../Data/roslyn/roslyn_1.csv', 'roslyn'),\n",
    "]\n",
    "losslist = [\n",
    "    (nn.BCEWithLogitsLoss(), 'BCE'),\n",
    "    (CustomizedBCELoss(), 'CBCE'),\n",
    "    (AsymmetricLossOptimized(), 'ASL'),\n",
    "]\n",
    "\n",
    "ckptlist = [\n",
    "    ('bert-base-uncased', 'Multi-triage'),  # just for tokenize\n",
    "    ('bert-base-uncased', ' Bert'),\n",
    "    ('roberta-base', 'Robert'),\n",
    "    ('albert-base-v2', 'albert'),\n",
    "]\n",
    "\n",
    "astlist = [\n",
    "    'no_AST',\n",
    "    # 'use_AST',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f5bcf-58f3-4100-b88f-f1f52b620be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 20/20 [00:34<00:00,  1.72s/it]\n",
      "100%|██████████| 178/178 [00:00<00:00, 548.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.08315895236191459\n",
      " test_acc:[0.9914924565995686, 0.9841036927164257]\n",
      " test_f1: [0.7464875521381351, 0.0029260144551772676]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.04s/it]\n",
      "100%|██████████| 178/178 [00:00<00:00, 491.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.192807698517706\n",
      " test_acc:[0.9911816257439314, 0.9730167064104195]\n",
      " test_f1: [0.7443590021458304, 0.3252574691984619]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet  Bert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:27<00:00, 64.39s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.04116710515044041\n",
      " test_acc:[0.9921141156319828, 0.9872106035773665]\n",
      " test_f1: [0.6769651465298621, 0.4813009334402033]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet  Bert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:37<00:00, 64.90s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 28.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 17.13844633906075\n",
      " test_acc:[0.9901225024394777, 0.984295212150958]\n",
      " test_f1: [0.7419702737434072, 0.5681322851241607]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Robert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:44<00:00, 65.21s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.048497773204626664\n",
      " test_acc:[0.9919644580798205, 0.9841462541162281]\n",
      " test_f1: [0.690307830434486, 0.0]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Robert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:54<00:00, 65.71s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 29.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 16.09969470742044\n",
      " test_acc:[0.9891324455148723, 0.9836461677979879]\n",
      " test_f1: [0.7334819687771934, 0.5725674298373816]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet albert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [23:25<00:00, 70.28s/it]\n",
      "100%|██████████| 178/178 [00:07<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.0445819696714955\n",
      " test_acc:[0.991998996627465, 0.9860880729857446]\n",
      " test_f1: [0.6804763791363463, 0.33973016652897636]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet albert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [23:34<00:00, 70.74s/it]\n",
      "100%|██████████| 178/178 [00:07<00:00, 23.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.162022064241139\n",
      " test_acc:[0.9897886417555016, 0.981911832027221]\n",
      " test_f1: [0.7233821751890152, 0.5343408129934141]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.52s/it]\n",
      "100%|██████████| 165/165 [00:00<00:00, 545.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.11704160900730072\n",
      " test_acc:[0.9703938487804292, 0.9714080344546926]\n",
      " test_f1: [0.6113625533222877, 0.0708456471918589]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:37<00:00,  1.88s/it]\n",
      "100%|██████████| 165/165 [00:00<00:00, 485.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 10.80376569863522\n",
      " test_acc:[0.9619090091098434, 0.9374608177127265]\n",
      " test_f1: [0.6181393157525095, 0.3284657550165212]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore  Bert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [19:46<00:00, 59.33s/it]\n",
      "100%|██████████| 165/165 [00:05<00:00, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.091461328778303\n",
      " test_acc:[0.9715756654739375, 0.9743991631450083]\n",
      " test_f1: [0.555063576446371, 0.3636511525008343]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore  Bert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 80%|████████  | 16/20 [16:56<04:14, 63.55s/it]\n",
      "100%|██████████| 165/165 [00:05<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 18.672516912402536\n",
      " test_acc:[0.9661514285838961, 0.9654127500273972]\n",
      " test_f1: [0.6344624430134864, 0.4512340457911784]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Robert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 4/20 [03:59<16:00, 60.00s/it]"
     ]
    }
   ],
   "source": [
    "for path in pathlist:\n",
    "    for ckpt in ckptlist:\n",
    "        for loss in losslist:\n",
    "            for ast in astlist:\n",
    "                is_t, u_ast = (ckpt[1] == 'Multi-triage'), (ast == 'use_AST') \n",
    "                if loss[1] == 'CBCE':\n",
    "                    loss[0] = CustomizedBCELoss(p_N=path[2]/2)\n",
    "\n",
    "                logname = ' '.join([path[1], ckpt[1], loss[1], ast])\n",
    "                print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            \n",
    "                train_imm(_path = path[0], _logname = '../res_log/' + logname + '.txt', \n",
    "                      _loss_fn = loss[0], _use_ast = u_ast, _is_textcnn = is_t, _ckpt = ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f758c22-1da8-4bac-917e-a7309461873c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617388cb-09b9-46f4-8843-be99446f8fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
