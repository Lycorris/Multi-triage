44643,"sql: safe to upgrade cluster queryIn recent conversations about upgrading clusters with @lucy-zhang and others, we realized that developers don't always know when their cluster is safe to upgrade. For example, we'd want to ensure that there are no schema changes running, no dead nodes, no unavailable ranges, etc. \r\n\r\nIn addition to piping this into the webui, it would be great if we could provide appdevs a query that lets them know if the cluster is safe to upgrade. Something like:\r\n\r\nIt could return a boolean flag that says a cluster is ready to upgrade/not ready to upgrade.",C-enhancement,irfansharif,"In recent conversations about upgrading clusters with @lucy-zhang and others, we realized that developers don't always know when their cluster is safe to upgrade. For example, we'd want to ensure that there are no schema changes running, no dead nodes, no unavailable ranges, etc. \r\n\r\nIn addition to piping this into the webui, it would be great if we could provide appdevs a query that lets them know if the cluster is safe to upgrade. Something like:\r\n```SQL\r\nshow cluster upgrade status\r\n```\r\nIt could return a boolean flag that says a cluster is ready to upgrade/not ready to upgrade.",SQL\r\nshow cluster upgrade status\r\n
44632,"Internal error for arithmetic operators and NULLIFConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the `SELECT` results in an internal error:\r\n\r\n```\r\nERROR: internal error: could not find overload for binary expression plus\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/typing.go:94: BinaryAllowsNullArgs()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/custom_funcs.go:1691: AllowNullArgs()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:12441: ConstructPlus()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:663: constructBinary()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:217: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/project.go:171: buildProjectionList()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:930: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:850: buildSelectStmtWithoutParens()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:823: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/with.go:29: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:822: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:253: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:420: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:795: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:678: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\nAlso other arithmetic operators are affected. I found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.",C-bug,rytaft,"Consider the following test case:\r\n\r\n```sql\r\nSELECT NULLIF(NULL, 0) + NULLIF(NULL, 0); -- internal error: could not find overload for binary expression plus\r\n```\r\n\r\nUnexpectedly, the `SELECT` results in an internal error:\r\n\r\n```\r\nERROR: internal error: could not find overload for binary expression plus\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/typing.go:94: BinaryAllowsNullArgs()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/custom_funcs.go:1691: AllowNullArgs()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:12441: ConstructPlus()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:663: constructBinary()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:217: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/project.go:171: buildProjectionList()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:930: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:850: buildSelectStmtWithoutParens()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:823: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/with.go:29: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:822: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:253: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:420: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:795: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:678: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\nAlso other arithmetic operators are affected. I found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.","sql\r\nSELECT NULLIF(NULL, 0) + NULLIF(NULL, 0); -- internal error: could not find overload for binary expression plus\r\n"
44625,Internal error for SUBSTRING with negative length and VECTORIZE=experimental_onConsider the following test case:\r\n\r\n\r\n\r\nThe `SELECT` unexpectedly results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: negative substring length -1 not allowed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:679: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:159: VectorizedInternalPanic()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/builtin_funcs.go:164: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:116: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/builtin_funcs.go:136: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/selection_ops.eg.go:1788: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\n\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.,C-bug,yuzefovich,"Consider the following test case:\r\n\r\n```sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0(c0) VALUES(0);\r\nSELECT * FROM t0 WHERE SUBSTRING('', 0, -1) = ''; -- unexpected error from the vectorized runtime: negative substring length -1 not allowed\r\n```\r\n\r\nThe `SELECT` unexpectedly results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: negative substring length -1 not allowed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:679: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:159: VectorizedInternalPanic()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/builtin_funcs.go:164: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:116: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/builtin_funcs.go:136: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/selection_ops.eg.go:1788: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\n\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.","sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0(c0) VALUES(0);\r\nSELECT * FROM t0 WHERE SUBSTRING('', 0, -1) = ''; -- unexpected error from the vectorized runtime: negative substring length -1 not allowed\r\n"
44624,"Complex ORDER BY clause results in internal error with  VECTORIZE=experimental_onConsider the following test case:\r\n\r\n\r\nUnexpectedly, the query results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: cannot overwrite value on flat Bytes: maxSetIndex=1, setIndex=0, consider using Reset\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:679: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/bytes.go:118: Set()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/vec.eg.go:410: Copy()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:233: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:116: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:131: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/sort.go:134: spool()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/sort.go:261: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/disk_spiller.go:141: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/disk_spiller.go:139: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.",C-bug,yuzefovich,"Consider the following test case:\r\n\r\n```sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 STRING, c1 BOOL, c2 INT);\r\nINSERT INTO t0(c0) VALUES('');\r\nINSERT INTO t0(rowid, c1, c0) VALUES(0, true, '');\r\nSELECT * FROM t0 ORDER BY CASE WHEN t0.c1 IS NULL THEN t0.c0 WHEN true THEN t0.c0 END; -- unexpected error from the vectorized runtime: cannot overwrite value on flat Bytes: maxSetIndex=1, setIndex=0, consider using Reset\r\n```\r\nUnexpectedly, the query results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: cannot overwrite value on flat Bytes: maxSetIndex=1, setIndex=0, consider using Reset\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:679: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/bytes.go:118: Set()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/vec.eg.go:410: Copy()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:233: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:116: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:131: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/sort.go:134: spool()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/sort.go:261: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/disk_spiller.go:141: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/disk_spiller.go:139: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.","sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 STRING, c1 BOOL, c2 INT);\r\nINSERT INTO t0(c0) VALUES('');\r\nINSERT INTO t0(rowid, c1, c0) VALUES(0, true, '');\r\nSELECT * FROM t0 ORDER BY CASE WHEN t0.c1 IS NULL THEN t0.c0 WHEN true THEN t0.c0 END; -- unexpected error from the vectorized runtime: cannot overwrite value on flat Bytes: maxSetIndex=1, setIndex=0, consider using Reset\r\n"
44621,"Internal error for ILIKE_ESCAPE and special charactersConsider the following testcase:\r\n\r\n\r\n\r\nUnexpectedly, the query results in an internal error with the following stacktrace:\r\n\r\n\r\n\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.",C-bug|O-community|S-3-erroneous-edge-case,yuzefovich,"Consider the following testcase:\r\n\r\n```sql\r\nSELECT ILIKE_ESCAPE('a', '\ua9d5', '\ufffd'); -- internal error: runtime error: index out of range [2] with length 1\r\n```\r\n\r\nUnexpectedly, the query results in an internal error with the following stacktrace:\r\n\r\n```sql\r\nERROR: internal error: runtime error: index out of range [2] with length 1\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/util/errorutil/catch.go:29: ShouldCatch()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:169: func1()\r\nruntime/panic.go:679: gopanic()\r\nruntime/panic.go:75: goPanicIndex()\r\nunicode/utf8/utf8.go:355: EncodeRune()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:4862: replaceCustomEscape()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:5090: Pattern()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/regexp_cache.go:62: GetRegexp()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:2521: ConvertLikeToRegexp()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:2503: MatchLikeEscape()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtins.go:1194: func79()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtins.go:4552: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:4008: Eval()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/fold_constants.go:353: FoldFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:13856: ConstructFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:476: buildFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:290: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/project.go:171: buildProjectionList()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:930: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:850: buildSelectStmtWithoutParens()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:823: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/with.go:29: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:822: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:253: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:420: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:795: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:678: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\n```\r\n\r\nI found this bug based on commit `031ab37f38a9095da3740c13a5ac03531f13608c`.","sql\r\nSELECT ILIKE_ESCAPE('a', '\ua9d5', '\ufffd'); -- internal error: runtime error: index out of range [2] with length 1\r\n"
44565,"Server exits on query with LEFT JOINConsider the following test case:\r\n\r\n\r\n\r\nAfter issuing this query, the server quits, without displaying an error message. I found this issue based on commit `11a129329e5b5bc8aa22e1a7db15c7d56d328885`.\r\n\r\n---- \r\nUPDATE: All the statements to reproduce the bug are now included in this bug report, considering that no release version is affected.",C-bug|S-2-temp-unavailability,RaduBerinde,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 INT);\r\nCREATE TABLE t1(c0 INT);\r\nSELECT * FROM t0 LEFT JOIN t1 ON true LIMIT -1; -- causes server to exit\r\n```\r\n\r\nAfter issuing this query, the server quits, without displaying an error message. I found this issue based on commit `11a129329e5b5bc8aa22e1a7db15c7d56d328885`.\r\n\r\n---- \r\nUPDATE: All the statements to reproduce the bug are now included in this bug report, considering that no release version is affected.",sql\r\nCREATE TABLE t0(c0 INT);\r\nCREATE TABLE t1(c0 INT);\r\nSELECT * FROM t0 LEFT JOIN t1 ON true LIMIT -1; -- causes server to exit\r\n
44547,"Internal error for NATURAL JOIN and VECTORIZE='experimental_on'Consider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the `SELECT` results in an error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:70: func1()\r\nruntime/panic.go:679: gopanic()\r\nruntime/iface.go:255: panicdottypeE()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/vec.go:202: Int64()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.eg.go:398: rehash()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.go:332: computeBuckets()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.go:363: lookupInitial()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:442: exec()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:223: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\n\r\nI found this bug based on commit `bf96530216efcaaafcd3e88db22089b4f0269480`.",C-bug,yuzefovich,"Consider the following test case:\r\n\r\n```sql\r\nSET SESSION VECTORIZE='experimental_on';\r\nCREATE TABLE t0(c0 INT4);\r\nCREATE TABLE t1(c0 INT8);\r\nINSERT INTO t0(c0) VALUES(0);\r\nINSERT INTO t1(c0) VALUES(0);\r\nSELECT * FROM t0 NATURAL JOIN t1; -- internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\n```\r\n\r\nUnexpectedly, the `SELECT` results in an error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:70: func1()\r\nruntime/panic.go:679: gopanic()\r\nruntime/iface.go:255: panicdottypeE()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/vec.go:202: Int64()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.eg.go:398: rehash()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.go:332: computeBuckets()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashtable.go:363: lookupInitial()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:442: exec()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:223: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:877: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:769: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1316: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1245: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:452: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1357: goexit()\r\n```\r\n\r\nI found this bug based on commit `bf96530216efcaaafcd3e88db22089b4f0269480`.","sql\r\nSET SESSION VECTORIZE='experimental_on';\r\nCREATE TABLE t0(c0 INT4);\r\nCREATE TABLE t1(c0 INT8);\r\nINSERT INTO t0(c0) VALUES(0);\r\nINSERT INTO t1(c0) VALUES(0);\r\nSELECT * FROM t0 NATURAL JOIN t1; -- internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\n"
44505,"sql: investigate online primary key changes timingI'm testing out running a online primary key change while running tpc-c as described in https://github.com/cockroachdb/cockroach/issues/44501.\r\n\r\nOn a tpc-c 2500 cluster, while running tpc-c with active load 2000, this online primary key change:\r\n\r\n\r\nIt took 1h23m to complete.\r\n\r\nIs this reasonable? \r\n\r\nThe schema of customers was:\r\n\r\nAfter the change it is:\r\n\r\nSo this online primary key change results in the following indexes being written:\r\n\r\n- New primary\r\n- Old primary\r\n- New customer_idx\r\n\r\nAre these done in parallel? \r\n\r\nFor comparison, I'm now running a create index:\r\n\r\nThis took 24m. \r\n\r\nRoughly 3*24 is 1h12m which is ~10 minutes faster than the time it took overall. ",C-investigation,solongordon|rohany,"I'm testing out running a online primary key change while running tpc-c as described in https://github.com/cockroachdb/cockroach/issues/44501.\r\n\r\nOn a tpc-c 2500 cluster, while running tpc-c with active load 2000, this online primary key change:\r\n\r\n```SQL\r\nALTER TABLE customer ALTER PRIMARY KEY USING COLUMNS (c_city, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n```\r\nIt took 1h23m to complete.\r\n\r\nIs this reasonable? \r\n\r\nThe schema of customers was:\r\n```SQL\r\nCREATE TABLE customer (\r\n             |     c_id INT8 NOT NULL,\r\n             |     c_d_id INT8 NOT NULL,\r\n             |     c_w_id INT8 NOT NULL,\r\n             |     c_first VARCHAR(16) NULL,\r\n             |     c_middle CHAR(2) NULL,\r\n             |     c_last VARCHAR(16) NULL,\r\n             |     c_street_1 VARCHAR(20) NULL,\r\n             |     c_street_2 VARCHAR(20) NULL,\r\n             |     c_city VARCHAR(20) NULL,\r\n             |     c_state CHAR(2) NULL,\r\n             |     c_zip CHAR(9) NULL,\r\n             |     c_phone CHAR(16) NULL,\r\n             |     c_since TIMESTAMP NULL,\r\n             |     c_credit CHAR(2) NULL,\r\n             |     c_credit_lim DECIMAL(12,2) NULL,\r\n             |     c_discount DECIMAL(4,4) NULL,\r\n             |     c_balance DECIMAL(12,2) NULL,\r\n             |     c_ytd_payment DECIMAL(12,2) NULL,\r\n             |     c_payment_cnt INT8 NULL,\r\n             |     c_delivery_cnt INT8 NULL,\r\n             |     c_data VARCHAR(500) NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (c_w_id ASC, c_d_id ASC, c_id ASC),\r\n             |     CONSTRAINT fk_c_w_id_ref_district FOREIGN KEY (c_w_id, c_d_id) REFERENCES district(d_w_id, d_id),\r\n             |     INDEX customer_idx (c_w_id ASC, c_d_id ASC, c_last ASC, c_first ASC),\r\n             |     FAMILY ""primary"" (c_id, c_d_id, c_w_id, c_first, c_middle, c_last, c_street_1, c_street_2, c_city, c_state, c_zip, c_phone, c_since, c_credit, c_credit_lim, c_discount, c_balance, c_ytd_payment, c_payment_cnt, c_delivery_cnt, c_data)\r\n             | )\r\n```\r\nAfter the change it is:\r\n```SQL\r\nroot@localhost:26257/tpcc> show create table customer;\r\n  table_name |                                                                                                               create_statement\r\n-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n  customer   | CREATE TABLE customer (\r\n             |     c_id INT8 NOT NULL,\r\n             |     c_d_id INT8 NOT NULL,\r\n             |     c_w_id INT8 NOT NULL,\r\n             |     c_first VARCHAR(16) NULL,\r\n             |     c_middle CHAR(2) NULL,\r\n             |     c_last VARCHAR(16) NULL,\r\n             |     c_street_1 VARCHAR(20) NULL,\r\n             |     c_street_2 VARCHAR(20) NULL,\r\n             |     c_city VARCHAR(20) NOT NULL,\r\n             |     c_state CHAR(2) NULL,\r\n             |     c_zip CHAR(9) NULL,\r\n             |     c_phone CHAR(16) NULL,\r\n             |     c_since TIMESTAMP NULL,\r\n             |     c_credit CHAR(2) NULL,\r\n             |     c_credit_lim DECIMAL(12,2) NULL,\r\n             |     c_discount DECIMAL(4,4) NULL,\r\n             |     c_balance DECIMAL(12,2) NULL,\r\n             |     c_ytd_payment DECIMAL(12,2) NULL,\r\n             |     c_payment_cnt INT8 NULL,\r\n             |     c_delivery_cnt INT8 NULL,\r\n             |     c_data VARCHAR(500) NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (c_city ASC, c_w_id ASC, c_d_id ASC, c_id ASC),\r\n             |     CONSTRAINT fk_c_w_id_ref_district FOREIGN KEY (c_w_id, c_d_id) REFERENCES district(d_w_id, d_id),\r\n             |     INDEX customer_idx (c_w_id ASC, c_d_id ASC, c_last ASC, c_first ASC),\r\n             |     FAMILY ""primary"" (c_id, c_d_id, c_w_id, c_first, c_middle, c_last, c_street_1, c_street_2, c_city, c_state, c_zip, c_phone, c_since, c_credit, c_credit_lim, c_discount, c_balance, c_ytd_payment, c_payment_cnt, c_delivery_cnt, c_data)\r\n             | )\r\n(1 row)\r\n\r\nTime: 380.683852ms\r\n```\r\nSo this online primary key change results in the following indexes being written:\r\n\r\n- New primary\r\n- Old primary\r\n- New customer_idx\r\n\r\nAre these done in parallel? \r\n\r\nFor comparison, I'm now running a create index:\r\n```SQL\r\nCREATE UNIQUE INDEX ON tpcc.customer (c_city ASC, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n```\r\nThis took 24m. \r\n\r\nRoughly 3*24 is 1h12m which is ~10 minutes faster than the time it took overall. ","SQL\r\nALTER TABLE customer ALTER PRIMARY KEY USING COLUMNS (c_city, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n"
44504,"sql: error result is ambiguous in tpcc after online primary key changeI'm testing out online primary key changes as described in https://github.com/cockroachdb/cockroach/issues/44501\r\nI ran this change:\r\n\r\nAfter completing an online primary key change, the workload failed with:\r\n```\r\nError: error in payment: ERROR: result is ambiguous (intent missing and record aborted) (SQLSTATE 40003)\r\nError:  exit status 1\r\n```",C-investigation,solongordon|rohany,"I'm testing out online primary key changes as described in https://github.com/cockroachdb/cockroach/issues/44501\r\nI ran this change:\r\n```SQL\r\nset experimental_enable_primary_key_changes =true;\r\nALTER TABLE customer ALTER COLUMN c_city set not null;\r\nALTER TABLE customer ALTER PRIMARY KEY USING COLUMNS (c_city, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n```\r\nAfter completing an online primary key change, the workload failed with:\r\n```\r\nError: error in payment: ERROR: result is ambiguous (intent missing and record aborted) (SQLSTATE 40003)\r\nError:  exit status 1\r\n```","SQL\r\nset experimental_enable_primary_key_changes =true;\r\nALTER TABLE customer ALTER COLUMN c_city set not null;\r\nALTER TABLE customer ALTER PRIMARY KEY USING COLUMNS (c_city, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n"
44501,sql: A 2m long 2s latency spike during online primary key changes with TPCCCluster setup for testing online primary key changes during TPC-C.\r\n\r\n\r\nIn another window:\r\n\r\n![image](https://user-images.githubusercontent.com/22278911/73392134-38e93280-42a7-11ea-9360-513200566d1b.png)\r\n\r\nversion V20.1.0-ALPHA.20191118-1665-GABBE4BC\r\ncluster andy-3node\r\n,C-investigation,solongordon|rohany,"Cluster setup for testing online primary key changes during TPC-C.\r\n```SQL\r\nroachprod create $CLUSTER -n 4 --clouds=aws  --aws-machine-type-ssd=c5d.4xlarge\r\nroachprod stage $CLUSTER:1-3 cockroach\r\nroachprod stage $CLUSTER:4 workload\r\nroachprod start $CLUSTER:1-3\r\nroachprod adminurl --open $CLUSTER:1\r\nroachprod run $CLUSTER:1 -- ""./cockroach workload fixtures import tpcc --warehouses=2500 --db=tpcc""\r\nroachprod run $CLUSTER:4 ""./workload run tpcc --ramp=5m --warehouses=2500 --active-warehouses=2000 --duration=15m --split --scatter {pgurl:1-3}""\r\n```\r\n\r\nIn another window:\r\n```SQL\r\nset experimental_enable_primary_key_changes =true;\r\nALTER TABLE customer ALTER COLUMN c_city set not null;\r\nALTER TABLE customer ALTER PRIMARY KEY USING COLUMNS (c_city, c_w_id ASC, c_d_id ASC, c_id ASC);\r\n```\r\n![image](https://user-images.githubusercontent.com/22278911/73392134-38e93280-42a7-11ea-9360-513200566d1b.png)\r\n\r\nversion V20.1.0-ALPHA.20191118-1665-GABBE4BC\r\ncluster andy-3node\r\n","SQL\r\nroachprod create $CLUSTER -n 4 --clouds=aws  --aws-machine-type-ssd=c5d.4xlarge\r\nroachprod stage $CLUSTER:1-3 cockroach\r\nroachprod stage $CLUSTER:4 workload\r\nroachprod start $CLUSTER:1-3\r\nroachprod adminurl --open $CLUSTER:1\r\nroachprod run $CLUSTER:1 -- ""./cockroach workload fixtures import tpcc --warehouses=2500 --db=tpcc""\r\nroachprod run $CLUSTER:4 ""./workload run tpcc --ramp=5m --warehouses=2500 --active-warehouses=2000 --duration=15m --split --scatter {pgurl:1-3}""\r\n"
44466,"Multi-record UPSERT inserts duplicate values in PRIMARY KEY, resulting in inconsistent resultsConsider the following testcase:\r\n\r\n\r\nA single row is fetched, while the following query fetches two records with a duplicate value for c0:\r\n\r\n\r\nI found this issue in CockroachDB based on commit `1917a12003daad100b491e2b0195d7df5909e63f`.",C-bug,rohany,"Consider the following testcase:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 INT PRIMARY KEY, c1 BOOL, c2 INT UNIQUE);\r\nINSERT INTO t0(c0) VALUES (0);\r\nUPSERT INTO t0(c2, c0) VALUES (0, 0), (1, 0);\r\nSELECT * FROM t0; -- {0 | NULL | 1}\r\n```\r\nA single row is fetched, while the following query fetches two records with a duplicate value for c0:\r\n\r\n```sql\r\nSELECT c0 FROM t0; -- {0, 0}\r\n```\r\nI found this issue in CockroachDB based on commit `1917a12003daad100b491e2b0195d7df5909e63f`.","sql\r\nCREATE TABLE t0(c0 INT PRIMARY KEY, c1 BOOL, c2 INT UNIQUE);\r\nINSERT INTO t0(c0) VALUES (0);\r\nUPSERT INTO t0(c2, c0) VALUES (0, 0), (1, 0);\r\nSELECT * FROM t0; -- {0 | NULL | 1}\r\n"
44465,"`to_json()` results don't match PostgresFor example the following query:\r\n\r\ncurrently returns a ""duplicate column"" error. \r\n\r\nAfter #41557  is merged, CockroachDB will return\r\n```\r\n   to_json\r\n------------------\r\n  {""column2"": 2}\r\n```\r\n\r\nbut Postgres returns\r\n```\r\n   to_json\r\n------------------\r\n{""column2"":1,""column2"":2}\r\n```\r\n\r\nSo, duplicate column names in the input of `to_json` need to be included as duplicate keys in the resulting JSON object.\r\n\r\nThe correct test cases have been added as a TODO comment in #41557 https://github.com/cockroachdb/cockroach/pull/41557/files#diff-e9dd4e05509f6f7fc35050a751db0ea0R327.\r\n\r\nI am willing to tackle this issue but feel free to re-assign if needed.",C-enhancement|A-sql-pgcompat,giorgosp,"For example the following query:\r\n```sql\r\nSELECT to_json(x.*) FROM (VALUES (1,2)) AS x(column2);\r\n```\r\ncurrently returns a ""duplicate column"" error. \r\n\r\nAfter #41557  is merged, CockroachDB will return\r\n```\r\n   to_json\r\n------------------\r\n  {""column2"": 2}\r\n```\r\n\r\nbut Postgres returns\r\n```\r\n   to_json\r\n------------------\r\n{""column2"":1,""column2"":2}\r\n```\r\n\r\nSo, duplicate column names in the input of `to_json` need to be included as duplicate keys in the resulting JSON object.\r\n\r\nThe correct test cases have been added as a TODO comment in #41557 https://github.com/cockroachdb/cockroach/pull/41557/files#diff-e9dd4e05509f6f7fc35050a751db0ea0R327.\r\n\r\nI am willing to tackle this issue but feel free to re-assign if needed.","sql\r\nSELECT to_json(x.*) FROM (VALUES (1,2)) AS x(column2);\r\n"
44418,"Internal error ""estimated distinct count must be non-zero""Consider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the SELECT results in an internal error with the following stacktrace:\r\n```\r\nERROR: internal error: estimated distinct count must be non-zero\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2291: finalizeFromRowCount()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:621: colStatScan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:338: colStat()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:193: colStatFromChild()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:786: colStatProject()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:347: colStat()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:193: colStatFromChild()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:252: colStatFromInput()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2864: updateDistinctCountFromUnappliedConjuncts()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2686: applyConstraintSet()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2584: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2597: applyFilter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:675: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:238: buildSelectProps()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.og.go:15467: MemoizeSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:1034: ConstructSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:1020: buildWhere()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:904: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:850: buildSelectStmtWithoutParens()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:823: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/with.go:29: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:822: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:253: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:299: buildReusableMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:395: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:795: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:678: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\n```\r\n\r\nI found this issue using CockroachDB based on commit `42351d55d6d4ef6974c2573ea56806c51c5592e8`.",C-bug,rytaft,"Consider the following test case:\r\n\r\n```sql\r\n\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0(rowid) VALUES (0), (1), (2);\r\nCREATE VIEW v0(c0) AS SELECT CASE WHEN t0.c0 > 0 THEN 1 ELSE t0.rowid END FROM t0;\r\nCREATE STATISTICS s0 FROM t0;\r\nDELETE FROM t0;\r\nINSERT INTO t0(rowid) VALUES (3);\r\nINSERT INTO t0(rowid) VALUES (4);\r\nCREATE STATISTICS s1 ON rowid FROM t0;\r\nSELECT * FROM v0 WHERE v0.c0 > 0; -- internal error: estimated distinct count must be non-zero\r\n```\r\n\r\nUnexpectedly, the SELECT results in an internal error with the following stacktrace:\r\n```\r\nERROR: internal error: estimated distinct count must be non-zero\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2291: finalizeFromRowCount()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:621: colStatScan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:338: colStat()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:193: colStatFromChild()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:786: colStatProject()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:347: colStat()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:193: colStatFromChild()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:252: colStatFromInput()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2864: updateDistinctCountFromUnappliedConjuncts()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2686: applyConstraintSet()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2584: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:2597: applyFilter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/statistics_builder.go:675: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:238: buildSelectProps()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.og.go:15467: MemoizeSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:1034: ConstructSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:1020: buildWhere()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:904: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:850: buildSelectStmtWithoutParens()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:823: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/with.go:29: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:822: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:253: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:299: buildReusableMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:395: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:795: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:678: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:471: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:101: execStmt()\r\n```\r\n\r\nI found this issue using CockroachDB based on commit `42351d55d6d4ef6974c2573ea56806c51c5592e8`.","sql\r\n\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0(rowid) VALUES (0), (1), (2);\r\nCREATE VIEW v0(c0) AS SELECT CASE WHEN t0.c0 > 0 THEN 1 ELSE t0.rowid END FROM t0;\r\nCREATE STATISTICS s0 FROM t0;\r\nDELETE FROM t0;\r\nINSERT INTO t0(rowid) VALUES (3);\r\nINSERT INTO t0(rowid) VALUES (4);\r\nCREATE STATISTICS s1 ON rowid FROM t0;\r\nSELECT * FROM v0 WHERE v0.c0 > 0; -- internal error: estimated distinct count must be non-zero\r\n"
44304,"Internal error for CASE expression and VECTORIZE=experimental_onConsider the following testcase:\r\n\r\n\r\n\r\nUnexpectedly, the `SELECT` triggers an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: Vec is of unknown type and should not be accessed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:522: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/unknown_vec.go:94: Nulls()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/const.eg.go:456: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:230: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:115: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:131: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:125: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:36: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:876: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:768: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:470: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI found this issue on trunk (commit `a47ef1f0d6bd9a7f8a24f9ba6176b4aab8b58575`).",C-bug|A-sql-vec,yuzefovich,"Consider the following testcase:\r\n\r\n```sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0 (c0) VALUES (0);\r\nSELECT * FROM t0 WHERE CASE WHEN t0.c0 > 0 THEN NULL END; --  internal error: unexpected error from the vectorized runtime: Vec is of unknown type and should not be accessed\r\n```\r\n\r\nUnexpectedly, the `SELECT` triggers an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: unexpected error from the vectorized runtime: Vec is of unknown type and should not be accessed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:77: func1()\r\nruntime/panic.go:522: gopanic()\r\ngithub.com/cockroachdb/cockroach/pkg/col/coldata/unknown_vec.go:94: Nulls()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/const.eg.go:456: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:230: func1()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/allocator.go:115: PerformOperation()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:131: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:125: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:36: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:93: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:149: next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:170: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:749: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:876: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:768: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:470: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI found this issue on trunk (commit `a47ef1f0d6bd9a7f8a24f9ba6176b4aab8b58575`).",sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 INT);\r\nINSERT INTO t0 (c0) VALUES (0);\r\nSELECT * FROM t0 WHERE CASE WHEN t0.c0 > 0 THEN NULL END; --  internal error: unexpected error from the vectorized runtime: Vec is of unknown type and should not be accessed\r\n
44296,"Incorrect result for IS NULL query on VIEW using SELECT DISTINCTConsider the following statements:\r\n\r\n\r\nUnexpectedly, two rows are fetched, even though `v0` contains only a single row:\r\n\r\n\r\n\r\nAlso the following query indicates that the predicate evaluates to `TRUE` on only a single row:\r\n\r\n\r\nThe query works correctly when removing the `UNIQUE` constraint from `t0`.\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.",C-bug|O-community|S-3-erroneous-edge-case,RaduBerinde,"Consider the following statements:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 INT UNIQUE);\r\nCREATE VIEW v0(c0) AS SELECT DISTINCT t0.c0 FROM t0;\r\nINSERT INTO t0 (c0) VALUES (NULL), (NULL);\r\nSELECT * FROM v0 WHERE v0.c0 IS NULL; -- expected: {NULL}, actual: {NULL, NULL}\r\n```\r\nUnexpectedly, two rows are fetched, even though `v0` contains only a single row:\r\n\r\n```sql\r\nSELECT * FROM v0; -- {NULL}\r\n```\r\n\r\nAlso the following query indicates that the predicate evaluates to `TRUE` on only a single row:\r\n\r\n```sql\r\nSELECT v0.c0 IS NULL FROM v0; -- {TRUE}\r\n```\r\nThe query works correctly when removing the `UNIQUE` constraint from `t0`.\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.","sql\r\nCREATE TABLE t0(c0 INT UNIQUE);\r\nCREATE VIEW v0(c0) AS SELECT DISTINCT t0.c0 FROM t0;\r\nINSERT INTO t0 (c0) VALUES (NULL), (NULL);\r\nSELECT * FROM v0 WHERE v0.c0 IS NULL; -- expected: {NULL}, actual: {NULL, NULL}\r\n"
44244,"Internal error setting tracing=trueConsider the following statement:\r\n\r\n\r\n\r\nUnexpectedly, the `SET` results in the following internal error:\r\n\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.",C-bug|S-3-ux-surprise,yuzefovich,"Consider the following statement:\r\n\r\n```sql\r\nSET SESSION tracing=true; -- ERROR: internal error: expected string for set tracing argument, not *tree.DBool\r\n```\r\n\r\nUnexpectedly, the `SET` results in the following internal error:\r\n\r\n```sql\r\nERROR: internal error: expected string for set tracing argument, not *tree.DBool\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:1184: runSetTracing()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:1134: runObserverStatement()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:73: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.","sql\r\nSET SESSION tracing=true; -- ERROR: internal error: expected string for set tracing argument, not *tree.DBool\r\n"
44207,"Incorrect result for query with IS NULL condition on UNIQUE column and VECTORIZE=experimental_onConsider the following statements:\r\n\r\n\r\n\r\nThe query fetches one row, although it is expected to fetch two. When removing the `UNIQUE` from `t0.c0`, the query works correctly. It also works correctly when `VECTORIZE` is not set to `experimental_on`. The following query demonstrates that the predicate evaluates to `TRUE` for two rows:\r\n\r\n\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.",C-bug,yuzefovich,"Consider the following statements:\r\n\r\n```sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t1(c0 INT);\r\nCREATE TABLE t0(c0 INT UNIQUE);\r\nINSERT INTO t1(c0) VALUES (0);\r\nINSERT INTO t0(c0) VALUES (NULL), (NULL);\r\nSELECT * FROM t0, t1 WHERE t0.c0 IS NULL; -- expected: two rows are fetched, actual: one row is fetched\r\n```\r\n\r\nThe query fetches one row, although it is expected to fetch two. When removing the `UNIQUE` from `t0.c0`, the query works correctly. It also works correctly when `VECTORIZE` is not set to `experimental_on`. The following query demonstrates that the predicate evaluates to `TRUE` for two rows:\r\n\r\n```sql\r\nSELECT t0.c0 IS NULL FROM t0, t1; -- {TRUE, TRUE}\r\n```\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.","sql\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t1(c0 INT);\r\nCREATE TABLE t0(c0 INT UNIQUE);\r\nINSERT INTO t1(c0) VALUES (0);\r\nINSERT INTO t0(c0) VALUES (NULL), (NULL);\r\nSELECT * FROM t0, t1 WHERE t0.c0 IS NULL; -- expected: two rows are fetched, actual: one row is fetched\r\n"
44203,"Query on VIEW with OFFSET NULL and WHERE condition involving CURRENT_USER() unexpectedly fetches a rowConsider the following test case:\r\n\r\n\r\n\r\nThe row should not be fetched, which is also the observed behavior when the `WHERE` clause is omitted:\r\n\r\nI executed the query locally, with the following user name:\r\n\r\n\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.",C-bug,RaduBerinde,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 BOOL);\r\nINSERT INTO t0(c0) VALUES (false);\r\nCREATE VIEW v0(c0) AS SELECT c0 FROM t0 WHERE t0.c0 OFFSET NULL;\r\nSELECT * FROM v0 WHERE CURRENT_USER() != ''; -- unexpected: a row is fetched\r\n```\r\n\r\nThe row should not be fetched, which is also the observed behavior when the `WHERE` clause is omitted:\r\n```sql\r\nSELECT * FROM v0; -- no row is fetched\r\n```\r\nI executed the query locally, with the following user name:\r\n\r\n```sql\r\nSELECT CURRENT_USER();\r\n  current_user\r\n----------------\r\n  root\r\n(1 row)\r\n```\r\n\r\nI found this issue based on commit `c76ad970e73e606bc55a372d93f9d2d6acb32c9c`.",sql\r\nCREATE TABLE t0(c0 BOOL);\r\nINSERT INTO t0(c0) VALUES (false);\r\nCREATE VIEW v0(c0) AS SELECT c0 FROM t0 WHERE t0.c0 OFFSET NULL;\r\nSELECT * FROM v0 WHERE CURRENT_USER() != ''; -- unexpected: a row is fetched\r\n
44181,"Internal error for BETWEEN operator and CAST to BYTESConsider the following statement:\r\n\r\n\r\nUnexpectedly, the SELECT results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: lookup for ComparisonExpr (('\\x')[bytes] <= ('')[string])[bool]'s CmpOp failed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/expr.go:526: memoizeFn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/expr.go:440: NewTypedComparisonExpr()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/scalar.go:228: buildComparison()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/scalar.go:91: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:368: buildValuesRows()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:345: buildValues()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:184: buildRelational()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/builder.go:131: build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/builder.go:100: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:170: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:741: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:624: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\nI found this bug using commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug|S-3-ux-surprise,RaduBerinde,"Consider the following statement:\r\n\r\n```sql\r\nSELECT '' BETWEEN ''::BYTES AND ''; -- ERROR: internal error: lookup for ComparisonExpr ((@1)[bytes] > ('')[string])[bool]'s CmpOp failed\r\n```\r\nUnexpectedly, the SELECT results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: lookup for ComparisonExpr (('\\x')[bytes] <= ('')[string])[bool]'s CmpOp failed\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/expr.go:526: memoizeFn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/expr.go:440: NewTypedComparisonExpr()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/scalar.go:228: buildComparison()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/scalar.go:91: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:368: buildValuesRows()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:345: buildValues()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational.go:184: buildRelational()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/builder.go:131: build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/builder.go:100: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:170: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:741: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:624: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\nI found this bug using commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",sql\r\nSELECT '' BETWEEN ''::BYTES AND ''; -- ERROR: internal error: lookup for ComparisonExpr ((@1)[bytes] > ('')[string])[bool]'s CmpOp failed\r\n
44154,"Incorrect result for BETWEEN SYMMETRIC queryConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the query fetches a row, even though the predicate should evaluate to `NULL`:\r\n\r\nWhen removing the `CHECK (true)` from `t0`, the query works correctly.\r\n\r\nI found this bug using commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug,RaduBerinde,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 BOOL UNIQUE, c1 BOOL CHECK (true));\r\nINSERT INTO t0(c0) VALUES (true);\r\nSELECT * FROM t0 WHERE t0.c0 AND (false NOT BETWEEN SYMMETRIC t0.c0 AND NULL AND true); -- unexpected: row is fetched\r\n```\r\n\r\nUnexpectedly, the query fetches a row, even though the predicate should evaluate to `NULL`:\r\n```sql\r\nSELECT t0.c0 AND (false NOT BETWEEN SYMMETRIC t0.c0 AND NULL AND true) FROM t0; -- NULL\r\n```\r\nWhen removing the `CHECK (true)` from `t0`, the query works correctly.\r\n\r\nI found this bug using commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.","sql\r\nCREATE TABLE t0(c0 BOOL UNIQUE, c1 BOOL CHECK (true));\r\nINSERT INTO t0(c0) VALUES (true);\r\nSELECT * FROM t0 WHERE t0.c0 AND (false NOT BETWEEN SYMMETRIC t0.c0 AND NULL AND true); -- unexpected: row is fetched\r\n"
44152,"TO_ENGLISH(-9223372036854775808) results in an internal errorConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the `SELECT` statement results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: runtime error: index out of range\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/util/errorutil/catch.go:29: ShouldCatch()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:169: func1()\r\nruntime/panic.go:522: gopanic()\r\nruntime/panic.go:44: panicindex()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtins.go:839: func57()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:3978: Eval()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/fold_constants.go:353: FoldFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:13856: ConstructFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:475: buildFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:290: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/project.go:171: buildProjectionList()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:866: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:786: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:287: func4()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:741: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:286: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:420: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:741: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:624: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug|S-3-productivity,otan,"Consider the following test case:\r\n\r\n```sql\r\nSELECT TO_ENGLISH(-9223372036854775808); -- internal error: runtime error: index out of range\r\n```\r\n\r\nUnexpectedly, the `SELECT` statement results in an internal error with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: runtime error: index out of range\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/util/errorutil/catch.go:29: ShouldCatch()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:169: func1()\r\nruntime/panic.go:522: gopanic()\r\nruntime/panic.go:44: panicindex()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/builtins/builtins.go:839: func57()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go:3978: Eval()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/fold_constants.go:353: FoldFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:13856: ConstructFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:475: buildFunction()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:290: buildScalar()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/project.go:171: buildProjectionList()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:866: buildSelectClause()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:786: buildSelect()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:287: func4()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:741: processWiths()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:286: buildStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:217: buildStmtAtRoot()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:188: Build()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:420: buildExecMemo()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/plan_opt.go:162: makeOptimizerPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:741: makeExecPlan()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:624: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",sql\r\nSELECT TO_ENGLISH(-9223372036854775808); -- internal error: runtime error: index out of range\r\n
44148,"DEFAULT value causes unexpected syntax error when executing INSERTConsider the following test case:\r\n\r\n\r\nUnexpectedly, the `INSERT` causes a syntax error:\r\n\r\n```\r\nERROR: at or near ""collate"": syntax error\r\nSQLSTATE: 42601\r\nDETAIL: source SQL:\r\nSET ROW (NULL BETWEEN '':::STRING COLLATE en AND '':::STRING)\r\n                                  ^\r\nHINT: try \\h SET SESSION\r\n```\r\nThe expression seems to be valid, as demonstrated by the following `SELECT`, which executes without errors:\r\n\r\n\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug|O-community|A-sql-optimizer,solongordon,"Consider the following test case:\r\n```sql\r\nCREATE TABLE t0(c0 BOOL DEFAULT (NULL BETWEEN ('' COLLATE en) AND ''));\r\nINSERT INTO t0(c0) VALUES(DEFAULT); -- ERROR: at or near ""collate"": syntax error\r\n```\r\n\r\nUnexpectedly, the `INSERT` causes a syntax error:\r\n\r\n```\r\nERROR: at or near ""collate"": syntax error\r\nSQLSTATE: 42601\r\nDETAIL: source SQL:\r\nSET ROW (NULL BETWEEN '':::STRING COLLATE en AND '':::STRING)\r\n                                  ^\r\nHINT: try \\h SET SESSION\r\n```\r\nThe expression seems to be valid, as demonstrated by the following `SELECT`, which executes without errors:\r\n\r\n```sql\r\nSELECT NULL BETWEEN ('' COLLATE en) AND ''; -- NULL\r\n```\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.","sql\r\nCREATE TABLE t0(c0 BOOL DEFAULT (NULL BETWEEN ('' COLLATE en) AND ''));\r\nINSERT INTO t0(c0) VALUES(DEFAULT); -- ERROR: at or near ""collate"": syntax error\r\n"
44142,"Unexpected error when using EXPERIMENTAL SCRUBConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the `EXPERIMENTAL SCRUB` statement results in an error `scrub-index: unsupported comparison operator: <collatedstring{en}> IS NOT DISTINCT FROM <unknown>`. When removing the column definition of c0, the statement executes as expected.\r\n\r\nI noticed that the statement is only [partially implemented](https://github.com/cockroachdb/cockroach/issues/10425), but this does not seem to be a documented or expected limitation.\r\n\r\nI found this potential issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug,thoszhang,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 STRING UNIQUE, c1 STRING COLLATE en PRIMARY KEY);\r\nEXPERIMENTAL SCRUB TABLE t0; -- ERROR: scrub-index: unsupported comparison operator: <collatedstring{en}> IS NOT DISTINCT FROM <unknown>\r\n```\r\n\r\nUnexpectedly, the `EXPERIMENTAL SCRUB` statement results in an error `scrub-index: unsupported comparison operator: <collatedstring{en}> IS NOT DISTINCT FROM <unknown>`. When removing the column definition of c0, the statement executes as expected.\r\n\r\nI noticed that the statement is only [partially implemented](https://github.com/cockroachdb/cockroach/issues/10425), but this does not seem to be a documented or expected limitation.\r\n\r\nI found this potential issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.","sql\r\nCREATE TABLE t0(c0 STRING UNIQUE, c1 STRING COLLATE en PRIMARY KEY);\r\nEXPERIMENTAL SCRUB TABLE t0; -- ERROR: scrub-index: unsupported comparison operator: <collatedstring{en}> IS NOT DISTINCT FROM <unknown>\r\n"
44137,"execinfra: internal error in expression that uses a CAST and COLLATEConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the SELECT results in a crash with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: ((-('1a')) COLLATE en::INT8) < 1:::INT8: unsupported unary operator: - <string> (desired <string>)\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:78: processExpression()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:170: InitWithRemapping()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:146: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:108: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:800: InitWithEvalCtx()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:776: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec/tablereader.go:93: newTableReader()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec/processors.go:138: NewProcessor()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:228: makeProcessor()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:81: setupProcessors()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:64: Setup()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql/server.go:352: setupFlow()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql/server.go:463: SetupLocalSyncFlow()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:252: setupFlows()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:354: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:823: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:715: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug,yuzefovich,"Consider the following test case:\r\n\r\n```SQL\r\nCREATE TABLE t0(c0 INT);\r\nSELECT * FROM t0 WHERE 1 > -(('1a' COLLATE en)::INT); -- crash\r\n```\r\n\r\nUnexpectedly, the SELECT results in a crash with the following stacktrace:\r\n\r\n```\r\nERROR: internal error: ((-('1a')) COLLATE en::INT8) < 1:::INT8: unsupported unary operator: - <string> (desired <string>)\r\nSQLSTATE: XX000\r\nDETAIL: stack trace:\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:78: processExpression()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:170: InitWithRemapping()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/expr.go:146: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:108: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:800: InitWithEvalCtx()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:776: Init()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec/tablereader.go:93: newTableReader()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec/processors.go:138: NewProcessor()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:228: makeProcessor()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:81: setupProcessors()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowflow/row_based_flow.go:64: Setup()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql/server.go:352: setupFlow()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql/server.go:463: SetupLocalSyncFlow()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:252: setupFlows()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:354: Run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977: PlanAndRun()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:823: execWithDistSQLEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:715: dispatchToExecutionEngine()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418: execStmtInOpenState()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100: execStmt()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283: execCmd()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212: run()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451: ServeConn()\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566: func1()\r\nruntime/asm_amd64.s:1337: goexit()\r\n```\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",SQL\r\nCREATE TABLE t0(c0 INT);\r\nSELECT * FROM t0 WHERE 1 > -(('1a' COLLATE en)::INT); -- crash\r\n
44133,"Crash when using VECTORIZE=experimental_onConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the query results in a crash (see stack trace below).\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.\r\n\r\n```\r\noveToDraining called in state StateTrailingMeta with err: received multiple headers\r\ngoroutine 443 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.getStacks(0x744cb00, 0xed5b6c637, 0x0, 0x840c90)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/get_stacks.go:25 +0xb1\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.(*loggerT).outputLogEntry(0x7449a40, 0xc000000004, 0x6cdb4a1, 0x1f, 0x24a, 0xc0188cfa80, 0x7a)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/clog.go:209 +0x968\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.addStructured(0x4a9bd80, 0xc01059e940, 0x4, 0x2, 0x416e782, 0x2e, 0xc00e6e1de8, 0x2, 0x2)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/structured.go:66 +0x2cc\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.logDepth(0x4a9bd80, 0xc01059e940, 0x1, 0xc000000004, 0x416e782, 0x2e, 0xc00e6e1de8, 0x2, 0x2)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/log.go:44 +0x8c\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.Fatalf(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/log.go:155\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra.(*ProcessorBase).MoveToDraining(0xc00a827200, 0x4a259a0, 0xc00f8b4b00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:586 +0x30c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec.(*valuesProcessor).Start(0xc00a827200, 0x4a9bd80, 0xc01059e940, 0xc007693180, 0x400)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/rowexec/values.go:81 +0x224\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*Columnarizer).Init(0xc0263c6700)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/columnarizer.go:85 +0x211\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*hashJoinEqOp).Init(0xc02167bd00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:221 +0x6c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*bufferOp).Init(0xc00f8b48e0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/buffer.go:48 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.constBoolOp.Init(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/const.eg.go:111\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*caseOp).Init(0xc035fc9c00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:99 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.selBoolOp.Init(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:121\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*boolVecToSelOp).Init(0xc01059efc0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:85 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*simpleProjectOp).Init(0xc0035e63c0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:89 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*Materializer).Start(0xc0263c6e00, 0x4a9bd80, 0xc01059e940, 0x749c4a0, 0x3b03440)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:132 +0x3e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra.(*ProcessorBase).Run(0xc0263c6e00, 0x4a9bd80, 0xc01059e940)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:748 +0x52\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra.(*FlowBase).Run(0xc0232f67e0, 0x4a9bd80, 0xc01059e940, 0x427ece8, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370 +0x1fe\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc000114780, 0xc00e09a120, 0xc0330ea1b0, 0xc00e6e2950, 0xc001152dc0, 0xc00d38e470, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375 +0x413\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc000114780, 0x4a9be40, 0xc00aea7650, 0xc00d38e470, 0xc00e09a120, 0xc0330ea1b0, 0x4a9db80, 0xc00d43f080, 0xc001152dc0, 0xc00d38e1d8)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977 +0x20a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0xc00d38e390, 0x3, 0x7fae74b8fc40, 0xc00a0ac680, 0xc00e6e2b01, 0x0, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:823 +0x370\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0xc00d38e390, 0x7fae74b8fc40, 0xc00a0ac680, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:715 +0x6af\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0x4ab3a80, 0xc00ad66dc0, 0xc00621c6a9, 0x4d, 0x0, 0x2, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418 +0xbc4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0x4ab3a80, 0xc00ad66dc0, 0xc00621c6a9, 0x4d, 0x0, 0x2, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100 +0x4f9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execCmd(0xc00d38e000, 0x4a9bd80, 0xc018f10440, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283 +0x1aa2\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc00d38e000, 0x4a9bd80, 0xc00c0bc5c0, 0xc000931540, 0x5400, 0x15000, 0xc0009315d8, 0xc00963a140, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212 +0x1c0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc000861900, 0x4a9bd80, 0xc00c0bc5c0, 0xc00d38e000, 0x5400, 0x15000, 0xc0009315d8, 0xc00963a140, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync.func1(0xc00a56a139, 0xc0082c2380, 0x4a9bd80, 0xc00c0bc5c0, 0xc00963a140, 0xc000861900, 0xc00a0ac000, 0x4ab1d00, 0xc0082c2360, 0xc00d280240, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566 +0x2ae\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:498 +0x17e\r\n\r\n```",C-bug|S-2,asubiotto,"Consider the following test case:\r\n\r\n```SQL\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 STRING);\r\nCREATE TABLE t1(c0 STRING UNIQUE NOT NULL);\r\nSELECT * FROM t0, t1 WHERE t0.c0 NOT BETWEEN t1.c0 AND '' AND (t1.c0 IS NULL); -- crash\r\n```\r\n\r\nUnexpectedly, the query results in a crash (see stack trace below).\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.\r\n\r\n```\r\noveToDraining called in state StateTrailingMeta with err: received multiple headers\r\ngoroutine 443 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.getStacks(0x744cb00, 0xed5b6c637, 0x0, 0x840c90)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/get_stacks.go:25 +0xb1\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.(*loggerT).outputLogEntry(0x7449a40, 0xc000000004, 0x6cdb4a1, 0x1f, 0x24a, 0xc0188cfa80, 0x7a)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/clog.go:209 +0x968\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.addStructured(0x4a9bd80, 0xc01059e940, 0x4, 0x2, 0x416e782, 0x2e, 0xc00e6e1de8, 0x2, 0x2)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/structured.go:66 +0x2cc\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.logDepth(0x4a9bd80, 0xc01059e940, 0x1, 0xc000000004, 0x416e782, 0x2e, 0xc00e6e1de8, 0x2, 0x2)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/log.go:44 +0x8c\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.Fatalf(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/util/log/log.go:155\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra.(*ProcessorBase).MoveToDraining(0xc00a827200, 0x4a259a0, 0xc00f8b4b00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:586 +0x30c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/rowexec.(*valuesProcessor).Start(0xc00a827200, 0x4a9bd80, 0xc01059e940, 0xc007693180, 0x400)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/rowexec/values.go:81 +0x224\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*Columnarizer).Init(0xc0263c6700)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/columnarizer.go:85 +0x211\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*hashJoinEqOp).Init(0xc02167bd00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/hashjoiner.go:221 +0x6c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*bufferOp).Init(0xc00f8b48e0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/buffer.go:48 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.constBoolOp.Init(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/const.eg.go:111\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*caseOp).Init(0xc035fc9c00)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/case.go:99 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.selBoolOp.Init(...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:121\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*boolVecToSelOp).Init(0xc01059efc0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/bool_vec_to_sel.go:85 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*simpleProjectOp).Init(0xc0035e63c0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/simple_project.go:89 +0x33\r\ngithub.com/cockroachdb/cockroach/pkg/sql/colexec.(*Materializer).Start(0xc0263c6e00, 0x4a9bd80, 0xc01059e940, 0x749c4a0, 0x3b03440)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:132 +0x3e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/execinfra.(*ProcessorBase).Run(0xc0263c6e00, 0x4a9bd80, 0xc01059e940)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:748 +0x52\r\ngithub.com/cockroachdb/cockroach/pkg/sql/flowinfra.(*FlowBase).Run(0xc0232f67e0, 0x4a9bd80, 0xc01059e940, 0x427ece8, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:370 +0x1fe\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc000114780, 0xc00e09a120, 0xc0330ea1b0, 0xc00e6e2950, 0xc001152dc0, 0xc00d38e470, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:375 +0x413\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc000114780, 0x4a9be40, 0xc00aea7650, 0xc00d38e470, 0xc00e09a120, 0xc0330ea1b0, 0x4a9db80, 0xc00d43f080, 0xc001152dc0, 0xc00d38e1d8)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:977 +0x20a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0xc00d38e390, 0x3, 0x7fae74b8fc40, 0xc00a0ac680, 0xc00e6e2b01, 0x0, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:823 +0x370\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0xc00d38e390, 0x7fae74b8fc40, 0xc00a0ac680, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:715 +0x6af\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0x4ab3a80, 0xc00ad66dc0, 0xc00621c6a9, 0x4d, 0x0, 0x2, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:418 +0xbc4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc00d38e000, 0x4a9be40, 0xc00aea7650, 0x4ab3a80, 0xc00ad66dc0, 0xc00621c6a9, 0x4d, 0x0, 0x2, 0x0, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:100 +0x4f9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execCmd(0xc00d38e000, 0x4a9bd80, 0xc018f10440, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1283 +0x1aa2\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc00d38e000, 0x4a9bd80, 0xc00c0bc5c0, 0xc000931540, 0x5400, 0x15000, 0xc0009315d8, 0xc00963a140, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1212 +0x1c0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc000861900, 0x4a9bd80, 0xc00c0bc5c0, 0xc00d38e000, 0x5400, 0x15000, 0xc0009315d8, 0xc00963a140, 0x0, 0x0)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:451 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync.func1(0xc00a56a139, 0xc0082c2380, 0x4a9bd80, 0xc00c0bc5c0, 0xc00963a140, 0xc000861900, 0xc00a0ac000, 0x4ab1d00, 0xc0082c2360, 0xc00d280240, ...)\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:566 +0x2ae\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync\r\n\t.../go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:498 +0x17e\r\n\r\n```","SQL\r\nSET SESSION VECTORIZE=experimental_on;\r\nCREATE TABLE t0(c0 STRING);\r\nCREATE TABLE t1(c0 STRING UNIQUE NOT NULL);\r\nSELECT * FROM t0, t1 WHERE t0.c0 NOT BETWEEN t1.c0 AND '' AND (t1.c0 IS NULL); -- crash\r\n"
44132,"Generated column causes query to omit a record in the result setConsider the following test case:\r\n\r\n\r\nUnexpectedly, the query does not fetch the single row from t0. This bug seems to be caused by/related to the generated column, since the query works correctly when removing `c1`:\r\n\r\n\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.",C-bug,RaduBerinde,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 BOOL UNIQUE, c1 INT AS (NULL) STORED);\r\nINSERT INTO t0 (c0) VALUES (true);\r\nSELECT * FROM t0 WHERE c0; -- expected: row is fetched, actual: row is not fetched\r\n```\r\nUnexpectedly, the query does not fetch the single row from t0. This bug seems to be caused by/related to the generated column, since the query works correctly when removing `c1`:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 BOOL UNIQUE);\r\nINSERT INTO t0 (c0) VALUES (true);\r\nSELECT * FROM t0 WHERE c0; -- row is fetched\r\n```\r\nI found this issue in commit `3bb183403b4bc75c25fbcfc69d7d35de76d2b984` on Ubuntu 19.04.","sql\r\nCREATE TABLE t0(c0 BOOL UNIQUE, c1 INT AS (NULL) STORED);\r\nINSERT INTO t0 (c0) VALUES (true);\r\nSELECT * FROM t0 WHERE c0; -- expected: row is fetched, actual: row is not fetched\r\n"
44123,"sql: Incorrect result for LIKE queryConsider the following test case:\r\n\r\n\r\n\r\nUnexpectedly, the query fetches a row. When removing the `UNIQUE` constraint, no row is fetched. That the predicate should evaluate to `FALSE` is also demonstrated by the following query:\r\n\r\n\r\nI built CockroachDB from source (commit `55c0e012b9539a327640e42ceaaf556b7a0e7b10`) on Ubuntu 19.04.",C-bug|S-0-visible-logical-error|T-sql-queries|E-quick-win,cucaroach,"Consider the following test case:\r\n\r\n```sql\r\nCREATE TABLE t0(c0 STRING UNIQUE);\r\nINSERT INTO t0 (c0) VALUES ('\\a');\r\nSELECT * FROM t0 WHERE c0 LIKE '\\a'; -- unexpected: row is fetched\r\n```\r\n\r\nUnexpectedly, the query fetches a row. When removing the `UNIQUE` constraint, no row is fetched. That the predicate should evaluate to `FALSE` is also demonstrated by the following query:\r\n\r\n```sql\r\nSELECT c0 LIKE '\\a' FROM t0; -- FALSE\r\n```\r\nI built CockroachDB from source (commit `55c0e012b9539a327640e42ceaaf556b7a0e7b10`) on Ubuntu 19.04.",sql\r\nCREATE TABLE t0(c0 STRING UNIQUE);\r\nINSERT INTO t0 (c0) VALUES ('\\a');\r\nSELECT * FROM t0 WHERE c0 LIKE '\\a'; -- unexpected: row is fetched\r\n
44032,"sql: add localtime functionSupport the Postgres function to select local time.\r\n\r\n\r\nCurrently, this is unimplemented:\r\n\r\nIn postgres:\r\n",C-enhancement|A-sql-pgcompat,otan,"Support the Postgres function to select local time.\r\n```SQL\r\nSELECT LOCALTIME;\r\ninvalid syntax: statement ignored: at or near ""localtime"": syntax error\r\nSQLSTATE: 42601\r\nDETAIL: source SQL:\r\nSELECT LOCALTIME\r\n       ^\r\nHINT: try \\h SELECT\r\n```\r\n\r\nCurrently, this is unimplemented:\r\n\r\nIn postgres:\r\n```SQL\r\nSELECT LOCALTIME;\r\n \r\n      time\r\n-----------------\r\n 00:52:40.227186\r\n(1 row)\r\n```","SQL\r\nSELECT LOCALTIME;\r\ninvalid syntax: statement ignored: at or near ""localtime"": syntax error\r\nSQLSTATE: 42601\r\nDETAIL: source SQL:\r\nSELECT LOCALTIME\r\n       ^\r\nHINT: try \\h SELECT\r\n"
44027,"sql: convert a timestamp to another time zoneSupport the postgres function to convert a timestamp to another time zone.\r\n\r\nCurrently, this is unimplemented:\r\n\r\nIt should return:\r\n```\r\n| timezone--------------------- 2016-06-01 03:00:00(1 row)\r\n-- | --\r\n```\r\n\r\n\r\n",C-enhancement|A-sql-pgcompat,otan,"Support the postgres function to convert a timestamp to another time zone.\r\n\r\nCurrently, this is unimplemented:\r\n```SQL\r\nSELECT timezone('America/New_York','2016-06-01 00:00');\r\nERROR: unknown signature: timezone(string, string)\r\nSQLSTATE: 42883\r\n```\r\nIt should return:\r\n```\r\n| timezone--------------------- 2016-06-01 03:00:00(1 row)\r\n-- | --\r\n```\r\n\r\n\r\n","SQL\r\nSELECT timezone('America/New_York','2016-06-01 00:00');\r\nERROR: unknown signature: timezone(string, string)\r\nSQLSTATE: 42883\r\n"
44026,"sql: add TIMEOFDAY() function and unimplemented telemetry Support the postgres function TIMEOFDAY:\r\n\r\nCurrently, this is unimplemented:\r\n\r\nIt should return the time of day in a strong format like:\r\n\r\n",C-enhancement|A-sql-pgcompat,otan,"Support the postgres function TIMEOFDAY:\r\n\r\nCurrently, this is unimplemented:\r\n```SQL\r\nSELECT TIMEOFDAY();\r\nERROR: unknown function: timeofday()\r\nSQLSTATE: 42883\r\n```\r\nIt should return the time of day in a strong format like:\r\n```SQL\r\nWed Jun 22 20:51:12.632420 2016 PDT\r\n```\r\n",SQL\r\nSELECT TIMEOFDAY();\r\nERROR: unknown function: timeofday()\r\nSQLSTATE: 42883\r\n
43936,"Error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64**Problem**\r\n\r\nVectorized engine unexpectedly fails with an error message about integer type conversions:\r\n\r\n```\r\npq: internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\n```\r\n\r\n**To Reproduce**\r\n\r\nUsing a CRDB 19.2.1 binary, load the ""employees"" data set:\r\n\r\n\r\n\r\nThen issue the following query:\r\n\r\n\r\n\r\nThis resulted in the following error message:\r\n\r\n```\r\nDETAIL: stack trace:\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:70: func1()\r\n/usr/local/go/src/runtime/panic.go:522: gopanic()\r\n/usr/local/go/src/runtime/iface.go:248: panicdottypeE()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/col/coldata/vec.go:185: Int64()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/vec_elem_to_datum.go:51: PhysicalTypeColElemToDatum()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:168: next()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:169: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:725: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:375: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:372: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:997: PlanAndRun()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:825: execWithDistSQLEngine()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:717: dispatchToExecutionEngine()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:416: execStmtInOpenState()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:98: execStmt()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1243: execCmd()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1172: run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:444: ServeConn()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:584: func1()\r\n/usr/local/go/src/runtime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI searched in the issue tracker for the `pq` error message above, and did not find a report of this issue.\r\n\r\n**Expected behavior**\r\n\r\nI expected the query to return a result.  Interestingly, running a very similar query on another table that calls `max()` on similar values does work:\r\n\r\n```\r\nSELECT MAX(e.emp_no) FROM employees AS e;\r\n   max    \r\n+--------+\r\n  499999  \r\n(1 row)\r\n```\r\n\r\nI tried the above because I thought (based on the error message) that CRDB was unhappy because of a particular 32-bit value somewhere.  Both the `employees` and `salaries` tables use `INT4` as the data type for the numeric values I was running `max()` on.  However, one query works, and one doesn't.\r\n\r\n```\r\nSHOW COLUMNS FROM salaries;\r\n  column_name | data_type | is_nullable | column_default | generation_expression |  indices  | is_hidden  \r\n+-------------+-----------+-------------+----------------+-----------------------+-----------+-----------+\r\n  emp_no      | INT4      |    false    | NULL           |                       | {primary} |   false    \r\n  salary      | INT4      |    false    | NULL           |                       | {}        |   false    \r\n  from_date   | DATE      |    false    | NULL           |                       | {primary} |   false    \r\n  to_date     | DATE      |    false    | NULL           |                       | {}        |   false    \r\n(4 rows)\r\n```\r\n```\r\nSHOW COLUMNS FROM employees;\r\n  column_name |  data_type  | is_nullable | column_default | generation_expression |  indices  | is_hidden  \r\n+-------------+-------------+-------------+----------------+-----------------------+-----------+-----------+\r\n  emp_no      | INT4        |    false    | NULL           |                       | {primary} |   false    \r\n  birth_date  | DATE        |    false    | NULL           |                       | {}        |   false    \r\n  first_name  | VARCHAR(14) |    false    | NULL           |                       | {}        |   false    \r\n  last_name   | VARCHAR(16) |    false    | NULL           |                       | {}        |   false    \r\n  gender      | STRING      |    false    | NULL           |                       | {}        |   false    \r\n  hire_date   | DATE        |    false    | NULL           |                       | {}        |   false    \r\n(6 rows)\r\n```\r\n\r\n**Environment**\r\n\r\nCRDB version 19.2.1 running in a local 1-node cluster on a Macbook (macOS Catalina 10.15.2):\r\n\r\n```\r\nSELECT VERSION();\r\n                                         version                                         \r\n+---------------------------------------------------------------------------------------+\r\n  CockroachDB CCL v19.2.1 (x86_64-apple-darwin14, built 2019/11/18 23:17:47, go1.12.12)  \r\n(1 row)\r\n```",C-bug,yuzefovich,"**Problem**\r\n\r\nVectorized engine unexpectedly fails with an error message about integer type conversions:\r\n\r\n```\r\npq: internal error: unexpected error from the vectorized runtime: interface conversion: coldata.column is []int32, not []int64\r\n```\r\n\r\n**To Reproduce**\r\n\r\nUsing a CRDB 19.2.1 binary, load the ""employees"" data set:\r\n\r\n```sql\r\nCREATE DATABASE IF NOT EXISTS employees;\r\nUSE employees;\r\nIMPORT MYSQLDUMP 'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/employees-db/mysqldump/employees-full.sql.gz';\r\n```\r\n\r\nThen issue the following query:\r\n\r\n```sql\r\nSELECT MAX(s.salary) FROM salaries AS s;\r\n```\r\n\r\nThis resulted in the following error message:\r\n\r\n```\r\nDETAIL: stack trace:\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:70: func1()\r\n/usr/local/go/src/runtime/panic.go:522: gopanic()\r\n/usr/local/go/src/runtime/iface.go:248: panicdottypeE()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/col/coldata/vec.go:185: Int64()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/vec_elem_to_datum.go:51: PhysicalTypeColElemToDatum()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:168: next()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:140: nextAdapter()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/execerror/error.go:91: CatchVectorizedRuntimeError()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/colexec/materializer.go:177: Next()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/base.go:169: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/execinfra/processorsbase.go:725: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/flowinfra/flow.go:375: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:372: Run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:997: PlanAndRun()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:825: execWithDistSQLEngine()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:717: dispatchToExecutionEngine()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:416: execStmtInOpenState()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:98: execStmt()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1243: execCmd()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1172: run()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:444: ServeConn()\r\n/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:584: func1()\r\n/usr/local/go/src/runtime/asm_amd64.s:1337: goexit()\r\n```\r\n\r\nI searched in the issue tracker for the `pq` error message above, and did not find a report of this issue.\r\n\r\n**Expected behavior**\r\n\r\nI expected the query to return a result.  Interestingly, running a very similar query on another table that calls `max()` on similar values does work:\r\n\r\n```\r\nSELECT MAX(e.emp_no) FROM employees AS e;\r\n   max    \r\n+--------+\r\n  499999  \r\n(1 row)\r\n```\r\n\r\nI tried the above because I thought (based on the error message) that CRDB was unhappy because of a particular 32-bit value somewhere.  Both the `employees` and `salaries` tables use `INT4` as the data type for the numeric values I was running `max()` on.  However, one query works, and one doesn't.\r\n\r\n```\r\nSHOW COLUMNS FROM salaries;\r\n  column_name | data_type | is_nullable | column_default | generation_expression |  indices  | is_hidden  \r\n+-------------+-----------+-------------+----------------+-----------------------+-----------+-----------+\r\n  emp_no      | INT4      |    false    | NULL           |                       | {primary} |   false    \r\n  salary      | INT4      |    false    | NULL           |                       | {}        |   false    \r\n  from_date   | DATE      |    false    | NULL           |                       | {primary} |   false    \r\n  to_date     | DATE      |    false    | NULL           |                       | {}        |   false    \r\n(4 rows)\r\n```\r\n```\r\nSHOW COLUMNS FROM employees;\r\n  column_name |  data_type  | is_nullable | column_default | generation_expression |  indices  | is_hidden  \r\n+-------------+-------------+-------------+----------------+-----------------------+-----------+-----------+\r\n  emp_no      | INT4        |    false    | NULL           |                       | {primary} |   false    \r\n  birth_date  | DATE        |    false    | NULL           |                       | {}        |   false    \r\n  first_name  | VARCHAR(14) |    false    | NULL           |                       | {}        |   false    \r\n  last_name   | VARCHAR(16) |    false    | NULL           |                       | {}        |   false    \r\n  gender      | STRING      |    false    | NULL           |                       | {}        |   false    \r\n  hire_date   | DATE        |    false    | NULL           |                       | {}        |   false    \r\n(6 rows)\r\n```\r\n\r\n**Environment**\r\n\r\nCRDB version 19.2.1 running in a local 1-node cluster on a Macbook (macOS Catalina 10.15.2):\r\n\r\n```\r\nSELECT VERSION();\r\n                                         version                                         \r\n+---------------------------------------------------------------------------------------+\r\n  CockroachDB CCL v19.2.1 (x86_64-apple-darwin14, built 2019/11/18 23:17:47, go1.12.12)  \r\n(1 row)\r\n```",sql\r\nCREATE DATABASE IF NOT EXISTS employees;\r\nUSE employees;\r\nIMPORT MYSQLDUMP 'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/employees-db/mysqldump/employees-full.sql.gz';\r\n
43752,"bulk-io: CTAS creates more rows than expected when row count in millions**Describe the problem**\r\n\r\nCustomer fielded a bug where a CTAS on 19.2.1 on a table with 35 million rows is reporting 10's of million rows more than expected.\r\n\r\nThey are not using it in a TXN and therefore bulk-io is used to write the table.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a local 3 node cluster\r\n2. generate dummy data with 35m rows (I used a python script).\r\n\r\n3. create extern directories in each local node store\r\n4. copy data.csv to each respective extern folder\r\n5. in the sql shell:\r\n\r\n\r\n\r\n**Expected behavior**\r\nA table with the exact amount of rows and data to be created.\r\n\r\n**Additional data / screenshots**\r\npossibly related to https://github.com/cockroachdb/cockroach/issues/28842\r\n\r\n**the bug does not occur when specifying a primary key**\r\n\r\n\r\n\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.2.1\r\n - Server OS: MacOS\r\n - Client app: Cockroach SQL\r\n\r\n\r\n",C-bug|S-2,dt,"**Describe the problem**\r\n\r\nCustomer fielded a bug where a CTAS on 19.2.1 on a table with 35 million rows is reporting 10's of million rows more than expected.\r\n\r\nThey are not using it in a TXN and therefore bulk-io is used to write the table.\r\n\r\n\r\n**To Reproduce**\r\n\r\n1. Create a local 3 node cluster\r\n2. generate dummy data with 35m rows (I used a python script).\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport uuid\r\nimport csv\r\n\r\nwith open('data.csv', mode='w') as data_file:\r\n    data_writer = csv.writer(data_file, delimiter=',', quotechar='""',quoting=csv.QUOTE_MINIMAL)\r\n    data_writer.writerow(['id','data'])\r\n    for i in range (35000000):\r\n        data_writer.writerow([uuid.uuid4(),'Testrowdata'])\r\n\r\n```\r\n3. create extern directories in each local node store\r\n4. copy data.csv to each respective extern folder\r\n5. in the sql shell:\r\n\r\n```sql\r\nroot@:26257/defaultdb> import table test_table (\r\nid string primary key,\r\ndata string)\r\ncsv data('nodelocal:///data.csv');\r\n        job_id       |  status   | fraction_completed |   rows   | index_entries | system_records |   bytes\r\n+--------------------+-----------+--------------------+----------+---------------+----------------+------------+\r\n  518623560092418049 | succeeded |                  1 | 35000001 |             0 |              0 | 2100000019\r\n(1 row)\r\n\r\nTime: 5m23.902097s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table;\r\n   count\r\n+----------+\r\n  35000001\r\n(1 row)\r\n\r\nTime: 8.174131s\r\n\r\nroot@:26257/defaultdb> create table test_table2 as select * from test_table;\r\nCREATE TABLE AS\r\n\r\nTime: 1m24.952231s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table2;\r\n   count\r\n+----------+\r\n  42190239\r\n(1 row)\r\n\r\nTime: 19.979619s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table2;\r\n   count\r\n+----------+\r\n  70000002\r\n(1 row)\r\n\r\nTime: 16.955686s\r\n```\r\n\r\n**Expected behavior**\r\nA table with the exact amount of rows and data to be created.\r\n\r\n**Additional data / screenshots**\r\npossibly related to https://github.com/cockroachdb/cockroach/issues/28842\r\n\r\n**the bug does not occur when specifying a primary key**\r\n\r\n```sql\r\nroot@:26257/defaultdb> select count(1) from test_table;\r\n   count\r\n+----------+\r\n  35000001\r\n(1 row)\r\n\r\nTime: 13.908131s\r\n\r\nroot@:26257/defaultdb> create table test_table3(id primary key,data) as select id,data from test_table;\r\nCREATE TABLE AS\r\n\r\nTime: 1m3.381818s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table;\r\n   count\r\n+----------+\r\n  35000001\r\n(1 row)\r\n\r\nTime: 7.996564s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table3;\r\n   count\r\n+----------+\r\n  35000001\r\n(1 row)\r\n\r\nTime: 13.764223s\r\n\r\nroot@:26257/defaultdb> select count(1) from test_table3;\r\n   count\r\n+----------+\r\n  35000001\r\n(1 row)\r\n\r\nTime: 17.487245s\r\n```\r\n\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.2.1\r\n - Server OS: MacOS\r\n - Client app: Cockroach SQL\r\n\r\n\r\n","python\r\n#!/usr/bin/env python3\r\n\r\nimport uuid\r\nimport csv\r\n\r\nwith open('data.csv', mode='w') as data_file:\r\n    data_writer = csv.writer(data_file, delimiter=',', quotechar='""',quoting=csv.QUOTE_MINIMAL)\r\n    data_writer.writerow(['id','data'])\r\n    for i in range (35000000):\r\n        data_writer.writerow([uuid.uuid4(),'Testrowdata'])\r\n\r\n"
43404,"sql: stack overflow with FK creation in transactionDeveloping PonyORM support for CockroachDB I discovered that CockroachDB crashes on the following SQL commands:\r\n\r\n\r\n\r\nIn the log file I can see the following:\r\n\r\n```log\r\nCockroachDB node starting at 2019-12-20 16:19:18.4851919 +0000 UTC (took 1.2s)\r\nbuild:               CCL v19.2.2 @ 2019/12/11 01:43:26 (go1.12.12)\r\nwebui:               http://mymachine:8080\r\nsql:                 postgresql://root@mymachine:26257?sslmode=disable\r\nRPC client flags:    cockroach.exe <client cmd> --host=mymachine:26257 --insecure\r\nlogs:                c:\\cockroachdb\\cockroach-data\\logs\r\ntemp dir:              @@c:\\cockroachdb\\cockroach-data\\cockroach-temp706949739\r\nexternal I/O path:   c:\\cockroachdb\\cockroach-data\\extern\r\nstore[0]:            path=c:\\cockroachdb\\cockroach-data\r\nstatus:              restarted pre-existing node\r\nclusterID:           465ba226-5719-46e4-9398-3a50a7c905c1\r\nnodeID:              1\r\nI191220 16:19:18.496607 510 server/server_update.go:53  [n1] no need to upgrade, cluster already at the newest version\r\nI191220 16:19:18.499808 512 sql/event_log.go:130  [n1] Event: ""node_restart"", target: 1, info: {Descriptor:{NodeID:1 Address:Win10-NewHome:26257 Attrs: Locality: ServerVersion:19.2 BuildTag:v19.2.2 StartedAt:1576858758240784200 LocalityAddress:[] ClusterName: SQLAddress:Win10-NewHome:26257} ClusterID:465ba226-5719-46e4-9398-3a50a7c905c1 StartedAt:1576858758240784200 LastUp:1576858746817624700}\r\nI191220 16:19:18.507259 455 sql/lease.go:1919  released orphaned table lease: {id:23 version:1 expiration:{Time:{wall:214599000 ext:63712455717 loc:<nil>}}}\r\nI191220 16:19:20.142335 134 gossip/gossip.go:1531  [n1] node has connected to cluster via gossip\r\nI191220 16:19:20.142463 134 storage/stores.go:259  [n1] wrote 0 node addresses to persistent storage\r\nruntime: goroutine stack exceeds 1000000000-byte limit\r\nfatal error: stack overflow\r\n\r\nruntime stack:\r\nruntime.throw(0x41e04f7, 0xe)\r\n\t/usr/local/go/src/runtime/panic.go:617 +0x79\r\nruntime.newstack()\r\n\t/usr/local/go/src/runtime/stack.go:1041 +0x6fe\r\nruntime.morestack()\r\n\t/usr/local/go/src/runtime/asm_amd64.s:429 +0x97\r\n\r\ngoroutine 605 [running]:\r\n...\r\n```\r\n\r\nThis is the full Python code to reproduce:\r\n\r\n\r\n\r\nCockroachDB instance was started as `cockroach.exe start-single-node --insecure`\r\n\r\nThe same error was on Windows 10 and Kubuntu 18.04. CockroachDB version is 19.2.2.\r\n\r\nI think this is pretty important issue, because self-referenced tables are quite common use-case.",C-bug|S-2-temp-unavailability,ajwerner,"Developing PonyORM support for CockroachDB I discovered that CockroachDB crashes on the following SQL commands:\r\n\r\n```sql\r\nCREATE TABLE ""x"" (""id"" INT8 PRIMARY KEY, ""parent_id"" INT8);\r\nCREATE INDEX ""idx_x__parent_id"" ON ""x"" (""parent_id"");\r\nALTER TABLE ""x"" ADD CONSTRAINT ""fk_x__parent_id"" FOREIGN KEY (""parent_id"") REFERENCES ""x"" (""id"") ON DELETE CASCADE;\r\n```\r\n\r\nIn the log file I can see the following:\r\n\r\n```log\r\nCockroachDB node starting at 2019-12-20 16:19:18.4851919 +0000 UTC (took 1.2s)\r\nbuild:               CCL v19.2.2 @ 2019/12/11 01:43:26 (go1.12.12)\r\nwebui:               http://mymachine:8080\r\nsql:                 postgresql://root@mymachine:26257?sslmode=disable\r\nRPC client flags:    cockroach.exe <client cmd> --host=mymachine:26257 --insecure\r\nlogs:                c:\\cockroachdb\\cockroach-data\\logs\r\ntemp dir:              @@c:\\cockroachdb\\cockroach-data\\cockroach-temp706949739\r\nexternal I/O path:   c:\\cockroachdb\\cockroach-data\\extern\r\nstore[0]:            path=c:\\cockroachdb\\cockroach-data\r\nstatus:              restarted pre-existing node\r\nclusterID:           465ba226-5719-46e4-9398-3a50a7c905c1\r\nnodeID:              1\r\nI191220 16:19:18.496607 510 server/server_update.go:53  [n1] no need to upgrade, cluster already at the newest version\r\nI191220 16:19:18.499808 512 sql/event_log.go:130  [n1] Event: ""node_restart"", target: 1, info: {Descriptor:{NodeID:1 Address:Win10-NewHome:26257 Attrs: Locality: ServerVersion:19.2 BuildTag:v19.2.2 StartedAt:1576858758240784200 LocalityAddress:[] ClusterName: SQLAddress:Win10-NewHome:26257} ClusterID:465ba226-5719-46e4-9398-3a50a7c905c1 StartedAt:1576858758240784200 LastUp:1576858746817624700}\r\nI191220 16:19:18.507259 455 sql/lease.go:1919  released orphaned table lease: {id:23 version:1 expiration:{Time:{wall:214599000 ext:63712455717 loc:<nil>}}}\r\nI191220 16:19:20.142335 134 gossip/gossip.go:1531  [n1] node has connected to cluster via gossip\r\nI191220 16:19:20.142463 134 storage/stores.go:259  [n1] wrote 0 node addresses to persistent storage\r\nruntime: goroutine stack exceeds 1000000000-byte limit\r\nfatal error: stack overflow\r\n\r\nruntime stack:\r\nruntime.throw(0x41e04f7, 0xe)\r\n\t/usr/local/go/src/runtime/panic.go:617 +0x79\r\nruntime.newstack()\r\n\t/usr/local/go/src/runtime/stack.go:1041 +0x6fe\r\nruntime.morestack()\r\n\t/usr/local/go/src/runtime/asm_amd64.s:429 +0x97\r\n\r\ngoroutine 605 [running]:\r\n...\r\n```\r\n\r\nThis is the full Python code to reproduce:\r\n\r\n```python\r\nimport psycopg2\r\n\r\nstatements = [\r\n\t'CREATE TABLE ""x"" (""id"" INT8 PRIMARY KEY, ""parent_id"" INT8)',\r\n\t'CREATE INDEX ""idx_x__parent_id"" ON ""x"" (""parent_id"")',\r\n\t'ALTER TABLE ""x"" ADD CONSTRAINT ""fk_x__parent_id"" FOREIGN KEY (""parent_id"") REFERENCES ""x"" (""id"") ON DELETE CASCADE',\r\n]\r\n\r\nparams = dict(user='root', host='localhost', port=26257, sslmode='disable', database='test1')\r\n\r\ncon = psycopg2.connect(**params)\r\ncursor = con.cursor()\r\n\r\nfor sql in statements:\r\n    print(sql)\r\n    cursor.execute(sql)\r\n```\r\n\r\nCockroachDB instance was started as `cockroach.exe start-single-node --insecure`\r\n\r\nThe same error was on Windows 10 and Kubuntu 18.04. CockroachDB version is 19.2.2.\r\n\r\nI think this is pretty important issue, because self-referenced tables are quite common use-case.","sql\r\nCREATE TABLE ""x"" (""id"" INT8 PRIMARY KEY, ""parent_id"" INT8);\r\nCREATE INDEX ""idx_x__parent_id"" ON ""x"" (""parent_id"");\r\nALTER TABLE ""x"" ADD CONSTRAINT ""fk_x__parent_id"" FOREIGN KEY (""parent_id"") REFERENCES ""x"" (""id"") ON DELETE CASCADE;\r\n"
43294,"Allow AS OF SYSTEM TIME (AOST) to be passed to EXPLAINIn version 19.2.1, if you stick `EXPLAIN` on the front of a query that includes `AS OF SYSTEM TIME`, it fails with the following error message:\r\n\r\n```\r\npq: AS OF SYSTEM TIME must be provided on a top-level statement\r\n```\r\n\r\nI would like to be able to take a query that includes `AS OF SYSTEM TIME` and pass it to `EXPLAIN` without having to edit out the AOST portion.\r\n\r\nMore generally, this was surprising to me because my expectation of `EXPLAIN` is that I can pretty much just ""slap an `EXPLAIN` on it"" with any arbitrary SQL query.\r\n\r\nIn the SQL shell, this is the difference between:\r\n\r\n1. up arrow\r\n2. ctrl-a (beginning of line)\r\n3. type ""EXPLAIN ""\r\n\r\nvs. doing a bunch of possibly painful inline edits, including maybe making a mistake so that the query that gets `EXPLAIN`ed does not exactly match your intended query, etc.\r\n\r\nFrom client code, it could also be unpleasant depending on your environment, since you couldn't just prepend an `EXPLAIN` e.g.\r\n\r\n\r\n\r\nIn this case, I was trying to see if / how the `EXPLAIN` output would actually differ if `AOST` were used on a query.  In [the AOST docs](https://www.cockroachlabs.com/docs/stable/as-of-system-time.html) we say re: performance that\r\n\r\n> This clause can be used to read historical data (also known as ""time travel queries"") and can also be advantageous for performance as it decreases transaction conflicts\r\n\r\nbut it's pretty vague.  I wanted to run an `EXPLAIN` to see if there were other performance implications to the query plan that could be seen vs. running the same query without AOST.\r\n\r\nIt also makes documenting things more verbose in some cases (such as work I'm doing on [this docs PR](https://github.com/cockroachdb/docs/pull/6114) since the workflow is not (1) run query, (2) slap an EXPLAIN on the query, it's the whole inline-edit-and-don't-make-a-mistake thing mentioned above, which also requires mentioning this limitation inline in those (unrelated) docs, which adds cognitive weight to the user.  \r\n\r\nFrom a UX perspective, this limitation is a bit unfortunate if we are expecting people to write AOST queries on a regular basis to get good performance, which I think we are.\r\n\r\nRelated issues: I searched around and did find #30534 which may be the technical reason for this limitation but is not about this particular use case.\r\n\r\nExact version info:\r\n```\r\nSELECT VERSION();\r\n                                         version                                         \r\n+---------------------------------------------------------------------------------------+\r\n  CockroachDB CCL v19.2.1 (x86_64-apple-darwin14, built 2019/11/18 23:17:47, go1.12.12)  \r\n(1 row)\r\n```",C-enhancement|A-sql-optimizer,RaduBerinde,"In version 19.2.1, if you stick `EXPLAIN` on the front of a query that includes `AS OF SYSTEM TIME`, it fails with the following error message:\r\n\r\n```\r\npq: AS OF SYSTEM TIME must be provided on a top-level statement\r\n```\r\n\r\nI would like to be able to take a query that includes `AS OF SYSTEM TIME` and pass it to `EXPLAIN` without having to edit out the AOST portion.\r\n\r\nMore generally, this was surprising to me because my expectation of `EXPLAIN` is that I can pretty much just ""slap an `EXPLAIN` on it"" with any arbitrary SQL query.\r\n\r\nIn the SQL shell, this is the difference between:\r\n\r\n1. up arrow\r\n2. ctrl-a (beginning of line)\r\n3. type ""EXPLAIN ""\r\n\r\nvs. doing a bunch of possibly painful inline edits, including maybe making a mistake so that the query that gets `EXPLAIN`ed does not exactly match your intended query, etc.\r\n\r\nFrom client code, it could also be unpleasant depending on your environment, since you couldn't just prepend an `EXPLAIN` e.g.\r\n\r\n```perl\r\n$query = 'EXPLAIN ' . $query;\r\n```\r\n\r\nIn this case, I was trying to see if / how the `EXPLAIN` output would actually differ if `AOST` were used on a query.  In [the AOST docs](https://www.cockroachlabs.com/docs/stable/as-of-system-time.html) we say re: performance that\r\n\r\n> This clause can be used to read historical data (also known as ""time travel queries"") and can also be advantageous for performance as it decreases transaction conflicts\r\n\r\nbut it's pretty vague.  I wanted to run an `EXPLAIN` to see if there were other performance implications to the query plan that could be seen vs. running the same query without AOST.\r\n\r\nIt also makes documenting things more verbose in some cases (such as work I'm doing on [this docs PR](https://github.com/cockroachdb/docs/pull/6114) since the workflow is not (1) run query, (2) slap an EXPLAIN on the query, it's the whole inline-edit-and-don't-make-a-mistake thing mentioned above, which also requires mentioning this limitation inline in those (unrelated) docs, which adds cognitive weight to the user.  \r\n\r\nFrom a UX perspective, this limitation is a bit unfortunate if we are expecting people to write AOST queries on a regular basis to get good performance, which I think we are.\r\n\r\nRelated issues: I searched around and did find #30534 which may be the technical reason for this limitation but is not about this particular use case.\r\n\r\nExact version info:\r\n```\r\nSELECT VERSION();\r\n                                         version                                         \r\n+---------------------------------------------------------------------------------------+\r\n  CockroachDB CCL v19.2.1 (x86_64-apple-darwin14, built 2019/11/18 23:17:47, go1.12.12)  \r\n(1 row)\r\n```",perl\r\n$query = 'EXPLAIN ' . $query;\r\n
42953,"cli: allow the ability to specify which nodes (by number) are added to the debug zip`cockroach debug zip` should have a parameter that accepts a selection of node numbers, similar to how roachprod works right now, and only construct the debug.zip from those. \r\n\r\ne.g.:\r\n\r\n\r\n\r\nI'm not sure if this should be on the cli team, so please triage appropriately.",C-enhancement,piyush-singh,"`cockroach debug zip` should have a parameter that accepts a selection of node numbers, similar to how roachprod works right now, and only construct the debug.zip from those. \r\n\r\ne.g.:\r\n\r\n```bash\r\n./cockroach debug zip --nodes=1,5,8\r\n./cockroach debug zip --nodes=1-5,8-10,12\r\n./cockroach debug zip --nodes=9\r\n```\r\n\r\nI'm not sure if this should be on the cli team, so please triage appropriately.","bash\r\n./cockroach debug zip --nodes=1,5,8\r\n./cockroach debug zip --nodes=1-5,8-10,12\r\n./cockroach debug zip --nodes=9\r\n"
42937,"Wrong timestamp to date cast for dates predating 1970-01-01**Describe the problem**\r\n\r\nWhen casting a timestamp before `1970-01-01` to date, a wrong result is produced\r\n\r\n**To Reproduce**\r\n\r\nRun this SQL query in PostgreSQL and CockroachDB:\r\n\r\n\r\n\r\nThe expected result (as observed in PostgreSQL) is that both timestamps are truncated to December 30:\r\n\r\n|ts|ts|\r\n|--|--|\r\n|1969-12-30 01:00:00|1969-12-30|\r\n|1970-12-30 01:00:00|1970-12-30|\r\n\r\nHowever, in CockroachDB, I'm getting:\r\n\r\n|ts|ts|\r\n|--|--|\r\n|1969-12-30 01:00:00|1969-12-31|\r\n|1970-12-30 01:00:00|1970-12-30|\r\n\r\nI'm using:\r\n\r\n\r\n> Europe/Berlin\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC",C-bug|A-sql-pgcompat|S-3-ux-surprise,otan,"**Describe the problem**\r\n\r\nWhen casting a timestamp before `1970-01-01` to date, a wrong result is produced\r\n\r\n**To Reproduce**\r\n\r\nRun this SQL query in PostgreSQL and CockroachDB:\r\n\r\n```sql\r\nselect ts, ts::date\r\nfrom (values \r\n  (timestamp '1969-12-30 01:00:00'),\r\n  (timestamp '1970-12-30 01:00:00')\r\n) t (ts);\r\n```\r\n\r\nThe expected result (as observed in PostgreSQL) is that both timestamps are truncated to December 30:\r\n\r\n|ts|ts|\r\n|--|--|\r\n|1969-12-30 01:00:00|1969-12-30|\r\n|1970-12-30 01:00:00|1970-12-30|\r\n\r\nHowever, in CockroachDB, I'm getting:\r\n\r\n|ts|ts|\r\n|--|--|\r\n|1969-12-30 01:00:00|1969-12-31|\r\n|1970-12-30 01:00:00|1970-12-30|\r\n\r\nI'm using:\r\n\r\n```sql\r\nshow timezone;\r\n```\r\n> Europe/Berlin\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC","sql\r\nselect ts, ts::date\r\nfrom (values \r\n  (timestamp '1969-12-30 01:00:00'),\r\n  (timestamp '1970-12-30 01:00:00')\r\n) t (ts);\r\n"
42936,"isodow date part in EXTRACT() not supported**Describe the problem**\r\n\r\nThis may well be a feature request, but since the documentation mentions this should work, I have reported a bug.\r\n\r\nIn PostgreSQL, I can extract the ISO day of week as follows:\r\n\r\n\r\n\r\nAccording to the documentation, this should be possible in CockroachDB as well:\r\nhttps://www.cockroachlabs.com/docs/stable/functions-and-operators.html#date-and-time-functions\r\n\r\n![image](https://user-images.githubusercontent.com/734593/70139098-0d284280-1692-11ea-9709-9c54a7d903fb.png)\r\n\r\nHowever, I'm getting:\r\n\r\n```\r\nSQL Error [22023]: ERROR: extract(): unsupported timespan: isodow\r\n```\r\n\r\nA few others are also not supported, including:\r\n\r\n- Millenium (easy to emulate)\r\n- Century (easy to emulate)\r\n- Decade (easy to emulate)\r\n- Isoyear\r\n- Julian\r\n\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC",C-investigation|A-sql-pgcompat,otan,"**Describe the problem**\r\n\r\nThis may well be a feature request, but since the documentation mentions this should work, I have reported a bug.\r\n\r\nIn PostgreSQL, I can extract the ISO day of week as follows:\r\n\r\n```sql\r\nselect extract(isodow from current_timestamp)\r\n```\r\n\r\nAccording to the documentation, this should be possible in CockroachDB as well:\r\nhttps://www.cockroachlabs.com/docs/stable/functions-and-operators.html#date-and-time-functions\r\n\r\n![image](https://user-images.githubusercontent.com/734593/70139098-0d284280-1692-11ea-9709-9c54a7d903fb.png)\r\n\r\nHowever, I'm getting:\r\n\r\n```\r\nSQL Error [22023]: ERROR: extract(): unsupported timespan: isodow\r\n```\r\n\r\nA few others are also not supported, including:\r\n\r\n- Millenium (easy to emulate)\r\n- Century (easy to emulate)\r\n- Decade (easy to emulate)\r\n- Isoyear\r\n- Julian\r\n\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC",sql\r\nselect extract(isodow from current_timestamp)\r\n
42935,"Wrong implementation of WINDOW ROWS clause**Describe the problem**\r\n\r\nIt seems that the `WINDOW` function `ROWS` clause is implemented incorrectly. It seems to behave like `RANGE`\r\n\r\n**To Reproduce**\r\n\r\nRun this query:\r\n\r\n\r\n\r\nThe result is:\r\n\r\n|a|b|no frame|range frame|rows frame|\r\n|-|-|--------|-----------|----------|\r\n|1|1|2|2|2|\r\n|2|1|2|2|2|\r\n|3|2|4|4|4|\r\n|4|2|4|4|4|\r\n\r\n(Observe the `rows frame` column)\r\n\r\nIn PostgreSQL and most other RDBMS, the result is:\r\n\r\n\r\n\r\nIf possible, provide steps to reproduce the behavior:\r\n\r\n**Environment:**\r\n - CockroachDB version: `CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)`\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC",C-bug|A-sql-pgcompat|A-sql-execution,yuzefovich,"**Describe the problem**\r\n\r\nIt seems that the `WINDOW` function `ROWS` clause is implemented incorrectly. It seems to behave like `RANGE`\r\n\r\n**To Reproduce**\r\n\r\nRun this query:\r\n\r\n```sql\r\nselect \r\n  a,\r\n  b,\r\n  count(*) over (order by b) as ""no frame"", \r\n  count(*) over (order by b range between unbounded preceding and current row) as ""range frame"", \r\n  count(*) over (order by b rows between unbounded preceding and current row) as ""rows frame""\r\nfrom (values (1, 1), (2, 1), (3, 2), (4, 2)) t (a, b)\r\norder by a, b; \r\n```\r\n\r\nThe result is:\r\n\r\n|a|b|no frame|range frame|rows frame|\r\n|-|-|--------|-----------|----------|\r\n|1|1|2|2|2|\r\n|2|1|2|2|2|\r\n|3|2|4|4|4|\r\n|4|2|4|4|4|\r\n\r\n(Observe the `rows frame` column)\r\n\r\nIn PostgreSQL and most other RDBMS, the result is:\r\n\r\n```sql\r\n|a|b|no frame|range frame|rows frame|\r\n|-|-|--------|-----------|----------|\r\n|1|1|2|2|1|\r\n|2|1|2|2|2|\r\n|3|2|4|4|3|\r\n|4|2|4|4|4|\r\n```\r\n\r\nIf possible, provide steps to reproduce the behavior:\r\n\r\n**Environment:**\r\n - CockroachDB version: `CockroachDB CCL v19.2.1 (x86_64-unknown-linux-gnu, built 2019/11/18 23:23:55, go1.12.12)`\r\n - Server OS: Linux on Docker on Windows\r\n - Client app: JDBC","sql\r\nselect \r\n  a,\r\n  b,\r\n  count(*) over (order by b) as ""no frame"", \r\n  count(*) over (order by b range between unbounded preceding and current row) as ""range frame"", \r\n  count(*) over (order by b rows between unbounded preceding and current row) as ""rows frame""\r\nfrom (values (1, 1), (2, 1), (3, 2), (4, 2)) t (a, b)\r\norder by a, b; \r\n"
42881,"sql: invalid string when converted to bytea**Describe the problem**\r\n\r\n:::BYTEA instead of ::BYTEA.\r\n\r\nThe former means ""this is the type of value"" the latter means ""convert to this type"" but when converting the value is first interpreted as a string and then converted to a bytea, and in this case, that string was invalid for some reason.\r\n\r\nSeeing the following errors: \r\n\r\n```\r\nclient_test.go:84: pq: error in argument for $3: invalid UTF-8 sequence\r\nclient_test.go:87: key not found\r\nclient_test.go:90: value changed\r\n```\r\n\r\n\r\n**To Reproduce**\r\n\r\n```\r\nCREATE TABLE t1 (\r\nbucket BYTES NOT NULL,\r\nfullpath BYTES NOT NULL,\r\nmetadata BYTES NOT NULL,\r\nCONSTRAINT ""primary"" PRIMARY KEY (bucket ASC, fullpath ASC),\r\nFAMILY ""primary"" (bucket, fullpath, metadata),\r\nCONSTRAINT check_fullpath CHECK (fullpath != '')\r\n)```\r\n\r\nThe golang code and SQL that I've reproduced the error with looks like this.\r\n\r\n\r\n\r\n**Expected behavior**\r\nShould work without having to change `::BYTEA` to `:::BYTEA`\r\n\r\n\r\n\r\n\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1\r\n\r\n\r\n\r\n",A-sql-execution,rohany,"**Describe the problem**\r\n\r\n:::BYTEA instead of ::BYTEA.\r\n\r\nThe former means ""this is the type of value"" the latter means ""convert to this type"" but when converting the value is first interpreted as a string and then converted to a bytea, and in this case, that string was invalid for some reason.\r\n\r\nSeeing the following errors: \r\n\r\n```\r\nclient_test.go:84: pq: error in argument for $3: invalid UTF-8 sequence\r\nclient_test.go:87: key not found\r\nclient_test.go:90: value changed\r\n```\r\n\r\n\r\n**To Reproduce**\r\n\r\n```\r\nCREATE TABLE t1 (\r\nbucket BYTES NOT NULL,\r\nfullpath BYTES NOT NULL,\r\nmetadata BYTES NOT NULL,\r\nCONSTRAINT ""primary"" PRIMARY KEY (bucket ASC, fullpath ASC),\r\nFAMILY ""primary"" (bucket, fullpath, metadata),\r\nCONSTRAINT check_fullpath CHECK (fullpath != '')\r\n)```\r\n\r\nThe golang code and SQL that I've reproduced the error with looks like this.\r\n\r\n```go\r\nfunc TestUTF8(t *testing.T) {\r\npgConn, err := sql.Open(""postgres"", ""postgres://root@localhost:26257/defaultdb?sslmode=disable"")\r\nif err != nil {\r\nt.Error(err)\r\n}\r\n\r\nbucket := []byte{}\r\nkey := []byte(""full/path/2"")\r\noldValue := []byte{0, 255, 255, 1}\r\nnewValue := []byte{0, 255, 255, 4}\r\n\r\ninsertResult, err := pgConn.Exec(""INSERT INTO t1 (bucket, fullpath, metadata) VALUES ($1::BYTEA, $2::BYTEA, $3::BYTEA);"", bucket, key, oldValue)\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nrowsAffected, err := insertResult.RowsAffected()\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nfmt.Printf(""ROWS AFFECTED: %d\\n"", rowsAffected)\r\n\r\nq := `\r\nWITH matching_key AS (\r\nSELECT * FROM t1 WHERE bucket = $1::BYTEA AND fullpath = $2::BYTEA\r\n), updated AS (\r\nUPDATE t1\r\nSET metadata = $4::BYTEA\r\nFROM matching_key mk\r\nWHERE t1.metadata = $3::BYTEA\r\nAND t1.bucket = mk.bucket\r\nAND t1.fullpath = mk.fullpath\r\nRETURNING 1\r\n)\r\nSELECT EXISTS(SELECT 1 FROM matching_key) AS key_present, EXISTS(SELECT 1 FROM updated) AS value_updated;\r\n`\r\n\r\nrow := pgConn.QueryRow(q, bucket, key, oldValue, newValue)\r\n\r\nvar keyPresent, valueUpdated bool\r\nerr = row.Scan(&keyPresent, &valueUpdated)\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nif !keyPresent {\r\nt.Error(errors.New(""key not found""))\r\n}\r\nif !valueUpdated {\r\nt.Error(errors.New(""value changed""))\r\n}\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nShould work without having to change `::BYTEA` to `:::BYTEA`\r\n\r\n\r\n\r\n\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1\r\n\r\n\r\n\r\n","go\r\nfunc TestUTF8(t *testing.T) {\r\npgConn, err := sql.Open(""postgres"", ""postgres://root@localhost:26257/defaultdb?sslmode=disable"")\r\nif err != nil {\r\nt.Error(err)\r\n}\r\n\r\nbucket := []byte{}\r\nkey := []byte(""full/path/2"")\r\noldValue := []byte{0, 255, 255, 1}\r\nnewValue := []byte{0, 255, 255, 4}\r\n\r\ninsertResult, err := pgConn.Exec(""INSERT INTO t1 (bucket, fullpath, metadata) VALUES ($1::BYTEA, $2::BYTEA, $3::BYTEA);"", bucket, key, oldValue)\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nrowsAffected, err := insertResult.RowsAffected()\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nfmt.Printf(""ROWS AFFECTED: %d\\n"", rowsAffected)\r\n\r\nq := `\r\nWITH matching_key AS (\r\nSELECT * FROM t1 WHERE bucket = $1::BYTEA AND fullpath = $2::BYTEA\r\n), updated AS (\r\nUPDATE t1\r\nSET metadata = $4::BYTEA\r\nFROM matching_key mk\r\nWHERE t1.metadata = $3::BYTEA\r\nAND t1.bucket = mk.bucket\r\nAND t1.fullpath = mk.fullpath\r\nRETURNING 1\r\n)\r\nSELECT EXISTS(SELECT 1 FROM matching_key) AS key_present, EXISTS(SELECT 1 FROM updated) AS value_updated;\r\n`\r\n\r\nrow := pgConn.QueryRow(q, bucket, key, oldValue, newValue)\r\n\r\nvar keyPresent, valueUpdated bool\r\nerr = row.Scan(&keyPresent, &valueUpdated)\r\nif err != nil {\r\nt.Error(err)\r\n}\r\nif !keyPresent {\r\nt.Error(errors.New(""key not found""))\r\n}\r\nif !valueUpdated {\r\nt.Error(errors.New(""value changed""))\r\n}\r\n}\r\n"
42849,"1PC CPuts suffering from lost updatesWhen running as a 1PC, a CPut can erroneously miss to consider the last value on its key. This is because we mistakenly ignore a write too old condition in some cases. The ignoring happens here:\r\nhttps://github.com/cockroachdb/cockroach/blob/c276b70cdff27b1bf4111f4b27b1459122804d70/pkg/storage/replica_evaluate.go#L344\r\n\r\nAn example of a bad scenario is the following:\r\n- txnA starts at ts 10\r\n- a Put of key a happens at ts 15\r\n- txnA does a CPut that writes the key if there's nothing there, in the same batch as an EndTransaction\r\n- the CPut evaluates at the ""read timestamp"" (so no key is seen) and so it erroneously succeeds, overwriting the first Put. A WriteTooOldError is returned from the CPut evaluation, which is turned into a WriteTooOld flag by `evaluateBatch()`\r\n- the EndTransaction is the evaluated and succeeds, despite the WriteTooOld flag\r\n\r\nThe conditions under which this happens are:\r\n- no reads before the EndTransaction batch. If there would be any reads, the EndTransaction would refuse to commit when the `WriteTooOld` flag is set on the txn at the end, insisting that the client refresh.\r\n- no pipelined writes that are unresolved by the time of the EndTransaction. If there's any such writes, the EndTransaction moves the txn record to STAGING (instead of COMMITTED), and the erroneous ignoring of the wto condition doesn't kick in (by a happy accident)?\r\n\r\nSo in 19.2 this leaves mostly 1PC txns vulnerable. But in 19.1 I think the second condition above doesn't stand, so maybe the problem was even worse?\r\n\r\nIt would appear the bug was introduced in #38668, which was backported to 19.1.5.\r\n\r\nHere's a test exemplifying the problem.\r\n\r\n",S-1,andreimatei,"When running as a 1PC, a CPut can erroneously miss to consider the last value on its key. This is because we mistakenly ignore a write too old condition in some cases. The ignoring happens here:\r\nhttps://github.com/cockroachdb/cockroach/blob/c276b70cdff27b1bf4111f4b27b1459122804d70/pkg/storage/replica_evaluate.go#L344\r\n\r\nAn example of a bad scenario is the following:\r\n- txnA starts at ts 10\r\n- a Put of key a happens at ts 15\r\n- txnA does a CPut that writes the key if there's nothing there, in the same batch as an EndTransaction\r\n- the CPut evaluates at the ""read timestamp"" (so no key is seen) and so it erroneously succeeds, overwriting the first Put. A WriteTooOldError is returned from the CPut evaluation, which is turned into a WriteTooOld flag by `evaluateBatch()`\r\n- the EndTransaction is the evaluated and succeeds, despite the WriteTooOld flag\r\n\r\nThe conditions under which this happens are:\r\n- no reads before the EndTransaction batch. If there would be any reads, the EndTransaction would refuse to commit when the `WriteTooOld` flag is set on the txn at the end, insisting that the client refresh.\r\n- no pipelined writes that are unresolved by the time of the EndTransaction. If there's any such writes, the EndTransaction moves the txn record to STAGING (instead of COMMITTED), and the erroneous ignoring of the wto condition doesn't kick in (by a happy accident)?\r\n\r\nSo in 19.2 this leaves mostly 1PC txns vulnerable. But in 19.1 I think the second condition above doesn't stand, so maybe the problem was even worse?\r\n\r\nIt would appear the bug was introduced in #38668, which was backported to 19.1.5.\r\n\r\nHere's a test exemplifying the problem.\r\n\r\n```go\r\nfunc TestXXX(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\tctx := context.Background()\r\n\r\n\ts, _, db := serverutils.StartServer(t, base.TestServerArgs{})\r\n\tdefer s.Stopper().Stop(ctx)\r\n\tkeyA := ""a""\r\n\ttxn := client.NewTxn(ctx, db, 1 /* gatewayNodeID */, client.RootTxn)\r\n\tlog.Infof(ctx, ""!!! test starting txn: %s"", txn.ID())\r\n\r\n\tif err := db.Put(ctx, keyA, ""a""); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tb := txn.NewBatch()\r\n\tb.CPut(keyA, ""b"", nil /* exp */)\r\n\tif err := txn.CommitInBatch(ctx, b); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tval, err := db.Get(ctx, keyA)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tv, err := val.Value.GetBytes()\r\n\trequire.NoError(t, err)\r\n\tlog.Infof(ctx, ""!!! val: %s"", string(v)) // This will be ""b"".\r\n```","go\r\nfunc TestXXX(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\tctx := context.Background()\r\n\r\n\ts, _, db := serverutils.StartServer(t, base.TestServerArgs{})\r\n\tdefer s.Stopper().Stop(ctx)\r\n\tkeyA := ""a""\r\n\ttxn := client.NewTxn(ctx, db, 1 /* gatewayNodeID */, client.RootTxn)\r\n\tlog.Infof(ctx, ""!!! test starting txn: %s"", txn.ID())\r\n\r\n\tif err := db.Put(ctx, keyA, ""a""); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tb := txn.NewBatch()\r\n\tb.CPut(keyA, ""b"", nil /* exp */)\r\n\tif err := txn.CommitInBatch(ctx, b); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tval, err := db.Get(ctx, keyA)\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\tv, err := val.Value.GetBytes()\r\n\trequire.NoError(t, err)\r\n\tlog.Infof(ctx, ""!!! val: %s"", string(v)) // This will be ""b"".\r\n"
42612,"sql: DATE_TRUNC('week', <value>) truncates to midnight Sunday rather than Monday`DATE_TRUNC('week', <value>)` truncates to midnight Sunday on cockroachdb but midnight Monday on PostgreSQL.\r\n\r\nThis was discovered while testing with Django: https://github.com/cockroachdb/django-cockroachdb/issues/92\r\n\r\nPostgreSQL:\r\n\r\ncockroach:\r\n",A-sql-pgcompat,otan,"`DATE_TRUNC('week', <value>)` truncates to midnight Sunday on cockroachdb but midnight Monday on PostgreSQL.\r\n\r\nThis was discovered while testing with Django: https://github.com/cockroachdb/django-cockroachdb/issues/92\r\n\r\nPostgreSQL:\r\n```sql\r\n> SELECT DATE_TRUNC('week', '2019-11-13'::timestamp);\r\n""2019-11-11 00:00:00""\r\n```\r\ncockroach:\r\n```sql\r\n> SELECT DATE_TRUNC('week', '2019-11-13'::timestamptz);\r\n         date_trunc          \r\n+---------------------------+\r\n  2019-11-10 00:00:00+00:00\r\n```","sql\r\n> SELECT DATE_TRUNC('week', '2019-11-13'::timestamp);\r\n""2019-11-11 00:00:00""\r\n"
42568,"pg_type incompatible with jdbc driver**Describe the problem**\r\n\r\nPostgreSQL JDBC driver fails to interpret types in the `pg_catalog` schema.\r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\norg.postgresql.util.PSQLException: Bad value for type long : numeric_in\r\n```\r\n\r\n**Expected behavior**\r\n\r\nShould print Type: `some name` is `numeric_in`\r\n\r\n**Additional data / screenshots**\r\n\r\nAs well it's possible to see the problem when connected to the database in IntelliJ with the Database plug-in and running the same query. That is `42.2.5` version of the driver.\r\n\r\n<img width=""1226"" alt=""screen"" src=""https://user-images.githubusercontent.com/156068/69163058-e264b880-0aed-11ea-8d3b-10171cd52f88.png"">\r\n\r\n**Environment:**\r\n\r\n - CCL v19.2.1 @ 2019/11/18 23:17:47 (go1.12.12)\r\n - macOS\r\n - OpenJ9 11 with PostgreSQL JDBC driver 42.2.8\r\n\r\n**Additional context**\r\n\r\nRoot failure was JRuby (Ruby on Rails) failing to gather required data.\r\n",C-investigation,apantel,"**Describe the problem**\r\n\r\nPostgreSQL JDBC driver fails to interpret types in the `pg_catalog` schema.\r\n\r\n**To Reproduce**\r\n\r\n```shell\r\nif [ ! -f cockroach ]; then\r\n  curl https://binaries.cockroachdb.com/cockroach-v19.2.1.darwin-10.9-amd64.tgz | tar -xJ\r\n  mv cockroach-*/cockroach cockroach\r\n  rmdir cockroach-*\r\nfi\r\n\r\n./cockroach start \\\r\n--insecure \\\r\n--store=cockroach-data \\\r\n--listen-addr=localhost:26257 \\\r\n--http-addr=localhost:9081\r\n```\r\n\r\n```sql\r\nCREATE DATABASE sample;\r\n```\r\n\r\n```java\r\nResultSet rs = this.dataSource.getConnection().prepareStatement(""SELECT * FROM pg_catalog.pg_type;"").executeQuery();\r\nwhile(rs.next()) {\r\n    this.logger.info(""Type: {} is {}"", rs.getObject(""typname""), rs.getObject(""typinput""));\r\n}\r\n```\r\n\r\n```\r\norg.postgresql.util.PSQLException: Bad value for type long : numeric_in\r\n```\r\n\r\n**Expected behavior**\r\n\r\nShould print Type: `some name` is `numeric_in`\r\n\r\n**Additional data / screenshots**\r\n\r\nAs well it's possible to see the problem when connected to the database in IntelliJ with the Database plug-in and running the same query. That is `42.2.5` version of the driver.\r\n\r\n<img width=""1226"" alt=""screen"" src=""https://user-images.githubusercontent.com/156068/69163058-e264b880-0aed-11ea-8d3b-10171cd52f88.png"">\r\n\r\n**Environment:**\r\n\r\n - CCL v19.2.1 @ 2019/11/18 23:17:47 (go1.12.12)\r\n - macOS\r\n - OpenJ9 11 with PostgreSQL JDBC driver 42.2.8\r\n\r\n**Additional context**\r\n\r\nRoot failure was JRuby (Ruby on Rails) failing to gather required data.\r\n",shell\r\nif [ ! -f cockroach ]; then\r\n  curl https://binaries.cockroachdb.com/cockroach-v19.2.1.darwin-10.9-amd64.tgz | tar -xJ\r\n  mv cockroach-*/cockroach cockroach\r\n  rmdir cockroach-*\r\nfi\r\n\r\n./cockroach start \\\r\n--insecure \\\r\n--store=cockroach-data \\\r\n--listen-addr=localhost:26257 \\\r\n--http-addr=localhost:9081\r\n
42556,"sql: panic with window function in 19.1.5**Describe the problem**\r\n\r\nSQL panic in 19.1.5, client is restarted and node crashes.\r\n\r\nDoes not occur in 19.2.\r\n\r\n**To Reproduce**\r\n\r\nWhat did you do? Describe in your own words.\r\n\r\n**Expected behavior**\r\nInsert to function properly.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1.5\r\n - Client app: Cockroach SQL\r\n\r\n**Additional context**\r\nThis is related to local mode.\r\n\r\nStack trace:\r\n```\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] a SQL panic has occurred while executing ""INSERT INTO t1 SELECT col1::INT8, col2::INT8, col3::INT8, col4::INT8, col5::INT8, col6::INT8, col7::INT8, col8::VARCHAR, col9::VARCHAR, col10::VARCHAR, col11::INT8, row_number() OVER (PARTITION BY col13 ORDER BY col14 DESC)::INT8 FROM t2"": windowNode can't be run in local mode\r\n*\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: INSERT INTO _ SELECT _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::VARCHAR, _::VARCHAR, _::VARCHAR, _::INT8, row_number() OVER (PARTITION BY _ ORDER BY _ DESC)::INT8 FROM _\r\n\r\ngoroutine 1536 [running]:\r\nruntime/debug.Stack(0x7289100, 0xc00ada7a40, 0xc000000003)\r\n\t/usr/local/go/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x7289100, 0xc00ada7a40, 0xc000493300, 0x697c960, 0xc00d9bb140, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:226 +0xb5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc000c38d80, 0x7289100, 0xc00ada7a40, 0x660d3a0, 0x7241b80)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:715 +0x2dd\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc000c38d80, 0x7289100, 0xc00ada7a40)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:434 +0x61\r\npanic(0x660d3a0, 0x7241b80)\r\n\t/usr/local/go/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*windowNode).Next(0xc009586c60, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0xc00fd41988, 0x400cc5d, 0x6749d00)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/window.go:175 +0x39\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*insertNode).BatchedNext(0xc00fd1c280, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x0, 0xab62440, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/insert.go:471 +0xcc\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*rowCountNode).startExec(0xc000426920, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x4016015, 0xc00ea65258)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_batch.go:173 +0xd0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec.func2(0x6b04308, 0x5, 0x728ae80, 0xc000426920, 0xc00fd41a98, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:496 +0x55\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal.func1(0xc00fab7ec0, 0x6b04308, 0x5, 0x728ae80, 0xc000426920)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:146 +0x5d\r\npanic(0x660d3a0, 0x7241b70)\r\n\t/usr/local/go/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*windowNode).startExec(0xc009586c60, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0xc00e019200, 0xc00fd41be8)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/window.go:171 +0x39\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec.func2(0x6b06f66, 0x6, 0x728b500, 0xc009586c60, 0xc0009d1260, 0xc00fd41bf8)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:496 +0x55\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal.func1(0xc00fab7ec0, 0x6b06f66, 0x6, 0x728b500, 0xc009586c60)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:146 +0x5d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728b500, 0xc009586c60, 0x6b06f66, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:613 +0x1a2\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visit(0xc00fab7ec0, 0x728b500, 0xc009586c60, 0x6b063f0, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:113 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728aac0, 0xc00fd1c280, 0x6b063f0, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:452 +0x47c6\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitConcrete(0xc00fab7ec0, 0x728aac0, 0xc00fd1c280)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:126 +0x81\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728ae80, 0xc000426920, 0x6b04308, 0x5)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:533 +0x4ceb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visit(0xc00fab7ec0, 0x728ae80, 0xc000426920, 0x203003, 0x4010c39)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:113 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.walkPlan(0x72891c0, 0xc00dab9770, 0x728ae80, 0xc000426920, 0x0, 0x6c75f28, 0x0, 0x0, 0x0, 0xc00dab9860, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:77 +0x1a4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec(0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x728ae80, 0xc000426920, 0xc00a046e58, 0xc00a046e50)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:499 +0x116\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Start(0xc00fcfaa00, 0x72891c0, 0xc00dab9770, 0x90c9de0, 0x660c620)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:124 +0xd9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*ProcessorBase).Run(0xc00fcfaa00, 0x72891c0, 0xc00dab9770)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go:800 +0x52\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).Run(0xc00fcf85a0, 0x72891c0, 0xc00dab9770, 0x6c75618, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:626 +0x1e9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc00081c780, 0xc00fab7ce0, 0xc009c7d710, 0xc00f2e0838, 0xc000ac58c0, 0xc000c39190, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:252 +0x8a0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc00081c780, 0x72891c0, 0xc00d97aae0, 0xc000c39190, 0xc00fab7ce0, 0xc009c7d710, 0x728ae80, 0xc000426920, 0xc000ac58c0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:839 +0x227\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0xc000c390d0, 0x2, 0xcb73dc0, 0xc0007d36b0, 0x0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:1125 +0x283\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0xc000c390d0, 0xcb73dc0, 0xc0007d36b0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:961 +0x658\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0x728e800, 0xc00ed4b2c0, 0xc00dd0ea58, 0x1d2, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:456 +0xdd0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0x728e800, 0xc00ed4b2c0, 0xc00dd0ea58, 0x1d2, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:102 +0x610\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc000c38d80, 0x7289100, 0xc00ada7a40, 0xc00004ee78, 0x5400, 0x15000, 0xc00004ef10, 0xc009bf9390, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1186 +0x21d4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc0003018c0, 0x7289100, 0xc00ada7a40, 0xc000c38d80, 0x5400, 0x15000, 0xc00004ef10, 0xc009bf9390, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:436 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync.func1(0xc009fc88d9, 0xc00a0b9c00, 0x7289100, 0xc00ada7a40, 0xc009bf9390, 0xc0003018c0, 0xc00a9c1200, 0x728ce40, 0xc00a0b9be0, 0xc0010a8600, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:580 +0x21f\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:515 +0x17b\r\n\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] Reported as error 02ee48e10bef47448abcbcfe9833ab9b\r\n*\r\ndriver: bad connection\r\nwarning: connection lost!\r\nopening new connection: all session settings will be lost\r\nwarning: error retrieving the transaction status: dial tcp [::1]:26257: connect: connection refused\r\nwarning: connection lost!\r\nopening new connection: all session settings will be lost\r\nwarning: error retrieving the database name: dial tcp [::1]:26257: connect: connection refused\r\n```",A-sql-execution,yuzefovich,"**Describe the problem**\r\n\r\nSQL panic in 19.1.5, client is restarted and node crashes.\r\n\r\nDoes not occur in 19.2.\r\n\r\n**To Reproduce**\r\n\r\nWhat did you do? Describe in your own words.\r\n```sql\r\nCREATE TABLE t1 (col1 INT8, col2 INT8, col3 INT8, col4 INT8, col5 INT8, col6 INT8, col7 INT8, col8 VARCHAR, col9 VARCHAR, col10 VARCHAR, col11 INT8, col12 INT8);\r\n\r\nCREATE TABLE t2 (\r\n\tcol1 INT8, col2 INT8, col3 INT8, col4 INT8, col5 INT8, col6 INT8, col7 INT8, col8 VARCHAR, col9 VARCHAR, col10 VARCHAR, col11 INT8, col12 INT8, col13 INT8, col14 INT8\r\n);\r\n\r\nINSERT\r\nINTO\r\n\tt1\r\nSELECT\r\n\tcol1::INT8,\r\n\tcol2::INT8,\r\n\tcol3::INT8,\r\n\tcol4::INT8,\r\n\tcol5::INT8,\r\n\tcol6::INT8,\r\n\tcol7::INT8,\r\n\tcol8::VARCHAR,\r\n\tcol9::VARCHAR,\r\n\tcol10::VARCHAR,\r\n\tcol11::INT8,\r\n\trow_number() OVER (PARTITION BY col13 ORDER BY col14 DESC)::INT8\r\nFROM\r\n\tt2;\r\n```\r\n**Expected behavior**\r\nInsert to function properly.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1.5\r\n - Client app: Cockroach SQL\r\n\r\n**Additional context**\r\nThis is related to local mode.\r\n\r\nStack trace:\r\n```\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] a SQL panic has occurred while executing ""INSERT INTO t1 SELECT col1::INT8, col2::INT8, col3::INT8, col4::INT8, col5::INT8, col6::INT8, col7::INT8, col8::VARCHAR, col9::VARCHAR, col10::VARCHAR, col11::INT8, row_number() OVER (PARTITION BY col13 ORDER BY col14 DESC)::INT8 FROM t2"": windowNode can't be run in local mode\r\n*\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: INSERT INTO _ SELECT _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::INT8, _::VARCHAR, _::VARCHAR, _::VARCHAR, _::INT8, row_number() OVER (PARTITION BY _ ORDER BY _ DESC)::INT8 FROM _\r\n\r\ngoroutine 1536 [running]:\r\nruntime/debug.Stack(0x7289100, 0xc00ada7a40, 0xc000000003)\r\n\t/usr/local/go/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x7289100, 0xc00ada7a40, 0xc000493300, 0x697c960, 0xc00d9bb140, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:226 +0xb5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc000c38d80, 0x7289100, 0xc00ada7a40, 0x660d3a0, 0x7241b80)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:715 +0x2dd\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc000c38d80, 0x7289100, 0xc00ada7a40)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:434 +0x61\r\npanic(0x660d3a0, 0x7241b80)\r\n\t/usr/local/go/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*windowNode).Next(0xc009586c60, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0xc00fd41988, 0x400cc5d, 0x6749d00)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/window.go:175 +0x39\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*insertNode).BatchedNext(0xc00fd1c280, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x0, 0xab62440, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/insert.go:471 +0xcc\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*rowCountNode).startExec(0xc000426920, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x4016015, 0xc00ea65258)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_batch.go:173 +0xd0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec.func2(0x6b04308, 0x5, 0x728ae80, 0xc000426920, 0xc00fd41a98, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:496 +0x55\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal.func1(0xc00fab7ec0, 0x6b04308, 0x5, 0x728ae80, 0xc000426920)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:146 +0x5d\r\npanic(0x660d3a0, 0x7241b70)\r\n\t/usr/local/go/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*windowNode).startExec(0xc009586c60, 0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0xc00e019200, 0xc00fd41be8)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/window.go:171 +0x39\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec.func2(0x6b06f66, 0x6, 0x728b500, 0xc009586c60, 0xc0009d1260, 0xc00fd41bf8)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:496 +0x55\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal.func1(0xc00fab7ec0, 0x6b06f66, 0x6, 0x728b500, 0xc009586c60)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:146 +0x5d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728b500, 0xc009586c60, 0x6b06f66, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:613 +0x1a2\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visit(0xc00fab7ec0, 0x728b500, 0xc009586c60, 0x6b063f0, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:113 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728aac0, 0xc00fd1c280, 0x6b063f0, 0x6)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:452 +0x47c6\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitConcrete(0xc00fab7ec0, 0x728aac0, 0xc00fd1c280)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:126 +0x81\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visitInternal(0xc00fab7ec0, 0x728ae80, 0xc000426920, 0x6b04308, 0x5)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:533 +0x4ceb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planVisitor).visit(0xc00fab7ec0, 0x728ae80, 0xc000426920, 0x203003, 0x4010c39)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:113 +0x8d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.walkPlan(0x72891c0, 0xc00dab9770, 0x728ae80, 0xc000426920, 0x0, 0x6c75f28, 0x0, 0x0, 0x0, 0xc00dab9860, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/walk.go:77 +0x1a4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.startExec(0x72891c0, 0xc00dab9770, 0xc00b7c5000, 0xc000c390d0, 0x728ae80, 0xc000426920, 0xc00a046e58, 0xc00a046e50)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:499 +0x116\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Start(0xc00fcfaa00, 0x72891c0, 0xc00dab9770, 0x90c9de0, 0x660c620)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:124 +0xd9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*ProcessorBase).Run(0xc00fcfaa00, 0x72891c0, 0xc00dab9770)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go:800 +0x52\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).Run(0xc00fcf85a0, 0x72891c0, 0xc00dab9770, 0x6c75618, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:626 +0x1e9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc00081c780, 0xc00fab7ce0, 0xc009c7d710, 0xc00f2e0838, 0xc000ac58c0, 0xc000c39190, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:252 +0x8a0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc00081c780, 0x72891c0, 0xc00d97aae0, 0xc000c39190, 0xc00fab7ce0, 0xc009c7d710, 0x728ae80, 0xc000426920, 0xc000ac58c0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:839 +0x227\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0xc000c390d0, 0x2, 0xcb73dc0, 0xc0007d36b0, 0x0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:1125 +0x283\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0xc000c390d0, 0xcb73dc0, 0xc0007d36b0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:961 +0x658\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0x728e800, 0xc00ed4b2c0, 0xc00dd0ea58, 0x1d2, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:456 +0xdd0\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc000c38d80, 0x72891c0, 0xc00d97aae0, 0x728e800, 0xc00ed4b2c0, 0xc00dd0ea58, 0x1d2, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:102 +0x610\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc000c38d80, 0x7289100, 0xc00ada7a40, 0xc00004ee78, 0x5400, 0x15000, 0xc00004ef10, 0xc009bf9390, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1186 +0x21d4\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc0003018c0, 0x7289100, 0xc00ada7a40, 0xc000c38d80, 0x5400, 0x15000, 0xc00004ef10, 0xc009bf9390, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:436 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync.func1(0xc009fc88d9, 0xc00a0b9c00, 0x7289100, 0xc00ada7a40, 0xc009bf9390, 0xc0003018c0, 0xc00a9c1200, 0x728ce40, 0xc00a0b9be0, 0xc0010a8600, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:580 +0x21f\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).processCommandsAsync\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:515 +0x17b\r\n\r\n*\r\n* ERROR: [n1,client=127.0.0.1:57582,user=root] Reported as error 02ee48e10bef47448abcbcfe9833ab9b\r\n*\r\ndriver: bad connection\r\nwarning: connection lost!\r\nopening new connection: all session settings will be lost\r\nwarning: error retrieving the transaction status: dial tcp [::1]:26257: connect: connection refused\r\nwarning: connection lost!\r\nopening new connection: all session settings will be lost\r\nwarning: error retrieving the database name: dial tcp [::1]:26257: connect: connection refused\r\n```","sql\r\nCREATE TABLE t1 (col1 INT8, col2 INT8, col3 INT8, col4 INT8, col5 INT8, col6 INT8, col7 INT8, col8 VARCHAR, col9 VARCHAR, col10 VARCHAR, col11 INT8, col12 INT8);\r\n\r\nCREATE TABLE t2 (\r\n\tcol1 INT8, col2 INT8, col3 INT8, col4 INT8, col5 INT8, col6 INT8, col7 INT8, col8 VARCHAR, col9 VARCHAR, col10 VARCHAR, col11 INT8, col12 INT8, col13 INT8, col14 INT8\r\n);\r\n\r\nINSERT\r\nINTO\r\n\tt1\r\nSELECT\r\n\tcol1::INT8,\r\n\tcol2::INT8,\r\n\tcol3::INT8,\r\n\tcol4::INT8,\r\n\tcol5::INT8,\r\n\tcol6::INT8,\r\n\tcol7::INT8,\r\n\tcol8::VARCHAR,\r\n\tcol9::VARCHAR,\r\n\tcol10::VARCHAR,\r\n\tcol11::INT8,\r\n\trow_number() OVER (PARTITION BY col13 ORDER BY col14 DESC)::INT8\r\nFROM\r\n\tt2;\r\n"
42510,"sql: flattening error objects into other errors erases telemetry data and user hintsI found this while looking for something else:\r\n\r\n\r\n\r\nBy flattening the error with `err.Error()` this constructor strips the error object of all its details, including stack traces but also **user-directed hints, links to documentation issues and telemetry data**.\r\n\r\nOther examples (non exhaustive):\r\n```\r\nexecute.go:                     return nil, pgerror.New(pgcode.WrongObjectType, err.Error())\r\nexport.go:                      return nil, pgerror.New(pgcode.InvalidParameterValue, err.Error())\r\nrowexec/hashjoiner.go:                                  err = pgerror.Wrapf(addErr, pgcode.OutOfMemory, ""while spilling: %v"", err)\r\nset_var.go:             return wrapSetVarError(""statement_timeout"", s, ""%v"", err)\r\n```\r\n\r\nThere are multiple ways to fix it:\r\n\r\n- prefer `errors.Wrap[f]` when no need to add a pg code\r\n- use `pgerror.WithCandidateCode` when need to add a pg code if there wasn't one already to start with\r\n- use `errors.Combine` or `errors.WithSecondaryError` to combine errors together\r\n\r\n\r\nI am fixing the first example at the top in #42509, but the other cases remain to be fixed.\r\n\r\ncc @cockroachdb/sql-execution ",C-bug|A-error-handling|T-sql-foundations|T-sql-queries,rafiss|e-mbrown,"I found this while looking for something else:\r\n\r\n```go\r\n func NewInvalidSchemaDefinitionError(err error) error {\r\n       return pgerror.New(pgcode.InvalidSchemaDefinition, err.Error())\r\n }\r\n```\r\n\r\nBy flattening the error with `err.Error()` this constructor strips the error object of all its details, including stack traces but also **user-directed hints, links to documentation issues and telemetry data**.\r\n\r\nOther examples (non exhaustive):\r\n```\r\nexecute.go:                     return nil, pgerror.New(pgcode.WrongObjectType, err.Error())\r\nexport.go:                      return nil, pgerror.New(pgcode.InvalidParameterValue, err.Error())\r\nrowexec/hashjoiner.go:                                  err = pgerror.Wrapf(addErr, pgcode.OutOfMemory, ""while spilling: %v"", err)\r\nset_var.go:             return wrapSetVarError(""statement_timeout"", s, ""%v"", err)\r\n```\r\n\r\nThere are multiple ways to fix it:\r\n\r\n- prefer `errors.Wrap[f]` when no need to add a pg code\r\n- use `pgerror.WithCandidateCode` when need to add a pg code if there wasn't one already to start with\r\n- use `errors.Combine` or `errors.WithSecondaryError` to combine errors together\r\n\r\n\r\nI am fixing the first example at the top in #42509, but the other cases remain to be fixed.\r\n\r\ncc @cockroachdb/sql-execution ","go\r\n func NewInvalidSchemaDefinitionError(err error) error {\r\n       return pgerror.New(pgcode.InvalidSchemaDefinition, err.Error())\r\n }\r\n"
42399,"ALTER INDEX IF EXISTS does not seem to work**Describe the problem**\r\n\r\nThe syntax `ALTER INDEX IF EXISTS` does not seem to work. It seems its implementation was forgotten in https://github.com/cockroachdb/cockroach/issues/2034\r\n\r\n**To Reproduce**\r\n\r\nRun this statement:\r\n\r\n\r\n\r\nI'm expecting it to be ignored, but I'm getting:\r\n\r\n```\r\nSQL Error [42704]: ERROR: index ""i1"" does not exist\r\n```\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.0-rc.4 (x86_64-unknown-linux-gnu, built 2019/11/05 20:40:29, go1.12.12)\r\n - Server OS: Linux in DOcker\r\n - Client app: JDBC",C-bug|A-sql-pgcompat|S-3-ux-surprise,rafiss,"**Describe the problem**\r\n\r\nThe syntax `ALTER INDEX IF EXISTS` does not seem to work. It seems its implementation was forgotten in https://github.com/cockroachdb/cockroach/issues/2034\r\n\r\n**To Reproduce**\r\n\r\nRun this statement:\r\n\r\n```sql\r\nalter index if exists i1 rename to i2;\r\n```\r\n\r\nI'm expecting it to be ignored, but I'm getting:\r\n\r\n```\r\nSQL Error [42704]: ERROR: index ""i1"" does not exist\r\n```\r\n\r\n**Environment:**\r\n - CockroachDB version: CockroachDB CCL v19.2.0-rc.4 (x86_64-unknown-linux-gnu, built 2019/11/05 20:40:29, go1.12.12)\r\n - Server OS: Linux in DOcker\r\n - Client app: JDBC",sql\r\nalter index if exists i1 rename to i2;\r\n
41924,"sql: the result column name for json_array_elements should be ""value""Found by @drewdeally \r\n\r\n\r\n\r\nerror:\r\n```\r\npq: column ""value"" does not exist\r\n```\r\n\r\nThis is because the result column for json_array_elements is not called ""value"" by default, unlike in pg.\r\n\r\nWorkaround is to use `jsonb_array_elements(jdata->'o') as value`\r\n",C-bug|E-easy|A-sql-pgcompat|good first issue|A-sql-optimizer|A-sql-builtins,rytaft,"Found by @drewdeally \r\n\r\n```sql\r\n with a_table(jdata) as (\r\nvalues (\r\n    '{\r\n        ""uid"":14105529,\r\n        ""o"":[\r\n            {""mid"":6551,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0},\r\n            {""mid"":7552,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0},\r\n            {""mid"":7553,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0}\r\n        ] }'::jsonb\r\n    )\r\n)\r\nselect jdata->'uid' as uid, value\r\nfrom a_table, jsonb_array_elements(jdata->'o')\r\nwhere (value->>'mid')::int > 7000;\r\n```\r\n\r\nerror:\r\n```\r\npq: column ""value"" does not exist\r\n```\r\n\r\nThis is because the result column for json_array_elements is not called ""value"" by default, unlike in pg.\r\n\r\nWorkaround is to use `jsonb_array_elements(jdata->'o') as value`\r\n","sql\r\n with a_table(jdata) as (\r\nvalues (\r\n    '{\r\n        ""uid"":14105529,\r\n        ""o"":[\r\n            {""mid"":6551,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0},\r\n            {""mid"":7552,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0},\r\n            {""mid"":7553,""ac"":1913,""ip"":""144.36.233.44"",""adw"":5,""at"":133000,""ad"":151015,""aid"":0}\r\n        ] }'::jsonb\r\n    )\r\n)\r\nselect jdata->'uid' as uid, value\r\nfrom a_table, jsonb_array_elements(jdata->'o')\r\nwhere (value->>'mid')::int > 7000;\r\n"
41387,"cli: cockroach demo doesn't return with globalNeither this:\r\n\r\nor this work:\r\n```\r\n ./cockroach demo \u2014nodes 9 \u2014global\r\nError: unknown command ""\u2014nodes"" for ""cockroach demo""\r\nFailed running ""demo""\r\n```",C-bug|S-3-ux-surprise|A-demo,rohany,"Neither this:\r\n```SQL\r\n./cockroach demo \u2014-nodes 9 \u2014global\r\nError: unknown command ""\u2014-nodes"" for ""cockroach demo""\r\nFailed running ""demo""\r\n```\r\nor this work:\r\n```\r\n ./cockroach demo \u2014nodes 9 \u2014global\r\nError: unknown command ""\u2014nodes"" for ""cockroach demo""\r\nFailed running ""demo""\r\n```","SQL\r\n./cockroach demo \u2014-nodes 9 \u2014global\r\nError: unknown command ""\u2014-nodes"" for ""cockroach demo""\r\nFailed running ""demo""\r\n"
41078,"sql: validity of sub-queries in special query contextstldr: this issue spells out the semantic rules around the `[...]` syntax, CTEs, mutations and hoisting of subqueries. **Comments welcome**\r\n\r\nTopics covered:\r\n\r\n- equivalence between `[...]` and WITH\r\n- CTE hoisting\r\n- mutations in CTEs and the pg/SQL restriction\r\n- the case of EXPLAIN\r\n\r\n### Intro\r\n\r\nCockroachDB's dialect allows a few _syntactic_ constructs that are not possible in PostgreSQL. The question then arises what is their _semantics_, if any. \r\n\r\nFor example, we like things like `select * from [explain ...] where field=...`, which postgres does not support, and that _seems_ at first to be well defined.  But when what happens with `select * from [explain select from [explain ...]]`? \r\n\r\n(summary from below: easy peasy, just compute the `explain` output depth first post-order)\r\n\r\nWe would also like things like `table [update ... returning ...] union all table [update ... returning ...]` but it looks suspicious: beyond the apparent clarity of this syntax, is it valid to optimize the _execution_ of this query to run the two updates concurrently?\r\n\r\nThen what about `select * from [insert into x table [insert into y values... returning ...] returning...]`?\r\n\r\n(summary from below: _it depends_)\r\n\r\n### Terminology\r\n\r\n- common table expression (_CTE_): the `xxx` part in `WITH a AS (xxx)`\r\n- CTE _alias_: the `xxx` part in `WITH xxx AS (SELECT ...)`\r\n- _scalar subquery_: a valid query enclosed in `(...)` in a scalar position, e.g. `xxx` in `SELECT 1 IN (xxx)`\r\n- _relational subquery_: a valid query enclosed in `(...)` in a relational position, e.g. `xxx` in `SELECT 1 FROM (xxx)`\r\n- _statement source_: a statement enclosed in `[...]` e.g. `SELECT 1 FROM [xxx]`\r\n- _correlated subquery_: a subquery that references columns defined out of it. For example, in `SELECT (SELECT x+y) FROM xy` where `x` and `y` are columns of `xy`, the subquery `SELECT x+y` is correlated.\r\n\r\n### Clarification on crdb's statement sources\r\n\r\nThe ""statement source"" syntax `[...]` is intended to be equivalent to relational subqueries and CTEs. \r\n\r\nIn fact it exists in CockroachDB mainly for historical reasons, as it was introduced at a time when CockroachDB did not support CTEs.\r\n\r\nFor technical reasons specific to how the SQL parser works, it's not possible to allow all the statement types that may be interesting to use as relational expression, inside `(...)`. However, these statement types _can_ be included syntactically as a CTE, so once the CTE syntax became available there was no unique reason to keep `[...]` other than, perhaps, conciseness and backward-compatibility.\r\n\r\n### Mapping statement sources to CTE\r\n\r\nIn fact, it's _always_ possible to perform a text-level substitution of all uses of `[...]` and replace it with `WITH ...`, for example:\r\n\r\n| Before | After | Notes |\r\n|---|---|---|\r\n| `table [xxx]` | `with a as (xxx) table a` | define CTE just above, replace statement source by CTE alias\r\n| `select * from [xxx]` | `with a as (xxx) select * from a` | ditto |\r\n| `insert into t table [xxx]` | `insert into t with a as (xxx) table a` | ditto, note we're not defining the CTE completely at the top yet. we'll discuss top-level CTEs later. |\r\n| `table [select * from [xxx]]` | with a as (xxx), b as (select * from a) table b` | transform statement source to CTE in the deeper level first, then the outer level |\r\n\r\nIn other words **the semantics of a statement source are those of a CTE defined at the same level**, there is nothing special with them that a CTE cannot do.\r\n\r\n### CTEs are actually more powerful\r\n\r\nIn fact, CTEs do something _more_ which statements sources cannot: it's possible to refer to a CTE alias more than once. For example I can write:\r\n\r\n`with a as (select 1) select * from a, a`\r\n\r\nand the results of `(select 1)` will be cross-joined with themselves.\r\n\r\nTo the naive reader, this may look&feel like we're just ""pasting"" the CTE definition to every place where the CTE alias is listed, so that it looks possible to reverse map a CTE to a statement source. But that's incorrect, because of this example:\r\n\r\n`with a as (insert into t values(1) returning *) select * from a, a`\r\n\r\nIn this case, PostgreSQL's semantics (and the SQL standard, actually) _guarantee_ that the INSERT is only performed once. \r\n\r\nThis means that a CTE also conceptually ""stores"" the intermediate results of its query somewhere and reuses the stored results every time the CTE alias is used. It does not re-compute the query every time. This is not possible to achieve with statement sources only.\r\n\r\n### Statement sources are not intended to be more powerful\r\n\r\nThe `[...]` syntax is a CockroachDB extension and we do not have much evidence it's used in the wild. We are attached to it because it makes our SQL tests much shorter and more convenient to it, but we don't have strong requirements on them other than ""don't make them do less than they could do in CockroachDB 2.0"".\r\n\r\nIn particular **the statement source syntax was introduced when CockroachDB did not support correlated subqueries**. This did not work in 2.0, PostgreSQL is not providing it, and nobody is asking for it. Therefore we do not have any requirement that statement sources work when the query inside is correlated. Remember this as we'll use this fact below.\r\n\r\n### Do not pay too much attention to statement sources\r\n\r\nBecause statement source syntax can always map to CTEs, we may as well consider that statement sources are transformed to a CTE *at the same scope level* very early during query transformation, before semantic analysis. \r\n\r\nThis enables reasoning about the semantics of SQL without taking statement source syntax into consideration. We'll simply focus on CTEs from now on.\r\n\r\n### CTE hoisting\r\n\r\nConsider the two following SQL queries:\r\n\r\n\r\n\r\n\r\nand\r\n\r\n\r\n\r\n\r\nWhat's the difference?  In the first case the CTE is _nested_, in the latter case the CTE is at the _top level_ of the query.\r\n\r\nFor various reasons related to performance, it is desirable during query optimization to _hoist_ nested CTEs from ""inside"" a query to the toplevel, or close to it.\r\n\r\nThe question then arises, when is this transformation valid?\r\n\r\nIn the example above, the two queries are equivalent and the transformation is possible. However, in the following case, not so much:\r\n\r\n\r\n\r\n(where `x` is a column from `t`.\r\n\r\nIn this example, it's not possible to transform this to:\r\n\r\n\r\nbecause the scope of `t` is limited to `select...` and therefore the `with ...` becomes invalid.\r\n\r\nIn general, hoisting a *correlated CTE* is problematic. To address this a query optimizer has two tools:\r\n\r\n1. as much as possible, a nested CTE can be ""inlined"" in the relational query tree, and the need for the WITH... aliasing disappears entirely\r\n2. in _theoretical_ cases where a CTE cannot be hoisted, it may be necessary to define a `LET ... IN ...` relational expression node that preserves the CTE constructs in the final query plan. \r\n\r\nHowever, SQL databases really do not like case 2 above, and we'd like to be in a position to _always_ hoist nested CTEs, or de-correlate them to inline them in the surrounding context.\r\n\r\n### The problem with mutations\r\n\r\n*In general, mutations cannot always be decorrelated when they are nested in a larger query context.*  (for various technical and theoretical reasons).\r\n\r\nSo what do we have here:\r\n1. it's possible to have a SQL mutation inside a CTE (e.g. `with a as (update...)`)\r\n2. we really want to decorrelated CTEs or hoist them to the top level, and not get stuck with nested CTEs in the final plan\r\n3. not all mutations can be decorrelated\r\n\r\n*what happens if there's a correlated + nested + mutation CTE in a query?**\r\n\r\nThis looks like we'd be stuck?\r\n\r\n### PostgreSQL to the rescue\r\n\r\nWell.... PostgreSQL and the SQL standard rescue us on this point:\r\n\r\n**These SQL dialects which we aim to support only support mutations in top-level CTEs**. \r\n\r\nThey don't even have to mention ""decorrelated"" - they use ""top level"" which is even more powerful. It's not possible to have a correlated query at the top level.\r\n\r\nIn other words, if a client asks them to run SQL that contains UPDATE etc in a CTE that's nested, they tell them ""the SQL syntax is not correct"".\r\n\r\nThis _semantic limitation_ that our upstream techs are forcing on their users is good for us, because we can keep it and it makes our lives easier.\r\n\r\nWith this limitation in place, we can simply reject a query like this:\r\n\r\n`select * from (with a as (update .... returning))` with an error that sounds like ""cannot use UPDATE in a CTE that's not at the top level"" or something like that.\r\n\r\n### Mutations in statement sources\r\n\r\nTo the astute reader the sections above land into a contradictory, unsound spot:\r\n\r\n- at the beginning we say ""It's desirable to support `select * from [insert into ... returning ...]`\r\n- then we say ""It's always possible to transform statement sources to a CTE at the same level"" so that query would be `select * from (with a as (insert ...) table a)`\r\n- then we say ""mutations in nested CTEs should be disallowed"" which means this desirable query becomes invalid?\r\n\r\nThat's no good?!\r\n\r\nWell to square the circle we can reliably fix this by replacing the rule ""no mutations in nested CTEs"" to either:\r\n\r\nA. ""no mutations in CTEs that cannot be hoisted + the remaining mutations are always hoisted"", or\r\nB. ""no mutations in correlated CTEs + the remaining mutations are always hoisted""\r\n\r\n(""no mutations"" = ""produce a planning error if a client tries that"")\r\n\r\nEither of these two rules will guarantee that a _nested_ mutation in a statement source syntax or CTE definition can become hoisted at the top level during query transformation.\r\n\r\nBoth of these two rules are more generous to clients than the base rule from PostgreSQL / SQL standard, so we're not losing anything regarding expressiveness by adopting them.\r\n\r\nThe question then becomes, which of the two variants to choose.\r\n\r\nI (@knz) personally recommend going for variant B. \r\n\r\nThe problem with variant A is that it opens the conversation of _which_ nested queries containing mutations can be hoisted (or de-correlated and then hoisted), and how. This is a super hard discussion and we'd be trying to solve a problem that nobody else in the pg world is otherwise trying to solve. \r\n\r\nVariant B of the rule is much simpler to implement as this determination can be done very early during query analysis, during scope analysis to be specific (optbuilder in CockroachDB). It's also much simpler to explain and teach, and to for users to get an intuition about.\r\n\r\n### Example\r\n\r\nConsider:\r\n\r\n\r\n\r\nThis transforms to\r\n```\r\nselect * from (with a as (insert into t values(x) returning *)) from xy\r\n```\r\n\r\nAnd then we see that `insert into t values (x)` is correlated and so by the rule above the query planning fails with ""INSERT not allowed in correlated CTE"".\r\n\r\nAnother example:\r\n\r\n\r\n\r\nThis transforms to\r\n\r\n\r\nHere the mutations are not correlated, and so all is well. CTE hoisting will pull them at the top level safely:\r\n\r\n\r\n\r\n### Multiple mutations in CTEs\r\n\r\nWhat we have so far is that **all nested mutations eventually make their way to a top level CTE definition** via tree transforms.\r\n\r\nIn fact it's valid pg SQL to have multiple mutations side by side as CTEs.\r\n\r\nIs it anywhere valid to *execute* two CTE mutations that do not depend on each other, concurrently?\r\n\r\nThe short answer is ""it depends"".\r\n\r\nThe long answer is, PostgreSQL does not do this. PostgreSQL executes its top level CTEs one after another, sequentially, so that multiple mutations never execute concurrently. So we can't just at pg and copy its concurrency rules. It doesn't have any.\r\n\r\nIf we make  up our own rules we'd need to be careful about two things:\r\n\r\n- CTE semantics say ""a mutation only happens once"" (referential equivalence)\r\n- if a 2nd mutation refers to the same table as the first, it should either see the data as if the other mutation had completely finished, or has not yet started (atomicity)\r\n\r\nSo I'd say we're free to execute mutations concurrently if we're able to prove during planning that they have no overlapping k/v ranges.\r\n\r\n### What about EXPLAIN(PLAN)\r\n\r\nThe SQL syntax `EXPLAIN(PLAN) xxx` means ""show the query plan for the SQL syntax `xxx`.\r\n\r\nThe result is a relational expression for a relation with 3 columns (tree, field, value) which contains a textual description of the query plan. **The query is not executed**.\r\n\r\nSince that result is itself relational it's possible (and desirable) to have it in a sub-query, for example `select * from [explain ...]` or `with a as (explain ...) table a`.\r\n\r\nThis raises a new question: what happens if we apply EXPLAIN to a mutation, then also place that EXPLAIN in a sub-query? for example `select * from [explain update ...]`\r\n\r\nThe answer to this can be constructed from the following basic principle: **EXPLAIN defines its own, fresh, top level scope for query planning**.\r\n\r\nFrom that principle alone we can derive the following rules:\r\n- the syntax passed after EXPLAIN must be valid if it was pasted at the top level of a SQL prompt\r\n- therefore, it does not permit correlated subqueries\r\n- the EXPLAIN itself cannot be correlated with its surroundings\r\n- during planning an EXPLAIN, the highest level at which _anything_ underneath can be hoisted, is the EXPLAIN itself\r\n- the EXPLAIN itself is uncorrelated and thus can be hoisted at the top level of the surrounding query\r\n- all the planning needed to produce the EXPLAIN results can be processed before the start of execution of the surrounding SQL query.\r\n\r\nSo the rules above about semantic constraints and hoisting continue to apply, unchanged:\r\n\r\n- ""no correlated mutation"" -> nothing special\r\n- ""mutation hoisted to top level"" -> ""mutation hoisted to _current_ top level"", which is going to be the nearest EXPLAIN if any\r\n- ditto for hoisting other things that we also want to hoist: _uncorrelated_ subqueries are typically also hoisted to a top level, _and_ we want them to be ""retained"" under EXPLAIN, so they're going to be hoisted to EXPLAIN and not above that.\r\n- if there are multiple nested EXPLAINs, for example `explain explain select 1`, we apply the planning recursively, using depth-first, post-order processing.\r\n\r\nThen we have:\r\n\r\n\r\n\r\nbecomes\r\n\r\n\r\nand \r\n\r\n\r\nbecomes\r\n```\r\nwith a as (explain with b as (update ...) select * from b)\r\nselect * from a\r\n```\r\n\r\n### What about EXPLAIN(ANALYZE)\r\n\r\nThat's a tough(er) cookie. In contrast to EXPLAIN(PLAN), EXPLAIN(ANALYSE) needs to run its query parameter.\r\n\r\nI don't have a clear opinion of this, but I think we'd get some mileage by sticking to the following rules:\r\n\r\n- EXPLAIN(ANALYZE) must be plannable like EXPLAIN(PLAN), i.e. no uncorrelated query underneath\r\n- guarantee semantically that the query it's given is only executed once and the results are stored.\r\n\r\nThis way we can always hoist EXPLAIN(ANALYZE), run the explains sequentially prior to the execution of the main query, and deliver the results of EXPLAIN via a spool node or temp table.",C-investigation|A-sql-optimizer|meta-issue|A-docs,justinj|knz|rytaft|RaduBerinde|andy-kimball,"tldr: this issue spells out the semantic rules around the `[...]` syntax, CTEs, mutations and hoisting of subqueries. **Comments welcome**\r\n\r\nTopics covered:\r\n\r\n- equivalence between `[...]` and WITH\r\n- CTE hoisting\r\n- mutations in CTEs and the pg/SQL restriction\r\n- the case of EXPLAIN\r\n\r\n### Intro\r\n\r\nCockroachDB's dialect allows a few _syntactic_ constructs that are not possible in PostgreSQL. The question then arises what is their _semantics_, if any. \r\n\r\nFor example, we like things like `select * from [explain ...] where field=...`, which postgres does not support, and that _seems_ at first to be well defined.  But when what happens with `select * from [explain select from [explain ...]]`? \r\n\r\n(summary from below: easy peasy, just compute the `explain` output depth first post-order)\r\n\r\nWe would also like things like `table [update ... returning ...] union all table [update ... returning ...]` but it looks suspicious: beyond the apparent clarity of this syntax, is it valid to optimize the _execution_ of this query to run the two updates concurrently?\r\n\r\nThen what about `select * from [insert into x table [insert into y values... returning ...] returning...]`?\r\n\r\n(summary from below: _it depends_)\r\n\r\n### Terminology\r\n\r\n- common table expression (_CTE_): the `xxx` part in `WITH a AS (xxx)`\r\n- CTE _alias_: the `xxx` part in `WITH xxx AS (SELECT ...)`\r\n- _scalar subquery_: a valid query enclosed in `(...)` in a scalar position, e.g. `xxx` in `SELECT 1 IN (xxx)`\r\n- _relational subquery_: a valid query enclosed in `(...)` in a relational position, e.g. `xxx` in `SELECT 1 FROM (xxx)`\r\n- _statement source_: a statement enclosed in `[...]` e.g. `SELECT 1 FROM [xxx]`\r\n- _correlated subquery_: a subquery that references columns defined out of it. For example, in `SELECT (SELECT x+y) FROM xy` where `x` and `y` are columns of `xy`, the subquery `SELECT x+y` is correlated.\r\n\r\n### Clarification on crdb's statement sources\r\n\r\nThe ""statement source"" syntax `[...]` is intended to be equivalent to relational subqueries and CTEs. \r\n\r\nIn fact it exists in CockroachDB mainly for historical reasons, as it was introduced at a time when CockroachDB did not support CTEs.\r\n\r\nFor technical reasons specific to how the SQL parser works, it's not possible to allow all the statement types that may be interesting to use as relational expression, inside `(...)`. However, these statement types _can_ be included syntactically as a CTE, so once the CTE syntax became available there was no unique reason to keep `[...]` other than, perhaps, conciseness and backward-compatibility.\r\n\r\n### Mapping statement sources to CTE\r\n\r\nIn fact, it's _always_ possible to perform a text-level substitution of all uses of `[...]` and replace it with `WITH ...`, for example:\r\n\r\n| Before | After | Notes |\r\n|---|---|---|\r\n| `table [xxx]` | `with a as (xxx) table a` | define CTE just above, replace statement source by CTE alias\r\n| `select * from [xxx]` | `with a as (xxx) select * from a` | ditto |\r\n| `insert into t table [xxx]` | `insert into t with a as (xxx) table a` | ditto, note we're not defining the CTE completely at the top yet. we'll discuss top-level CTEs later. |\r\n| `table [select * from [xxx]]` | with a as (xxx), b as (select * from a) table b` | transform statement source to CTE in the deeper level first, then the outer level |\r\n\r\nIn other words **the semantics of a statement source are those of a CTE defined at the same level**, there is nothing special with them that a CTE cannot do.\r\n\r\n### CTEs are actually more powerful\r\n\r\nIn fact, CTEs do something _more_ which statements sources cannot: it's possible to refer to a CTE alias more than once. For example I can write:\r\n\r\n`with a as (select 1) select * from a, a`\r\n\r\nand the results of `(select 1)` will be cross-joined with themselves.\r\n\r\nTo the naive reader, this may look&feel like we're just ""pasting"" the CTE definition to every place where the CTE alias is listed, so that it looks possible to reverse map a CTE to a statement source. But that's incorrect, because of this example:\r\n\r\n`with a as (insert into t values(1) returning *) select * from a, a`\r\n\r\nIn this case, PostgreSQL's semantics (and the SQL standard, actually) _guarantee_ that the INSERT is only performed once. \r\n\r\nThis means that a CTE also conceptually ""stores"" the intermediate results of its query somewhere and reuses the stored results every time the CTE alias is used. It does not re-compute the query every time. This is not possible to achieve with statement sources only.\r\n\r\n### Statement sources are not intended to be more powerful\r\n\r\nThe `[...]` syntax is a CockroachDB extension and we do not have much evidence it's used in the wild. We are attached to it because it makes our SQL tests much shorter and more convenient to it, but we don't have strong requirements on them other than ""don't make them do less than they could do in CockroachDB 2.0"".\r\n\r\nIn particular **the statement source syntax was introduced when CockroachDB did not support correlated subqueries**. This did not work in 2.0, PostgreSQL is not providing it, and nobody is asking for it. Therefore we do not have any requirement that statement sources work when the query inside is correlated. Remember this as we'll use this fact below.\r\n\r\n### Do not pay too much attention to statement sources\r\n\r\nBecause statement source syntax can always map to CTEs, we may as well consider that statement sources are transformed to a CTE *at the same scope level* very early during query transformation, before semantic analysis. \r\n\r\nThis enables reasoning about the semantics of SQL without taking statement source syntax into consideration. We'll simply focus on CTEs from now on.\r\n\r\n### CTE hoisting\r\n\r\nConsider the two following SQL queries:\r\n\r\n\r\n```sql\r\nselect x+1 from (with a as (select 1 as x) table a)\r\n```\r\n\r\nand\r\n\r\n```sql\r\nwith a as (select 1 as x) select x+1 from table a\r\n```\r\n\r\n\r\nWhat's the difference?  In the first case the CTE is _nested_, in the latter case the CTE is at the _top level_ of the query.\r\n\r\nFor various reasons related to performance, it is desirable during query optimization to _hoist_ nested CTEs from ""inside"" a query to the toplevel, or close to it.\r\n\r\nThe question then arises, when is this transformation valid?\r\n\r\nIn the example above, the two queries are equivalent and the transformation is possible. However, in the following case, not so much:\r\n\r\n```sql\r\nselect 1 = (with a as (select x) table a) from t\r\n```\r\n\r\n(where `x` is a column from `t`.\r\n\r\nIn this example, it's not possible to transform this to:\r\n```sql\r\nwith a as (select x)\r\nselect 1 = (table a) from t\r\n```\r\n\r\nbecause the scope of `t` is limited to `select...` and therefore the `with ...` becomes invalid.\r\n\r\nIn general, hoisting a *correlated CTE* is problematic. To address this a query optimizer has two tools:\r\n\r\n1. as much as possible, a nested CTE can be ""inlined"" in the relational query tree, and the need for the WITH... aliasing disappears entirely\r\n2. in _theoretical_ cases where a CTE cannot be hoisted, it may be necessary to define a `LET ... IN ...` relational expression node that preserves the CTE constructs in the final query plan. \r\n\r\nHowever, SQL databases really do not like case 2 above, and we'd like to be in a position to _always_ hoist nested CTEs, or de-correlate them to inline them in the surrounding context.\r\n\r\n### The problem with mutations\r\n\r\n*In general, mutations cannot always be decorrelated when they are nested in a larger query context.*  (for various technical and theoretical reasons).\r\n\r\nSo what do we have here:\r\n1. it's possible to have a SQL mutation inside a CTE (e.g. `with a as (update...)`)\r\n2. we really want to decorrelated CTEs or hoist them to the top level, and not get stuck with nested CTEs in the final plan\r\n3. not all mutations can be decorrelated\r\n\r\n*what happens if there's a correlated + nested + mutation CTE in a query?**\r\n\r\nThis looks like we'd be stuck?\r\n\r\n### PostgreSQL to the rescue\r\n\r\nWell.... PostgreSQL and the SQL standard rescue us on this point:\r\n\r\n**These SQL dialects which we aim to support only support mutations in top-level CTEs**. \r\n\r\nThey don't even have to mention ""decorrelated"" - they use ""top level"" which is even more powerful. It's not possible to have a correlated query at the top level.\r\n\r\nIn other words, if a client asks them to run SQL that contains UPDATE etc in a CTE that's nested, they tell them ""the SQL syntax is not correct"".\r\n\r\nThis _semantic limitation_ that our upstream techs are forcing on their users is good for us, because we can keep it and it makes our lives easier.\r\n\r\nWith this limitation in place, we can simply reject a query like this:\r\n\r\n`select * from (with a as (update .... returning))` with an error that sounds like ""cannot use UPDATE in a CTE that's not at the top level"" or something like that.\r\n\r\n### Mutations in statement sources\r\n\r\nTo the astute reader the sections above land into a contradictory, unsound spot:\r\n\r\n- at the beginning we say ""It's desirable to support `select * from [insert into ... returning ...]`\r\n- then we say ""It's always possible to transform statement sources to a CTE at the same level"" so that query would be `select * from (with a as (insert ...) table a)`\r\n- then we say ""mutations in nested CTEs should be disallowed"" which means this desirable query becomes invalid?\r\n\r\nThat's no good?!\r\n\r\nWell to square the circle we can reliably fix this by replacing the rule ""no mutations in nested CTEs"" to either:\r\n\r\nA. ""no mutations in CTEs that cannot be hoisted + the remaining mutations are always hoisted"", or\r\nB. ""no mutations in correlated CTEs + the remaining mutations are always hoisted""\r\n\r\n(""no mutations"" = ""produce a planning error if a client tries that"")\r\n\r\nEither of these two rules will guarantee that a _nested_ mutation in a statement source syntax or CTE definition can become hoisted at the top level during query transformation.\r\n\r\nBoth of these two rules are more generous to clients than the base rule from PostgreSQL / SQL standard, so we're not losing anything regarding expressiveness by adopting them.\r\n\r\nThe question then becomes, which of the two variants to choose.\r\n\r\nI (@knz) personally recommend going for variant B. \r\n\r\nThe problem with variant A is that it opens the conversation of _which_ nested queries containing mutations can be hoisted (or de-correlated and then hoisted), and how. This is a super hard discussion and we'd be trying to solve a problem that nobody else in the pg world is otherwise trying to solve. \r\n\r\nVariant B of the rule is much simpler to implement as this determination can be done very early during query analysis, during scope analysis to be specific (optbuilder in CockroachDB). It's also much simpler to explain and teach, and to for users to get an intuition about.\r\n\r\n### Example\r\n\r\nConsider:\r\n\r\n```sql\r\nselect * from [insert into t values(x) returning *] from xy\r\n```\r\n\r\nThis transforms to\r\n```\r\nselect * from (with a as (insert into t values(x) returning *)) from xy\r\n```\r\n\r\nAnd then we see that `insert into t values (x)` is correlated and so by the rule above the query planning fails with ""INSERT not allowed in correlated CTE"".\r\n\r\nAnother example:\r\n\r\n```sql\r\nselect * from [insert into t [insert into u values (1) returning *] returning *]\r\n```\r\n\r\nThis transforms to\r\n```sql\r\nselect * from (\r\n  with a as (insert into u values (1) returning *),\r\n  b as (insert into t table a returning *)\r\n  table b\r\n)\r\n```\r\n\r\nHere the mutations are not correlated, and so all is well. CTE hoisting will pull them at the top level safely:\r\n\r\n```sql\r\nwith a as (insert into u values (1) returning *),\r\nb as (insert into t table a returning *)\r\nselect * from b\r\n```\r\n\r\n### Multiple mutations in CTEs\r\n\r\nWhat we have so far is that **all nested mutations eventually make their way to a top level CTE definition** via tree transforms.\r\n\r\nIn fact it's valid pg SQL to have multiple mutations side by side as CTEs.\r\n\r\nIs it anywhere valid to *execute* two CTE mutations that do not depend on each other, concurrently?\r\n\r\nThe short answer is ""it depends"".\r\n\r\nThe long answer is, PostgreSQL does not do this. PostgreSQL executes its top level CTEs one after another, sequentially, so that multiple mutations never execute concurrently. So we can't just at pg and copy its concurrency rules. It doesn't have any.\r\n\r\nIf we make  up our own rules we'd need to be careful about two things:\r\n\r\n- CTE semantics say ""a mutation only happens once"" (referential equivalence)\r\n- if a 2nd mutation refers to the same table as the first, it should either see the data as if the other mutation had completely finished, or has not yet started (atomicity)\r\n\r\nSo I'd say we're free to execute mutations concurrently if we're able to prove during planning that they have no overlapping k/v ranges.\r\n\r\n### What about EXPLAIN(PLAN)\r\n\r\nThe SQL syntax `EXPLAIN(PLAN) xxx` means ""show the query plan for the SQL syntax `xxx`.\r\n\r\nThe result is a relational expression for a relation with 3 columns (tree, field, value) which contains a textual description of the query plan. **The query is not executed**.\r\n\r\nSince that result is itself relational it's possible (and desirable) to have it in a sub-query, for example `select * from [explain ...]` or `with a as (explain ...) table a`.\r\n\r\nThis raises a new question: what happens if we apply EXPLAIN to a mutation, then also place that EXPLAIN in a sub-query? for example `select * from [explain update ...]`\r\n\r\nThe answer to this can be constructed from the following basic principle: **EXPLAIN defines its own, fresh, top level scope for query planning**.\r\n\r\nFrom that principle alone we can derive the following rules:\r\n- the syntax passed after EXPLAIN must be valid if it was pasted at the top level of a SQL prompt\r\n- therefore, it does not permit correlated subqueries\r\n- the EXPLAIN itself cannot be correlated with its surroundings\r\n- during planning an EXPLAIN, the highest level at which _anything_ underneath can be hoisted, is the EXPLAIN itself\r\n- the EXPLAIN itself is uncorrelated and thus can be hoisted at the top level of the surrounding query\r\n- all the planning needed to produce the EXPLAIN results can be processed before the start of execution of the surrounding SQL query.\r\n\r\nSo the rules above about semantic constraints and hoisting continue to apply, unchanged:\r\n\r\n- ""no correlated mutation"" -> nothing special\r\n- ""mutation hoisted to top level"" -> ""mutation hoisted to _current_ top level"", which is going to be the nearest EXPLAIN if any\r\n- ditto for hoisting other things that we also want to hoist: _uncorrelated_ subqueries are typically also hoisted to a top level, _and_ we want them to be ""retained"" under EXPLAIN, so they're going to be hoisted to EXPLAIN and not above that.\r\n- if there are multiple nested EXPLAINs, for example `explain explain select 1`, we apply the planning recursively, using depth-first, post-order processing.\r\n\r\nThen we have:\r\n\r\n```sql\r\nexplain select * from [update ...]\r\n```\r\n\r\nbecomes\r\n```sql\r\nexplain with a as (update...) select * from a\r\n```\r\n\r\nand \r\n```sql\r\nselect * from [explain select * from [update ...]]\r\n```\r\n\r\nbecomes\r\n```\r\nwith a as (explain with b as (update ...) select * from b)\r\nselect * from a\r\n```\r\n\r\n### What about EXPLAIN(ANALYZE)\r\n\r\nThat's a tough(er) cookie. In contrast to EXPLAIN(PLAN), EXPLAIN(ANALYSE) needs to run its query parameter.\r\n\r\nI don't have a clear opinion of this, but I think we'd get some mileage by sticking to the following rules:\r\n\r\n- EXPLAIN(ANALYZE) must be plannable like EXPLAIN(PLAN), i.e. no uncorrelated query underneath\r\n- guarantee semantically that the query it's given is only executed once and the results are stored.\r\n\r\nThis way we can always hoist EXPLAIN(ANALYZE), run the explains sequentially prior to the execution of the main query, and deliver the results of EXPLAIN via a spool node or temp table.",sql\r\nselect x+1 from (with a as (select 1 as x) table a)\r\n
40693,"sql: users with select permissions can alter tables and partitions with zone configurations In our docs, we specify that \r\n\r\nWe don't appear to follow that. Here is an example of creating users and databases:\r\n\r\nAndy is able to alter tables and partitions:\r\n\r\nProof of no create table privileges:\r\n\r\n",C-bug|A-sql-privileges|S-2,solongordon,"In our docs, we specify that ```Currently, only members of the admin role can configure replication zones. By default, the root user belongs to the admin role.```\r\n\r\nWe don't appear to follow that. Here is an example of creating users and databases:\r\n```SQL\r\ncreate user andy;\r\ncreate database foo;\r\nuse foo;\r\ncreate table t (a int);\r\nALTER TABLE t PARTITION BY LIST (rowid) ( PARTITION new_york VALUES IN ('1'), PARTITION chicago VALUES IN ('2'), PARTITION seattle VALUES IN ('3') );\r\ngrant select on table t to andy;\r\n```\r\nAndy is able to alter tables and partitions:\r\n```SQL\r\n./cockroach sql --insecure --user=andy\r\nuse foo;\r\nALTER TABLE t CONFIGURE ZONE USING gc.ttlseconds = 600;\r\nALTER PARTITION seattle OF TABLE t CONFIGURE ZONE USING constraints='[]';\r\n```\r\nProof of no create table privileges:\r\n```SQL\r\ncreate table t2 (a int);\r\npq: user andy does not have CREATE privilege on database foo\r\n```\r\n","Currently, only members of the admin role can configure replication zones. By default, the root user belongs to the admin role."
40544,sql: expand wildcard (@*) to SHOW RANGES @jseldess suggested we extend the wildcard syntax to SHOW RANGES:\r\n\r\n>It would be great to be able to run a single show ranges statement for all indexes of a table:\r\n\r\n>or for all partitions of a table:\r\n,C-enhancement|A-partitioning|A-sql-execution,rohany,@jseldess suggested we extend the wildcard syntax to SHOW RANGES:\r\n\r\n>It would be great to be able to run a single show ranges statement for all indexes of a table:\r\n```SQL\r\nSHOW RANGES FROM INDEX movr.vehicles@*\r\n```\r\n>or for all partitions of a table:\r\n```SQL\r\nSELECT * FROM [SHOW RANGES FROM INDEX movr.vehicles@*] WHERE  NOT LIKE '%Prefix%';\r\n```,SQL\r\nSHOW RANGES FROM INDEX movr.vehicles@*\r\n
40427,"cli: demo --with-load doesn't work with -MovRThis does not work:\r\n\r\nIt returns this text:\r\n```\r\nError: unknown flag: --with-load\r\nFailed running ""demo movr""\r\n```\r\nThis does work:\r\n",C-bug|S-3-ux-surprise|A-partitioning|A-demo,rohany,"This does not work:\r\n```SQL\r\n./cockroach demo movr --nodes 9 --with-load --demo-locality=region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=useast,az=1:region=us-central,az=1:region=us-west,az=2\r\n```\r\nIt returns this text:\r\n```\r\nError: unknown flag: --with-load\r\nFailed running ""demo movr""\r\n```\r\nThis does work:\r\n```SQL\r\n./cockroach demo --nodes 9 --with-load --demo-locality=region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=useast,az=1:region=us-central,az=1:region=us-west,az=2\r\n```","SQL\r\n./cockroach demo movr --nodes 9 --with-load --demo-locality=region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=us-east,az=1:region=us-central,az=1:region=us-west,az=2:region=useast,az=1:region=us-central,az=1:region=us-west,az=2\r\n"
40410,"sql: panic in row fetcher during UPSERTSentry Issue: [COCKROACHDB-11N](https://sentry.io/organizations/cockroach-labs/issues/1171576571/?referrer=github_integration)\n\nVersion 19.1.3\n\n\n\n```\n*log.safeError: conn_executor.go:434: panic while executing 1 statements: UPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__): caused by <redacted>\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 434, in func1\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fetcher.go"", line 768, in processKV\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fetcher.go"", line 1062, in NextRow\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go"", line 165, in Next\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/joinreader.go"", line 413, in performLookup\n...\n(25 additional frame(s) were not displayed)\n\nconn_executor.go:434: panic while executing 1 statements: UPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__): caused by <redacted>\n```",C-bug|S-2-temp-unavailability|A-sql-mutations|O-sentry,rohany,"Sentry Issue: [COCKROACHDB-11N](https://sentry.io/organizations/cockroach-labs/issues/1171576571/?referrer=github_integration)\n\nVersion 19.1.3\n\n```sql\nUPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__)\n```\n\n```\n*log.safeError: conn_executor.go:434: panic while executing 1 statements: UPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__): caused by <redacted>\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 434, in func1\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fetcher.go"", line 768, in processKV\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fetcher.go"", line 1062, in NextRow\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go"", line 165, in Next\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/joinreader.go"", line 413, in performLookup\n...\n(25 additional frame(s) were not displayed)\n\nconn_executor.go:434: panic while executing 1 statements: UPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__): caused by <redacted>\n```","sql\nUPSERT INTO _(_, _, _, _, _, _, _, _, _, _, _, _, _, _) VALUES (_, _, __more10__), (__more4__)\n"
40409,"sql: (v19.1.0) panic in window function executionSentry Issue: [COCKROACHDB-11Z](https://sentry.io/organizations/cockroach-labs/issues/1187187584/?referrer=github_integration)\r\n\r\nVersion 19.1.0\r\n\r\n\r\n\r\n```\r\n*log.safeError: conn_executor.go:431: panic while executing 1 statements: CREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._): caused by <redacted>\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 431, in func1\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/window.go"", line 175, in Next\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/create_table.go"", line 279, in startExec\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/plan.go"", line 496, in func2\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/walk.go"", line 145, in func1\r\n...\r\n(25 additional frame(s) were not displayed)\r\n\r\nconn_executor.go:431: panic while executing 1 statements: CREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._): caused by <redacted>\r\n```",C-bug|S-2-temp-unavailability|A-sql-execution|O-sentry,yuzefovich,"Sentry Issue: [COCKROACHDB-11Z](https://sentry.io/organizations/cockroach-labs/issues/1187187584/?referrer=github_integration)\r\n\r\nVersion 19.1.0\r\n\r\n```sql\r\nCREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._)\r\n```\r\n\r\n```\r\n*log.safeError: conn_executor.go:431: panic while executing 1 statements: CREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._): caused by <redacted>\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 431, in func1\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/window.go"", line 175, in Next\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/create_table.go"", line 279, in startExec\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/plan.go"", line 496, in func2\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/walk.go"", line 145, in func1\r\n...\r\n(25 additional frame(s) were not displayed)\r\n\r\nconn_executor.go:431: panic while executing 1 statements: CREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._): caused by <redacted>\r\n```","sql\r\nCREATE TABLE _ AS (SELECT row_number() OVER (), * FROM _._._)\r\n"
40408,"sql: panic ""joinReader specified with visibility %+v | distsqlpb.ScanVisibility"" Sentry Issue: [COCKROACHDB-120](https://sentry.io/organizations/cockroach-labs/issues/1187984986/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n\r\n\r\n```\r\n*log.safeError: joinreader.go:130: joinReader specified with visibility %+v | distsqlpb.ScanVisibility\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/joinreader.go"", line 130, in newJoinReader\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go"", line 1016, in newProcessor\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 349, in makeProcessor\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 424, in setupProcessors\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 524, in setup\r\n...\r\n(9 additional frame(s) were not displayed)\r\n\r\n(0) joinreader.go:130: joinReader specified with visibility %+v | distsqlpb.ScanVisibility\r\n(1) statement: SELECT _.* FROM _ JOIN _ ON _._ = _._ JOIN _ ON _._ = _._ WHERE (_._ = $1)\r\n(see stack traces in additional data)\r\n```",C-bug|A-sql-execution|O-sentry|S-3,jordanlewis,"Sentry Issue: [COCKROACHDB-120](https://sentry.io/organizations/cockroach-labs/issues/1187984986/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n```sql\r\nSELECT _.* FROM _ JOIN _ ON _._ = _._ JOIN _ ON _._ = _._ WHERE (_._ = $1)\r\n```\r\n\r\n```\r\n*log.safeError: joinreader.go:130: joinReader specified with visibility %+v | distsqlpb.ScanVisibility\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/joinreader.go"", line 130, in newJoinReader\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go"", line 1016, in newProcessor\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 349, in makeProcessor\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 424, in setupProcessors\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go"", line 524, in setup\r\n...\r\n(9 additional frame(s) were not displayed)\r\n\r\n(0) joinreader.go:130: joinReader specified with visibility %+v | distsqlpb.ScanVisibility\r\n(1) statement: SELECT _.* FROM _ JOIN _ ON _._ = _._ JOIN _ ON _._ = _._ WHERE (_._ = $1)\r\n(see stack traces in additional data)\r\n```",sql\r\nSELECT _.* FROM _ JOIN _ ON _._ = _._ JOIN _ ON _._ = _._ WHERE (_._ = $1)\r\n
40407,"opt: error ""expression does not have exactly one column"" while decorrelating subquerySentry Issue: [COCKROACHDB-11V](https://sentry.io/organizations/cockroach-labs/issues/1179884521/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n\r\n\r\n\r\n```\r\n*log.safeError: decorrelate.go:704: expression does not have exactly one column | string\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/decorrelate.go"", line 704, in referenceSingleColumn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/decorrelate.go"", line 680, in ConstructAnyCondition\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go"", line 190, in ConstructSelect\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go"", line 724, in buildWhere\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/mutation_builder.go"", line 198, in buildInputForUpdateOrDelete\r\n...\r\n(12 additional frame(s) were not displayed)\r\n\r\n(0) decorrelate.go:704: expression does not have exactly one column | string\r\n(1) statement: WITH _ AS (SELECT _._._ FROM _._ INNER JOIN _._ ON _._._ = _._._ WHERE (_ = _) AND (_ <= _::TIMESTAMPTZ) ORDER BY _ DESC LIMIT _) DELETE FROM _._ WHERE _ = ANY (SELECT _ FROM _)\r\n(see stack traces in additional data)\r\n```",C-bug|A-sql-optimizer|S-2-temp-unavailability|O-sentry,rytaft,"Sentry Issue: [COCKROACHDB-11V](https://sentry.io/organizations/cockroach-labs/issues/1179884521/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n```sql\r\nWITH _ AS (SELECT _._._ FROM _._ INNER JOIN _._ ON _._._ = _._._ WHERE (_ = _) AND (_ <= _::TIMESTAMPTZ) ORDER BY _ DESC LIMIT _) DELETE FROM _._ WHERE _ = ANY (SELECT _ FROM _)\r\n```\r\n\r\n\r\n```\r\n*log.safeError: decorrelate.go:704: expression does not have exactly one column | string\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/decorrelate.go"", line 704, in referenceSingleColumn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/decorrelate.go"", line 680, in ConstructAnyCondition\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go"", line 190, in ConstructSelect\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go"", line 724, in buildWhere\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/mutation_builder.go"", line 198, in buildInputForUpdateOrDelete\r\n...\r\n(12 additional frame(s) were not displayed)\r\n\r\n(0) decorrelate.go:704: expression does not have exactly one column | string\r\n(1) statement: WITH _ AS (SELECT _._._ FROM _._ INNER JOIN _._ ON _._._ = _._._ WHERE (_ = _) AND (_ <= _::TIMESTAMPTZ) ORDER BY _ DESC LIMIT _) DELETE FROM _._ WHERE _ = ANY (SELECT _ FROM _)\r\n(see stack traces in additional data)\r\n```",sql\r\nWITH _ AS (SELECT _._._ FROM _._ INNER JOIN _._ ON _._._ = _._._ WHERE (_ = _) AND (_ <= _::TIMESTAMPTZ) ORDER BY _ DESC LIMIT _) DELETE FROM _._ WHERE _ = ANY (SELECT _ FROM _)\r\n
40406,"opt: error ""referenced table %d not in provided table map"" while planning fk existence check in mutationSentry Issue: [COCKROACHDB-124](https://sentry.io/organizations/cockroach-labs/issues/1189696831/?referrer=github_integration)\n\nVersion 19.1.3\n\n\n\n```\n*log.safeError: fk_existence_base.go:126: referenced table %d not in provided table map %+v | sqlbase.ID; row.FkTableMetadata\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fk_existence_base.go"", line 126, in makeFkExistenceCheckBaseHelper\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fk_existence_insert.go"", line 69, in makeFkExistenceCheckHelperForInsert\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/writer.go"", line 81, in MakeInserter\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt_exec_factory.go"", line 1123, in ConstructInsert\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational_builder.go"", line 1346, in buildInsert\n...\n(11 additional frame(s) were not displayed)\n\n(0) fk_existence_base.go:126: referenced table %d not in provided table map %+v | sqlbase.ID; row.FkTableMetadata\n(1) statement: INSERT INTO _(_, _, _, _) VALUES ($1, $2, __more2__)\n(see stack traces in additional data)\n```",C-bug|A-sql-optimizer|S-2-temp-unavailability|O-sentry,solongordon,"Sentry Issue: [COCKROACHDB-124](https://sentry.io/organizations/cockroach-labs/issues/1189696831/?referrer=github_integration)\n\nVersion 19.1.3\n\n```sql\nINSERT INTO (...) VALUES (...)\n```\n\n```\n*log.safeError: fk_existence_base.go:126: referenced table %d not in provided table map %+v | sqlbase.ID; row.FkTableMetadata\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fk_existence_base.go"", line 126, in makeFkExistenceCheckBaseHelper\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/fk_existence_insert.go"", line 69, in makeFkExistenceCheckHelperForInsert\n  File ""github.com/cockroachdb/cockroach/pkg/sql/row/writer.go"", line 81, in MakeInserter\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt_exec_factory.go"", line 1123, in ConstructInsert\n  File ""github.com/cockroachdb/cockroach/pkg/sql/opt/exec/execbuilder/relational_builder.go"", line 1346, in buildInsert\n...\n(11 additional frame(s) were not displayed)\n\n(0) fk_existence_base.go:126: referenced table %d not in provided table map %+v | sqlbase.ID; row.FkTableMetadata\n(1) statement: INSERT INTO _(_, _, _, _) VALUES ($1, $2, __more2__)\n(see stack traces in additional data)\n```",sql\nINSERT INTO (...) VALUES (...)\n
40402,"sql: panic while evaluating top level subquery prior to main executionSentry Issue: [COCKROACHDB-12A](https://sentry.io/organizations/cockroach-labs/issues/1195637015/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n\r\n\r\n```\r\n*log.safeError: subquery.go:48: invalid index %d for %q | int; *tree.Subquery\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/subquery.go"", line 48, in EvalSubquery\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlplan/expression.go"", line 124, in VisitPre\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/walk.go"", line 680, in WalkExpr\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlplan/expression.go"", line 91, in MakeExpression\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsql_physical_planner.go"", line 885, in initTableReaderSpec\r\n...\r\n(22 additional frame(s) were not displayed)\r\n\r\n(0) subquery.go:48: invalid index %d for %q | int; *tree.Subquery\r\n(1) statement: SELECT (SELECT _ FROM _ LEFT JOIN _ AS _ RIGHT JOIN _ AS _ FULL JOIN _ AS _ ON _ ON _._ CROSS JOIN _ AS _ INNER JOIN _ AS _ ON _ ON EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON EXISTS (SELECT _ FROM _))) FROM _ AS _\r\n(see stack traces in additional data)\r\n```",C-bug|A-sql-optimizer|S-2-temp-unavailability|O-sentry,yuzefovich,"Sentry Issue: [COCKROACHDB-12A](https://sentry.io/organizations/cockroach-labs/issues/1195637015/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n```sql\r\nSELECT \r\n  (SELECT _ \r\n   FROM _ \r\n    LEFT JOIN _ AS _ \r\n    RIGHT JOIN _ AS _ \r\n    FULL JOIN _ AS _ ON _ \r\n    ON _._ \r\n    CROSS JOIN _ AS _ \r\n    INNER JOIN _ AS _ ON _ \r\n    ON EXISTS (\r\n         SELECT _ \r\n           FROM _ AS _ \r\n           LEFT JOIN _ ON EXISTS (SELECT _ FROM _)))\r\n FROM _ AS _\r\n```\r\n\r\n```\r\n*log.safeError: subquery.go:48: invalid index %d for %q | int; *tree.Subquery\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/subquery.go"", line 48, in EvalSubquery\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlplan/expression.go"", line 124, in VisitPre\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/walk.go"", line 680, in WalkExpr\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsqlplan/expression.go"", line 91, in MakeExpression\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/distsql_physical_planner.go"", line 885, in initTableReaderSpec\r\n...\r\n(22 additional frame(s) were not displayed)\r\n\r\n(0) subquery.go:48: invalid index %d for %q | int; *tree.Subquery\r\n(1) statement: SELECT (SELECT _ FROM _ LEFT JOIN _ AS _ RIGHT JOIN _ AS _ FULL JOIN _ AS _ ON _ ON _._ CROSS JOIN _ AS _ INNER JOIN _ AS _ ON _ ON EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON EXISTS (SELECT _ FROM _))) FROM _ AS _\r\n(see stack traces in additional data)\r\n```",sql\r\nSELECT \r\n  (SELECT _ \r\n   FROM _ \r\n    LEFT JOIN _ AS _ \r\n    RIGHT JOIN _ AS _ \r\n    FULL JOIN _ AS _ ON _ \r\n    ON _._ \r\n    CROSS JOIN _ AS _ \r\n    INNER JOIN _ AS _ ON _ \r\n    ON EXISTS (\r\n         SELECT _ \r\n           FROM _ AS _ \r\n           LEFT JOIN _ ON EXISTS (SELECT _ FROM _)))\r\n FROM _ AS _\r\n
40401,"sql: panic in datum compare inside apply joinSentry Issue: [COCKROACHDB-12B](https://sentry.io/organizations/cockroach-labs/issues/1195648548/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n\r\n\r\n```\r\n*log.safeError: conn_executor.go:434: panic while executing 1 statements: SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ FULL JOIN _ AS _ ON _._ OR ((SELECT _) IN (SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON NOT _._ HAVING bool_or(_._)))) CROSS JOIN _ AS _): caused by <redacted>\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 434, in func1\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/datum.go"", line 2608, in Compare\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 2101, in cmpOpScalarFn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 2106, in cmpOpScalarEQFn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 3611, in Eval\r\n...\r\n(32 additional frame(s) were not displayed)\r\n\r\nconn_executor.go:434: panic while executing 1 statements: SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ FULL JOIN _ AS _ ON _._ OR ((SELECT _) IN (SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON NOT _._ HAVING bool_or(_._)))) CROSS JOIN _ AS _): caused by <redacted>\r\n```",C-bug|S-2-temp-unavailability|A-sql-execution|O-sentry,rafiss,"Sentry Issue: [COCKROACHDB-12B](https://sentry.io/organizations/cockroach-labs/issues/1195648548/?referrer=github_integration)\r\n\r\nVersion 19.1.4\r\n\r\n```sql\r\nSELECT _ FROM _ AS _ \r\n WHERE \r\n  EXISTS(SELECT _ \r\n      FROM _ FULL JOIN _ AS _ \r\n        ON _._ OR (\r\n          (SELECT _) IN (\r\n               SELECT _ \r\n               FROM _ AS _ \r\n               WHERE \r\n                 EXISTS (SELECT _ \r\n                     FROM _ AS _ \r\n                     LEFT JOIN _ \r\n                     ON NOT _._ \r\n                     HAVING bool_or(_._))))\r\n       CROSS JOIN _ AS _)\r\n```\r\n\r\n```\r\n*log.safeError: conn_executor.go:434: panic while executing 1 statements: SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ FULL JOIN _ AS _ ON _._ OR ((SELECT _) IN (SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON NOT _._ HAVING bool_or(_._)))) CROSS JOIN _ AS _): caused by <redacted>\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go"", line 434, in func1\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/datum.go"", line 2608, in Compare\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 2101, in cmpOpScalarFn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 2106, in cmpOpScalarEQFn\r\n  File ""github.com/cockroachdb/cockroach/pkg/sql/sem/tree/eval.go"", line 3611, in Eval\r\n...\r\n(32 additional frame(s) were not displayed)\r\n\r\nconn_executor.go:434: panic while executing 1 statements: SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ FULL JOIN _ AS _ ON _._ OR ((SELECT _) IN (SELECT _ FROM _ AS _ WHERE EXISTS (SELECT _ FROM _ AS _ LEFT JOIN _ ON NOT _._ HAVING bool_or(_._)))) CROSS JOIN _ AS _): caused by <redacted>\r\n```",sql\r\nSELECT _ FROM _ AS _ \r\n WHERE \r\n  EXISTS(SELECT _ \r\n      FROM _ FULL JOIN _ AS _ \r\n        ON _._ OR (\r\n          (SELECT _) IN (\r\n               SELECT _ \r\n               FROM _ AS _ \r\n               WHERE \r\n                 EXISTS (SELECT _ \r\n                     FROM _ AS _ \r\n                     LEFT JOIN _ \r\n                     ON NOT _._ \r\n                     HAVING bool_or(_._))))\r\n       CROSS JOIN _ AS _)\r\n
40387,sql: new partiton @* doesn't work with table/poor error messageI tried out the new @* syntax from https://github.com/cockroachdb/cockroach/issues/39357#issuecomment-526744308 via:\r\n\r\nI observed:\r\n\r\nIt's confusing to me that we chose the INDEX over the TABLE here given that it applies to the entire table. I'd expect to either alias and be able to use either or at the very least to get a warning with the new syntax that you need to use INDEX when using the @* syntax.,C-enhancement|A-partitioning,solongordon,"I tried out the new @* syntax from https://github.com/cockroachdb/cockroach/issues/39357#issuecomment-526744308 via:\r\n```SQL\r\n./cockroach demo movr --nodes 3 --demo-locality=region=us-east,az=1:region=us-central,az=1:region=us-west,az=2\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nI observed:\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.rides@* CONFIGURE ZONE USING constraints='[+region=us-east,+az=1]'; ALTER PARTITION chicago OF TABLE rides@* CONFIGURE ZONE USING constraints='[+region=us-central,+az=1]'; ALTER PARTITION seattle OF TABLE movr.rides@* CONFIGURE ZONE USING constraints='[+region=us-west,+az=2]';\r\ninvalid syntax: statement ignored: at or near ""@"": syntax error\r\nDETAIL: source SQL:\r\nALTER PARTITION new_york OF TABLE movr.rides@* CONFIGURE ZONE USING constraints='[+region=us-east,+az=1]'\r\n                                            ^\r\nHINT: try \\h ALTER PARTITION\r\n```\r\nIt's confusing to me that we chose the INDEX over the TABLE here given that it applies to the entire table. I'd expect to either alias and be able to use either or at the very least to get a warning with the new syntax that you need to use INDEX when using the @* syntax.","SQL\r\n./cockroach demo movr --nodes 3 --demo-locality=region=us-east,az=1:region=us-central,az=1:region=us-west,az=2\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
40384,"sql: add comment to SHOW CREATE TABLE highlight no zone configsDevelopers can forget to alter zone configs after partitioning a table. For example, if a user did this:\r\n\r\nThen tried to check their table to for completion, they would see no zone configs:\r\n\r\nHopefully, this would spark them to remember to add zone configs, but there is no warning present. If they checked via SHOW PARTITIONS, they would receive a visual cue:\r\n\r\nCould we add table comments or some other warning to SHOW CREATE TABLE only when there is a partitioning statement but no zone config statements to say something like:\r\n`no zone configs applied`\r\n",C-enhancement|A-partitioning,rohany,"Developers can forget to alter zone configs after partitioning a table. For example, if a user did this:\r\n```SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nThen tried to check their table to for completion, they would see no zone configs:\r\n```SQL\r\nshow create table users;\r\n  table_name |                      create_statement\r\n+------------+-------------------------------------------------------------+\r\n  users      | CREATE TABLE users (\r\n             |     id UUID NOT NULL,\r\n             |     city VARCHAR NOT NULL,\r\n             |     name VARCHAR NULL,\r\n             |     address VARCHAR NULL,\r\n             |     credit_card VARCHAR NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n             |     FAMILY ""primary"" (id, city, name, address, credit_card)\r\n             | ) PARTITION BY LIST (city) (\r\n             |     PARTITION new_york VALUES IN (('new york')),\r\n             |     PARTITION chicago VALUES IN (('chicago')),\r\n             |     PARTITION seattle VALUES IN (('seattle'))\r\n             | )\r\n(1 row)\r\n```\r\nHopefully, this would spark them to remember to add zone configs, but there is no warning present. If they checked via SHOW PARTITIONS, they would receive a visual cue:\r\n```SQL\r\nroot@127.0.0.1:54289/movr> show partitions from table users;\r\n  database_name | table_name | partition_name | parent_partition | column_names |  index_name   | partition_value | zone_config\r\n+---------------+------------+----------------+------------------+--------------+---------------+-----------------+-------------+\r\n  movr          | users      | new_york       | NULL             | city         | users@primary | ('new york')    | NULL\r\n  movr          | users      | chicago        | NULL             | city         | users@primary | ('chicago')     | NULL\r\n  movr          | users      | seattle        | NULL             | city         | users@primary | ('seattle')     | NULL\r\n(3 rows)\r\n\r\nTime: 10.575ms\r\n```\r\nCould we add table comments or some other warning to SHOW CREATE TABLE only when there is a partitioning statement but no zone config statements to say something like:\r\n`no zone configs applied`\r\n","SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
40383,"sql: add command that partitions all indexes of a table in the same way using @*Similarly to https://github.com/cockroachdb/cockroach/issues/40382, we should aim to take advantage of #39357 (via 12a33d7) in which we added support for a new command that will apply zone configs to all indexes on a table with the same partition name. For example:\r\n\r\nIn the status quo, you must alter the partitions of each index. For example:\r\n\r\nAnd:\r\n```\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nIt would be great to be able to do this instead:\r\n\r\nAnd then return this:\r\n",C-enhancement|A-partitioning|A-sql-execution,solongordon,"Similarly to https://github.com/cockroachdb/cockroach/issues/40382, we should aim to take advantage of #39357 (via 12a33d7) in which we added support for a new command that will apply zone configs to all indexes on a table with the same partition name. For example:\r\n```SQL\r\nALTER PARTITION new_york OF INDEX rides@* CONFIGURE ZONE USING constraints='[+region=us-east,+az=1]';\r\n```\r\nIn the status quo, you must alter the partitions of each index. For example:\r\n```SQL\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nAnd:\r\n```\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nIt would be great to be able to do this instead:\r\n```SQL\r\nALTER INDEX rides@* PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nAnd then return this:\r\n```SQL\r\nSHOW PARTITIONS FROM INDEX rides@*;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |                 zone_config\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+---------------------------------------------+\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n```","SQL\r\nALTER PARTITION new_york OF INDEX rides@* CONFIGURE ZONE USING constraints='[+region=us-east,+az=1]';\r\n"
40382,sql: add a show partitions from index table@* command In https://github.com/cockroachdb/cockroach/issues/39357 (via https://github.com/cockroachdb/cockroach/pull/39750/commits/12a33d730847c3d6b9a0c243eedf0aa286c207a7) we added support for a new command that will apply zone configs to all indexes on a table with the same partition name. For example:\r\n\r\n\r\n\r\n\r\n\r\nA user might also want to be able to verify this via the `SHOW PARTITIONS FROM INDEX` statement but has no way of specifying multiple indexes. \r\n\r\nWe could extend this to do something like: \r\n\r\n,C-enhancement|A-partitioning|A-sql-execution,solongordon,"In https://github.com/cockroachdb/cockroach/issues/39357 (via https://github.com/cockroachdb/cockroach/pull/39750/commits/12a33d730847c3d6b9a0c243eedf0aa286c207a7) we added support for a new command that will apply zone configs to all indexes on a table with the same partition name. For example:\r\n\r\n```SQL\r\nshow partitions from table rides;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |                 zone_config\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+---------------------------------------------+\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n(9 rows)\r\n\r\nTime: 19.839ms\r\n```\r\n```SQL\r\nALTER PARTITION new_york OF INDEX rides@* CONFIGURE ZONE USING constraints='[+region=us-east,+az=1]'; ALTER PARTITION chicago OF INDEX rides@* CONFIGURE ZONE USING constraints='[+region=us-central,+az=1]'; ALTER PARTITION seattle OF INDEX rides@* CONFIGURE ZONE USING constraints='[+region=us-west,+az=2]';\r\nCONFIGURE ZONE 1\r\n\r\nTime: 109.829ms\r\n```\r\n```SQL\r\nroot@127.0.0.1:52788/movr> show partitions from table rides;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |                 zone_config\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+---------------------------------------------+\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n(9 rows)\r\n```\r\n\r\nA user might also want to be able to verify this via the `SHOW PARTITIONS FROM INDEX` statement but has no way of specifying multiple indexes. \r\n\r\nWe could extend this to do something like: \r\n```SQL\r\nSHOW PARTITIONS FROM INDEX rides@*;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |                 zone_config\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+---------------------------------------------+\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n```\r\n","SQL\r\nshow partitions from table rides;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |                 zone_config\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+---------------------------------------------+\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | constraints = '[+region=us-east, +az=1]'\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | constraints = '[+region=us-central, +az=1]'\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | constraints = '[+region=us-west, +az=2]'\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n(9 rows)\r\n\r\nTime: 19.839ms\r\n"
40367,"storage: must never reuse state from old replicaIDsI was looking at the range merge tests and in particular\r\nTestStoreRangeMergeAddReplicaRace (at its time of introduction; it has since\r\nchanged due to learners and lost a lot of its context), see\r\n\r\nce936508d150f1caa5ca94fb3a66ba84e4555e3e\r\n\r\nThe worry is roughly that when a snapshot that does *not* contain a merge is\r\nsitting on some store (which is not part of the range) and this store is then\r\nadded to the range, the replica will likely blow up while applying the merge\r\ntrigger, is the subsumed replica is very unlikely to be present - this colocation\r\nis only achieved for the duration of the merge txn and for the stores that\r\nare members of the range.\r\n\r\nAt the time of the below comment, the way such a situation would come to be is\r\nbecause preemptive snapshots were sent before starting the replica change txn.\r\nSo you'd send a snapshot, then do a merge, and (before we introduced the range\r\ndesc generation for that purpose) the upreplication could succeed, boom.\r\n\r\nHowever, this sequence of events might unfold just the same without any preemptive snaps:\r\n\r\n1. add replica on store\r\n2. remove replica from store; replicaGC is delayed\r\n3. run a merge\r\n4. readd store\r\n\r\nHere, in 4 I don't mean ""re-add as a learner and promote to voter""! It's enough to\r\nreadd it as a learner. Raft will start catching up the old snapshot and boom, we\r\nhit the problem above.\r\n\r\nThis actually seems like a likely problem to occur in practice if replicas are\r\nremoved and re-added frequently while merges occur, though the fact that we still\r\nsend explicit learner snapshots could mask it somewhat (that snapshot will postdate\r\nthe merge, so it can hide the problem by fast-forwarding the old replica past the\r\nmerge).\r\n\r\nFixing this is probably not easy. We want to ""not reuse old snapshots"", i.e. when the\r\nreplicaID changes we want to delete all our state and start anew. This is far from a\r\ntrivial change, but it's in the spirit of changes that we want to make anyway to lower\r\nthe complexity in the storage package. Anyway the first step here is reproducing the\r\nproblem.\r\n\r\ncc @danhhz @nvanbenschoten\r\n\r\n",C-bug|A-kv-distribution,ajwerner,"I was looking at the range merge tests and in particular\r\nTestStoreRangeMergeAddReplicaRace (at its time of introduction; it has since\r\nchanged due to learners and lost a lot of its context), see\r\n\r\nce936508d150f1caa5ca94fb3a66ba84e4555e3e\r\n\r\nThe worry is roughly that when a snapshot that does *not* contain a merge is\r\nsitting on some store (which is not part of the range) and this store is then\r\nadded to the range, the replica will likely blow up while applying the merge\r\ntrigger, is the subsumed replica is very unlikely to be present - this colocation\r\nis only achieved for the duration of the merge txn and for the stores that\r\nare members of the range.\r\n\r\nAt the time of the below comment, the way such a situation would come to be is\r\nbecause preemptive snapshots were sent before starting the replica change txn.\r\nSo you'd send a snapshot, then do a merge, and (before we introduced the range\r\ndesc generation for that purpose) the upreplication could succeed, boom.\r\n\r\nHowever, this sequence of events might unfold just the same without any preemptive snaps:\r\n\r\n1. add replica on store\r\n2. remove replica from store; replicaGC is delayed\r\n3. run a merge\r\n4. readd store\r\n\r\nHere, in 4 I don't mean ""re-add as a learner and promote to voter""! It's enough to\r\nreadd it as a learner. Raft will start catching up the old snapshot and boom, we\r\nhit the problem above.\r\n\r\nThis actually seems like a likely problem to occur in practice if replicas are\r\nremoved and re-added frequently while merges occur, though the fact that we still\r\nsend explicit learner snapshots could mask it somewhat (that snapshot will postdate\r\nthe merge, so it can hide the problem by fast-forwarding the old replica past the\r\nmerge).\r\n\r\nFixing this is probably not easy. We want to ""not reuse old snapshots"", i.e. when the\r\nreplicaID changes we want to delete all our state and start anew. This is far from a\r\ntrivial change, but it's in the spirit of changes that we want to make anyway to lower\r\nthe complexity in the storage package. Anyway the first step here is reproducing the\r\nproblem.\r\n\r\ncc @danhhz @nvanbenschoten\r\n\r\n```go\r\n// TestStoreRangeMergeAddReplicaRace verifies that an add replica request that\r\n// occurs concurrently with a merge is aborted.\r\n//\r\n// To see why aborting the add replica request is necessary, consider two\r\n// adjacent and collocated ranges, Q and R. Say the replicate queue decides to\r\n// rebalance Q onto store S4. It will initiate a ChangeReplicas command that\r\n// will send S4 a preemptive snapshot, then launch a replica-change transaction\r\n// to update R's range descriptor with the new replica. Now say the merge queue\r\n// decides to merge Q and R after the preemptive snapshot of Q has been sent to\r\n// S4 but before the replica-change transaction has started. The merge can\r\n// succeed because the ranges are still collocated. (The new replica of Q is\r\n// only considered added once the replica-change transaction commits.) If the\r\n// replica-change transaction were to commit, the new replica of Q on S4 would\r\n// have a snapshot of Q that predated the merge. In order to catch up, it would\r\n// need to apply the merge trigger, but the merge trigger will explode because\r\n// S4 does not have a replica of R.\r\n//\r\n// To avoid this scenario, ChangeReplicas commands will abort if they discover\r\n// the range descriptor has changed between when the snapshot is sent and when\r\n// the replica-change transaction starts.\r\n//\r\n// There is a particularly diabolical edge case here. Consider the same\r\n// situation as above, except that Q and R merge together and then split at\r\n// exactly the same key, all before the replica-change transaction starts. Q's\r\n// range descriptor will have the same start key, end key, and next replica ID\r\n// that it did when the preemptive snapshot started. That is, it will look\r\n// unchanged! To protect against this, range descriptors contain a generation\r\n// counter, which is incremented on every split or merge. The presence of this\r\n// counter means that ChangeReplicas commands can detect and abort if any merges\r\n// have occurred since the preemptive snapshot, even if the sequence of splits\r\n// or merges left the keyspan of the range unchanged. This diabolical edge case\r\n// is tested here.\r\n```","go\r\n// TestStoreRangeMergeAddReplicaRace verifies that an add replica request that\r\n// occurs concurrently with a merge is aborted.\r\n//\r\n// To see why aborting the add replica request is necessary, consider two\r\n// adjacent and collocated ranges, Q and R. Say the replicate queue decides to\r\n// rebalance Q onto store S4. It will initiate a ChangeReplicas command that\r\n// will send S4 a preemptive snapshot, then launch a replica-change transaction\r\n// to update R's range descriptor with the new replica. Now say the merge queue\r\n// decides to merge Q and R after the preemptive snapshot of Q has been sent to\r\n// S4 but before the replica-change transaction has started. The merge can\r\n// succeed because the ranges are still collocated. (The new replica of Q is\r\n// only considered added once the replica-change transaction commits.) If the\r\n// replica-change transaction were to commit, the new replica of Q on S4 would\r\n// have a snapshot of Q that predated the merge. In order to catch up, it would\r\n// need to apply the merge trigger, but the merge trigger will explode because\r\n// S4 does not have a replica of R.\r\n//\r\n// To avoid this scenario, ChangeReplicas commands will abort if they discover\r\n// the range descriptor has changed between when the snapshot is sent and when\r\n// the replica-change transaction starts.\r\n//\r\n// There is a particularly diabolical edge case here. Consider the same\r\n// situation as above, except that Q and R merge together and then split at\r\n// exactly the same key, all before the replica-change transaction starts. Q's\r\n// range descriptor will have the same start key, end key, and next replica ID\r\n// that it did when the preemptive snapshot started. That is, it will look\r\n// unchanged! To protect against this, range descriptors contain a generation\r\n// counter, which is incremented on every split or merge. The presence of this\r\n// counter means that ChangeReplicas commands can detect and abort if any merges\r\n// have occurred since the preemptive snapshot, even if the sequence of splits\r\n// or merges left the keyspan of the range unchanged. This diabolical edge case\r\n// is tested here.\r\n"
39767,"sql: add range size to show rangesWe now have a more mature `SHOW RANGES` that provides good information:\r\n\r\n\r\nHowever, this command could also be expanded to show range size. This recently came up with a customer seeking a way to get this information outside of the webui. cc @roncrdb ",C-enhancement|A-sql-execution,rohany,"We now have a more mature `SHOW RANGES` that provides good information:\r\n```SQL\r\nroot@127.0.0.1:63253/movr> show ranges from table rides;\r\n        start_key       |        end_key        | range_id | replicas | lease_holder |    locality\r\n+-----------------------+-----------------------+----------+----------+--------------+-----------------+\r\n  NULL                  | /""chicago""            |       23 | {1,2,3}  |            1 | region=us-east1\r\n  /""chicago""            | /""chicago""/PrefixEnd  |       28 | {1,2,3}  |            1 | region=us-east1\r\n  /""chicago""/PrefixEnd  | /""new york""           |       31 | {1,2,3}  |            1 | region=us-east1\r\n  /""new york""           | /""new york""/PrefixEnd |       27 | {1,2,3}  |            1 | region=us-east1\r\n  /""new york""/PrefixEnd | /""seattle""            |       29 | {1,2,3}  |            1 | region=us-east1\r\n  /""seattle""            | /""seattle""/PrefixEnd  |       30 | {1,2,3}  |            1 | region=us-east1\r\n  /""seattle""/PrefixEnd  | NULL                  |       32 | {1,2,3}  |            1 | region=us-east1\r\n(7 rows)\r\n\r\nTime: 31.467ms\r\n```\r\n\r\nHowever, this command could also be expanded to show range size. This recently came up with a customer seeking a way to get this information outside of the webui. cc @roncrdb ","SQL\r\nroot@127.0.0.1:63253/movr> show ranges from table rides;\r\n        start_key       |        end_key        | range_id | replicas | lease_holder |    locality\r\n+-----------------------+-----------------------+----------+----------+--------------+-----------------+\r\n  NULL                  | /""chicago""            |       23 | {1,2,3}  |            1 | region=us-east1\r\n  /""chicago""            | /""chicago""/PrefixEnd  |       28 | {1,2,3}  |            1 | region=us-east1\r\n  /""chicago""/PrefixEnd  | /""new york""           |       31 | {1,2,3}  |            1 | region=us-east1\r\n  /""new york""           | /""new york""/PrefixEnd |       27 | {1,2,3}  |            1 | region=us-east1\r\n  /""new york""/PrefixEnd | /""seattle""            |       29 | {1,2,3}  |            1 | region=us-east1\r\n  /""seattle""            | /""seattle""/PrefixEnd  |       30 | {1,2,3}  |            1 | region=us-east1\r\n  /""seattle""/PrefixEnd  | NULL                  |       32 | {1,2,3}  |            1 | region=us-east1\r\n(7 rows)\r\n\r\nTime: 31.467ms\r\n"
39703,"has_column_privilege() uses column index instead of attnum**Describe the problem**\r\n\r\nIt seems that CockroachDB interprets the second argument of `has_column_privilege(table, column, privilege)` as a column index instead of an attribute number. This is different from PostgreSQL.\r\n\r\nHere is an excerpt from the [PostgreSQL docs for the function](https://www.postgresql.org/docs/current/functions-info.html):\r\n\r\n> `has_column_privilege` checks whether a user can access a column in a particular way. Its argument possibilities are analogous to `has_table_privilege`, with the addition that the column can be specified **either by name or attribute number**. \r\n\r\n**To Reproduce**\r\n\r\n\r\n\r\n**Environment:**\r\n - CCL v19.1.4 @ 2019/08/06 15:29:00 (go1.11.6)\r\n - macOS 10.14.6\r\n - Postico \r\n\r\n**Additional context**\r\n\r\nThis breaks compatibility with Postico. (I'm the developer of Postico)\r\n\r\nPostgreSQL does not return an error when an invalid attnum is provided -- it just returns true. I think that the CockroachDB behavior of returning an error is better than PostgreSQL's behavior of ignoring errors, otherwise we probably wouldn't have discovered this issue.",C-bug|A-sql-pgcompat|S-3-productivity,yuzefovich,"**Describe the problem**\r\n\r\nIt seems that CockroachDB interprets the second argument of `has_column_privilege(table, column, privilege)` as a column index instead of an attribute number. This is different from PostgreSQL.\r\n\r\nHere is an excerpt from the [PostgreSQL docs for the function](https://www.postgresql.org/docs/current/functions-info.html):\r\n\r\n> `has_column_privilege` checks whether a user can access a column in a particular way. Its argument possibilities are analogous to `has_table_privilege`, with the addition that the column can be specified **either by name or attribute number**. \r\n\r\n**To Reproduce**\r\n\r\n```sql\r\ndrop table if exists hcp_test;\r\n\r\ncreate table hcp_test(a int, b int, c int);\r\n\r\nalter table hcp_test drop column b;\r\n\r\nselect attname, attnum from pg_attribute where attrelid = 'hcp_test'::regclass;\r\n/*\r\n attname  | attnum\r\n ---------+-----\r\n a        | 1\r\n c        | 3\r\n rowid    | 4\r\n*/\r\n\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 1, 'SELECT'); \r\n/* TRUE */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 2, 'SELECT')\r\n/* TRUE (I expect this to fail because there is no attnum 2) */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 3, 'SELECT');\r\n/* TRUE */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 4, 'SELECT');\r\n/* ERROR:  has_column_privilege(): column 4 of relation hcp_test does not exist */\r\n```\r\n\r\n**Environment:**\r\n - CCL v19.1.4 @ 2019/08/06 15:29:00 (go1.11.6)\r\n - macOS 10.14.6\r\n - Postico \r\n\r\n**Additional context**\r\n\r\nThis breaks compatibility with Postico. (I'm the developer of Postico)\r\n\r\nPostgreSQL does not return an error when an invalid attnum is provided -- it just returns true. I think that the CockroachDB behavior of returning an error is better than PostgreSQL's behavior of ignoring errors, otherwise we probably wouldn't have discovered this issue.","sql\r\ndrop table if exists hcp_test;\r\n\r\ncreate table hcp_test(a int, b int, c int);\r\n\r\nalter table hcp_test drop column b;\r\n\r\nselect attname, attnum from pg_attribute where attrelid = 'hcp_test'::regclass;\r\n/*\r\n attname  | attnum\r\n ---------+-----\r\n a        | 1\r\n c        | 3\r\n rowid    | 4\r\n*/\r\n\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 1, 'SELECT'); \r\n/* TRUE */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 2, 'SELECT')\r\n/* TRUE (I expect this to fail because there is no attnum 2) */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 3, 'SELECT');\r\n/* TRUE */\r\n\r\nselect has_column_privilege('hcp_test'::regclass, 4, 'SELECT');\r\n/* ERROR:  has_column_privilege(): column 4 of relation hcp_test does not exist */\r\n"
39667,"sql: creating a table and adding an index within a single transaction produces problemsThe following has a number of issues and affects 19.1.x as well as master.\r\n\r\n\r\n\r\nThere are a number of issues here:\r\n\r\n1) if you delay a bit between issuing the create table and create index, the transaction will fail and require a retry.\r\n\r\n```\r\nroot@:26257/defaultdb  OPEN> commit;\r\npq: restart transaction: TransactionRetryWithProtoRefreshError: TransactionRetryError: retry txn (RETRY_SERIALIZABLE): id=3754c52d key=/Table/SystemConfigSpan/Start rw=true pri=0.00095111 stat=PENDING epo=0 ts=1565796278.946017000,1 orig=1565796271.134457000,0 min=1565796271.134457000,0 max=1565796271.134457000,0 wto=false seq=17\r\n```\r\n2) If the full transaction is committed, it will leave a job in a pending state and never complete it.\r\n\r\n![Screen Shot 2019-08-14 at 11 27 39](https://user-images.githubusercontent.com/1614265/63034019-92d77680-be86-11e9-8736-78577a6deb59.png)\r\n\r\n3) Subsequent schema changes to the table, may not take effect, even if they show up as passing.\r\nThis is the problem that flyway migrations keep experiencing.  Columns not being added is the most common.\r\n\r\n\r\n\r\n",C-bug|A-schema-changes,dt,"The following has a number of issues and affects 19.1.x as well as master.\r\n\r\n```sql\r\nBEGIN;\r\nCREATE TABLE test (\r\n    id                 UUID DEFAULT gen_random_uuid() PRIMARY KEY,\r\n    segment_key        UUID\r\n);\r\nCREATE INDEX idx ON test (segment_key);\r\nCOMMIT;\r\n```\r\n\r\nThere are a number of issues here:\r\n\r\n1) if you delay a bit between issuing the create table and create index, the transaction will fail and require a retry.\r\n\r\n```\r\nroot@:26257/defaultdb  OPEN> commit;\r\npq: restart transaction: TransactionRetryWithProtoRefreshError: TransactionRetryError: retry txn (RETRY_SERIALIZABLE): id=3754c52d key=/Table/SystemConfigSpan/Start rw=true pri=0.00095111 stat=PENDING epo=0 ts=1565796278.946017000,1 orig=1565796271.134457000,0 min=1565796271.134457000,0 max=1565796271.134457000,0 wto=false seq=17\r\n```\r\n2) If the full transaction is committed, it will leave a job in a pending state and never complete it.\r\n\r\n![Screen Shot 2019-08-14 at 11 27 39](https://user-images.githubusercontent.com/1614265/63034019-92d77680-be86-11e9-8736-78577a6deb59.png)\r\n\r\n3) Subsequent schema changes to the table, may not take effect, even if they show up as passing.\r\nThis is the problem that flyway migrations keep experiencing.  Columns not being added is the most common.\r\n\r\n\r\n\r\n","sql\r\nBEGIN;\r\nCREATE TABLE test (\r\n    id                 UUID DEFAULT gen_random_uuid() PRIMARY KEY,\r\n    segment_key        UUID\r\n);\r\nCREATE INDEX idx ON test (segment_key);\r\nCOMMIT;\r\n"
39490,"sql: partitions persist after dropping index  I was using Movr to test out partitioning and indexes in https://github.com/cockroachdb/cockroach/issues/39478.\r\n\r\nI created an index, partitioned that index, and then dropped that index. Then, I recreated the exact same index but the original partitions were still present.\r\n\r\nCreate index:\r\n\r\nThen alter index:\r\n\r\n\r\nAlter partition\r\n\r\nDrop index\r\n\r\nSet sql_safe_updates=false and recreate the index:\r\n\r\nShow partitions:\r\n\r\nI think the partitions should have been removed when the index was removed. \r\n",C-bug|S-3-ux-surprise|A-partitioning|A-sql-execution,solongordon,"I was using Movr to test out partitioning and indexes in https://github.com/cockroachdb/cockroach/issues/39478.\r\n\r\nI created an index, partitioned that index, and then dropped that index. Then, I recreated the exact same index but the original partitions were still present.\r\n\r\nCreate index:\r\n```SQL\r\nroot@10.142.0.78:26257/movr> create index on rides (vehicle_city, rider_id, revenue);\r\nCREATE INDEX\r\n\r\nTime: 8.628774396s\r\n\r\nroot@10.142.0.78:26257/movr> show indexes from rides;\r\n  table_name |                  index_name                   | non_unique | seq_in_index | column_name  | direction | storing | implicit\r\n+------------+-----------------------------------------------+------------+--------------+--------------+-----------+---------+----------+\r\n  rides      | primary                                       |   false    |            1 | city         | ASC       |  false  |  false\r\n  rides      | primary                                       |   false    |            2 | id           | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            1 | city         | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            2 | rider_id     | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            3 | id           | ASC       |  false  |   true\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            1 | vehicle_city | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            2 | vehicle_id   | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            3 | city         | ASC       |  false  |   true\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            4 | id           | ASC       |  false  |   true\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            1 | vehicle_city | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            2 | rider_id     | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            3 | revenue      | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            4 | city         | ASC       |  false  |   true\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            5 | id           | ASC       |  false  |   true\r\n(14 rows)\r\n\r\nTime: 43.625964ms\r\n```\r\nThen alter index:\r\n```SQL\r\nALTER INDEX rides_vehicle_city_rider_id_revenue_idx PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\nALTER INDEX\r\n\r\nTime: 912.789166ms\r\n```\r\n```SQL\r\nroot@10.142.0.78:26257/movr> show partitions from index rides@rides_vehicle_city_rider_id_revenue_idx;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                  index_name                   | partition_value | zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------+-----------------+------------------+\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('seattle')     | NULL\r\n(3 rows)\r\n```\r\nAlter partition\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-west1]';\r\nCONFIGURE ZONE 1\r\n```\r\nDrop index\r\n```SQL\r\nroot@10.142.0.78:26257/movr> drop index rides@rides_auto_index_fk_vehicle_city_ref_vehicles\r\ncascade;\r\nDROP INDEX\r\n\r\nTime: 8.439766675s\r\n```\r\nSet sql_safe_updates=false and recreate the index:\r\n```SQL\r\nroot@10.142.0.78:26257/movr> set sql_safe_updates=false;\r\nSET\r\n\r\nTime: 67.777001ms\r\n\r\nroot@10.142.0.78:26257/movr> create index on rides (vehicle_city, rider_id, revenue);\r\nCREATE INDEX\r\n\r\nTime: 11.830363829s\r\n```\r\nShow partitions:\r\n```SQL\r\nroot@10.142.0.78:26257/movr> show partitions from index rides@rides_vehicle_city_rider_id_revenue_idx;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                  index_name                   | partition_value | zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------+-----------------+------------------+\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('new york')    | NULL\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('chicago')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_vehicle_city_rider_id_revenue_idx | ('seattle')     | NULL\r\n(3 rows)\r\n\r\nTime: 683.433104ms\r\n```\r\nI think the partitions should have been removed when the index was removed. \r\n","SQL\r\nroot@10.142.0.78:26257/movr> create index on rides (vehicle_city, rider_id, revenue);\r\nCREATE INDEX\r\n\r\nTime: 8.628774396s\r\n\r\nroot@10.142.0.78:26257/movr> show indexes from rides;\r\n  table_name |                  index_name                   | non_unique | seq_in_index | column_name  | direction | storing | implicit\r\n+------------+-----------------------------------------------+------------+--------------+--------------+-----------+---------+----------+\r\n  rides      | primary                                       |   false    |            1 | city         | ASC       |  false  |  false\r\n  rides      | primary                                       |   false    |            2 | id           | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            1 | city         | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            2 | rider_id     | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_city_ref_users            |    true    |            3 | id           | ASC       |  false  |   true\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            1 | vehicle_city | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            2 | vehicle_id   | ASC       |  false  |  false\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            3 | city         | ASC       |  false  |   true\r\n  rides      | rides_auto_index_fk_vehicle_city_ref_vehicles |    true    |            4 | id           | ASC       |  false  |   true\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            1 | vehicle_city | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            2 | rider_id     | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            3 | revenue      | ASC       |  false  |  false\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            4 | city         | ASC       |  false  |   true\r\n  rides      | rides_vehicle_city_rider_id_revenue_idx       |    true    |            5 | id           | ASC       |  false  |   true\r\n(14 rows)\r\n\r\nTime: 43.625964ms\r\n"
39478,"sql: no warning when creating secondary index on partitioned tableI'm testing out this PR https://github.com/cockroachdb/cockroach/pull/39154 by partitioning Movr:\r\n\r\nand setting zone configs:\r\n\r\n\r\nThen I tried to make a new index on rides:\r\n```\r\nroot@10.142.0.78:26257/movr> create index on rides (rider_id, revenue) storing (city);\r\npq: index ""rides_rider_id_revenue_idx"" already contains column ""city""\r\nDETAIL: column ""city"" is part of the primary index and therefore implicit in all indexes\r\nroot@10.142.0.78:26257/movr> create index on rides (rider_id, revenue);\r\nCREATE INDEX\r\n\r\nTime: 7.040415307s\r\n```\r\nThis succeded with no warning. ",C-bug|S-3-ux-surprise|A-partitioning|A-sql-execution,solongordon,"I'm testing out this PR https://github.com/cockroachdb/cockroach/pull/39154 by partitioning Movr:\r\n```SQL\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nand setting zone configs:\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.rides CONFIGURE ZONE USING constraints='[+region=us-west1]';\r\n```\r\n\r\nThen I tried to make a new index on rides:\r\n```\r\nroot@10.142.0.78:26257/movr> create index on rides (rider_id, revenue) storing (city);\r\npq: index ""rides_rider_id_revenue_idx"" already contains column ""city""\r\nDETAIL: column ""city"" is part of the primary index and therefore implicit in all indexes\r\nroot@10.142.0.78:26257/movr> create index on rides (rider_id, revenue);\r\nCREATE INDEX\r\n\r\nTime: 7.040415307s\r\n```\r\nThis succeded with no warning. ","SQL\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
39477,"sql: update error message when zone configs not applied I'm partitioning the Movr table. I've setup the partitions as follows:\r\n\r\nThis is great because you can now reuse the same partitioning name. However, it causes a new problem when applying zone constraints.\r\n\r\nWe can show these constraints:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from database movr;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |   zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+-----------------------+\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n  movr          | users      | chicago        | NULL             | city         | users@primary                                       | ('chicago')     | [+region=us-central1]\r\n  movr          | users      | new_york       | NULL             | city         | users@primary                                       | ('new york')    | [+region=us-east1]\r\n  movr          | users      | seattle        | NULL             | city         | users@primary                                       | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@primary                                    | ('chicago')     | NULL\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('chicago')     | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@primary                                    | ('new york')    | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('new york')    | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@primary                                    | ('seattle')     | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('seattle')     | NULL\r\n(18 rows)\r\n\r\nTime: 615.713865ms\r\n```\r\nAnd we can then add them to a table that also has indexes:\r\n\r\nNow the show statement:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from database movr;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |   zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+-----------------------+\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n  movr          | users      | chicago        | NULL             | city         | users@primary                                       | ('chicago')     | [+region=us-central1]\r\n  movr          | users      | new_york       | NULL             | city         | users@primary                                       | ('new york')    | [+region=us-east1]\r\n  movr          | users      | seattle        | NULL             | city         | users@primary                                       | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@primary                                    | ('chicago')     | [+region=us-central1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('chicago')     | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@primary                                    | ('new york')    | [+region=us-east1]\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('new york')    | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@primary                                    | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('seattle')     | NULL\r\n(18 rows)\r\n```\r\nBut this does not apply to all of the indexes! In fact, there is no way to make the zone configs apply to the indexes because previously we did it to something like new_york_idx and that isn't specified anymore.\r\n\r\n",C-bug|S-3-ux-surprise|A-partitioning|A-sql-execution,solongordon,"I'm partitioning the Movr table. I've setup the partitions as follows:\r\n```SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER TABLE vehicles PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX vehicles_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nThis is great because you can now reuse the same partitioning name. However, it causes a new problem when applying zone constraints.\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-west1]';\r\n```\r\nWe can show these constraints:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from database movr;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |   zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+-----------------------+\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n  movr          | users      | chicago        | NULL             | city         | users@primary                                       | ('chicago')     | [+region=us-central1]\r\n  movr          | users      | new_york       | NULL             | city         | users@primary                                       | ('new york')    | [+region=us-east1]\r\n  movr          | users      | seattle        | NULL             | city         | users@primary                                       | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@primary                                    | ('chicago')     | NULL\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('chicago')     | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@primary                                    | ('new york')    | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('new york')    | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@primary                                    | ('seattle')     | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('seattle')     | NULL\r\n(18 rows)\r\n\r\nTime: 615.713865ms\r\n```\r\nAnd we can then add them to a table that also has indexes:\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.vehicles CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.vehicles CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.vehicles CONFIGURE ZONE USING constraints='[+region=us-west1]';\r\n```\r\nNow the show statement:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from database movr;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                     index_name                      | partition_value |   zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+-----------------------------------------------------+-----------------+-----------------------+\r\n  movr          | rides      | chicago        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('chicago')     | NULL\r\n  movr          | rides      | chicago        | NULL             | city         | rides@primary                                       | ('chicago')     | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@primary                                       | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('new york')    | NULL\r\n  movr          | rides      | new_york       | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('new york')    | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@rides_auto_index_fk_city_ref_users            | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | city         | rides@primary                                       | ('seattle')     | NULL\r\n  movr          | rides      | seattle        | NULL             | vehicle_city | rides@rides_auto_index_fk_vehicle_city_ref_vehicles | ('seattle')     | NULL\r\n  movr          | users      | chicago        | NULL             | city         | users@primary                                       | ('chicago')     | [+region=us-central1]\r\n  movr          | users      | new_york       | NULL             | city         | users@primary                                       | ('new york')    | [+region=us-east1]\r\n  movr          | users      | seattle        | NULL             | city         | users@primary                                       | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@primary                                    | ('chicago')     | [+region=us-central1]\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('chicago')     | NULL\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@primary                                    | ('new york')    | [+region=us-east1]\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('new york')    | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@primary                                    | ('seattle')     | [+region=us-west1]\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users      | ('seattle')     | NULL\r\n(18 rows)\r\n```\r\nBut this does not apply to all of the indexes! In fact, there is no way to make the zone configs apply to the indexes because previously we did it to something like new_york_idx and that isn't specified anymore.\r\n\r\n","SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER TABLE vehicles PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX vehicles_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER TABLE rides PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n\r\nALTER INDEX rides_auto_index_fk_vehicle_city_ref_vehicles PARTITION BY LIST (vehicle_city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
39476,"sql: improve error message for show partitions from indexIn running Movr, I partition an index:\r\n\r\nI want to see how that index works:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from index vehicles_auto_index_fk_city_ref_users;\r\npq: relation """" does not exist\r\n```\r\nSince indexes can have the same name on different tables, we can't identify which index to report on. Instead, we assume that their is an imaginary table in front of this index. \r\nUsers need to use the index hint syntax to find a result:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from index vehicles@vehicles_auto_index_fk_city_ref_users;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                   index_name                   | partition_value | zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+------------------------------------------------+-----------------+------------------+\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('new york')    | NULL\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('chicago')     | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('seattle')     | NULL\r\n(3 rows)\r\n\r\nTime: 453.712389ms\r\n```\r\nWe should improve upon the previous error message to say something like ""no table specified. Specify a table by using the hint syntax of foo@foo_index""",C-enhancement|A-partitioning|A-sql-execution,rohany,"In running Movr, I partition an index:\r\n```SQL\r\nALTER INDEX vehicles_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\nI want to see how that index works:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from index vehicles_auto_index_fk_city_ref_users;\r\npq: relation """" does not exist\r\n```\r\nSince indexes can have the same name on different tables, we can't identify which index to report on. Instead, we assume that their is an imaginary table in front of this index. \r\nUsers need to use the index hint syntax to find a result:\r\n```\r\nroot@10.142.0.78:26257/movr> show partitions from index vehicles@vehicles_auto_index_fk_city_ref_users;\r\n  database_name | table_name | partition_name | parent_partition | column_names |                   index_name                   | partition_value | zone_constraints\r\n+---------------+------------+----------------+------------------+--------------+------------------------------------------------+-----------------+------------------+\r\n  movr          | vehicles   | new_york       | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('new york')    | NULL\r\n  movr          | vehicles   | chicago        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('chicago')     | NULL\r\n  movr          | vehicles   | seattle        | NULL             | city         | vehicles@vehicles_auto_index_fk_city_ref_users | ('seattle')     | NULL\r\n(3 rows)\r\n\r\nTime: 453.712389ms\r\n```\r\nWe should improve upon the previous error message to say something like ""no table specified. Specify a table by using the hint syntax of foo@foo_index""","SQL\r\nALTER INDEX vehicles_auto_index_fk_city_ref_users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
39076,"sql: add constraints to SHOW CREATE TABLESplitting this off of https://github.com/cockroachdb/cockroach/issues/38436\r\n\r\nFor Movr, first we partition the users table:\r\n\r\n\r\n\r\nThen we add constraints:\r\n\r\n\r\n\r\n`SHOW CREATE TABLE` users currently displays:\r\n```\r\nroot@localhost:26257/movr> show create table users;\r\n  table_name |                      create_statement\r\n+------------+-------------------------------------------------------------+\r\n  users      | CREATE TABLE users (\r\n             |     id UUID NOT NULL,\r\n             |     city VARCHAR NOT NULL,\r\n             |     name VARCHAR NULL,\r\n             |     address VARCHAR NULL,\r\n             |     credit_card VARCHAR NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n             |     FAMILY ""primary"" (id, city, name, address, credit_card)\r\n             | ) PARTITION BY LIST (city) (\r\n             |     PARTITION new_york VALUES IN (('new york')),\r\n             |     PARTITION chicago VALUES IN (('chicago')),\r\n             |     PARTITION seattle VALUES IN (('seattle'))\r\n             | )\r\n(1 row)\r\n\r\nTime: 239.957759ms\r\n```\r\n\r\nPost adding in constraints I'm imagining:\r\n```\r\nroot@localhost:26257/movr> show create table users;\r\n  table_name |                      create_statement\r\n+------------+-------------------------------------------------------------+\r\n  users      | CREATE TABLE users (\r\n             |     id UUID NOT NULL,\r\n             |     city VARCHAR NOT NULL,\r\n             |     name VARCHAR NULL,\r\n             |     address VARCHAR NULL,\r\n             |     credit_card VARCHAR NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n             |     FAMILY ""primary"" (id, city, name, address, credit_card)\r\n             | ) PARTITION BY LIST (city) (\r\n             |     PARTITION new_york VALUES IN (('new york')),\r\n             |     PARTITION chicago VALUES IN (('chicago')),\r\n             |     PARTITION seattle VALUES IN (('seattle'))\r\n             | )\r\n(1 row)\r\n\r\nTime: 239.957759ms\r\n```\r\nWith some in-line comments that show constraints for the table or partitions.\r\n",C-enhancement|A-partitioning|A-sql-execution,rohany,"Splitting this off of https://github.com/cockroachdb/cockroach/issues/38436\r\n\r\nFor Movr, first we partition the users table:\r\n\r\n```SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\n\r\nThen we add constraints:\r\n\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-west1]';`) \r\n```\r\n\r\n`SHOW CREATE TABLE` users currently displays:\r\n```\r\nroot@localhost:26257/movr> show create table users;\r\n  table_name |                      create_statement\r\n+------------+-------------------------------------------------------------+\r\n  users      | CREATE TABLE users (\r\n             |     id UUID NOT NULL,\r\n             |     city VARCHAR NOT NULL,\r\n             |     name VARCHAR NULL,\r\n             |     address VARCHAR NULL,\r\n             |     credit_card VARCHAR NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n             |     FAMILY ""primary"" (id, city, name, address, credit_card)\r\n             | ) PARTITION BY LIST (city) (\r\n             |     PARTITION new_york VALUES IN (('new york')),\r\n             |     PARTITION chicago VALUES IN (('chicago')),\r\n             |     PARTITION seattle VALUES IN (('seattle'))\r\n             | )\r\n(1 row)\r\n\r\nTime: 239.957759ms\r\n```\r\n\r\nPost adding in constraints I'm imagining:\r\n```\r\nroot@localhost:26257/movr> show create table users;\r\n  table_name |                      create_statement\r\n+------------+-------------------------------------------------------------+\r\n  users      | CREATE TABLE users (\r\n             |     id UUID NOT NULL,\r\n             |     city VARCHAR NOT NULL,\r\n             |     name VARCHAR NULL,\r\n             |     address VARCHAR NULL,\r\n             |     credit_card VARCHAR NULL,\r\n             |     CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n             |     FAMILY ""primary"" (id, city, name, address, credit_card)\r\n             | ) PARTITION BY LIST (city) (\r\n             |     PARTITION new_york VALUES IN (('new york')),\r\n             |     PARTITION chicago VALUES IN (('chicago')),\r\n             |     PARTITION seattle VALUES IN (('seattle'))\r\n             | )\r\n(1 row)\r\n\r\nTime: 239.957759ms\r\n```\r\nWith some in-line comments that show constraints for the table or partitions.\r\n","SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
39075,"sql: show constraints doesn't show all constraintsFor Movr, first we partition the users table:\r\n\r\n\r\n\r\nThen we add constraints:\r\n\r\n\r\n\r\nIf you run `SHOW CONSTRAINTS FROM users` after adding constraints you will not see them:\r\n```\r\nroot@localhost:26257/movr> show constraints from users;\r\n  table_name | constraint_name | constraint_type |            details             | validated\r\n+------------+-----------------+-----------------+--------------------------------+-----------+\r\n  users      | primary         | PRIMARY KEY     | PRIMARY KEY (city ASC, id ASC) |   true\r\n(1 row)\r\n\r\nTime: 239.074341ms\r\n```",C-bug|S-3-ux-surprise|A-partitioning|A-sql-execution,rohany,"For Movr, first we partition the users table:\r\n\r\n```SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n```\r\n\r\nThen we add constraints:\r\n\r\n```SQL\r\nALTER PARTITION new_york OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-east1]'; ALTER PARTITION chicago OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-central1]'; ALTER PARTITION seattle OF TABLE movr.users CONFIGURE ZONE USING constraints='[+region=us-west1]';`) \r\n```\r\n\r\nIf you run `SHOW CONSTRAINTS FROM users` after adding constraints you will not see them:\r\n```\r\nroot@localhost:26257/movr> show constraints from users;\r\n  table_name | constraint_name | constraint_type |            details             | validated\r\n+------------+-----------------+-----------------+--------------------------------+-----------+\r\n  users      | primary         | PRIMARY KEY     | PRIMARY KEY (city ASC, id ASC) |   true\r\n(1 row)\r\n\r\nTime: 239.074341ms\r\n```","SQL\r\nALTER TABLE users PARTITION BY LIST (city) ( PARTITION new_york VALUES IN ('new york'), PARTITION chicago VALUES IN ('chicago'), PARTITION seattle VALUES IN ('seattle') );\r\n"
38818,"sql: fatal error: runtime: out of memory on specifying the order of aggregations **Describe the problem**\r\n\r\nI attempted to specify the order of aggregations (https://github.com/cockroachdb/cockroach/issues/23620)using MOVR data (https://github.com/cockroachdb/movr) and killed a node.\r\n![image](https://user-images.githubusercontent.com/22278911/61068865-08a87800-a3d9-11e9-9913-9b1fef893bb8.png)\r\n```\r\nfatal error: runtime: out of memory\r\n\r\nruntime stack:\r\nruntime.throw(0x395a6b2, 0x16)\r\n\t/usr/local/go/src/runtime/panic.go:617 +0x72 fp=0x7f26555fe958 sp=0x7f26555fe928 pc=0x8404a2\r\nruntime.sysMap(0xc704000000, 0x4000000, 0x6666358)\r\n\t/usr/local/go/src/runtime/mem_linux.go:170 +0xc7 fp=0x7f26555fe998 sp=0x7f26555fe958 pc=0x82aaf7\r\nruntime.(*mheap).sysAlloc(0x6621c60, 0x2000, 0x6621c70, 0x1)\r\n\t/usr/local/go/src/runtime/malloc.go:633 +0x1cd fp=0x7f26555fea40 sp=0x7f26555fe998 pc=0x81da7d\r\nruntime.(*mheap).grow(0x6621c60, 0x1, 0x0)\r\n\t/usr/local/go/src/runtime/mheap.go:1222 +0x42 fp=0x7f26555fea98 sp=0x7f26555fea40 pc=0x837e12\r\nruntime.(*mheap).allocSpanLocked(0x6621c60, 0x1, 0x6666368, 0x7f25afb6ceb8)\r\n\t/usr/local/go/src/runtime/mheap.go:1150 +0x37f fp=0x7f26555fead0 sp=0x7f26555fea98 pc=0x837cff\r\nruntime.(*mheap).alloc_m(0x6621c60, 0x1, 0x860006, 0x7f25afb6ceb8)\r\n\t/usr/local/go/src/runtime/mheap.go:977 +0xc2 fp=0x7f26555feb20 sp=0x7f26555fead0 pc=0x837352\r\nruntime.(*mheap).alloc.func1()\r\n\t/usr/local/go/src/runtime/mheap.go:1048 +0x4c fp=0x7f26555feb58 sp=0x7f26555feb20 pc=0x86baec\r\nruntime.systemstack(0x7f2670ea6750)\r\n\t/usr/local/go/src/runtime/asm_amd64.s:351 +0x66 fp=0x7f26555feb60 sp=0x7f26555feb58 pc=0x86de26\r\nruntime.mstart()\r\n\t/usr/local/go/src/runtime/proc.go:1153 fp=0x7f26555feb68 sp=0x7f26555feb60 pc=0x844a50\r\n```\r\n\r\n**To Reproduce**\r\nexport CLUSTER=andy-3node\r\nroachprod create $CLUSTER -n 4 --clouds=aws  --aws-machine-type-ssd=c5d.4xlarge\r\nroachprod run $CLUSTER --  ""sudo umount /mnt/data1; sudo mount -o discard,defaults,nobarrier /mnt/data1/; mount | grep /mnt/data1""\r\nroachprod stage $CLUSTER:1-3 cockroach \r\nroachprod stage $CLUSTER:4 workload\r\nroachprod start $CLUSTER:1-3\r\nroachprod adminurl --open $CLUSTER:1\r\nroachprod sql $CLUSTER:1\r\ncreate database movr;\r\nuse movr;\r\n``` SQL\r\nIMPORT TABLE users (\r\n        id UUID NOT NULL,\r\n        city VARCHAR NOT NULL,\r\n        name VARCHAR NULL,\r\n        address VARCHAR NULL,\r\n        credit_card VARCHAR NULL,\r\n        CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n        FAMILY ""primary"" (id, city, name, address, credit_card)\r\n)\r\nCSV DATA (\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.0.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.1.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.2.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.3.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.4.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.5.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.6.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.7.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.8.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.9.csv',\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.10.csv');\r\n```\r\n\r\n\r\nALTER TABLE vehicles ADD CONSTRAINT fk_city_ref_users FOREIGN KEY (city, owner_id) REFERENCES users (city, id);\r\nALTER TABLE rides ADD CONSTRAINT fk_city_ref_users FOREIGN KEY (city, rider_id) REFERENCES users (city, id);\r\nALTER TABLE rides ADD CONSTRAINT fk_vehicle_city_ref_vehicles FOREIGN KEY (vehicle_city, vehicle_id) REFERENCES vehicles (city, id);\r\nALTER TABLE vehicles VALIDATE CONSTRAINT fk_city_ref_users;\r\nALTER TABLE rides VALIDATE CONSTRAINT fk_city_ref_users;\r\nALTER TABLE rides VALIDATE CONSTRAINT fk_vehicle_city_ref_vehicles;\r\n\r\n\r\n**Environment:**\r\nv19.2.0-alpha.20190606-669-gbf2075c\r\nLog from the dead node:\r\n[cockroach.log](https://github.com/cockroachdb/cockroach/files/3383080/cockroach.log)\r\n\r\n",C-bug|A-sql-execution|S-2,yuzefovich,"**Describe the problem**\r\n\r\nI attempted to specify the order of aggregations (https://github.com/cockroachdb/cockroach/issues/23620)using MOVR data (https://github.com/cockroachdb/movr) and killed a node.\r\n![image](https://user-images.githubusercontent.com/22278911/61068865-08a87800-a3d9-11e9-9913-9b1fef893bb8.png)\r\n```\r\nfatal error: runtime: out of memory\r\n\r\nruntime stack:\r\nruntime.throw(0x395a6b2, 0x16)\r\n\t/usr/local/go/src/runtime/panic.go:617 +0x72 fp=0x7f26555fe958 sp=0x7f26555fe928 pc=0x8404a2\r\nruntime.sysMap(0xc704000000, 0x4000000, 0x6666358)\r\n\t/usr/local/go/src/runtime/mem_linux.go:170 +0xc7 fp=0x7f26555fe998 sp=0x7f26555fe958 pc=0x82aaf7\r\nruntime.(*mheap).sysAlloc(0x6621c60, 0x2000, 0x6621c70, 0x1)\r\n\t/usr/local/go/src/runtime/malloc.go:633 +0x1cd fp=0x7f26555fea40 sp=0x7f26555fe998 pc=0x81da7d\r\nruntime.(*mheap).grow(0x6621c60, 0x1, 0x0)\r\n\t/usr/local/go/src/runtime/mheap.go:1222 +0x42 fp=0x7f26555fea98 sp=0x7f26555fea40 pc=0x837e12\r\nruntime.(*mheap).allocSpanLocked(0x6621c60, 0x1, 0x6666368, 0x7f25afb6ceb8)\r\n\t/usr/local/go/src/runtime/mheap.go:1150 +0x37f fp=0x7f26555fead0 sp=0x7f26555fea98 pc=0x837cff\r\nruntime.(*mheap).alloc_m(0x6621c60, 0x1, 0x860006, 0x7f25afb6ceb8)\r\n\t/usr/local/go/src/runtime/mheap.go:977 +0xc2 fp=0x7f26555feb20 sp=0x7f26555fead0 pc=0x837352\r\nruntime.(*mheap).alloc.func1()\r\n\t/usr/local/go/src/runtime/mheap.go:1048 +0x4c fp=0x7f26555feb58 sp=0x7f26555feb20 pc=0x86baec\r\nruntime.systemstack(0x7f2670ea6750)\r\n\t/usr/local/go/src/runtime/asm_amd64.s:351 +0x66 fp=0x7f26555feb60 sp=0x7f26555feb58 pc=0x86de26\r\nruntime.mstart()\r\n\t/usr/local/go/src/runtime/proc.go:1153 fp=0x7f26555feb68 sp=0x7f26555feb60 pc=0x844a50\r\n```\r\n\r\n**To Reproduce**\r\nexport CLUSTER=andy-3node\r\nroachprod create $CLUSTER -n 4 --clouds=aws  --aws-machine-type-ssd=c5d.4xlarge\r\nroachprod run $CLUSTER --  ""sudo umount /mnt/data1; sudo mount -o discard,defaults,nobarrier /mnt/data1/; mount | grep /mnt/data1""\r\nroachprod stage $CLUSTER:1-3 cockroach \r\nroachprod stage $CLUSTER:4 workload\r\nroachprod start $CLUSTER:1-3\r\nroachprod adminurl --open $CLUSTER:1\r\nroachprod sql $CLUSTER:1\r\ncreate database movr;\r\nuse movr;\r\n``` SQL\r\nIMPORT TABLE users (\r\n        id UUID NOT NULL,\r\n        city VARCHAR NOT NULL,\r\n        name VARCHAR NULL,\r\n        address VARCHAR NULL,\r\n        credit_card VARCHAR NULL,\r\n        CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n        FAMILY ""primary"" (id, city, name, address, credit_card)\r\n)\r\nCSV DATA (\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.0.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.1.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.2.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.3.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.4.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.5.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.6.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.7.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.8.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.9.csv',\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/users/n1.10.csv');\r\n```\r\n```SQL\r\nIMPORT TABLE vehicles (\r\n        id UUID NOT NULL,\r\n        city VARCHAR NOT NULL,\r\n        type VARCHAR NULL,\r\n        owner_id UUID NULL,\r\n        creation_time TIMESTAMP NULL,\r\n        status VARCHAR NULL,\r\n        current_location VARCHAR NULL,\r\n        ext JSONB NULL,\r\n        CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n        INDEX vehicles_auto_index_fk_city_ref_users (city ASC, owner_id ASC),\r\n        INVERTED INDEX ix_vehicle_ext (ext),\r\n        FAMILY ""primary"" (id, city, type, owner_id, creation_time, status, current_location, ext)\r\n)                                                                                                                                                                \r\nCSV DATA ('https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/vehicles/n1.0.csv');\r\n```\r\n```SQL\r\nIMPORT TABLE rides (\r\n        id UUID NOT NULL,\r\n        city VARCHAR NOT NULL,\r\n        vehicle_city VARCHAR NULL,\r\n        rider_id UUID NULL,\r\n        vehicle_id UUID NULL,\r\n        start_address VARCHAR NULL,\r\n        end_address VARCHAR NULL,\r\n        start_time TIMESTAMP NULL,\r\n        end_time TIMESTAMP NULL,\r\n        revenue DECIMAL(10,2) NULL,\r\n        CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n        INDEX rides_auto_index_fk_city_ref_users (city ASC, rider_id ASC),\r\n        INDEX rides_auto_index_fk_vehicle_city_ref_vehicles (vehicle_city ASC, vehicle_id ASC),\r\n        FAMILY ""primary"" (id, city, vehicle_city, rider_id, vehicle_id, start_address, end_address, start_time, end_time, revenue),\r\n        CONSTRAINT check_vehicle_city_city CHECK (vehicle_city = city)\r\n) \r\nCSV DATA (\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.0.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.1.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.2.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.3.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.4.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.5.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.6.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.7.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.8.csv', \r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.9.csv',\r\n'https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/rides/n1.10.csv');\r\n```\r\nALTER TABLE vehicles ADD CONSTRAINT fk_city_ref_users FOREIGN KEY (city, owner_id) REFERENCES users (city, id);\r\nALTER TABLE rides ADD CONSTRAINT fk_city_ref_users FOREIGN KEY (city, rider_id) REFERENCES users (city, id);\r\nALTER TABLE rides ADD CONSTRAINT fk_vehicle_city_ref_vehicles FOREIGN KEY (vehicle_city, vehicle_id) REFERENCES vehicles (city, id);\r\nALTER TABLE vehicles VALIDATE CONSTRAINT fk_city_ref_users;\r\nALTER TABLE rides VALIDATE CONSTRAINT fk_city_ref_users;\r\nALTER TABLE rides VALIDATE CONSTRAINT fk_vehicle_city_ref_vehicles;\r\n```SQL\r\nSELECT array_agg(city ORDER BY revenue), array_agg(city\r\nORDER BY city) FROM rides limit 5;\r\n```\r\n\r\n**Environment:**\r\nv19.2.0-alpha.20190606-669-gbf2075c\r\nLog from the dead node:\r\n[cockroach.log](https://github.com/cockroachdb/cockroach/files/3383080/cockroach.log)\r\n\r\n","SQL\r\nIMPORT TABLE vehicles (\r\n        id UUID NOT NULL,\r\n        city VARCHAR NOT NULL,\r\n        type VARCHAR NULL,\r\n        owner_id UUID NULL,\r\n        creation_time TIMESTAMP NULL,\r\n        status VARCHAR NULL,\r\n        current_location VARCHAR NULL,\r\n        ext JSONB NULL,\r\n        CONSTRAINT ""primary"" PRIMARY KEY (city ASC, id ASC),\r\n        INDEX vehicles_auto_index_fk_city_ref_users (city ASC, owner_id ASC),\r\n        INVERTED INDEX ix_vehicle_ext (ext),\r\n        FAMILY ""primary"" (id, city, type, owner_id, creation_time, status, current_location, ext)\r\n)                                                                                                                                                                \r\nCSV DATA ('https://s3-us-west-1.amazonaws.com/cockroachdb-movr/datasets/movr-1m/vehicles/n1.0.csv');\r\n"
38778,"Node pegs CPU after slow latch acquisitions grew**Describe the problem**\r\n\r\nAfter ingesting some sample data, one of the CockroachDB nodes starts eating CPU and while some SQL queries can succeed, others hang forever.\r\n\r\n**To Reproduce**\r\n\r\nI'm experimenting with a 6 node cluster on 3 physical machines (two nodes per machines to see foreign keys' impact).\r\nI set up some tables and started to insert data. After some time, the queries hang and CRDB processes settle (using minimal CPU) except one, which spins on one CPU (6277, 6276 is the other node on the machine):\r\n\r\n```\r\n  PID USERNAME       THR PRI NICE   SIZE    RES STATE   C   TIME    WCPU COMMAND\r\n 6277 root            75  20    0  1488M  1348M uwait  16 686:38 100.49% cockroachoss\r\n 6276 root            76  20    0  1344M  1223M uwait   6 486:17   0.78% cockroachoss\r\n```\r\n\r\nI've just started experimenting with CRDB but could see different errors. This is one of them, so I'm not sure I can reproduce this.\r\n\r\nAnyways, the ingest connects to all nodes on 256 threads total (equal number of connections and the queries are load balanced between them).\r\n\r\nThe tables are:\r\n\r\n\r\nData is inserted with the following python functions:\r\n\r\n\r\nThese are called in the threads with the helper from here: https://www.cockroachlabs.com/docs/stable/build-a-python-app-with-cockroachdb.html#transaction-with-retry-logic\r\n\r\n**Expected behavior**\r\nI would expect CRDB not to hang up.\r\n\r\n**Additional data / screenshots**\r\nScreenshot showing how much queries could succeed:\r\n![image](https://user-images.githubusercontent.com/820331/60922204-861a9e00-a29c-11e9-9348-756678668737.png)\r\nand a big spike in service latency and everything stops.\r\nThe app didn't get back anything, the queries just hung around.\r\nWhen I stopped the app and restarted, some new queries can succeed, then the same happens (as it can be seen on the right side).\r\n\r\nThe graphs show a big jump in goroutine count then a big drop in GC count (no further activity):\r\n![image](https://user-images.githubusercontent.com/820331/60922415-f0cbd980-a29c-11e9-8516-e6addb296222.png)\r\n\r\nThere's a spike in distributed queries as well:\r\n![image](https://user-images.githubusercontent.com/820331/60922497-2670c280-a29d-11e9-952e-3cf9f0786bca.png)\r\n\r\nLive bytes grow steadily:\r\n![image](https://user-images.githubusercontent.com/820331/60922620-6b94f480-a29d-11e9-8797-71d0da4089ae.png)\r\n\r\nRanges:\r\n![image](https://user-images.githubusercontent.com/820331/60922694-954e1b80-a29d-11e9-9353-fd5d9a79c2a7.png)\r\n\r\n![image](https://user-images.githubusercontent.com/820331/60922740-b1ea5380-a29d-11e9-8443-1507ec4c0001.png)\r\n\r\nCompaction?\r\n![image](https://user-images.githubusercontent.com/820331/60922828-e4944c00-a29d-11e9-9287-4dc6c1ab2ef5.png)\r\n\r\nAnd slow latch acquisitions:\r\n![image](https://user-images.githubusercontent.com/820331/60922877-fa097600-a29d-11e9-9b94-3d8c022f4b56.png)\r\n\r\nNode constantly logs:\r\n```\r\nW190709 21:11:02.706159 175 server/node.go:880  [n2,summaries] health alerts detected: {Alerts:[{StoreID:2 Category:METRICS Description:requests.slow.latch Value:254 XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}] XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}\r\n```\r\n\r\nStarting with this one:\r\n```\r\nW190709 16:19:42.699317 175 server/node.go:880  [n2,summaries] health alerts detected: {Alerts:[{StoreID:2 Category:METRICS Description:requests.slow.latch Value:508 XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}] XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}\r\n```\r\n\r\nOn the node's goroutine page (http://localhost:8888/debug/pprof/ui/goroutine/1/peek) I can see:\r\nhttps://gist.github.com/bra-fsn/ec41cb66a16e93cc97cf36d5b29d941d\r\n\r\nOn the profile page:\r\nhttps://gist.github.com/bra-fsn/e6f4ba6892520e39cd81f1245fa7cd17\r\n\r\nAfter restarting the problematic node, everything works fine again.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1.2\r\n - Server OS: FreeBSD 11/amd64\r\n - Client app python/psycopg2\r\n",O-community,ajkr,"**Describe the problem**\r\n\r\nAfter ingesting some sample data, one of the CockroachDB nodes starts eating CPU and while some SQL queries can succeed, others hang forever.\r\n\r\n**To Reproduce**\r\n\r\nI'm experimenting with a 6 node cluster on 3 physical machines (two nodes per machines to see foreign keys' impact).\r\nI set up some tables and started to insert data. After some time, the queries hang and CRDB processes settle (using minimal CPU) except one, which spins on one CPU (6277, 6276 is the other node on the machine):\r\n\r\n```\r\n  PID USERNAME       THR PRI NICE   SIZE    RES STATE   C   TIME    WCPU COMMAND\r\n 6277 root            75  20    0  1488M  1348M uwait  16 686:38 100.49% cockroachoss\r\n 6276 root            76  20    0  1344M  1223M uwait   6 486:17   0.78% cockroachoss\r\n```\r\n\r\nI've just started experimenting with CRDB but could see different errors. This is one of them, so I'm not sure I can reproduce this.\r\n\r\nAnyways, the ingest connects to all nodes on 256 threads total (equal number of connections and the queries are load balanced between them).\r\n\r\nThe tables are:\r\n```sql\r\n    CREATE TABLE objects (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \tid INT8 NOT NULL,\r\n    \tsize INT8 NULL,\r\n    \tdisk_size INT8 NOT NULL,\r\n    \tcompression INT8 NULL,\r\n    \tatime INT8 NOT NULL DEFAULT CAST(now():::TIMESTAMPTZ AS INT8),\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, id ASC),\r\n    \tINDEX ix_objects_atime (atime ASC),\r\n    \tFAMILY ""primary"" (store, hash, id, size, disk_size, compression, atime)\r\n    )\r\n    CREATE TABLE object_locations (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \tid INT8 NOT NULL,\r\n    \tdisk INT8 NOT NULL,\r\n    \tpos INT8 NOT NULL,\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, id ASC, disk ASC),\r\n    \tINDEX ix_object_locations_disk (disk ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, id) REFERENCES objects (store, hash, id) ON DELETE CASCADE,\r\n    \tFAMILY ""primary"" (store, hash, id, disk, pos)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, id)\r\n    CREATE TABLE names (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \toid INT8 NOT NULL,\r\n    \tbucket INT8 NOT NULL,\r\n    \tid VARCHAR NOT NULL,\r\n    \thttp_headers JSONB NULL,\r\n    \tipacl JSONB NULL,\r\n    \tmax_downloads INT8 NULL,\r\n    \tcounter INT8 NOT NULL DEFAULT 0:::INT8,\r\n    \tatime INT8 NOT NULL DEFAULT CAST(now():::TIMESTAMPTZ AS INT8),\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, oid ASC, bucket ASC, id ASC),\r\n    \tINDEX ix_n_b_i (bucket ASC, id ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, oid) REFERENCES objects (store, hash, id),\r\n    \tFAMILY ""primary"" (store, hash, oid, bucket, id, http_headers, ipacl, max_downloads, counter, atime)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, oid)\r\n    CREATE TABLE object_actions (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \toid INT8 NOT NULL,\r\n    \tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n    \tdate TIMESTAMP NOT NULL DEFAULT current_timestamp():::TIMESTAMP,\r\n    \texecute_after TIMESTAMP NOT NULL DEFAULT current_timestamp():::TIMESTAMP,\r\n    \taction VARCHAR NOT NULL,\r\n    \tdata JSONB NULL,\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, oid ASC, id ASC),\r\n    \tINDEX ix_object_actions_execute_after (execute_after ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, oid) REFERENCES objects (store, hash, id) ON DELETE CASCADE,\r\n    \tFAMILY ""primary"" (store, hash, oid, id, date, execute_after, action, data)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, oid)\r\n```\r\n\r\nData is inserted with the following python functions:\r\n```python\r\ndef insert_object_iv(conn, store_id, object_hash, bucket_id, name, size, disks, offsets):\r\n    # with conn:\r\n        with conn.cursor() as cur:\r\n            sql = """"""insert into objects (store, hash, id, size, disk_size)\r\n                select %(store)s, %(hash)s, ifnull(max(id)+1,0), %(size)s, %(disk_size)s\r\n            from objects where store=%(store)s and hash=%(hash)s returning id;""""""\r\n            cur.execute(sql, {'store': store_id, 'hash': psycopg2.Binary(object_hash), 'size': size, 'disk_size': size})\r\n            object_id = cur.fetchone()[0]\r\n            if object_id:\r\n                # if object_id is not 0, schedule an out of band dedup\r\n                action = 'dedup'\r\n                sql = ""insert into object_actions (store, hash, oid, action) values (%s, %s, %s, %s)""\r\n                cur.execute(sql, (store_id, psycopg2.Binary(object_hash), object_id, action))\r\n\r\n            sql = ""insert into names (store, hash, oid, bucket, id) values (%s, %s, %s, %s, %s)""\r\n            cur.execute(sql, (store_id, psycopg2.Binary(object_hash), object_id, bucket_id, name))\r\n\r\n            tup = [[store_id, psycopg2.Binary(object_hash), object_id, disk, offset] for disk, offset in offsets.items()]\r\n            args_str = b','.join(cur.mogrify(""(%s,%s,%s,%s,%s)"", x) for x in tup)\r\n            sql = b""insert into object_locations (store, hash, id, disk, pos) values "" + args_str\r\n            cur.execute(sql)\r\n\r\n\r\ndef delete_object_iv(conn, store, bucket, name):\r\n    # with conn:\r\n        with conn.cursor() as cur:\r\n            sql = """"""delete from names where bucket=%(bucket)s and id=%(name)s returning store, hash, oid""""""\r\n            cur.execute(sql, {'store': store, 'bucket': bucket, 'name': name})\r\n            res = cur.fetchone()\r\n            if not res:\r\n                return\r\n            store, object_hash, object_id = res\r\n            sql = ""delete from objects where store=%(store)s and hash=%(hash)s and id=%(oid)s and not exists (select 1 from names where store=%(store)s and hash=%(hash)s and oid=%(oid)s)""\r\n            cur.execute(sql, {'store': store, 'hash': object_hash, 'oid': object_id})\r\n```\r\n\r\nThese are called in the threads with the helper from here: https://www.cockroachlabs.com/docs/stable/build-a-python-app-with-cockroachdb.html#transaction-with-retry-logic\r\n\r\n**Expected behavior**\r\nI would expect CRDB not to hang up.\r\n\r\n**Additional data / screenshots**\r\nScreenshot showing how much queries could succeed:\r\n![image](https://user-images.githubusercontent.com/820331/60922204-861a9e00-a29c-11e9-9348-756678668737.png)\r\nand a big spike in service latency and everything stops.\r\nThe app didn't get back anything, the queries just hung around.\r\nWhen I stopped the app and restarted, some new queries can succeed, then the same happens (as it can be seen on the right side).\r\n\r\nThe graphs show a big jump in goroutine count then a big drop in GC count (no further activity):\r\n![image](https://user-images.githubusercontent.com/820331/60922415-f0cbd980-a29c-11e9-8516-e6addb296222.png)\r\n\r\nThere's a spike in distributed queries as well:\r\n![image](https://user-images.githubusercontent.com/820331/60922497-2670c280-a29d-11e9-952e-3cf9f0786bca.png)\r\n\r\nLive bytes grow steadily:\r\n![image](https://user-images.githubusercontent.com/820331/60922620-6b94f480-a29d-11e9-8797-71d0da4089ae.png)\r\n\r\nRanges:\r\n![image](https://user-images.githubusercontent.com/820331/60922694-954e1b80-a29d-11e9-9353-fd5d9a79c2a7.png)\r\n\r\n![image](https://user-images.githubusercontent.com/820331/60922740-b1ea5380-a29d-11e9-8443-1507ec4c0001.png)\r\n\r\nCompaction?\r\n![image](https://user-images.githubusercontent.com/820331/60922828-e4944c00-a29d-11e9-9287-4dc6c1ab2ef5.png)\r\n\r\nAnd slow latch acquisitions:\r\n![image](https://user-images.githubusercontent.com/820331/60922877-fa097600-a29d-11e9-9b94-3d8c022f4b56.png)\r\n\r\nNode constantly logs:\r\n```\r\nW190709 21:11:02.706159 175 server/node.go:880  [n2,summaries] health alerts detected: {Alerts:[{StoreID:2 Category:METRICS Description:requests.slow.latch Value:254 XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}] XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}\r\n```\r\n\r\nStarting with this one:\r\n```\r\nW190709 16:19:42.699317 175 server/node.go:880  [n2,summaries] health alerts detected: {Alerts:[{StoreID:2 Category:METRICS Description:requests.slow.latch Value:508 XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}] XXX_NoUnkeyedLiteral:{} XXX_sizecache:0}\r\n```\r\n\r\nOn the node's goroutine page (http://localhost:8888/debug/pprof/ui/goroutine/1/peek) I can see:\r\nhttps://gist.github.com/bra-fsn/ec41cb66a16e93cc97cf36d5b29d941d\r\n\r\nOn the profile page:\r\nhttps://gist.github.com/bra-fsn/e6f4ba6892520e39cd81f1245fa7cd17\r\n\r\nAfter restarting the problematic node, everything works fine again.\r\n\r\n**Environment:**\r\n - CockroachDB version 19.1.2\r\n - Server OS: FreeBSD 11/amd64\r\n - Client app python/psycopg2\r\n","sql\r\n    CREATE TABLE objects (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \tid INT8 NOT NULL,\r\n    \tsize INT8 NULL,\r\n    \tdisk_size INT8 NOT NULL,\r\n    \tcompression INT8 NULL,\r\n    \tatime INT8 NOT NULL DEFAULT CAST(now():::TIMESTAMPTZ AS INT8),\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, id ASC),\r\n    \tINDEX ix_objects_atime (atime ASC),\r\n    \tFAMILY ""primary"" (store, hash, id, size, disk_size, compression, atime)\r\n    )\r\n    CREATE TABLE object_locations (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \tid INT8 NOT NULL,\r\n    \tdisk INT8 NOT NULL,\r\n    \tpos INT8 NOT NULL,\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, id ASC, disk ASC),\r\n    \tINDEX ix_object_locations_disk (disk ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, id) REFERENCES objects (store, hash, id) ON DELETE CASCADE,\r\n    \tFAMILY ""primary"" (store, hash, id, disk, pos)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, id)\r\n    CREATE TABLE names (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \toid INT8 NOT NULL,\r\n    \tbucket INT8 NOT NULL,\r\n    \tid VARCHAR NOT NULL,\r\n    \thttp_headers JSONB NULL,\r\n    \tipacl JSONB NULL,\r\n    \tmax_downloads INT8 NULL,\r\n    \tcounter INT8 NOT NULL DEFAULT 0:::INT8,\r\n    \tatime INT8 NOT NULL DEFAULT CAST(now():::TIMESTAMPTZ AS INT8),\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, oid ASC, bucket ASC, id ASC),\r\n    \tINDEX ix_n_b_i (bucket ASC, id ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, oid) REFERENCES objects (store, hash, id),\r\n    \tFAMILY ""primary"" (store, hash, oid, bucket, id, http_headers, ipacl, max_downloads, counter, atime)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, oid)\r\n    CREATE TABLE object_actions (\r\n    \tstore INT8 NOT NULL,\r\n    \thash BYTES NOT NULL,\r\n    \toid INT8 NOT NULL,\r\n    \tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n    \tdate TIMESTAMP NOT NULL DEFAULT current_timestamp():::TIMESTAMP,\r\n    \texecute_after TIMESTAMP NOT NULL DEFAULT current_timestamp():::TIMESTAMP,\r\n    \taction VARCHAR NOT NULL,\r\n    \tdata JSONB NULL,\r\n    \tCONSTRAINT ""primary"" PRIMARY KEY (store ASC, hash ASC, oid ASC, id ASC),\r\n    \tINDEX ix_object_actions_execute_after (execute_after ASC),\r\n    \tCONSTRAINT fk_store_ref_objects FOREIGN KEY (store, hash, oid) REFERENCES objects (store, hash, id) ON DELETE CASCADE,\r\n    \tFAMILY ""primary"" (store, hash, oid, id, date, execute_after, action, data)\r\n    ) INTERLEAVE IN PARENT objects (store, hash, oid)\r\n"
38768,"DDL statement on one table prevents stmt on another unrelated table in same transaction# Describe the problem\r\n\r\nInside a single transaction, DDL statements about one table prevent modifications on another totally unrelated table. \r\n\r\n**This is a regression that happened somewhere between 2.1.4 (which worked) and 19.1.2 (which exhibits the bug).**\r\n\r\n# To Reproduce\r\n\r\nPut the following in a `repro.sql` file:\r\n\r\n\r\n\r\nThen execute it on Cockroach 19.1.2:\r\n\r\n\r\n\r\nThis will fail with the following message: `pq: relation ""test.public.a"" does not exist`\r\n\r\nThe error is raised when executing the DROP INDEX statement in the second transaction. But tables `a` and `b` have no relation whatsoever so an operation on `a` shouldn't prevent an operation on `b`.\r\n\r\nThe error message is also very confusing: the mention of `b` makes it seem that the error happens during the DROP TABLE statement, but it is actually about the DROP INDEX statement.\r\n\r\nCurrently it is necessary to split the second transaction into 2 different transactions for it to execute properly.\r\n\r\nIf executed on a Cockroach 2.1.4 server, the execution suceeds.\r\n\r\n# Environment:\r\n - CockroachDB version 19.1.2\r\n - Server OS: macOS 10.14.5\r\n - Client app: `cockroach sql`\r\n",C-investigation,pbardea,"# Describe the problem\r\n\r\nInside a single transaction, DDL statements about one table prevent modifications on another totally unrelated table. \r\n\r\n**This is a regression that happened somewhere between 2.1.4 (which worked) and 19.1.2 (which exhibits the bug).**\r\n\r\n# To Reproduce\r\n\r\nPut the following in a `repro.sql` file:\r\n\r\n```sql\r\nBEGIN;\r\n\r\nCREATE TABLE a ();\r\n\r\nCREATE TABLE b (\r\n    key INT\r\n);\r\nCREATE INDEX b_idx ON b (key);\r\n\r\nCOMMIT;\r\n\r\n--------------------------------------------\r\n\r\nBEGIN;\r\n\r\nDROP TABLE a;\r\nDROP INDEX b_idx CASCADE;\r\n\r\nCOMMIT;\r\n```\r\n\r\nThen execute it on Cockroach 19.1.2:\r\n\r\n```bash\r\ncockroach sql --insecure -e ""drop database test; create database test;"" && cockroach sql --insecure -d test < repro.sql\r\n```\r\n\r\nThis will fail with the following message: `pq: relation ""test.public.a"" does not exist`\r\n\r\nThe error is raised when executing the DROP INDEX statement in the second transaction. But tables `a` and `b` have no relation whatsoever so an operation on `a` shouldn't prevent an operation on `b`.\r\n\r\nThe error message is also very confusing: the mention of `b` makes it seem that the error happens during the DROP TABLE statement, but it is actually about the DROP INDEX statement.\r\n\r\nCurrently it is necessary to split the second transaction into 2 different transactions for it to execute properly.\r\n\r\nIf executed on a Cockroach 2.1.4 server, the execution suceeds.\r\n\r\n# Environment:\r\n - CockroachDB version 19.1.2\r\n - Server OS: macOS 10.14.5\r\n - Client app: `cockroach sql`\r\n",sql\r\nBEGIN;\r\n\r\nCREATE TABLE a ();\r\n\r\nCREATE TABLE b (\r\n    key INT\r\n);\r\nCREATE INDEX b_idx ON b (key);\r\n\r\nCOMMIT;\r\n\r\n--------------------------------------------\r\n\r\nBEGIN;\r\n\r\nDROP TABLE a;\r\nDROP INDEX b_idx CASCADE;\r\n\r\nCOMMIT;\r\n
38758,"Listening URL file is written after process is backgrounded**Describe the problem**\r\nWhen running CockroachDB with the `--background` and `--listening-url-file` flags, the listening URL file isn't written until *after* control is returned to the shell. This makes it awkward to actually read the URL from the file, which makes `--background` less useful (especially in tests, where I'm starting databases on ephemeral ports).\r\n\r\nIt's easy to work around this by retrying until the file appears, but that's a bit distasteful.\r\n\r\n**To Reproduce**\r\nThis bash script fails reliably on my machine:\r\n\r\n\r\n**Expected behavior**\r\nI expected the listening URL file to be written out before control is returned to the shell. If that's not possible, it'd be nice to clarify the `cockroach start --help` output.\r\n\r\n**Additional data / screenshots**\r\nNone.\r\n\r\n**Environment:**\r\n - CockroachDB version: 19.1.1\r\n```sh\r\n$ cockroach version\r\nBuild Tag:    v19.1.1\r\nBuild Time:   2019/05/15 20:21:46\r\nDistribution: CCL\r\nPlatform:     darwin amd64 (x86_64-apple-darwin14)\r\nGo Version:   go1.11.6\r\nC Compiler:   4.2.1 Compatible Clang 3.8.0 (tags/RELEASE_380/final)\r\nBuild SHA-1:  c8bda1de440cfe90cf23a433119d77795cfa0047\r\nBuild Type:   release\r\n```\r\n - Server OS: MacOS\r\n - Client app: `cockroach` CLI\r\n\r\n**Additional context**\r\nThis doesn't have any production impact, but it forces me to choose between flaky tests or an ugly workaround.",S-3-ux-surprise|O-support,knz,"**Describe the problem**\r\nWhen running CockroachDB with the `--background` and `--listening-url-file` flags, the listening URL file isn't written until *after* control is returned to the shell. This makes it awkward to actually read the URL from the file, which makes `--background` less useful (especially in tests, where I'm starting databases on ephemeral ports).\r\n\r\nIt's easy to work around this by retrying until the file appears, but that's a bit distasteful.\r\n\r\n**To Reproduce**\r\nThis bash script fails reliably on my machine:\r\n```bash\r\n#!/usr/bin/env bash\r\nset -euo pipefail\r\n\r\nrm -f connfile\r\ncockroach start --background --insecure --listening-url-file connfile >/dev/null 1>&2\r\ncat connfile > /dev/null\r\n```\r\n\r\n**Expected behavior**\r\nI expected the listening URL file to be written out before control is returned to the shell. If that's not possible, it'd be nice to clarify the `cockroach start --help` output.\r\n\r\n**Additional data / screenshots**\r\nNone.\r\n\r\n**Environment:**\r\n - CockroachDB version: 19.1.1\r\n```sh\r\n$ cockroach version\r\nBuild Tag:    v19.1.1\r\nBuild Time:   2019/05/15 20:21:46\r\nDistribution: CCL\r\nPlatform:     darwin amd64 (x86_64-apple-darwin14)\r\nGo Version:   go1.11.6\r\nC Compiler:   4.2.1 Compatible Clang 3.8.0 (tags/RELEASE_380/final)\r\nBuild SHA-1:  c8bda1de440cfe90cf23a433119d77795cfa0047\r\nBuild Type:   release\r\n```\r\n - Server OS: MacOS\r\n - Client app: `cockroach` CLI\r\n\r\n**Additional context**\r\nThis doesn't have any production impact, but it forces me to choose between flaky tests or an ugly workaround.",bash\r\n#!/usr/bin/env bash\r\nset -euo pipefail\r\n\r\nrm -f connfile\r\ncockroach start --background --insecure --listening-url-file connfile >/dev/null 1>&2\r\ncat connfile > /dev/null\r\n
38436,sql: SHOW PARTITIONS FOR <table> or <index> or <database>**Table**\r\nIt is not documented but users can run: \r\n``` SQL\r\nSelect * from crdb_internal.partitions \r\njoin crdb_internal.tables on partitions.table_id=tables.table_id \r\nwhere tables.name='mytable'\r\n```\r\n**Database**\r\nIt is not documented but users can run: \r\n\r\n\r\nWe should make SHOW statements that allow for users to see this information easily.,C-enhancement|A-partitioning|A-sql-syntax,rohany,**Table**\r\nIt is not documented but users can run: \r\n``` SQL\r\nSelect * from crdb_internal.partitions \r\njoin crdb_internal.tables on partitions.table_id=tables.table_id \r\nwhere tables.name='mytable'\r\n```\r\n**Database**\r\nIt is not documented but users can run: \r\n```SQL\r\nSelect partitions.name from crdb_internal.partitions \r\njoin crdb_internal.tables on partitions.table_id=tables.table_id \r\nwhere database_name='mydb\r\n```\r\n\r\nWe should make SHOW statements that allow for users to see this information easily.,SQL\r\nSelect partitions.name from crdb_internal.partitions \r\njoin crdb_internal.tables on partitions.table_id=tables.table_id \r\nwhere database_name='mydb\r\n
37866,"storage: unexpected COMMITTED state, likely due to parallel commitsWe've seen a few instances of [this assertion](https://github.com/cockroachdb/cockroach/blob/db9c1217a6967fcac2d135cf0f24a4265dc76d77/pkg/kv/txn_coord_sender.go#L912-L914) fire because a TxnCoordSender in the `txnPending` state has a COMMITTED transaction proto. The assertion failure looks like:\r\n```\r\nunexpected txn state: ""sql txn"" id=2fb6a2e8 key=/Table/60/1/1875/0 rw=true pri=0.00955542 stat=COMMITTED epo=1 ts=1558613168.062142550,0 orig=1558613168.062142550,0 max=1558613168.562142550,0 wto=false seq=7 int=6\r\n```\r\n\r\nI've been able to reproduce this once by running TPC-C. When attempting to do so, I also saw a `TransactionStatusError(""already committed"")` error from [cmd_end_transaction.go:211](https://github.com/cockroachdb/cockroach/blob/db9c1217a6967fcac2d135cf0f24a4265dc76d77/pkg/storage/batcheval/cmd_end_transaction.go#L210-L211).\r\n\r\nIt's likely that these errors have the same underlying cause. Somehow a transaction record is being `COMMITTED` when a transaction coordinator does not expect it to be, which is probably related to parallel commits somehow. The first error comes when the coordinator's heartbeat finds the `COMMITTED` record and the second error comes when an EndTransactionRequest finds the `COMMITTED` record. The cause of the unexpected `COMMITTED` status is currently unknown.\r\n\r\nI've [downgraded](https://github.com/cockroachdb/cockroach/pull/37867) the assertion to an error so that it is easier to debug in roachtest failures. Past that point, I won't be around for the next week to do too much debugging and track down the root cause. If this causes enough instability that we deem it necessary to temporarily disable parallel commits, here's the patch to do so (cc. @tbg):\r\n<details><summary>Click to expand</summary>\r\n\r\n\r\n\r\n</details>",C-bug|A-kv-client,nvanbenschoten,"We've seen a few instances of [this assertion](https://github.com/cockroachdb/cockroach/blob/db9c1217a6967fcac2d135cf0f24a4265dc76d77/pkg/kv/txn_coord_sender.go#L912-L914) fire because a TxnCoordSender in the `txnPending` state has a COMMITTED transaction proto. The assertion failure looks like:\r\n```\r\nunexpected txn state: ""sql txn"" id=2fb6a2e8 key=/Table/60/1/1875/0 rw=true pri=0.00955542 stat=COMMITTED epo=1 ts=1558613168.062142550,0 orig=1558613168.062142550,0 max=1558613168.562142550,0 wto=false seq=7 int=6\r\n```\r\n\r\nI've been able to reproduce this once by running TPC-C. When attempting to do so, I also saw a `TransactionStatusError(""already committed"")` error from [cmd_end_transaction.go:211](https://github.com/cockroachdb/cockroach/blob/db9c1217a6967fcac2d135cf0f24a4265dc76d77/pkg/storage/batcheval/cmd_end_transaction.go#L210-L211).\r\n\r\nIt's likely that these errors have the same underlying cause. Somehow a transaction record is being `COMMITTED` when a transaction coordinator does not expect it to be, which is probably related to parallel commits somehow. The first error comes when the coordinator's heartbeat finds the `COMMITTED` record and the second error comes when an EndTransactionRequest finds the `COMMITTED` record. The cause of the unexpected `COMMITTED` status is currently unknown.\r\n\r\nI've [downgraded](https://github.com/cockroachdb/cockroach/pull/37867) the assertion to an error so that it is easier to debug in roachtest failures. Past that point, I won't be around for the next week to do too much debugging and track down the root cause. If this causes enough instability that we deem it necessary to temporarily disable parallel commits, here's the patch to do so (cc. @tbg):\r\n<details><summary>Click to expand</summary>\r\n\r\n```go\r\ndiff --git a/pkg/kv/dist_sender_server_test.go b/pkg/kv/dist_sender_server_test.go\r\nindex 708ae2c762..0f269569bf 100644\r\n--- a/pkg/kv/dist_sender_server_test.go\r\n+++ b/pkg/kv/dist_sender_server_test.go\r\n@@ -2442,7 +2442,7 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t// Parallel commits do not support the canForwardSerializableTimestamp\r\n \t\t\t// optimization. That's ok because we need to removed that optimization\r\n \t\t\t// anyway. See #36431.\r\n-\t\t\ttxnCoordRetry: true,\r\n+\t\t\ttxnCoordRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with forwarded timestamp and cput"",\r\n@@ -2478,9 +2478,10 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t\tb.Put(""c"", ""put"")\r\n \t\t\t\treturn txn.CommitInBatch(ctx, b) // both puts will succeed, et will retry from get\r\n \t\t\t},\r\n+\t\t\ttxnCoordRetry: true,\r\n \t\t\t// Client-side retry required as this will be a mixed success due\r\n \t\t\t// to parallel commits.\r\n-\t\t\tclientRetry: true,\r\n+\t\t\tclientRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with forwarded timestamp and cput and delete range"",\r\n@@ -2513,7 +2514,7 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t// Parallel commits do not support the canForwardSerializableTimestamp\r\n \t\t\t// optimization. That's ok because we need to removed that optimization\r\n \t\t\t// anyway. See #36431.\r\n-\t\t\ttxnCoordRetry: true,\r\n+\t\t\ttxnCoordRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with write too old and failed cput"",\r\n@@ -2627,9 +2628,10 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t\treturn txn.CommitInBatch(ctx, b)\r\n \t\t\t},\r\n \t\t\tfilter: newUncertaintyFilter(roachpb.Key([]byte(""c""))),\r\n+\t\t\ttxnCoordRetry: true,\r\n \t\t\t// Client-side retry required as this will be a mixed success due\r\n \t\t\t// to parallel commits.\r\n-\t\t\tclientRetry: true,\r\n+\t\t\tclientRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with uncertainty interval error and get conflict"",\r\ndiff --git a/pkg/kv/txn_interceptor_committer.go b/pkg/kv/txn_interceptor_committer.go\r\nindex 8860cca99d..cccd182457 100644\r\n--- a/pkg/kv/txn_interceptor_committer.go\r\n+++ b/pkg/kv/txn_interceptor_committer.go\r\n@@ -28,7 +28,7 @@ import (\r\n var parallelCommitsEnabled = settings.RegisterBoolSetting(\r\n \t""kv.transaction.parallel_commits_enabled"",\r\n \t""if enabled, transactional commits will be parallelized with transactional writes"",\r\n-\ttrue,\r\n+\tfalse,\r\n )\r\n \r\n // txnCommitter is a txnInterceptor that concerns itself with committing and\r\ndiff --git a/pkg/kv/txn_interceptor_committer_test.go b/pkg/kv/txn_interceptor_committer_test.go\r\nindex d02efc9937..1e7df7c496 100644\r\n--- a/pkg/kv/txn_interceptor_committer_test.go\r\n+++ b/pkg/kv/txn_interceptor_committer_test.go\r\n@@ -28,9 +28,11 @@ import (\r\n )\r\n \r\n func makeMockTxnCommitter() (txnCommitter, *mockLockedSender) {\r\n+\tst := cluster.MakeTestingClusterSettings()\r\n+\tparallelCommitsEnabled.Override(&st.SV, true)\r\n \tmockSender := &mockLockedSender{}\r\n \treturn txnCommitter{\r\n-\t\tst:      cluster.MakeTestingClusterSettings(),\r\n+\t\tst:      st,\r\n \t\tstopper: stop.NewStopper(),\r\n \t\twrapped: mockSender,\r\n \t\tmu:      new(syncutil.Mutex),\r\n\r\n```\r\n\r\n</details>","go\r\ndiff --git a/pkg/kv/dist_sender_server_test.go b/pkg/kv/dist_sender_server_test.go\r\nindex 708ae2c762..0f269569bf 100644\r\n--- a/pkg/kv/dist_sender_server_test.go\r\n+++ b/pkg/kv/dist_sender_server_test.go\r\n@@ -2442,7 +2442,7 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t// Parallel commits do not support the canForwardSerializableTimestamp\r\n \t\t\t// optimization. That's ok because we need to removed that optimization\r\n \t\t\t// anyway. See #36431.\r\n-\t\t\ttxnCoordRetry: true,\r\n+\t\t\ttxnCoordRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with forwarded timestamp and cput"",\r\n@@ -2478,9 +2478,10 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t\tb.Put(""c"", ""put"")\r\n \t\t\t\treturn txn.CommitInBatch(ctx, b) // both puts will succeed, et will retry from get\r\n \t\t\t},\r\n+\t\t\ttxnCoordRetry: true,\r\n \t\t\t// Client-side retry required as this will be a mixed success due\r\n \t\t\t// to parallel commits.\r\n-\t\t\tclientRetry: true,\r\n+\t\t\tclientRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with forwarded timestamp and cput and delete range"",\r\n@@ -2513,7 +2514,7 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t// Parallel commits do not support the canForwardSerializableTimestamp\r\n \t\t\t// optimization. That's ok because we need to removed that optimization\r\n \t\t\t// anyway. See #36431.\r\n-\t\t\ttxnCoordRetry: true,\r\n+\t\t\ttxnCoordRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with write too old and failed cput"",\r\n@@ -2627,9 +2628,10 @@ func TestTxnCoordSenderRetries(t *testing.T) {\r\n \t\t\t\treturn txn.CommitInBatch(ctx, b)\r\n \t\t\t},\r\n \t\t\tfilter: newUncertaintyFilter(roachpb.Key([]byte(""c""))),\r\n+\t\t\ttxnCoordRetry: true,\r\n \t\t\t// Client-side retry required as this will be a mixed success due\r\n \t\t\t// to parallel commits.\r\n-\t\t\tclientRetry: true,\r\n+\t\t\tclientRetry: false,\r\n \t\t},\r\n \t\t{\r\n \t\t\tname: ""multi-range batch with uncertainty interval error and get conflict"",\r\ndiff --git a/pkg/kv/txn_interceptor_committer.go b/pkg/kv/txn_interceptor_committer.go\r\nindex 8860cca99d..cccd182457 100644\r\n--- a/pkg/kv/txn_interceptor_committer.go\r\n+++ b/pkg/kv/txn_interceptor_committer.go\r\n@@ -28,7 +28,7 @@ import (\r\n var parallelCommitsEnabled = settings.RegisterBoolSetting(\r\n \t""kv.transaction.parallel_commits_enabled"",\r\n \t""if enabled, transactional commits will be parallelized with transactional writes"",\r\n-\ttrue,\r\n+\tfalse,\r\n )\r\n \r\n // txnCommitter is a txnInterceptor that concerns itself with committing and\r\ndiff --git a/pkg/kv/txn_interceptor_committer_test.go b/pkg/kv/txn_interceptor_committer_test.go\r\nindex d02efc9937..1e7df7c496 100644\r\n--- a/pkg/kv/txn_interceptor_committer_test.go\r\n+++ b/pkg/kv/txn_interceptor_committer_test.go\r\n@@ -28,9 +28,11 @@ import (\r\n )\r\n \r\n func makeMockTxnCommitter() (txnCommitter, *mockLockedSender) {\r\n+\tst := cluster.MakeTestingClusterSettings()\r\n+\tparallelCommitsEnabled.Override(&st.SV, true)\r\n \tmockSender := &mockLockedSender{}\r\n \treturn txnCommitter{\r\n-\t\tst:      cluster.MakeTestingClusterSettings(),\r\n+\t\tst:      st,\r\n \t\tstopper: stop.NewStopper(),\r\n \t\twrapped: mockSender,\r\n \t\tmu:      new(syncutil.Mutex),\r\n\r\n"
37373,opt: Crash on LEFT JOIN with SELECT sub-query**Describe the problem**\r\n\r\nI'm experiencing a consistent crash when executing a LEFT JOIN query with a sub-SELECT query.\r\n\r\n**To Reproduce**\r\n\r\nSet up the following schema:\r\n\r\n\r\nThen execute this query:\r\n\r\n\r\n\r\nRemoving either the `LEFT JOIN` or the `SELECT` subquery fixes it.\r\n\r\n**Expected behavior**\r\nI expect this to return a result without crashing\r\n\r\n**Additional data / screenshots**\r\n[cockroach.log](https://github.com/cockroachdb/cockroach/files/3155008/cockroach.log)\r\n\r\n**Environment:**\r\n - CockroachDB version 2.1.4\r\n - Server OS: macOS 10.14.4\r\n - Client app `cockroach sql`,C-bug|S-2-temp-unavailability,rytaft,"**Describe the problem**\r\n\r\nI'm experiencing a consistent crash when executing a LEFT JOIN query with a sub-SELECT query.\r\n\r\n**To Reproduce**\r\n\r\nSet up the following schema:\r\n```sql\r\nCREATE TABLE elements (\r\n  id VARCHAR(100) NOT NULL,\r\n  parent_id VARCHAR(100),\r\n  display TEXT,\r\n\r\n  PRIMARY KEY (id)\r\n);\r\n\r\nCREATE TABLE element_properties (\r\n  element_id VARCHAR(100) NOT NULL,\r\n  property VARCHAR(100) NOT NULL,\r\n  value VARCHAR(100),\r\n\r\n  CONSTRAINT fk_element  FOREIGN KEY (element_id) REFERENCES elements (id)\r\n);\r\n```\r\n\r\nThen execute this query:\r\n\r\n```sql\r\nSELECT *\r\nFROM elements e\r\nLEFT JOIN element_properties p\r\nON e.id = p.element_id\r\nWHERE (\r\n    e.id = 'third' OR\r\n    e.id IN (SELECT e1.parent_id FROM elements e1 WHERE e1.id = 'third')\r\n);\r\n```\r\n\r\nRemoving either the `LEFT JOIN` or the `SELECT` subquery fixes it.\r\n\r\n**Expected behavior**\r\nI expect this to return a result without crashing\r\n\r\n**Additional data / screenshots**\r\n[cockroach.log](https://github.com/cockroachdb/cockroach/files/3155008/cockroach.log)\r\n\r\n**Environment:**\r\n - CockroachDB version 2.1.4\r\n - Server OS: macOS 10.14.4\r\n - Client app `cockroach sql`","sql\r\nCREATE TABLE elements (\r\n  id VARCHAR(100) NOT NULL,\r\n  parent_id VARCHAR(100),\r\n  display TEXT,\r\n\r\n  PRIMARY KEY (id)\r\n);\r\n\r\nCREATE TABLE element_properties (\r\n  element_id VARCHAR(100) NOT NULL,\r\n  property VARCHAR(100) NOT NULL,\r\n  value VARCHAR(100),\r\n\r\n  CONSTRAINT fk_element  FOREIGN KEY (element_id) REFERENCES elements (id)\r\n);\r\n"
37365,"unable to insert zero-length array**The problem**\r\n\r\nThe lib/pq golang driver is unable to insert an empty array with CRDB 19.1, although it was able to do so with CRDB 2.1.6.\r\n\r\n**To Reproduce**\r\n\r\nFor the statement:\r\n\r\n\r\n\r\nInserting an empty array (but not nil/null) in go:\r\n\r\n\r\n\r\nWith CockroachDB 19.1, an error comes back from the CRDB connection saying:\r\n\r\n```\r\npq: error in argument for $1: only 1-dimension arrays supported\r\n```\r\n\r\nHowever, with `cockroach sql`, it is possible like this:\r\n\r\n\r\n\r\nIt is notable that the explicit conversion is required in CRDB, although I believe this was the case with 2.1.6 already, for which lib/pq was able to insert an empty array:\r\n\r\n\r\n\r\nUnfortunately, the ""cockroach-sql-exec"" log (with `sql.trace.log_statement_execute = true`) does not show what the query is, presumably because it does not get far enough in processing.\r\n\r\nI reported the issue at https://github.com/lib/pq/issues/862, but since the error message is coming back from CRDB, it does not seem to be an issue with the driver.\r\n\r\n**Environment:**\r\n - CockroachDB CCL v19.1.0 (x86_64-unknown-linux-gnu, built 2019/04/29 18:36:40, go1.11.6)\r\n - Server OS: [e.g. Arch Linux x64]\r\n - Client: golang via github.com/lib/pq\r\n",O-community,jordanlewis,"**The problem**\r\n\r\nThe lib/pq golang driver is unable to insert an empty array with CRDB 19.1, although it was able to do so with CRDB 2.1.6.\r\n\r\n**To Reproduce**\r\n\r\nFor the statement:\r\n\r\n```sql\r\nINSERT INTO tablex (some_strings) VALUES ($1);\r\n```\r\n\r\nInserting an empty array (but not nil/null) in go:\r\n\r\n```go\r\nstmt.QueryRow(pq.StringArray([]string{}))\r\n```\r\n\r\nWith CockroachDB 19.1, an error comes back from the CRDB connection saying:\r\n\r\n```\r\npq: error in argument for $1: only 1-dimension arrays supported\r\n```\r\n\r\nHowever, with `cockroach sql`, it is possible like this:\r\n\r\n```sql\r\n> INSERT INTO tablex (some_strings) VALUES ('{}'::TEXT[]);\r\nINSERT 1\r\n```\r\n\r\nIt is notable that the explicit conversion is required in CRDB, although I believe this was the case with 2.1.6 already, for which lib/pq was able to insert an empty array:\r\n\r\n```sql\r\n> INSERT INTO tablex (some_strings) VALUES ('{}');\r\npq: value type string doesn't match type STRING[] of column ""some_strings""\r\n```\r\n\r\nUnfortunately, the ""cockroach-sql-exec"" log (with `sql.trace.log_statement_execute = true`) does not show what the query is, presumably because it does not get far enough in processing.\r\n\r\nI reported the issue at https://github.com/lib/pq/issues/862, but since the error message is coming back from CRDB, it does not seem to be an issue with the driver.\r\n\r\n**Environment:**\r\n - CockroachDB CCL v19.1.0 (x86_64-unknown-linux-gnu, built 2019/04/29 18:36:40, go1.11.6)\r\n - Server OS: [e.g. Arch Linux x64]\r\n - Client: golang via github.com/lib/pq\r\n",sql\r\nINSERT INTO tablex (some_strings) VALUES ($1);\r\n
37054,config: default zone config should not be a globalWe have a global default for zone config which is changed by tests. Code in various layers uses `config.DefaultZoneConfig()` directly. This leads to problem with parallel tests; this is a good example (in `TestServer.Start`)\r\n\r\n\r\n\r\nThe default zone config should be a property of a server/cluster rather than a global.,C-enhancement|E-easy|E-starter,jeffrey-xiao,We have a global default for zone config which is changed by tests. Code in various layers uses `config.DefaultZoneConfig()` directly. This leads to problem with parallel tests; this is a good example (in `TestServer.Start`)\r\n\r\n```go\r\n\t// TODO(andrei): Running two TestServers concurrently with\r\n\t// PartOfCluster==false can result in the default zone config not be reset\r\n\t// properly. It would be nice if this were more robust.\r\n\tif !params.PartOfCluster {\r\n\t\t// Change the replication requirements so we don't get log spam about ranges\r\n\t\t// not being replicated enough.\r\n\t\tcfg := config.DefaultZoneConfig()\r\n\t\tcfg.NumReplicas = proto.Int32(1)\r\n\t\tfn := config.TestingSetDefaultZoneConfig(cfg)\r\n\t\tparams.Stopper.AddCloser(stop.CloserFn(fn))\r\n\t}\r\n```\r\n\r\nThe default zone config should be a property of a server/cluster rather than a global.,go\r\n\t// TODO(andrei): Running two TestServers concurrently with\r\n\t// PartOfCluster==false can result in the default zone config not be reset\r\n\t// properly. It would be nice if this were more robust.\r\n\tif !params.PartOfCluster {\r\n\t\t// Change the replication requirements so we don't get log spam about ranges\r\n\t\t// not being replicated enough.\r\n\t\tcfg := config.DefaultZoneConfig()\r\n\t\tcfg.NumReplicas = proto.Int32(1)\r\n\t\tfn := config.TestingSetDefaultZoneConfig(cfg)\r\n\t\tparams.Stopper.AddCloser(stop.CloserFn(fn))\r\n\t}\r\n
36842,"sql: olap query hitting context canceled/can't complete I'm trying to run an analytics query on registration cluster backup spun up in AWS (with no other traffic on m4.xlarge) but I keep seeing:\r\n`pq: communication error: rpc error: code = Canceled desc = context canceled\r\nroot@cockroachdb-public:26257/registration> pod default/cockroachdb-22073 terminated (Error)`\r\n\r\n\r\nHere is the `explain (opt, env)`:\r\n```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text\r\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n  Version: CockroachDB CCL v19.1.0-rc.2 (x86_64-unknown-linux-gnu, built 2019/04/05 00:36:18, go1.11.6)\r\n\r\n  CREATE TABLE zonecfgs (\r\n      cluster UUID NOT NULL,\r\n      zone INT NOT NULL,\r\n      ""timestamp"" TIMESTAMP NOT NULL DEFAULT now():::TIMESTAMP,\r\n      cfg JSONB NULL,\r\n      CONSTRAINT ""primary"" PRIMARY KEY (cluster ASC, zone ASC, ""timestamp"" DESC),\r\n      FAMILY ""primary"" (cluster, zone, ""timestamp"", cfg)\r\n  );\r\n\r\n  ALTER TABLE registration.public.zonecfgs INJECT STATISTICS '[\r\n      {\r\n          ""columns"": [\r\n              ""cluster""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 820850,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""zone""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 860,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""timestamp""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 152173206,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      }\r\n  ]';\r\n\r\n  CREATE TABLE clusters (\r\n      id UUID NOT NULL,\r\n      firstseen TIMESTAMP NULL DEFAULT now():::TIMESTAMP,\r\n      lastseen TIMESTAMP NOT NULL DEFAULT now():::TIMESTAMP,\r\n      pings INT NOT NULL DEFAULT 0:::INT,\r\n      insecure BOOL NULL,\r\n      maxnodeid INT NULL,\r\n      licensetype STRING NULL,\r\n      internal BOOL NULL,\r\n      CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n      FAMILY ""primary"" (id, firstseen, lastseen, pings, insecure, maxnodeid, licensetype, internal)\r\n  );\r\n\r\n  ALTER TABLE registration.public.clusters INJECT STATISTICS '[\r\n      {\r\n          ""columns"": [\r\n              ""id""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2865945,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""firstseen""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2845027,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""lastseen""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2457566,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""pings""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 6750,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""insecure""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 388391,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""maxnodeid""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 129,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 433074,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""licensetype""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 6,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 800363,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""internal""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 1097500,\r\n          ""row_count"": 2862353\r\n      }\r\n  ]';\r\n\r\n  EXPLAIN (OPT, ENV) SELECT d.month, sum(d.constraints) AS constraints, sum(d.lease_preferences) AS lease_preferences, sum(d.""both"") AS both, sum(d.neither) AS neither FROM (SELECT x.month, x.cluster, max(x.constraints) AS constraints, max(x.lease_preferences) AS lease_preferences, max(x.""both"") AS both, max(x.neither) AS neither FROM (SELECT z.cluster, substring(z.timestamp::STRING, 1, 7) AS month, IF((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null'), 1, 0) AS constraints, IF((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null'), 1, 0) AS lease_preferences, IF(((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null')) AND ((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null')), 1, 0) AS both, IF((NOT ((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null'))) AND (NOT ((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null'))), 1, 0) AS neither FROM registration.zonecfgs AS z INNER JOIN clusters AS c ON z.cluster = c.id WHERE ((c.lastseen::DATE - c.firstseen::DATE) >= '7') AND ((c.internal != true) OR (c.internal IS NULL))) AS x GROUP BY x.cluster, x.month) AS d GROUP BY d.month;\r\n  ----\r\n  group-by\r\n   \u251c\u2500\u2500 group-by\r\n   \u2502    \u251c\u2500\u2500 project\r\n   \u2502    \u2502    \u251c\u2500\u2500 inner-join (lookup zonecfgs)\r\n   \u2502    \u2502    \u2502    \u251c\u2500\u2500 select\r\n   \u2502    \u2502    \u2502    \u2502    \u251c\u2500\u2500 scan c\r\n   \u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500 filters\r\n   \u2502    \u2502    \u2502    \u2502         \u251c\u2500\u2500 (lastseen::DATE - firstseen::DATE) >= 7\r\n   \u2502    \u2502    \u2502    \u2502         \u2514\u2500\u2500 (internal != true) OR (internal IS NULL)\r\n   \u2502    \u2502    \u2502    \u2514\u2500\u2500 filters (true)\r\n   \u2502    \u2502    \u2514\u2500\u2500 projections\r\n   \u2502    \u2502         \u251c\u2500\u2500 substring(timestamp::STRING, 1, 7)\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE (((cfg->'constraints') != 'null') AND (json_array_length(cfg->'constraints') > 0)) AND ((((cfg->'constraints')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE (((cfg->'lease_preferences') != 'null') AND (json_array_length(cfg->'lease_preferences') > 0)) AND ((((cfg->'lease_preferences')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE ((((((cfg->'lease_preferences') != 'null') AND (json_array_length(cfg->'lease_preferences') > 0)) AND ((((cfg->'lease_preferences')->0)->'constraints') != 'null')) AND ((cfg->'constraints') != 'null')) AND (json_array_length(cfg->'constraints') > 0)) AND ((((cfg->'constraints')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u2514\u2500\u2500 CASE (((cfg @> '{""lease_preferences"": null}') OR (json_array_length(cfg->'lease_preferences') <= 0)) OR (((cfg->'lease_preferences')->0) @> '{""constraints"": null}')) AND (((cfg @> '{""constraints"": null}') OR (json_array_length(cfg->'constraints') <= 0)) OR (((cfg->'constraints')->0) @> '{""constraints"": null}')) WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2514\u2500\u2500 aggregations\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: constraints\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: lease_preferences\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: both\r\n   \u2502         \u2514\u2500\u2500 max\r\n   \u2502              \u2514\u2500\u2500 variable: neither\r\n   \u2514\u2500\u2500 aggregations\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u2514\u2500\u2500 sum\r\n             \u2514\u2500\u2500 variable: max\r\n(187 rows)\r\n\r\nTime: 64.115101ms\r\n```",C-bug|S-2-temp-unavailability|A-sql-execution,solongordon,"I'm trying to run an analytics query on registration cluster backup spun up in AWS (with no other traffic on m4.xlarge) but I keep seeing:\r\n`pq: communication error: rpc error: code = Canceled desc = context canceled\r\nroot@cockroachdb-public:26257/registration> pod default/cockroachdb-22073 terminated (Error)`\r\n\r\n```SQL\r\nSELECT\r\n    d.month,\r\n    sum(d.constraints) AS constraints,\r\n    sum(d.lease_preferences) AS lease_preferences,\r\n    sum(d.both) AS both,\r\n    sum(d.neither) AS neither\r\nFROM\r\n    (\r\n        SELECT\r\n            x.month,\r\n            x.cluster,\r\n            max(x.constraints) AS constraints,\r\n            max(x.lease_preferences) AS lease_preferences,\r\n            max(x.both) AS both,\r\n            max(x.neither) AS neither\r\n        FROM\r\n            (\r\n                SELECT\r\n                    z.cluster,\r\n                    substring(z.timestamp::STRING, 1, 7)\r\n                        AS month,\r\n                    IF(\r\n                        z.cfg->'constraints' != 'null'\r\n                        AND json_array_length(\r\n                                z.cfg->'constraints'\r\n                            )\r\n                            > 0\r\n                        AND z.cfg->'constraints'->0->'constraints'\r\n                            != 'null',\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS constraints,\r\n                    IF(\r\n                        z.cfg->'lease_preferences' != 'null'\r\n                        AND json_array_length(\r\n                                z.cfg->'lease_preferences'\r\n                            )\r\n                            > 0\r\n                        AND z.cfg->'lease_preferences'->0->'constraints'\r\n                            != 'null',\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS lease_preferences,\r\n                    IF(\r\n                        (\r\n                            z.cfg->'lease_preferences'\r\n                            != 'null'\r\n                            AND json_array_length(\r\n                                    z.cfg->'lease_preferences'\r\n                                )\r\n                                > 0\r\n                            AND z.cfg->'lease_preferences'->0->'constraints'\r\n                                != 'null'\r\n                        )\r\n                        AND (\r\n                                z.cfg->'constraints'\r\n                                != 'null'\r\n                                AND json_array_length(\r\n                                        z.cfg->'constraints'\r\n                                    )\r\n                                    > 0\r\n                                AND z.cfg->'constraints'->0->'constraints'\r\n                                    != 'null'\r\n                            ),\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS both,\r\n                    IF(\r\n                        NOT\r\n                            (\r\n                                z.cfg->'lease_preferences'\r\n                                != 'null'\r\n                                AND json_array_length(\r\n                                        z.cfg->'lease_preferences'\r\n                                    )\r\n                                    > 0\r\n                                AND z.cfg->'lease_preferences'->0->'constraints'\r\n                                    != 'null'\r\n                            )\r\n                        AND NOT\r\n                                (\r\n                                    z.cfg->'constraints'\r\n                                    != 'null'\r\n                                    AND json_array_length(\r\n                                            z.cfg->'constraints'\r\n                                        )\r\n                                        > 0\r\n                                    AND z.cfg->'constraints'->0->'constraints'\r\n                                        != 'null'\r\n                                ),\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS neither\r\n                FROM\r\n                    registration.zonecfgs AS z\r\n                    INNER JOIN clusters AS c\r\n                    ON z.cluster = c.id\r\n                WHERE\r\n                    (c.lastseen::DATE - c.firstseen::DATE)\r\n                    >= '7'\r\n                    AND (\r\n                            c.internal != true\r\n                            OR c.internal IS NULL\r\n                        )\r\n            )\r\n                AS x\r\n        GROUP BY\r\n            x.cluster, x.month\r\n    )\r\n        AS d\r\nGROUP BY\r\n    d.month;\r\n```\r\nHere is the `explain (opt, env)`:\r\n```                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text\r\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\r\n  Version: CockroachDB CCL v19.1.0-rc.2 (x86_64-unknown-linux-gnu, built 2019/04/05 00:36:18, go1.11.6)\r\n\r\n  CREATE TABLE zonecfgs (\r\n      cluster UUID NOT NULL,\r\n      zone INT NOT NULL,\r\n      ""timestamp"" TIMESTAMP NOT NULL DEFAULT now():::TIMESTAMP,\r\n      cfg JSONB NULL,\r\n      CONSTRAINT ""primary"" PRIMARY KEY (cluster ASC, zone ASC, ""timestamp"" DESC),\r\n      FAMILY ""primary"" (cluster, zone, ""timestamp"", cfg)\r\n  );\r\n\r\n  ALTER TABLE registration.public.zonecfgs INJECT STATISTICS '[\r\n      {\r\n          ""columns"": [\r\n              ""cluster""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 820850,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""zone""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 860,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""timestamp""\r\n          ],\r\n          ""created_at"": ""2019-04-15 14:08:26.858097+00:00"",\r\n          ""distinct_count"": 152173206,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 154033788\r\n      }\r\n  ]';\r\n\r\n  CREATE TABLE clusters (\r\n      id UUID NOT NULL,\r\n      firstseen TIMESTAMP NULL DEFAULT now():::TIMESTAMP,\r\n      lastseen TIMESTAMP NOT NULL DEFAULT now():::TIMESTAMP,\r\n      pings INT NOT NULL DEFAULT 0:::INT,\r\n      insecure BOOL NULL,\r\n      maxnodeid INT NULL,\r\n      licensetype STRING NULL,\r\n      internal BOOL NULL,\r\n      CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n      FAMILY ""primary"" (id, firstseen, lastseen, pings, insecure, maxnodeid, licensetype, internal)\r\n  );\r\n\r\n  ALTER TABLE registration.public.clusters INJECT STATISTICS '[\r\n      {\r\n          ""columns"": [\r\n              ""id""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2865945,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""firstseen""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2845027,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""lastseen""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2457566,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""pings""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 6750,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 0,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""insecure""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 388391,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""maxnodeid""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 129,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 433074,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""licensetype""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 6,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 800363,\r\n          ""row_count"": 2862353\r\n      },\r\n      {\r\n          ""columns"": [\r\n              ""internal""\r\n          ],\r\n          ""created_at"": ""2019-04-15 13:51:33.119775+00:00"",\r\n          ""distinct_count"": 2,\r\n          ""histo_col_type"": """",\r\n          ""name"": ""__auto__"",\r\n          ""null_count"": 1097500,\r\n          ""row_count"": 2862353\r\n      }\r\n  ]';\r\n\r\n  EXPLAIN (OPT, ENV) SELECT d.month, sum(d.constraints) AS constraints, sum(d.lease_preferences) AS lease_preferences, sum(d.""both"") AS both, sum(d.neither) AS neither FROM (SELECT x.month, x.cluster, max(x.constraints) AS constraints, max(x.lease_preferences) AS lease_preferences, max(x.""both"") AS both, max(x.neither) AS neither FROM (SELECT z.cluster, substring(z.timestamp::STRING, 1, 7) AS month, IF((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null'), 1, 0) AS constraints, IF((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null'), 1, 0) AS lease_preferences, IF(((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null')) AND ((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null')), 1, 0) AS both, IF((NOT ((((z.cfg->'lease_preferences') != 'null') AND (json_array_length(z.cfg->'lease_preferences') > 0)) AND ((((z.cfg->'lease_preferences')->0)->'constraints') != 'null'))) AND (NOT ((((z.cfg->'constraints') != 'null') AND (json_array_length(z.cfg->'constraints') > 0)) AND ((((z.cfg->'constraints')->0)->'constraints') != 'null'))), 1, 0) AS neither FROM registration.zonecfgs AS z INNER JOIN clusters AS c ON z.cluster = c.id WHERE ((c.lastseen::DATE - c.firstseen::DATE) >= '7') AND ((c.internal != true) OR (c.internal IS NULL))) AS x GROUP BY x.cluster, x.month) AS d GROUP BY d.month;\r\n  ----\r\n  group-by\r\n   \u251c\u2500\u2500 group-by\r\n   \u2502    \u251c\u2500\u2500 project\r\n   \u2502    \u2502    \u251c\u2500\u2500 inner-join (lookup zonecfgs)\r\n   \u2502    \u2502    \u2502    \u251c\u2500\u2500 select\r\n   \u2502    \u2502    \u2502    \u2502    \u251c\u2500\u2500 scan c\r\n   \u2502    \u2502    \u2502    \u2502    \u2514\u2500\u2500 filters\r\n   \u2502    \u2502    \u2502    \u2502         \u251c\u2500\u2500 (lastseen::DATE - firstseen::DATE) >= 7\r\n   \u2502    \u2502    \u2502    \u2502         \u2514\u2500\u2500 (internal != true) OR (internal IS NULL)\r\n   \u2502    \u2502    \u2502    \u2514\u2500\u2500 filters (true)\r\n   \u2502    \u2502    \u2514\u2500\u2500 projections\r\n   \u2502    \u2502         \u251c\u2500\u2500 substring(timestamp::STRING, 1, 7)\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE (((cfg->'constraints') != 'null') AND (json_array_length(cfg->'constraints') > 0)) AND ((((cfg->'constraints')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE (((cfg->'lease_preferences') != 'null') AND (json_array_length(cfg->'lease_preferences') > 0)) AND ((((cfg->'lease_preferences')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u251c\u2500\u2500 CASE ((((((cfg->'lease_preferences') != 'null') AND (json_array_length(cfg->'lease_preferences') > 0)) AND ((((cfg->'lease_preferences')->0)->'constraints') != 'null')) AND ((cfg->'constraints') != 'null')) AND (json_array_length(cfg->'constraints') > 0)) AND ((((cfg->'constraints')->0)->'constraints') != 'null') WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2502         \u2514\u2500\u2500 CASE (((cfg @> '{""lease_preferences"": null}') OR (json_array_length(cfg->'lease_preferences') <= 0)) OR (((cfg->'lease_preferences')->0) @> '{""constraints"": null}')) AND (((cfg @> '{""constraints"": null}') OR (json_array_length(cfg->'constraints') <= 0)) OR (((cfg->'constraints')->0) @> '{""constraints"": null}')) WHEN true THEN 1 ELSE 0 END\r\n   \u2502    \u2514\u2500\u2500 aggregations\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: constraints\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: lease_preferences\r\n   \u2502         \u251c\u2500\u2500 max\r\n   \u2502         \u2502    \u2514\u2500\u2500 variable: both\r\n   \u2502         \u2514\u2500\u2500 max\r\n   \u2502              \u2514\u2500\u2500 variable: neither\r\n   \u2514\u2500\u2500 aggregations\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u251c\u2500\u2500 sum\r\n        \u2502    \u2514\u2500\u2500 variable: max\r\n        \u2514\u2500\u2500 sum\r\n             \u2514\u2500\u2500 variable: max\r\n(187 rows)\r\n\r\nTime: 64.115101ms\r\n```","SQL\r\nSELECT\r\n    d.month,\r\n    sum(d.constraints) AS constraints,\r\n    sum(d.lease_preferences) AS lease_preferences,\r\n    sum(d.both) AS both,\r\n    sum(d.neither) AS neither\r\nFROM\r\n    (\r\n        SELECT\r\n            x.month,\r\n            x.cluster,\r\n            max(x.constraints) AS constraints,\r\n            max(x.lease_preferences) AS lease_preferences,\r\n            max(x.both) AS both,\r\n            max(x.neither) AS neither\r\n        FROM\r\n            (\r\n                SELECT\r\n                    z.cluster,\r\n                    substring(z.timestamp::STRING, 1, 7)\r\n                        AS month,\r\n                    IF(\r\n                        z.cfg->'constraints' != 'null'\r\n                        AND json_array_length(\r\n                                z.cfg->'constraints'\r\n                            )\r\n                            > 0\r\n                        AND z.cfg->'constraints'->0->'constraints'\r\n                            != 'null',\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS constraints,\r\n                    IF(\r\n                        z.cfg->'lease_preferences' != 'null'\r\n                        AND json_array_length(\r\n                                z.cfg->'lease_preferences'\r\n                            )\r\n                            > 0\r\n                        AND z.cfg->'lease_preferences'->0->'constraints'\r\n                            != 'null',\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS lease_preferences,\r\n                    IF(\r\n                        (\r\n                            z.cfg->'lease_preferences'\r\n                            != 'null'\r\n                            AND json_array_length(\r\n                                    z.cfg->'lease_preferences'\r\n                                )\r\n                                > 0\r\n                            AND z.cfg->'lease_preferences'->0->'constraints'\r\n                                != 'null'\r\n                        )\r\n                        AND (\r\n                                z.cfg->'constraints'\r\n                                != 'null'\r\n                                AND json_array_length(\r\n                                        z.cfg->'constraints'\r\n                                    )\r\n                                    > 0\r\n                                AND z.cfg->'constraints'->0->'constraints'\r\n                                    != 'null'\r\n                            ),\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS both,\r\n                    IF(\r\n                        NOT\r\n                            (\r\n                                z.cfg->'lease_preferences'\r\n                                != 'null'\r\n                                AND json_array_length(\r\n                                        z.cfg->'lease_preferences'\r\n                                    )\r\n                                    > 0\r\n                                AND z.cfg->'lease_preferences'->0->'constraints'\r\n                                    != 'null'\r\n                            )\r\n                        AND NOT\r\n                                (\r\n                                    z.cfg->'constraints'\r\n                                    != 'null'\r\n                                    AND json_array_length(\r\n                                            z.cfg->'constraints'\r\n                                        )\r\n                                        > 0\r\n                                    AND z.cfg->'constraints'->0->'constraints'\r\n                                        != 'null'\r\n                                ),\r\n                        1,\r\n                        0\r\n                    )\r\n                        AS neither\r\n                FROM\r\n                    registration.zonecfgs AS z\r\n                    INNER JOIN clusters AS c\r\n                    ON z.cluster = c.id\r\n                WHERE\r\n                    (c.lastseen::DATE - c.firstseen::DATE)\r\n                    >= '7'\r\n                    AND (\r\n                            c.internal != true\r\n                            OR c.internal IS NULL\r\n                        )\r\n            )\r\n                AS x\r\n        GROUP BY\r\n            x.cluster, x.month\r\n    )\r\n        AS d\r\nGROUP BY\r\n    d.month;\r\n"
36170,sql: EXPLAIN no longer shows the filter inside an index-join\r\n\r\nThe filter after the index-join no longer shows up.\r\n\r\nIn 2.1.x it looked like this:\r\n```\r\n       tree       | field  |  description   \r\n+-----------------+--------+---------------+\r\n  render          |        |                \r\n   \u2514\u2500\u2500 index-join |        |                \r\n        \u251c\u2500\u2500 scan  |        |                \r\n        \u2502         | table  | abc@abc_a_idx  \r\n        \u2502         | spans  | /1-/2          \r\n        \u2514\u2500\u2500 scan  |        |                \r\n                  | table  | abc@primary    \r\n                  | filter | c > 1          \r\n```,C-bug|S-3-ux-surprise,RaduBerinde,"```sql\r\ncreate table abc (a int, b int, c int, index (a) storing b);\r\nset optimizer = off;\r\nexplain select * from abc WHERE a=1 AND c>1;\r\n       tree       | field |  description   \r\n+-----------------+-------+---------------+\r\n  render          |       |                \r\n   \u2514\u2500\u2500 index-join |       |                \r\n        \u2502         | table | abc@primary    \r\n        \u2514\u2500\u2500 scan  |       |                \r\n                  | table | abc@abc_a_idx  \r\n                  | spans | /1-/2          \r\n(6 rows)\r\n```\r\n\r\nThe filter after the index-join no longer shows up.\r\n\r\nIn 2.1.x it looked like this:\r\n```\r\n       tree       | field  |  description   \r\n+-----------------+--------+---------------+\r\n  render          |        |                \r\n   \u2514\u2500\u2500 index-join |        |                \r\n        \u251c\u2500\u2500 scan  |        |                \r\n        \u2502         | table  | abc@abc_a_idx  \r\n        \u2502         | spans  | /1-/2          \r\n        \u2514\u2500\u2500 scan  |        |                \r\n                  | table  | abc@primary    \r\n                  | filter | c > 1          \r\n```","sql\r\ncreate table abc (a int, b int, c int, index (a) storing b);\r\nset optimizer = off;\r\nexplain select * from abc WHERE a=1 AND c>1;\r\n       tree       | field |  description   \r\n+-----------------+-------+---------------+\r\n  render          |       |                \r\n   \u2514\u2500\u2500 index-join |       |                \r\n        \u2502         | table | abc@primary    \r\n        \u2514\u2500\u2500 scan  |       |                \r\n                  | table | abc@abc_a_idx  \r\n                  | spans | /1-/2          \r\n(6 rows)\r\n"
35451,"Index selection does make unfortunate decisions since version 2.1.0The table layout for the minimal example to reproduce the problem is:\r\n```\r\n""CREATE TABLE my_table (\r\n    id INT NOT NULL,\r\n    value INT NOT NULL,\r\n    name STRING NULL,\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    INDEX value_idx (value ASC),\r\n    FAMILY ""primary"" (id, value, name)\r\n)\r\n```\r\n\r\nThe following query, filtering values with a list, uses the index `value_idx` fine up to version `2.0.6` of CockroachDB:\r\n```\r\nSELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21);\r\n```\r\nBut since version `2.1.0` this results in a linear table scan instead.\r\n\r\nInterestingly with one value less in the list, it's still OK, i.e., also versions `>= 2.1.0` then use the index:\r\n```\r\nSELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20);\r\n```\r\n\r\nI found out about this when updating (a 3-node cluster running on Kubernetes) from `2.0.3` to `2.1.5`. Previously fast queries suddenly timed out. The affected table had around 50M rows, so the automatic decision to not use the index in that case does not make much sense.\r\n\r\nOne possible workaround to make the DB use the index for all queries, is to use index hinting:\r\n```\r\nSELECT * FROM db.my_table@value_idx\r\n```\r\n\r\nBut implementing this everywhere is quite inconvenient, especially with auto-generating queries in the attached microservices.\r\n\r\nSo I hope that this can be solved in the DB directly.\r\n\r\n---\r\n\r\nHere is everything needed to reproduce the issue locally:\r\n\r\n`docker-compose.yml`:\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n\r\n  cockroachdb.v.2.0.6:\r\n    image: cockroachdb/cockroach:v2.0.6\r\n    command: start --insecure --host cockroachdb.v.2.0.6\r\n\r\n  cockroachdb.v.2.1.0:\r\n    image: cockroachdb/cockroach:v2.1.0\r\n    command: start --insecure --host cockroachdb.v.2.1.0\r\n\r\n  cockroachdb.v.2.1.5:\r\n    image: cockroachdb/cockroach:v2.1.5\r\n    command: start --insecure --host cockroachdb.v.2.1.5\r\n\r\n  test:\r\n    image: cockroachdb/cockroach:v2.1.5\r\n    depends_on:\r\n      - cockroachdb.v.2.0.6\r\n      - cockroachdb.v.2.1.0\r\n      - cockroachdb.v.2.1.5\r\n    volumes:\r\n      - ./:/tests\r\n    entrypoint: /tests/run_tests.sh\r\n```\r\n\r\n`run_tests.sh`\r\n\r\n\r\nWhen ran with\r\n\r\n\r\nthe output shows that version 2.0.6 is using the index for both queries, but version 2.1.0 (as well as 2.1.5) is ignoring it for the second query:\r\n\r\n```\r\n==================================\r\nUSING cockroachdb.v.2.0.6\r\n------------------\r\nTree   Field   Description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\nTree   Field   Description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/22\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n==================================\r\nUSING cockroachdb.v.2.1.0\r\n------------------\r\ntree   field   description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\ntree   field   description\r\nscan\r\n       table   my_table@primary\r\n       spans   ALL\r\n       filter  value IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21)\r\n==================================\r\nUSING cockroachdb.v.2.1.5\r\n------------------\r\ntree   field   description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\ntree   field   description\r\nscan\r\n       table   my_table@primary\r\n       spans   ALL\r\n       filter  value IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21)\r\n```",C-investigation|A-sql-optimizer,andy-kimball,"The table layout for the minimal example to reproduce the problem is:\r\n```\r\n""CREATE TABLE my_table (\r\n    id INT NOT NULL,\r\n    value INT NOT NULL,\r\n    name STRING NULL,\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    INDEX value_idx (value ASC),\r\n    FAMILY ""primary"" (id, value, name)\r\n)\r\n```\r\n\r\nThe following query, filtering values with a list, uses the index `value_idx` fine up to version `2.0.6` of CockroachDB:\r\n```\r\nSELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21);\r\n```\r\nBut since version `2.1.0` this results in a linear table scan instead.\r\n\r\nInterestingly with one value less in the list, it's still OK, i.e., also versions `>= 2.1.0` then use the index:\r\n```\r\nSELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20);\r\n```\r\n\r\nI found out about this when updating (a 3-node cluster running on Kubernetes) from `2.0.3` to `2.1.5`. Previously fast queries suddenly timed out. The affected table had around 50M rows, so the automatic decision to not use the index in that case does not make much sense.\r\n\r\nOne possible workaround to make the DB use the index for all queries, is to use index hinting:\r\n```\r\nSELECT * FROM db.my_table@value_idx\r\n```\r\n\r\nBut implementing this everywhere is quite inconvenient, especially with auto-generating queries in the attached microservices.\r\n\r\nSo I hope that this can be solved in the DB directly.\r\n\r\n---\r\n\r\nHere is everything needed to reproduce the issue locally:\r\n\r\n`docker-compose.yml`:\r\n```\r\nversion: '3'\r\n\r\nservices:\r\n\r\n  cockroachdb.v.2.0.6:\r\n    image: cockroachdb/cockroach:v2.0.6\r\n    command: start --insecure --host cockroachdb.v.2.0.6\r\n\r\n  cockroachdb.v.2.1.0:\r\n    image: cockroachdb/cockroach:v2.1.0\r\n    command: start --insecure --host cockroachdb.v.2.1.0\r\n\r\n  cockroachdb.v.2.1.5:\r\n    image: cockroachdb/cockroach:v2.1.5\r\n    command: start --insecure --host cockroachdb.v.2.1.5\r\n\r\n  test:\r\n    image: cockroachdb/cockroach:v2.1.5\r\n    depends_on:\r\n      - cockroachdb.v.2.0.6\r\n      - cockroachdb.v.2.1.0\r\n      - cockroachdb.v.2.1.5\r\n    volumes:\r\n      - ./:/tests\r\n    entrypoint: /tests/run_tests.sh\r\n```\r\n\r\n`run_tests.sh`\r\n```bash\r\n#!/usr/bin/env bash\r\n\r\ndeclare -a hosts=(""cockroachdb.v.2.0.6"" ""cockroachdb.v.2.1.0"" ""cockroachdb.v.2.1.5"")\r\n\r\nfor host in ""${hosts[@]}""\r\ndo\r\n    echo ==================================\r\n    echo USING $host\r\n\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'CREATE DATABASE db; CREATE USER usr; GRANT ALL ON DATABASE db TO usr;' > /dev/null 2>&1\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'SET DATABASE = db; CREATE TABLE my_table (id INT NOT NULL, value INT NOT NULL, name STRING NULL, CONSTRAINT ""primary"" PRIMARY KEY (id ASC), INDEX value_idx (value ASC));' > /dev/null 2>&1\r\n    echo ------------------\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'EXPLAIN SELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20);'\r\n    echo ------------------\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'EXPLAIN SELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21);'\r\n\r\ndone\r\n```\r\n\r\nWhen ran with\r\n```bash\r\ndocker-compose -f docker-compose.yml up test\r\n```\r\n\r\nthe output shows that version 2.0.6 is using the index for both queries, but version 2.1.0 (as well as 2.1.5) is ignoring it for the second query:\r\n\r\n```\r\n==================================\r\nUSING cockroachdb.v.2.0.6\r\n------------------\r\nTree   Field   Description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\nTree   Field   Description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/22\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n==================================\r\nUSING cockroachdb.v.2.1.0\r\n------------------\r\ntree   field   description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\ntree   field   description\r\nscan\r\n       table   my_table@primary\r\n       spans   ALL\r\n       filter  value IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21)\r\n==================================\r\nUSING cockroachdb.v.2.1.5\r\n------------------\r\ntree   field   description\r\nindex-join\r\n"" \u251c\u2500\u2500 scan""\r\n"" \u2502""   table   my_table@value_idx\r\n"" \u2502""   spans   /1-/21\r\n"" \u2514\u2500\u2500 scan""\r\n       table   my_table@primary\r\n------------------\r\ntree   field   description\r\nscan\r\n       table   my_table@primary\r\n       spans   ALL\r\n       filter  value IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21)\r\n```","bash\r\n#!/usr/bin/env bash\r\n\r\ndeclare -a hosts=(""cockroachdb.v.2.0.6"" ""cockroachdb.v.2.1.0"" ""cockroachdb.v.2.1.5"")\r\n\r\nfor host in ""${hosts[@]}""\r\ndo\r\n    echo ==================================\r\n    echo USING $host\r\n\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'CREATE DATABASE db; CREATE USER usr; GRANT ALL ON DATABASE db TO usr;' > /dev/null 2>&1\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'SET DATABASE = db; CREATE TABLE my_table (id INT NOT NULL, value INT NOT NULL, name STRING NULL, CONSTRAINT ""primary"" PRIMARY KEY (id ASC), INDEX value_idx (value ASC));' > /dev/null 2>&1\r\n    echo ------------------\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'EXPLAIN SELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20);'\r\n    echo ------------------\r\n    /cockroach/cockroach sql --host=$host --insecure -e 'EXPLAIN SELECT * FROM db.my_table WHERE value in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21);'\r\n\r\ndone\r\n"
35364,"sql: CBO fails to validate/enforce schema constraints prior to running CHECK constraintsThis is a regression from the HP code. Found while investigating #35040.\r\n\r\n\r\n\r\nThe problem is that column type constraints (width precision etc) must be applied (and the results truncated apropriately) prior to running the CHECK constraints.\r\n\r\nThis is non-trivial to fix in CBO yet as it requires a SQL built-in function able to apply column restrictions (this is not the same as performing a cast) and we don't have code to that effect yet.\r\n\r\nMy recommendation would be to temporarily disable CHECK planning in the CBO for 19.1, and improve the code towards a better fix post-release.\r\n\r\n",C-bug|A-sql-pgcompat|A-sql-mutations|S-3-erroneous-edge-case,knz|andy-kimball,"This is a regression from the HP code. Found while investigating #35040.\r\n\r\n```sql\r\nCREATE TABLE d(x DECIMAL(1,0) CHECK (x = 0));\r\nINSERT INTO d(x) VALUES (0); -- ok\r\nUPDATE d SET x = 0.1; -- OK in HP and PostgreSQL, fails with CBO enabled\r\n```\r\n\r\nThe problem is that column type constraints (width precision etc) must be applied (and the results truncated apropriately) prior to running the CHECK constraints.\r\n\r\nThis is non-trivial to fix in CBO yet as it requires a SQL built-in function able to apply column restrictions (this is not the same as performing a cast) and we don't have code to that effect yet.\r\n\r\nMy recommendation would be to temporarily disable CHECK planning in the CBO for 19.1, and improve the code towards a better fix post-release.\r\n\r\n","sql\r\nCREATE TABLE d(x DECIMAL(1,0) CHECK (x = 0));\r\nINSERT INTO d(x) VALUES (0); -- ok\r\nUPDATE d SET x = 0.1; -- OK in HP and PostgreSQL, fails with CBO enabled\r\n"
35177,"sql/opt: crash while planning `//` binop over two CASE expressionsThe following SQL crashes the CBO:\r\n\r\n\r\n\r\nStack trace:\r\n\r\n```\r\n        panic: could not find overload for binary expression floor-div [recovered]\r\n\r\n\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc001664000, 0x383dc40, 0xc00164f140, 0x2ba9c80, 0xc000476690)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:721 +0x36d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc001664000, 0x383dc40, 0xc00164f140)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:427 +0x61\r\npanic(0x2ba9c80, 0xc000476690)\r\n        /usr/lib/go-1.11/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc0014ba990)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:142 +0x9a\r\npanic(0x2ba9c80, 0xc000476690)\r\n        /usr/lib/go-1.11/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.BinaryAllowsNullArgs(0xc001650035, 0x385b120, 0x550eec0, 0x385b120, 0x550eec0, 0x43fb247b)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/typing.go:80 +0x132\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).AllowNullArgs(0xc001664ba8, 0x5500035, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0x2e06080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/custom_funcs.go:1090 +0x93\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructFloorDiv(0xc001664b98, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0xc0014b9d01, 0x0)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:12298 +0x4d6\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).constructBinary(0xc001222690, 0x7, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0x385ae20, 0x550eec0,\r\n0xc001657ba0, 0xc0012b7080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:635 +0x383\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildScalar(0xc001222690, 0x3858180, 0xc00173d340, 0xc0000f2800, 0x0, 0x0, 0x0, 0x385e540, 0xc0012b7080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:195 +0xc7a\r\n```\r\n\r\nThis doesn't repro with `set optimizer = off`",C-bug|A-sql-optimizer,rytaft|andy-kimball,"The following SQL crashes the CBO:\r\n\r\n```sql\r\nVALUES (\r\n        -3993911422377595656\r\n        << array_position(ARRAY[-0.48442661814333066, 0.08491291889752639, -0.017100705152083395, -1.5940068386193618], NULL)\r\n       ),\r\n       (-805133545430125191),\r\n       (\r\n        4085601939627203299\r\n        - CASE\r\n            WHEN false THEN CASE WHEN NULL THEN 3326209048762244771 ELSE -254531168219194454 END\r\n            ELSE NULL\r\n            END\r\n            // CASE WHEN NULL THEN 6010968360109510481 ELSE NULL END\r\n       );\r\n```\r\n\r\nStack trace:\r\n\r\n```\r\n        panic: could not find overload for binary expression floor-div [recovered]\r\n\r\n\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc001664000, 0x383dc40, 0xc00164f140, 0x2ba9c80, 0xc000476690)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:721 +0x36d\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc001664000, 0x383dc40, 0xc00164f140)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:427 +0x61\r\npanic(0x2ba9c80, 0xc000476690)\r\n        /usr/lib/go-1.11/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc0014ba990)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:142 +0x9a\r\npanic(0x2ba9c80, 0xc000476690)\r\n        /usr/lib/go-1.11/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.BinaryAllowsNullArgs(0xc001650035, 0x385b120, 0x550eec0, 0x385b120, 0x550eec0, 0x43fb247b)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/typing.go:80 +0x132\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).AllowNullArgs(0xc001664ba8, 0x5500035, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0x2e06080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/custom_funcs.go:1090 +0x93\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructFloorDiv(0xc001664b98, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0xc0014b9d01, 0x0)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:12298 +0x4d6\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).constructBinary(0xc001222690, 0x7, 0x385f740, 0xc001657ba0, 0x385f740, 0xc001657ba0, 0x385ae20, 0x550eec0,\r\n0xc001657ba0, 0xc0012b7080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:635 +0x383\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildScalar(0xc001222690, 0x3858180, 0xc00173d340, 0xc0000f2800, 0x0, 0x0, 0x0, 0x385e540, 0xc0012b7080)\r\n        /home/spill/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/scalar.go:195 +0xc7a\r\n```\r\n\r\nThis doesn't repro with `set optimizer = off`","sql\r\nVALUES (\r\n        -3993911422377595656\r\n        << array_position(ARRAY[-0.48442661814333066, 0.08491291889752639, -0.017100705152083395, -1.5940068386193618], NULL)\r\n       ),\r\n       (-805133545430125191),\r\n       (\r\n        4085601939627203299\r\n        - CASE\r\n            WHEN false THEN CASE WHEN NULL THEN 3326209048762244771 ELSE -254531168219194454 END\r\n            ELSE NULL\r\n            END\r\n            // CASE WHEN NULL THEN 6010968360109510481 ELSE NULL END\r\n       );\r\n"
35040,"sql: INSERT ON CONFLICT can insert NULLs into NOT NULL columns**Describe the problem**\r\n\r\nThe following sequence crashes CockroachDB:\r\n\r\n\r\nBacktrace:\r\n```\r\n*\r\n* ERROR: [n1,client=[::1]:60292,user=root] a SQL panic has occurred while executing ""SELECT * FROM test"": Non-nullable column ""test:pos"" with no value! Index scanned was ""primary"" with the index key columns (key) and the values ('key')\r\n*\r\n*\r\n* ERROR: [n1,client=[::1]:60292,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: SELECT * FROM _; caused by Non-nullable column ""test:pos"" with no value! Index scanned was ""primary"" with the index key columns (key) and the values ('key')\r\n\r\ngoroutine 90 [running]:\r\nruntime/debug.Stack(0x6d171e0, 0xc0052a4400, 0x3)\r\n\t/usr/local/Cellar/go/1.11.4/libexec/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x6d171e0, 0xc0052a4400, 0xc0005b5980, 0x64db4c0, 0xc00612f860, 0x1)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:213 +0xa6\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc005dbc000, 0x6d171e0, 0xc0052a4400, 0x61f4de0, 0xc0068db560)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:645 +0x2df\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc005dbc000, 0x6d171e0, 0xc0052a4400)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:387 +0x61\r\npanic(0x61f4de0, 0xc0068db560)\r\n\t/usr/local/Cellar/go/1.11.4/libexec/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*RowFetcher).finalizeRow(0xc0069b7c98, 0x6d172a0, 0xc00612f530)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/rowfetcher.go:1247 +0xa37\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*RowFetcher).NextRow(0xc0069b7c98, 0x6d172a0, 0xc00612f530, 0xc00612f560, 0x1, 0x1, 0x0, 0x10001, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/rowfetcher.go:997 +0x19a\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*rowFetcherWrapper).Next(0xc0078b15c0, 0x6d172a0, 0xc00612f530, 0xc006cf2c60, 0xc00612f4d0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go:144 +0x4a\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*tableReader).Next(0xc0069b7800, 0x0, 0x6d172a0, 0x6d172a0, 0xc00612f530)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go:247 +0x4c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.Run(0x6d172a0, 0xc00612f3b0, 0x6d20660, 0xc0069b7800, 0x6cf43a0, 0xc005044380)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/base.go:172 +0x35\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*ProcessorBase).Run(0xc0069b7800, 0x6d172a0, 0xc00612f3b0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go:731 +0x96\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).StartSync(0xc0062ea8c0, 0x6d172a0, 0xc00612f3b0, 0x6785fb8, 0xc000253880, 0x6cf4060)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:607 +0x19a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc0007d1600, 0xc006bd0d20, 0xc005024e10, 0xc0075f29e8, 0xc005c31680, 0xc005dbc4b0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:253 +0x8cb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc0007d1600, 0x6d172a0, 0xc005e2d1d0, 0xc005dbc4b0, 0xc006bd0d20, 0xc005024e10, 0x6d0a3a0, 0xc000775500, 0xc005c31680)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:758 +0x24c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0xc005dbc418, 0x3, 0xab9c398, 0xc005025050, 0x1, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:982 +0x26b\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:824 +0xa6a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:402 +0xaa7\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:96 +0x333\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc005dbc000, 0x6d171e0, 0xc0052a4400, 0xc00063f998, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1118 +0x2140\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc000830870, 0x6d171e0, 0xc0052a4400, 0xc005dbc000, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:389 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func4(0xc000830870, 0x6d171e0, 0xc0052a4400, 0xc005dbc000, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0xc0004c9100, 0xc0004c6da4)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:313 +0x81\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:312 +0x1018\r\n```\r\n\r\nPlease describe the issue you observed, and any steps we can take to reproduce it:\r\n\r\n**To Reproduce**\r\n\r\nWhat did you do? Describe in your own words.\r\n\r\nRun the set of statements posted above.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nSecond `EXECUTE X` should fail to insert NULL value into a NON-NULL column.\r\n\r\n**Environment:**\r\n - CockroachDB version:  CCL v2.1.4 @ 2019/01/21 23:00:37 (go1.11.4)\r\n - Server OS: macOS\r\n - Client app `cocroach sql`\r\n",C-bug|S-2,knz,"**Describe the problem**\r\n\r\nThe following sequence crashes CockroachDB:\r\n```sql\r\nCREATE TABLE test(\r\n  key STRING NOT NULL,\r\n  pos INT NOT NULL,\r\n  token STRING NOT NULL,\r\n  PRIMARY KEY(key)\r\n);\r\n\r\nPREPARE X AS\r\nINSERT INTO test(key, pos, token)\r\nVALUES ($1, $2, $3)\r\nON CONFLICT (key)\r\nDO UPDATE SET pos = CASE\r\n  WHEN excluded.token = test.token THEN excluded.pos\r\n  ELSE NULL\r\nEND\r\nRETURNING NOTHING;\r\n\r\nEXECUTE X('key', 123, 'hello');\r\nEXECUTE X('key', 123, 'bye');\r\nSELECT * FROM test;\r\n```\r\n\r\nBacktrace:\r\n```\r\n*\r\n* ERROR: [n1,client=[::1]:60292,user=root] a SQL panic has occurred while executing ""SELECT * FROM test"": Non-nullable column ""test:pos"" with no value! Index scanned was ""primary"" with the index key columns (key) and the values ('key')\r\n*\r\n*\r\n* ERROR: [n1,client=[::1]:60292,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: SELECT * FROM _; caused by Non-nullable column ""test:pos"" with no value! Index scanned was ""primary"" with the index key columns (key) and the values ('key')\r\n\r\ngoroutine 90 [running]:\r\nruntime/debug.Stack(0x6d171e0, 0xc0052a4400, 0x3)\r\n\t/usr/local/Cellar/go/1.11.4/libexec/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x6d171e0, 0xc0052a4400, 0xc0005b5980, 0x64db4c0, 0xc00612f860, 0x1)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:213 +0xa6\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc005dbc000, 0x6d171e0, 0xc0052a4400, 0x61f4de0, 0xc0068db560)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:645 +0x2df\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc005dbc000, 0x6d171e0, 0xc0052a4400)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:387 +0x61\r\npanic(0x61f4de0, 0xc0068db560)\r\n\t/usr/local/Cellar/go/1.11.4/libexec/src/runtime/panic.go:513 +0x1b9\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*RowFetcher).finalizeRow(0xc0069b7c98, 0x6d172a0, 0xc00612f530)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/rowfetcher.go:1247 +0xa37\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*RowFetcher).NextRow(0xc0069b7c98, 0x6d172a0, 0xc00612f530, 0xc00612f560, 0x1, 0x1, 0x0, 0x10001, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/rowfetcher.go:997 +0x19a\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*rowFetcherWrapper).Next(0xc0078b15c0, 0x6d172a0, 0xc00612f530, 0xc006cf2c60, 0xc00612f4d0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go:144 +0x4a\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*tableReader).Next(0xc0069b7800, 0x0, 0x6d172a0, 0x6d172a0, 0xc00612f530)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/tablereader.go:247 +0x4c\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.Run(0x6d172a0, 0xc00612f3b0, 0x6d20660, 0xc0069b7800, 0x6cf43a0, 0xc005044380)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/base.go:172 +0x35\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*ProcessorBase).Run(0xc0069b7800, 0x6d172a0, 0xc00612f3b0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/processors.go:731 +0x96\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).StartSync(0xc0062ea8c0, 0x6d172a0, 0xc00612f3b0, 0x6785fb8, 0xc000253880, 0x6cf4060)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:607 +0x19a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc0007d1600, 0xc006bd0d20, 0xc005024e10, 0xc0075f29e8, 0xc005c31680, 0xc005dbc4b0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:253 +0x8cb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc0007d1600, 0x6d172a0, 0xc005e2d1d0, 0xc005dbc4b0, 0xc006bd0d20, 0xc005024e10, 0x6d0a3a0, 0xc000775500, 0xc005c31680)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:758 +0x24c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0xc005dbc418, 0x3, 0xab9c398, 0xc005025050, 0x1, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:982 +0x26b\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:824 +0xa6a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:402 +0xaa7\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc005dbc000, 0x6d172a0, 0xc005e2d1d0, 0x6d1a920, 0xc00690d300, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:96 +0x333\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc005dbc000, 0x6d171e0, 0xc0052a4400, 0xc00063f998, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1118 +0x2140\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc000830870, 0x6d171e0, 0xc0052a4400, 0xc005dbc000, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0x0, 0x0)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:389 +0xce\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func4(0xc000830870, 0x6d171e0, 0xc0052a4400, 0xc005dbc000, 0x5400, 0x15000, 0xc00063fa30, 0xc0004c90f0, 0xc0004c9100, 0xc0004c6da4)\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:313 +0x81\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/private/tmp/cockroach-20190121-61877-1tcqxrf/cockroach-v2.1.4/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:312 +0x1018\r\n```\r\n\r\nPlease describe the issue you observed, and any steps we can take to reproduce it:\r\n\r\n**To Reproduce**\r\n\r\nWhat did you do? Describe in your own words.\r\n\r\nRun the set of statements posted above.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nSecond `EXECUTE X` should fail to insert NULL value into a NON-NULL column.\r\n\r\n**Environment:**\r\n - CockroachDB version:  CCL v2.1.4 @ 2019/01/21 23:00:37 (go1.11.4)\r\n - Server OS: macOS\r\n - Client app `cocroach sql`\r\n","sql\r\nCREATE TABLE test(\r\n  key STRING NOT NULL,\r\n  pos INT NOT NULL,\r\n  token STRING NOT NULL,\r\n  PRIMARY KEY(key)\r\n);\r\n\r\nPREPARE X AS\r\nINSERT INTO test(key, pos, token)\r\nVALUES ($1, $2, $3)\r\nON CONFLICT (key)\r\nDO UPDATE SET pos = CASE\r\n  WHEN excluded.token = test.token THEN excluded.pos\r\n  ELSE NULL\r\nEND\r\nRETURNING NOTHING;\r\n\r\nEXECUTE X('key', 123, 'hello');\r\nEXECUTE X('key', 123, 'bye');\r\nSELECT * FROM test;\r\n"
34862,"foreign key update and delete actions are not reflected in pg_constraint tableFK's update and delete actions works fine, but not reflected in `pg_constraint` table.\r\n\r\nSteps to reproduce:\r\n\r\n\r\nresults:\r\n\r\nconname\t| confupdtype\t| confdeltype\r\n-----------|----------------|-------------\r\nfk_foo_bar |\ta  |\ta\r\n\r\nfrom the postgres [docs](https://www.postgresql.org/docs/current/catalog-pg-constraint.html):\r\n`confupdtype` - Foreign key update action code: a = no action, r = restrict, c = cascade, n = set null, d = set default\r\n`confdeltype` - Foreign key deletion action code: a = no action, r = restrict, c = cascade, n = set null, d = set default",C-bug|A-sql-pgcompat|A-sql-vtables,BramGruneir,"FK's update and delete actions works fine, but not reflected in `pg_constraint` table.\r\n\r\nSteps to reproduce:\r\n\r\n```sql\r\nCREATE TABLE ""foo"" (""id"" INT PRIMARY KEY)\r\n\r\nCREATE TABLE ""bar"" (""id"" INT PRIMARY KEY, ""fooId"" INT)\r\n\r\nCREATE INDEX ""idx_fooId"" ON ""bar"" (""fooId"") \r\n\r\nALTER TABLE ""bar"" \r\n     ADD CONSTRAINT ""fk_foo_bar"" \r\n     FOREIGN KEY (""fooId"") REFERENCES ""foo""(""id"") \r\n     ON UPDATE SET NULL\r\n     ON DELETE SET NULL\r\n\r\nSELECT ""conname"", ""confupdtype"", ""confdeltype"" FROM ""pg_constraint""\r\n```\r\nresults:\r\n\r\nconname\t| confupdtype\t| confdeltype\r\n-----------|----------------|-------------\r\nfk_foo_bar |\ta  |\ta\r\n\r\nfrom the postgres [docs](https://www.postgresql.org/docs/current/catalog-pg-constraint.html):\r\n`confupdtype` - Foreign key update action code: a = no action, r = restrict, c = cascade, n = set null, d = set default\r\n`confdeltype` - Foreign key deletion action code: a = no action, r = restrict, c = cascade, n = set null, d = set default","sql\r\nCREATE TABLE ""foo"" (""id"" INT PRIMARY KEY)\r\n\r\nCREATE TABLE ""bar"" (""id"" INT PRIMARY KEY, ""fooId"" INT)\r\n\r\nCREATE INDEX ""idx_fooId"" ON ""bar"" (""fooId"") \r\n\r\nALTER TABLE ""bar"" \r\n     ADD CONSTRAINT ""fk_foo_bar"" \r\n     FOREIGN KEY (""fooId"") REFERENCES ""foo""(""id"") \r\n     ON UPDATE SET NULL\r\n     ON DELETE SET NULL\r\n\r\nSELECT ""conname"", ""confupdtype"", ""confdeltype"" FROM ""pg_constraint""\r\n"
32880,"Cluster is unresponsive after running big UPDATE**Describe the problem**\r\nDB Version: v2.1.0\r\n\r\nNot to face  issue #32524, I created a new column with a new family with name `content_`:\r\n\r\n\r\nAnd started to copy data from column `content` to `content_` from local connection(`cockroach sql`):\r\n\r\n\r\n\r\nThis table's size is 50gb, mostly data is in this `content` column.\r\n\r\nAfter 8 hours, `update` statement failed with `bad connection`, but at background database\r\nis still running and 3-node cluster is unresponsive with all running at high CPU usages. I can connect to db, but waits forever after running a simple `SELECT` query.\r\n\r\nI restarted 3 nodes one-by-one but after being accessible about 1 min, all nodes are again in high CPU usage and SQL's do not return, waits forever.\r\n\r\n`SHOW QUERIES`, `SHOW SESSIONS` and `SHOW JOBS` does not list anything related to that `update`.\r\n\r\nAdmin GUI showed node2  is in suspect mode but GUI is not stable, mostly can not get data from db. But it shows that:\r\n\r\n```\r\nTOTAL RANGES: 2896\r\nUNDER-REPLICATED RANGES: 2896 \r\nUNAVAILABLE RANGES: 2896\r\n```\r\n\r\n`cockroach sql` and apps can connect to db, but no SQL's waits forever.\r\n\r\nIf this update has resulted successfully, I will upgrade to 2.1.1 but now I'm stuck.\r\n\r\nHow can I cancel that `update` and make cluster responsive to applications?\r\n\r\n**Environment:**\r\n - CockroachDB version 2.1.0\r\n - Server OS: CENTOS 7\r\n - Client app `cockroach sql`\r\n\r\nIn the instance(n1) I run `update`, log shows:\r\n```\r\nE181206 01:23:36.107128 1814 storage/queue.go:893  [n1,split] 4 replicas failing with ""could not find valid split key""\r\nI181206 01:28:09.612178 578 server/status/runtime.go:465  [n1] runtime stats: 56 GiB RSS, 559 goroutines, 1.5 GiB/23 GiB/26 GiB GO alloc/idle/total, 30 GiB/31 GiB CGO alloc/total, 394.9 CGO/sec, 416.6/116.7 %(u/s)time, 0.0 %gc (6x), 27 MiB/2.0 MiB (r/w)net\r\nI181206 01:28:10.120526 394 storage/replica_proposal.go:211  [n1,s1,r416/1:/Table/29{6-7}] new range lease repl=(n1,s1):1 seq=8 start=1544059690.115944706,0 epo=47 pro=1544059690.115966847,0 following repl=(n3,s3):2 seq=7 start=1544010597.866347323,0 epo=38 pro=1544034658.823830841,0\r\n```\r\n\r\n\r\nFrom Node2 logs:\r\n```\r\nI181206 01:33:00.647690 160355 storage/replica_consistency.go:127  [n2,consistencyChecker,s2,r703/3:/System/tsd/cr.store.addsstab\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543403973184009866 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}\r\nI181206 01:33:05.467994 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 4.2 GiB/18 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 192.0 CGO/sec, 665.6/165.0 %(u/s)time, 0.7 %gc (8x), 153 MiB/153 MiB (r/w)net\r\nI181206 01:33:15.476942 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.6 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 173.5 CGO/sec, 659.5/156.0 %(u/s)time, 6.4 %gc (6x), 142 MiB/177 MiB (r/w)net\r\nI181206 01:33:26.246809 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 357 goroutines, 1.5 GiB/21 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 244.7 CGO/sec, 669.9/161.5 %(u/s)time, 3.2 %gc (11x), 105 MiB/128 MiB (r/w)net\r\nI181206 01:33:36.255723 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 4.9 GiB/17 GiB/23 GiB GO alloc/idle/total, 20 GiB/21 GiB CGO alloc/total, 242.3 CGO/sec, 641.8/172.2 %(u/s)time, 4.4 %gc (10x), 132 MiB/142 MiB (r/w)net\r\nI181206 01:33:46.265414 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 3.2 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/19 GiB CGO alloc/total, 233.4 CGO/sec, 639.6/144.1 %(u/s)time, 3.1 %gc (9x), 143 MiB/165 MiB (r/w)net\r\nW181206 01:33:46.696472 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nE181206 01:33:56.336386 13741 storage/queue.go:893  [n2,split] 3 replicas failing with ""could not find valid split key""\r\nI181206 01:33:56.519327 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 355 goroutines, 1.6 GiB/20 GiB/23 GiB GO alloc/idle/total, 18 GiB/19 GiB CGO alloc/total, 164.2 CGO/sec, 640.6/144.8 %(u/s)time, 3.1 %gc (11x), 144 MiB/154 MiB (r/w)net\r\nI181206 01:34:07.257700 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 356 goroutines, 4.0 GiB/18 GiB/23 GiB GO alloc/idle/total, 20 GiB/21 GiB CGO alloc/total, 206.9 CGO/sec, 670.7/157.6 %(u/s)time, 2.3 %gc (6x), 179 MiB/179 MiB (r/w)net\r\nI181206 01:34:17.281184 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 1.7 GiB/20 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 187.9 CGO/sec, 633.4/150.9 %(u/s)time, 10.8 %gc (9x), 184 MiB/206 MiB (r/w)net\r\nI181206 01:34:27.288739 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.5 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 410.7 CGO/sec, 657.1/166.3 %(u/s)time, 7.5 %gc (7x), 142 MiB/172 MiB (r/w)net\r\nI181206 01:34:37.815132 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 579 goroutines, 5.1 GiB/17 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 493.7 CGO/sec, 662.8/162.5 %(u/s)time, 0.9 %gc (7x), 114 MiB/136 MiB (r/w)net\r\nI181206 01:34:47.869768 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.8 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/19 GiB CGO alloc/total, 310.8 CGO/sec, 634.9/142.6 %(u/s)time, 4.8 %gc (9x), 169 MiB/421 MiB (r/w)net\r\nW181206 01:34:54.734657 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nI181206 01:34:57.907615 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 1.9 GiB/20 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 197.6 CGO/sec, 656.0/148.0 %(u/s)time, 4.4 %gc (12x), 151 MiB/172 MiB (r/w)net\r\nI181206 01:35:07.914425 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.6 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 487.4 CGO/sec, 659.9/176.5 %(u/s)time, 2.1 %gc (11x), 119 MiB/156 MiB (r/w)net\r\nI181206 01:35:17.923219 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 1.9 GiB/20 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 200.1 CGO/sec, 617.7/137.0 %(u/s)time, 4.7 %gc (11x), 154 MiB/174 MiB (r/w)net\r\nI181206 01:35:27.933038 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.2 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 332.5 CGO/sec, 665.7/158.8 %(u/s)time, 5.1 %gc (12x), 130 MiB/140 MiB (r/w)net\r\nI181206 01:35:37.943017 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.1 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 235.8 CGO/sec, 663.7/158.3 %(u/s)time, 5.0 %gc (9x), 118 MiB/130 MiB (r/w)net\r\nW181206 01:35:41.139856 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nI181206 01:35:47.950253 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 4.5 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 194.7 CGO/sec, 669.1/157.1 %(u/s)time, 3.6 %gc (8x), 106 MiB/116 MiB (r/w)net\r\nI181206 01:35:57.958833 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 5.7 GiB/16 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 200.5 CGO/sec, 669.9/165.2 %(u/s)time, 2.3 %gc (8x), 128 MiB/161 MiB (r/w)net\r\nI181206 01:35:59.927040 181681 storage/replica_consistency.go:127  [n2,consistencyChecker,s2,r1793/3:/System/tsd/cr.node.sql.mem.i\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543409937986831216 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}            \r\nE181206 01:59:03.369743 2415 storage/queue.go:793  [n2,raftlog,s2,r2595/3:/Table/428/1/""{r\\x96\u2026-sl\\xf\u2026}] context deadline exceeded\r\nE181206 01:59:03.369807 2417 storage/replica_range_lease.go:301  [n2,s2,r2595/3:/Table/428/1/""{r\\x96\u2026-sl\\xf\u2026}] context canceled\r\nE181206 01:59:14.490278 2626 storage/queue.go:793  [n2,raftlog,s2,r1975/3:/Table/428/1/""{d\\x98\u2026-es^f\ufffd\u2026}] context deadline exceeded\r\nE181206 01:59:18.274515 2430 storage/replica_range_lease.go:301  [n2,s2,r3435/3:/Table/428/1/""\\x0{e\\xb\u2026-fq2\\\u2026}] context canceled\r\nE181206 01:59:19.205304 2615 storage/replica_range_lease.go:301  [n2,s2,r2530/3:/Table/428/1/""\\x11\\xe{c0\u2026-fv\u2026}] context canceled\r\nE181206 01:59:29.807019 2659 storage/replica_range_lease.go:301  [n2,s2,r785/3:/System/tsd/cr.store.raft.pro\u2026] context canceled\r\nE181206 01:59:29.807081 2658 storage/queue.go:793  [n2,raftlog,s2,r785/3:/System/tsd/cr.store.raft.pro\u2026] context deadline exceeded\r\nE181206 01:59:40.605072 2633 storage/replica_range_lease.go:301  [n2,s2,r2998/3:/System/tsd/cr.store.range.r\u2026] context canceled\r\nE181206 02:00:09.748861 2744 storage/replica_range_lease.go:301  [n2,s2,r2544/3:/System/tsd/cr.node.sql.mem.c\u2026] context canceled\r\nE181206 02:00:09.748902 2792 storage/queue.go:793  [n2,raftlog,s2,r2544/3:/System/tsd/cr.node.sql.mem.c\u2026] context deadline exceeded\r\n                                                                                                                                                                           \r\n```\r\nFrom Node3 logs:\r\n```\r\n....\r\nI181206 01:33:20.045083 606 storage/replica_proposal.go:211  [n3,s3,r2188/2:/Table/428/1/""{R\\xe9\xe2<80>\xa6-S\\xdd\xe2<80>\xa6}] new range lease repl=(n3,s3):2 seq=21 start=1544060000.041692717,0 epo=41 pro=1544060000.041701297,0 following repl=(n1,s1):1 seq=20 start=1544036652.279190727,0 epo=43 pro=1544036652.279194466,0\r\nI181206 01:33:20.048924 359 storage/replica_proposal.go:211  [n3,s3,r494/2:/Table/37{4-5}] new range lease repl=(n3,s3):2 seq=12 start=1544060000.045747909,0 epo=41 pro=1544060000.045757386,0 following repl=(n1,s1):1 seq=11 start=1544037150.363487202,0 epo=43 pro=1544037150.363490949,0\r\nW181206 01:33:21.847272 697 server/node.go:886  [n3,summaries] health alerts detected: {Alerts:[{StoreID:3 Category:METRICS Description:requests.slow.raft Value:3} {StoreID:3 Category:METRICS Description:requests.slow.lease Value:5} {StoreID:3 Category:METRICS Description:queue.replicate.process.failure Value:3245}]}\r\nI181206 01:33:22.097114 695 server/status/runtime.go:465  [n3] runtime stats: 18 GiB RSS, 821 goroutines, 2.5 GiB/3.3 GiB/6.1 GiB GO alloc/idle/total, 11 GiB/11 GiB CGO alloc/total, 693.6 CGO/sec, 9.6/3.1 %(u/s)time, 0.0 %gc (0x), 74 MiB/3.0 MiB (r/w)net\r\nI181206 01:33:23.843997 449 storage/replica_proposal.go:211  [n3,s3,r2176/2:/System/tsd/cr.store.raft.pro\xe2<80>\xa6] new range lease repl=(n3,s3):2 seq=7 start=1544058881.001239157,0 epo=41 pro=1544060003.838539421,0 following repl=(n3,s3):2 seq=6 start=1544009790.472476417,0 epo=38 pro=1544034729.504088813,0\r\nI181206 01:33:23.871420 61178 storage/replica_consistency.go:127  [n3,consistencyChecker,s3,r2176/2:/System/tsd/cr.store.raft.pro\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543362096047685420 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}                                                                                                                                                                                       \r\n```\r\n\r\n",C-investigation,nvanbenschoten,"**Describe the problem**\r\nDB Version: v2.1.0\r\n\r\nNot to face  issue #32524, I created a new column with a new family with name `content_`:\r\n\r\n```sql\r\nALTER TABLE requests ADD COLUMN content_ BYTES CREATE FAMILY fcontent;\r\n```\r\nAnd started to copy data from column `content` to `content_` from local connection(`cockroach sql`):\r\n\r\n```sql\r\nUPDATE requests SET content_ = content;\r\n```\r\n\r\nThis table's size is 50gb, mostly data is in this `content` column.\r\n\r\nAfter 8 hours, `update` statement failed with `bad connection`, but at background database\r\nis still running and 3-node cluster is unresponsive with all running at high CPU usages. I can connect to db, but waits forever after running a simple `SELECT` query.\r\n\r\nI restarted 3 nodes one-by-one but after being accessible about 1 min, all nodes are again in high CPU usage and SQL's do not return, waits forever.\r\n\r\n`SHOW QUERIES`, `SHOW SESSIONS` and `SHOW JOBS` does not list anything related to that `update`.\r\n\r\nAdmin GUI showed node2  is in suspect mode but GUI is not stable, mostly can not get data from db. But it shows that:\r\n\r\n```\r\nTOTAL RANGES: 2896\r\nUNDER-REPLICATED RANGES: 2896 \r\nUNAVAILABLE RANGES: 2896\r\n```\r\n\r\n`cockroach sql` and apps can connect to db, but no SQL's waits forever.\r\n\r\nIf this update has resulted successfully, I will upgrade to 2.1.1 but now I'm stuck.\r\n\r\nHow can I cancel that `update` and make cluster responsive to applications?\r\n\r\n**Environment:**\r\n - CockroachDB version 2.1.0\r\n - Server OS: CENTOS 7\r\n - Client app `cockroach sql`\r\n\r\nIn the instance(n1) I run `update`, log shows:\r\n```\r\nE181206 01:23:36.107128 1814 storage/queue.go:893  [n1,split] 4 replicas failing with ""could not find valid split key""\r\nI181206 01:28:09.612178 578 server/status/runtime.go:465  [n1] runtime stats: 56 GiB RSS, 559 goroutines, 1.5 GiB/23 GiB/26 GiB GO alloc/idle/total, 30 GiB/31 GiB CGO alloc/total, 394.9 CGO/sec, 416.6/116.7 %(u/s)time, 0.0 %gc (6x), 27 MiB/2.0 MiB (r/w)net\r\nI181206 01:28:10.120526 394 storage/replica_proposal.go:211  [n1,s1,r416/1:/Table/29{6-7}] new range lease repl=(n1,s1):1 seq=8 start=1544059690.115944706,0 epo=47 pro=1544059690.115966847,0 following repl=(n3,s3):2 seq=7 start=1544010597.866347323,0 epo=38 pro=1544034658.823830841,0\r\n```\r\n\r\n\r\nFrom Node2 logs:\r\n```\r\nI181206 01:33:00.647690 160355 storage/replica_consistency.go:127  [n2,consistencyChecker,s2,r703/3:/System/tsd/cr.store.addsstab\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543403973184009866 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}\r\nI181206 01:33:05.467994 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 4.2 GiB/18 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 192.0 CGO/sec, 665.6/165.0 %(u/s)time, 0.7 %gc (8x), 153 MiB/153 MiB (r/w)net\r\nI181206 01:33:15.476942 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.6 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 173.5 CGO/sec, 659.5/156.0 %(u/s)time, 6.4 %gc (6x), 142 MiB/177 MiB (r/w)net\r\nI181206 01:33:26.246809 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 357 goroutines, 1.5 GiB/21 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 244.7 CGO/sec, 669.9/161.5 %(u/s)time, 3.2 %gc (11x), 105 MiB/128 MiB (r/w)net\r\nI181206 01:33:36.255723 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 4.9 GiB/17 GiB/23 GiB GO alloc/idle/total, 20 GiB/21 GiB CGO alloc/total, 242.3 CGO/sec, 641.8/172.2 %(u/s)time, 4.4 %gc (10x), 132 MiB/142 MiB (r/w)net\r\nI181206 01:33:46.265414 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 3.2 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/19 GiB CGO alloc/total, 233.4 CGO/sec, 639.6/144.1 %(u/s)time, 3.1 %gc (9x), 143 MiB/165 MiB (r/w)net\r\nW181206 01:33:46.696472 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nE181206 01:33:56.336386 13741 storage/queue.go:893  [n2,split] 3 replicas failing with ""could not find valid split key""\r\nI181206 01:33:56.519327 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 355 goroutines, 1.6 GiB/20 GiB/23 GiB GO alloc/idle/total, 18 GiB/19 GiB CGO alloc/total, 164.2 CGO/sec, 640.6/144.8 %(u/s)time, 3.1 %gc (11x), 144 MiB/154 MiB (r/w)net\r\nI181206 01:34:07.257700 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 356 goroutines, 4.0 GiB/18 GiB/23 GiB GO alloc/idle/total, 20 GiB/21 GiB CGO alloc/total, 206.9 CGO/sec, 670.7/157.6 %(u/s)time, 2.3 %gc (6x), 179 MiB/179 MiB (r/w)net\r\nI181206 01:34:17.281184 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 1.7 GiB/20 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 187.9 CGO/sec, 633.4/150.9 %(u/s)time, 10.8 %gc (9x), 184 MiB/206 MiB (r/w)net\r\nI181206 01:34:27.288739 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.5 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 410.7 CGO/sec, 657.1/166.3 %(u/s)time, 7.5 %gc (7x), 142 MiB/172 MiB (r/w)net\r\nI181206 01:34:37.815132 531 server/status/runtime.go:465  [n2] runtime stats: 42 GiB RSS, 579 goroutines, 5.1 GiB/17 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 493.7 CGO/sec, 662.8/162.5 %(u/s)time, 0.9 %gc (7x), 114 MiB/136 MiB (r/w)net\r\nI181206 01:34:47.869768 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.8 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/19 GiB CGO alloc/total, 310.8 CGO/sec, 634.9/142.6 %(u/s)time, 4.8 %gc (9x), 169 MiB/421 MiB (r/w)net\r\nW181206 01:34:54.734657 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nI181206 01:34:57.907615 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 1.9 GiB/20 GiB/23 GiB GO alloc/idle/total, 17 GiB/18 GiB CGO alloc/total, 197.6 CGO/sec, 656.0/148.0 %(u/s)time, 4.4 %gc (12x), 151 MiB/172 MiB (r/w)net\r\nI181206 01:35:07.914425 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.6 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 487.4 CGO/sec, 659.9/176.5 %(u/s)time, 2.1 %gc (11x), 119 MiB/156 MiB (r/w)net\r\nI181206 01:35:17.923219 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 1.9 GiB/20 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 200.1 CGO/sec, 617.7/137.0 %(u/s)time, 4.7 %gc (11x), 154 MiB/174 MiB (r/w)net\r\nI181206 01:35:27.933038 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.2 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 332.5 CGO/sec, 665.7/158.8 %(u/s)time, 5.1 %gc (12x), 130 MiB/140 MiB (r/w)net\r\nI181206 01:35:37.943017 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 3.1 GiB/19 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 235.8 CGO/sec, 663.7/158.3 %(u/s)time, 5.0 %gc (9x), 118 MiB/130 MiB (r/w)net\r\nW181206 01:35:41.139856 460 storage/store_rebalancer.go:227  [n2,s2,store-rebalancer] StorePool missing descriptor for local store\r\nI181206 01:35:47.950253 531 server/status/runtime.go:465  [n2] runtime stats: 40 GiB RSS, 353 goroutines, 4.5 GiB/18 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 194.7 CGO/sec, 669.1/157.1 %(u/s)time, 3.6 %gc (8x), 106 MiB/116 MiB (r/w)net\r\nI181206 01:35:57.958833 531 server/status/runtime.go:465  [n2] runtime stats: 41 GiB RSS, 353 goroutines, 5.7 GiB/16 GiB/23 GiB GO alloc/idle/total, 19 GiB/20 GiB CGO alloc/total, 200.5 CGO/sec, 669.9/165.2 %(u/s)time, 2.3 %gc (8x), 128 MiB/161 MiB (r/w)net\r\nI181206 01:35:59.927040 181681 storage/replica_consistency.go:127  [n2,consistencyChecker,s2,r1793/3:/System/tsd/cr.node.sql.mem.i\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543409937986831216 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}            \r\nE181206 01:59:03.369743 2415 storage/queue.go:793  [n2,raftlog,s2,r2595/3:/Table/428/1/""{r\\x96\u2026-sl\\xf\u2026}] context deadline exceeded\r\nE181206 01:59:03.369807 2417 storage/replica_range_lease.go:301  [n2,s2,r2595/3:/Table/428/1/""{r\\x96\u2026-sl\\xf\u2026}] context canceled\r\nE181206 01:59:14.490278 2626 storage/queue.go:793  [n2,raftlog,s2,r1975/3:/Table/428/1/""{d\\x98\u2026-es^f\ufffd\u2026}] context deadline exceeded\r\nE181206 01:59:18.274515 2430 storage/replica_range_lease.go:301  [n2,s2,r3435/3:/Table/428/1/""\\x0{e\\xb\u2026-fq2\\\u2026}] context canceled\r\nE181206 01:59:19.205304 2615 storage/replica_range_lease.go:301  [n2,s2,r2530/3:/Table/428/1/""\\x11\\xe{c0\u2026-fv\u2026}] context canceled\r\nE181206 01:59:29.807019 2659 storage/replica_range_lease.go:301  [n2,s2,r785/3:/System/tsd/cr.store.raft.pro\u2026] context canceled\r\nE181206 01:59:29.807081 2658 storage/queue.go:793  [n2,raftlog,s2,r785/3:/System/tsd/cr.store.raft.pro\u2026] context deadline exceeded\r\nE181206 01:59:40.605072 2633 storage/replica_range_lease.go:301  [n2,s2,r2998/3:/System/tsd/cr.store.range.r\u2026] context canceled\r\nE181206 02:00:09.748861 2744 storage/replica_range_lease.go:301  [n2,s2,r2544/3:/System/tsd/cr.node.sql.mem.c\u2026] context canceled\r\nE181206 02:00:09.748902 2792 storage/queue.go:793  [n2,raftlog,s2,r2544/3:/System/tsd/cr.node.sql.mem.c\u2026] context deadline exceeded\r\n                                                                                                                                                                           \r\n```\r\nFrom Node3 logs:\r\n```\r\n....\r\nI181206 01:33:20.045083 606 storage/replica_proposal.go:211  [n3,s3,r2188/2:/Table/428/1/""{R\\xe9\xe2<80>\xa6-S\\xdd\xe2<80>\xa6}] new range lease repl=(n3,s3):2 seq=21 start=1544060000.041692717,0 epo=41 pro=1544060000.041701297,0 following repl=(n1,s1):1 seq=20 start=1544036652.279190727,0 epo=43 pro=1544036652.279194466,0\r\nI181206 01:33:20.048924 359 storage/replica_proposal.go:211  [n3,s3,r494/2:/Table/37{4-5}] new range lease repl=(n3,s3):2 seq=12 start=1544060000.045747909,0 epo=41 pro=1544060000.045757386,0 following repl=(n1,s1):1 seq=11 start=1544037150.363487202,0 epo=43 pro=1544037150.363490949,0\r\nW181206 01:33:21.847272 697 server/node.go:886  [n3,summaries] health alerts detected: {Alerts:[{StoreID:3 Category:METRICS Description:requests.slow.raft Value:3} {StoreID:3 Category:METRICS Description:requests.slow.lease Value:5} {StoreID:3 Category:METRICS Description:queue.replicate.process.failure Value:3245}]}\r\nI181206 01:33:22.097114 695 server/status/runtime.go:465  [n3] runtime stats: 18 GiB RSS, 821 goroutines, 2.5 GiB/3.3 GiB/6.1 GiB GO alloc/idle/total, 11 GiB/11 GiB CGO alloc/total, 693.6 CGO/sec, 9.6/3.1 %(u/s)time, 0.0 %gc (0x), 74 MiB/3.0 MiB (r/w)net\r\nI181206 01:33:23.843997 449 storage/replica_proposal.go:211  [n3,s3,r2176/2:/System/tsd/cr.store.raft.pro\xe2<80>\xa6] new range lease repl=(n3,s3):2 seq=7 start=1544058881.001239157,0 epo=41 pro=1544060003.838539421,0 following repl=(n3,s3):2 seq=6 start=1544009790.472476417,0 epo=38 pro=1544034729.504088813,0\r\nI181206 01:33:23.871420 61178 storage/replica_consistency.go:127  [n3,consistencyChecker,s3,r2176/2:/System/tsd/cr.store.raft.pro\xe2<80>\xa6] triggering stats recomputation to resolve delta of {ContainsEstimates:true LastUpdateNanos:1543362096047685420 IntentAge:0 GCBytesAge:0 LiveBytes:0 LiveCount:0 KeyBytes:0 KeyCount:0 ValBytes:0 ValCount:0 IntentBytes:0 IntentCount:0 SysBytes:0 SysCount:0}                                                                                                                                                                                       \r\n```\r\n\r\n",sql\r\nALTER TABLE requests ADD COLUMN content_ BYTES CREATE FAMILY fcontent;\r\n
32524,"v2.1.1,ERR: split failed while applying backpressure: could not find valid split key**Describe the problem**\r\n\r\nAfter upgrading 2.1.1, table updates failed on our system with error:\r\n\r\n```\r\nInternalError: (psycopg2.InternalError) split failed while applying backpressure: could not find valid split key\r\n [SQL: 'UPDATE requests SET q_checked_on=%(q_checked_on)s WHERE requests.id = %(id_1)s'] [parameters: {'q_checked_on': datetime.datetime(2018, 11, 21, 0, 50, 6, 316859), 'id_1': UUID('323ea26d-8105-45ac-9984-7ec8ecd8900e')}]\r\n```\r\nTarget is not a cluster, standalone local CRDB database(with --insecure).\r\n\r\nThis is log from db:\r\n```\r\nW181121 07:52:54.991908 535049 storage/replica_backpressure.go:135  [n1,s1,r191/1:/Table/77/1/""2{>\\xa2\xe2<80>\xa6-pgA\\x\xe2<80>\xa6}] applying backpressure to limit range growth on batch [txn: 5260ab3a], BeginTransaction [/Table/77/1/""2>\\xa2m\\x81\\x05E\\xac\\x99\\x84~\\xc8\\xec\xd8<90>\\x0e""/0,/Min), Put [/Table/77/1/""2>\\xa2m\\x81\\x05E\\xac\\x99\\x84~\\xc8\\xec\xd8<90>\\x0e""/0,/Min)\r\nW181121 07:52:54.992258 535049 internal/client/txn.go:532  [n1,client=127.0.0.1:32300,user=xxxx] failure aborting transaction: split failed while applying backpressure: could not find valid split key; abort caused by: split failed while applying backpressure: could not find valid split key\r\n```\r\n\r\n**To Reproduce**\r\n\r\n\r\n**Environment:**\r\nCockroachDB CCL v2.1.1 (x86_64-unknown-linux-gnu, built 2018/11/19 18:24:21, go1.10.3)\r\nCentOS 7.5.1804\r\n\r\n**Additional context**\r\n\r\nAfter setting \r\n\r\n\r\nclients started to work normally.",C-investigation,nvanbenschoten,"**Describe the problem**\r\n\r\nAfter upgrading 2.1.1, table updates failed on our system with error:\r\n\r\n```\r\nInternalError: (psycopg2.InternalError) split failed while applying backpressure: could not find valid split key\r\n [SQL: 'UPDATE requests SET q_checked_on=%(q_checked_on)s WHERE requests.id = %(id_1)s'] [parameters: {'q_checked_on': datetime.datetime(2018, 11, 21, 0, 50, 6, 316859), 'id_1': UUID('323ea26d-8105-45ac-9984-7ec8ecd8900e')}]\r\n```\r\nTarget is not a cluster, standalone local CRDB database(with --insecure).\r\n\r\nThis is log from db:\r\n```\r\nW181121 07:52:54.991908 535049 storage/replica_backpressure.go:135  [n1,s1,r191/1:/Table/77/1/""2{>\\xa2\xe2<80>\xa6-pgA\\x\xe2<80>\xa6}] applying backpressure to limit range growth on batch [txn: 5260ab3a], BeginTransaction [/Table/77/1/""2>\\xa2m\\x81\\x05E\\xac\\x99\\x84~\\xc8\\xec\xd8<90>\\x0e""/0,/Min), Put [/Table/77/1/""2>\\xa2m\\x81\\x05E\\xac\\x99\\x84~\\xc8\\xec\xd8<90>\\x0e""/0,/Min)\r\nW181121 07:52:54.992258 535049 internal/client/txn.go:532  [n1,client=127.0.0.1:32300,user=xxxx] failure aborting transaction: split failed while applying backpressure: could not find valid split key; abort caused by: split failed while applying backpressure: could not find valid split key\r\n```\r\n\r\n**To Reproduce**\r\n\r\n```sql\r\nCREATE TABLE requests (\r\n\tid UUID NOT NULL,\r\n        q_checked_on TIMESTAMP NULL,\r\n        content BYTES NULL,\r\n        CONSTRAINT requests_pkey PRIMARY KEY (id ASC));\r\nUPDATE requests SET q_checked_on=%(q_checked_on)s WHERE requests.id = %(id_1)s\r\n```\r\n**Environment:**\r\nCockroachDB CCL v2.1.1 (x86_64-unknown-linux-gnu, built 2018/11/19 18:24:21, go1.10.3)\r\nCentOS 7.5.1804\r\n\r\n**Additional context**\r\n\r\nAfter setting \r\n\r\n```sql\r\nSET CLUSTER SETTING kv.range.backpressure_range_size_multiplier=0;\r\n```\r\nclients started to work normally.","sql\r\nCREATE TABLE requests (\r\n\tid UUID NOT NULL,\r\n        q_checked_on TIMESTAMP NULL,\r\n        content BYTES NULL,\r\n        CONSTRAINT requests_pkey PRIMARY KEY (id ASC));\r\nUPDATE requests SET q_checked_on=%(q_checked_on)s WHERE requests.id = %(id_1)s\r\n"
32275,sql: ALTER TABLE ... DROP STORED can be run on non-computed columnsWe should expect the second two `alter table` commands to fail.\r\n\r\n\r\n\r\nhttps://github.com/cockroachdb/cockroach/blob/5a43cbf833c9e653818d519d04ba6672d8cd4485/pkg/sql/alter_table.go#L742-L743 should check `col.IsComputed()`.,C-bug|A-schema-changes,bobvawter,"We should expect the second two `alter table` commands to fail.\r\n\r\n```sql\r\ncreate table a (a int primary key, b int as (a+1) stored);\r\n-- OK\r\nalter table a alter column b drop stored;\r\n-- Should fail\r\nalter table a alter column a drop stored;\r\n-- Should fail\r\nalter table a alter column b drop stored;\r\n```\r\n\r\nhttps://github.com/cockroachdb/cockroach/blob/5a43cbf833c9e653818d519d04ba6672d8cd4485/pkg/sql/alter_table.go#L742-L743 should check `col.IsComputed()`.","sql\r\ncreate table a (a int primary key, b int as (a+1) stored);\r\n-- OK\r\nalter table a alter column b drop stored;\r\n-- Should fail\r\nalter table a alter column a drop stored;\r\n-- Should fail\r\nalter table a alter column b drop stored;\r\n"
32254,"sql: GRANT admin TO user command hung SQL shell**Describe the problem**\r\n\r\nOn a one-node cluster running on master (a05f15b7223642484ef0364d81ddc462c306e954), I added an enterprise license and tried adding my user to the `admin` role. However, when I did so, the command never completed in my SQL shell, it just hung there for minutes on end:\r\n\r\n\r\n\r\nThe node's logs were full of the following log spam, where it just kept repeating the same message every couple hundred milliseconds ad infinitum:\r\n\r\n```\r\nI181113 09:24:40.895659 483 sql/lease.go:345  [n1,client=[::1]:62257,user=root] publish: descID=23 (role_members) version=2 mtime=2018-11-13 09:24:40.89189 +0000 UTC\r\nI181113 09:24:40.901066 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:40.901484 1659 sql/lease.go:876  new lease: 23(""role_members"") ver=2:1542101372.003237783,0, refcount=0\r\nI181113 09:24:40.927960 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:40.976218 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.062664 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.254359 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.472894 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.696796 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\n...\r\n```\r\n\r\nThe command had successfully taken effect -- the user `alex` was able to do things that required the admin role, but the statement never finished. When I ctrl+C'ed the shell, the log spam stopped with this final message:\r\n\r\n```\r\nW181113 09:26:36.644777 483 sql/schema_changer.go:796  [n1,client=[::1]:62257,user=root,scExec] aborted in distSender: context canceled\r\n```\r\n\r\n**To Reproduce**\r\n\r\nStart a one node cluster at the SHA above. Create a user. Set up an enterprise license. Try to grant the `admin` role to that user.\r\n\r\n**Expected behavior**\r\n\r\nThe command to return.\r\n\r\n@mberhault @vivekmenezes ",C-bug|A-sql-privileges|A-schema-descriptors|A-schema-changes,vivekmenezes,"**Describe the problem**\r\n\r\nOn a one-node cluster running on master (a05f15b7223642484ef0364d81ddc462c306e954), I added an enterprise license and tried adding my user to the `admin` role. However, when I did so, the command never completed in my SQL shell, it just hung there for minutes on end:\r\n\r\n```sql\r\nroot@:26257/defaultdb> grant admin to alex;\r\n```\r\n\r\nThe node's logs were full of the following log spam, where it just kept repeating the same message every couple hundred milliseconds ad infinitum:\r\n\r\n```\r\nI181113 09:24:40.895659 483 sql/lease.go:345  [n1,client=[::1]:62257,user=root] publish: descID=23 (role_members) version=2 mtime=2018-11-13 09:24:40.89189 +0000 UTC\r\nI181113 09:24:40.901066 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:40.901484 1659 sql/lease.go:876  new lease: 23(""role_members"") ver=2:1542101372.003237783,0, refcount=0\r\nI181113 09:24:40.927960 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:40.976218 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.062664 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.254359 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.472894 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\nI181113 09:24:41.696796 483 sql/lease.go:315  publish (1 leases): desc=[{role_members 23 1}]\r\n...\r\n```\r\n\r\nThe command had successfully taken effect -- the user `alex` was able to do things that required the admin role, but the statement never finished. When I ctrl+C'ed the shell, the log spam stopped with this final message:\r\n\r\n```\r\nW181113 09:26:36.644777 483 sql/schema_changer.go:796  [n1,client=[::1]:62257,user=root,scExec] aborted in distSender: context canceled\r\n```\r\n\r\n**To Reproduce**\r\n\r\nStart a one node cluster at the SHA above. Create a user. Set up an enterprise license. Try to grant the `admin` role to that user.\r\n\r\n**Expected behavior**\r\n\r\nThe command to return.\r\n\r\n@mberhault @vivekmenezes ",sql\r\nroot@:26257/defaultdb> grant admin to alex;\r\n
31882,"opt: incorrect execbuilder configuration of required orderings when optional columns are presentConsider the query:\r\n\r\nGroupBy requires ordering `+c opt(a,b)` and the index can provide this ordering:\r\n```\r\ngroup-by\r\n \u251c\u2500\u2500 columns: a:1(int) b:2(int) s:6(int[])\r\n \u251c\u2500\u2500 grouping columns: a:1(int) b:2(int)\r\n \u251c\u2500\u2500 internal-ordering: +3 opt(1,2)\r\n \u251c\u2500\u2500 stats: [rows=1000, distinct(1,2)=1000, null(1,2)=19.9]\r\n \u251c\u2500\u2500 cost: 1130\r\n \u251c\u2500\u2500 key: (1,2)\r\n \u251c\u2500\u2500 fd: (1,2)-->(6)\r\n \u251c\u2500\u2500 prune: (6)\r\n \u251c\u2500\u2500 scan abcd@ab\r\n \u2502    \u251c\u2500\u2500 columns: a:1(int) b:2(int) c:3(int) d:4(int)\r\n \u2502    \u251c\u2500\u2500 stats: [rows=1000, distinct(1,2)=1000, null(1,2)=19.9]\r\n \u2502    \u251c\u2500\u2500 cost: 1090\r\n \u2502    \u251c\u2500\u2500 ordering: +3 opt(1,2)\r\n \u2502    \u2514\u2500\u2500 prune: (1-4)\r\n \u2514\u2500\u2500 aggregations\r\n      \u2514\u2500\u2500 array-agg [type=int[], outer=(4)]\r\n           \u2514\u2500\u2500 variable: d [type=int]\r\n```\r\n\r\nThe problem is when we configure the required ordering of the scan:\r\n```\r\ngroup           \xb7            \xb7             (a, b, s)     \xb7\r\n \u2502              aggregate 0  a             \xb7             \xb7\r\n \u2502              aggregate 1  b             \xb7             \xb7\r\n \u2502              aggregate 2  array_agg(d)  \xb7             \xb7\r\n \u2502              group by     @1-@2         \xb7             \xb7\r\n \u2514\u2500\u2500 render     \xb7            \xb7             (a, b, d)     \xb7\r\n      \u2502         render 0     a             \xb7             \xb7\r\n      \u2502         render 1     b             \xb7             \xb7\r\n      \u2502         render 2     d             \xb7             \xb7\r\n      \u2514\u2500\u2500 scan  \xb7            \xb7             (a, b, c, d)  +c\r\n\xb7               table        abcd@ab       \xb7             \xb7\r\n\xb7               spans        ALL           \xb7             \xb7\r\n```\r\nThe index can cannot actually provide `+c`; it can provide `+a,+b,+c`. This information is used to configure distsql input synchronizers so this can result in incorrect results and/or `incorrectly ordered stream` errors.",C-bug|A-sql-optimizer,RaduBerinde,"Consider the query:\r\n```sql\r\nCREATE TABLE abcd (a INT, b INT, c INT, d INT, INDEX abc(a, b, c) STORING (d))\r\nSELECT a, b, ARRAY_AGG(d) AS s FROM (SELECT * FROM abcd ORDER BY c) GROUP BY a, b\r\n```\r\nGroupBy requires ordering `+c opt(a,b)` and the index can provide this ordering:\r\n```\r\ngroup-by\r\n \u251c\u2500\u2500 columns: a:1(int) b:2(int) s:6(int[])\r\n \u251c\u2500\u2500 grouping columns: a:1(int) b:2(int)\r\n \u251c\u2500\u2500 internal-ordering: +3 opt(1,2)\r\n \u251c\u2500\u2500 stats: [rows=1000, distinct(1,2)=1000, null(1,2)=19.9]\r\n \u251c\u2500\u2500 cost: 1130\r\n \u251c\u2500\u2500 key: (1,2)\r\n \u251c\u2500\u2500 fd: (1,2)-->(6)\r\n \u251c\u2500\u2500 prune: (6)\r\n \u251c\u2500\u2500 scan abcd@ab\r\n \u2502    \u251c\u2500\u2500 columns: a:1(int) b:2(int) c:3(int) d:4(int)\r\n \u2502    \u251c\u2500\u2500 stats: [rows=1000, distinct(1,2)=1000, null(1,2)=19.9]\r\n \u2502    \u251c\u2500\u2500 cost: 1090\r\n \u2502    \u251c\u2500\u2500 ordering: +3 opt(1,2)\r\n \u2502    \u2514\u2500\u2500 prune: (1-4)\r\n \u2514\u2500\u2500 aggregations\r\n      \u2514\u2500\u2500 array-agg [type=int[], outer=(4)]\r\n           \u2514\u2500\u2500 variable: d [type=int]\r\n```\r\n\r\nThe problem is when we configure the required ordering of the scan:\r\n```\r\ngroup           \xb7            \xb7             (a, b, s)     \xb7\r\n \u2502              aggregate 0  a             \xb7             \xb7\r\n \u2502              aggregate 1  b             \xb7             \xb7\r\n \u2502              aggregate 2  array_agg(d)  \xb7             \xb7\r\n \u2502              group by     @1-@2         \xb7             \xb7\r\n \u2514\u2500\u2500 render     \xb7            \xb7             (a, b, d)     \xb7\r\n      \u2502         render 0     a             \xb7             \xb7\r\n      \u2502         render 1     b             \xb7             \xb7\r\n      \u2502         render 2     d             \xb7             \xb7\r\n      \u2514\u2500\u2500 scan  \xb7            \xb7             (a, b, c, d)  +c\r\n\xb7               table        abcd@ab       \xb7             \xb7\r\n\xb7               spans        ALL           \xb7             \xb7\r\n```\r\nThe index can cannot actually provide `+c`; it can provide `+a,+b,+c`. This information is used to configure distsql input synchronizers so this can result in incorrect results and/or `incorrectly ordered stream` errors.","sql\r\nCREATE TABLE abcd (a INT, b INT, c INT, d INT, INDEX abc(a, b, c) STORING (d))\r\nSELECT a, b, ARRAY_AGG(d) AS s FROM (SELECT * FROM abcd ORDER BY c) GROUP BY a, b\r\n"
31823,"production: qps stalls on adriaticI deployed a new v2.1 release candidate a couple hours ago, and adriatic has had some qps stalls since (it also had some before the push, on yesterday's release candidate, too though):\r\n\r\n<img width=""930"" alt=""screen shot 2018-10-24 at 2 16 39 pm"" src=""https://user-images.githubusercontent.com/7085343/47455481-8ff48380-d797-11e8-9d1f-fa3fbf5b1e7d.png"">\r\n\r\nI was able to grab the queries that are stuck this time around. They don't change across multiple `SHOW QUERIES` calls:\r\n\r\n\r\n\r\nThere are only two ""problem ranges"", and they're both just non-active leases from a node's previous liveness epoch; nothing too suspicious.\r\n\r\nI'm continuing to investigate.",C-investigation,a-robinson,"I deployed a new v2.1 release candidate a couple hours ago, and adriatic has had some qps stalls since (it also had some before the push, on yesterday's release candidate, too though):\r\n\r\n<img width=""930"" alt=""screen shot 2018-10-24 at 2 16 39 pm"" src=""https://user-images.githubusercontent.com/7085343/47455481-8ff48380-d797-11e8-9d1f-fa3fbf5b1e7d.png"">\r\n\r\nI was able to grab the queries that are stuck this time around. They don't change across multiple `SHOW QUERIES` calls:\r\n\r\n```sql\r\nroot@:26257/defaultdb> select * from [show queries] order by start limit 10;\r\n              query_id             | node_id | user_name |              start               |                                                                                                                    query                                                                                                                     | client_address  | application_name | distributed |   phase\r\n+----------------------------------+---------+-----------+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+-------------+-----------+\r\n  1560a13726f6cf9a0000000000000006 |       6 | root      | 2018-10-24 19:12:10.809581+00:00 | UPDATE ""order"" SET o_carrier_id = 6 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227105), (3, 227939), (5, 227362), (8, 227767), (9, 227020), (1, 228429), (4, 226625), (6, 227548), (7, 227412), (10, 228144))) RETURNING o_d_id, o_c_id  | 127.0.0.1:57286 |                  |    false    | executing\r\n  1560a137446e3dbc0000000000000006 |       6 | root      | 2018-10-24 19:12:11.301989+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((7, 227412), (8, 227767), (10, 228144), (1, 228429), (3, 227939), (4, 226625), (6, 227548), (2, 227105), (5, 227362), (9, 227020))) RETURNING o_d_id, o_c_id  | 127.0.0.1:57268 |                  |    false    | executing\r\n  1560a137d084fc280000000000000004 |       4 | root      | 2018-10-24 19:12:13.654228+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 7)) AND (ol_o_id BETWEEN (227432 - 20) AND (227432 - 1))) AND (s_quantity < 16))   | 127.0.0.1:56480 |                  |    true     | executing\r\n  1560a137f99ce9840000000000000005 |       5 | root      | 2018-10-24 19:12:14.343641+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((8, 227768), (9, 227021), (1, 228430), (3, 227940), (5, 227363), (6, 227549), (7, 227413), (10, 228145), (2, 227106), (4, 226626))) RETURNING o_d_id, o_c_id  | 127.0.0.1:34926 |                  |    false    | executing\r\n  1560a137feac302e0000000000000004 |       4 | root      | 2018-10-24 19:12:14.428539+00:00 | UPDATE ""order"" SET o_carrier_id = 3 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227940), (4, 226626), (5, 227363), (10, 228145), (1, 228430), (2, 227106), (6, 227549), (7, 227413), (8, 227768), (9, 227021))) RETURNING o_d_id, o_c_id  | 127.0.0.1:56486 |                  |    false    | executing\r\n  1560a138c42aca6a0000000000000003 |       3 | root      | 2018-10-24 19:12:17.741935+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 5)) AND (ol_o_id BETWEEN (227380 - 20) AND (227380 - 1))) AND (s_quantity < 14))   | 127.0.0.1:35326 |                  |    true     | executing\r\n  1560a13f74022c600000000000000006 |       6 | root      | 2018-10-24 19:12:46.461861+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227115), (3, 227949), (5, 227372), (7, 227422), (8, 227777), (9, 227030), (10, 228154), (1, 228439), (4, 226635), (6, 227558))) RETURNING o_d_id, o_c_id | 127.0.0.1:57284 |                  |    false    | executing\r\n  1560a13fb978cf620000000000000006 |       6 | root      | 2018-10-24 19:12:47.627277+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227559), (8, 227778), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (3, 227950), (4, 226636), (5, 227373), (7, 227423))) RETURNING o_d_id, o_c_id | 127.0.0.1:57288 |                  |    false    | executing\r\n  1560a1405ca733470000000000000004 |       4 | root      | 2018-10-24 19:12:50.365056+00:00 | UPDATE ""order"" SET o_carrier_id = 5 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227950), (4, 226636), (5, 227373), (7, 227423), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (6, 227559), (8, 227778))) RETURNING o_d_id, o_c_id  | 127.0.0.1:56482 |                  |    false    | executing\r\n  1560a1437024a1180000000000000001 |       1 | root      | 2018-10-24 19:13:03.576917+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228443), (4, 226639), (5, 227376), (6, 227562), (9, 227034), (2, 227119), (3, 227953), (7, 227426), (8, 227781), (10, 228158))) RETURNING o_d_id, o_c_id | 127.0.0.1:33552 |                  |    false    | executing\r\n(10 rows)\r\n\r\nTime: 8.425971ms\r\n\r\nroot@:26257/defaultdb> select * from [show queries] order by start limit 25;\r\n              query_id             | node_id | user_name |              start               |                                                                                                                                                                                                                                                                                                  query                                                                                                                                                                                                                                                                                                  | client_address  | application_name | distributed |   phase\r\n+----------------------------------+---------+-----------+----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+-------------+-----------+\r\n  1560a13726f6cf9a0000000000000006 |       6 | root      | 2018-10-24 19:12:10.809581+00:00 | UPDATE ""order"" SET o_carrier_id = 6 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227105), (3, 227939), (5, 227362), (8, 227767), (9, 227020), (1, 228429), (4, 226625), (6, 227548), (7, 227412), (10, 228144))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57286 |                  |    false    | executing\r\n  1560a137446e3dbc0000000000000006 |       6 | root      | 2018-10-24 19:12:11.301989+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((7, 227412), (8, 227767), (10, 228144), (1, 228429), (3, 227939), (4, 226625), (6, 227548), (2, 227105), (5, 227362), (9, 227020))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57268 |                  |    false    | executing\r\n  1560a137d084fc280000000000000004 |       4 | root      | 2018-10-24 19:12:13.654228+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 7)) AND (ol_o_id BETWEEN (227432 - 20) AND (227432 - 1))) AND (s_quantity < 16))                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:56480 |                  |    true     | executing\r\n  1560a137f99ce9840000000000000005 |       5 | root      | 2018-10-24 19:12:14.343641+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((8, 227768), (9, 227021), (1, 228430), (3, 227940), (5, 227363), (6, 227549), (7, 227413), (10, 228145), (2, 227106), (4, 226626))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:34926 |                  |    false    | executing\r\n  1560a137feac302e0000000000000004 |       4 | root      | 2018-10-24 19:12:14.428539+00:00 | UPDATE ""order"" SET o_carrier_id = 3 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227940), (4, 226626), (5, 227363), (10, 228145), (1, 228430), (2, 227106), (6, 227549), (7, 227413), (8, 227768), (9, 227021))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56486 |                  |    false    | executing\r\n  1560a138c42aca6a0000000000000003 |       3 | root      | 2018-10-24 19:12:17.741935+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 5)) AND (ol_o_id BETWEEN (227380 - 20) AND (227380 - 1))) AND (s_quantity < 14))                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:35326 |                  |    true     | executing\r\n  1560a13f74022c600000000000000006 |       6 | root      | 2018-10-24 19:12:46.461861+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227115), (3, 227949), (5, 227372), (7, 227422), (8, 227777), (9, 227030), (10, 228154), (1, 228439), (4, 226635), (6, 227558))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57284 |                  |    false    | executing\r\n  1560a13fb978cf620000000000000006 |       6 | root      | 2018-10-24 19:12:47.627277+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227559), (8, 227778), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (3, 227950), (4, 226636), (5, 227373), (7, 227423))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57288 |                  |    false    | executing\r\n  1560a1405ca733470000000000000004 |       4 | root      | 2018-10-24 19:12:50.365056+00:00 | UPDATE ""order"" SET o_carrier_id = 5 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227950), (4, 226636), (5, 227373), (7, 227423), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (6, 227559), (8, 227778))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56482 |                  |    false    | executing\r\n  1560a1437024a1180000000000000001 |       1 | root      | 2018-10-24 19:13:03.576917+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228443), (4, 226639), (5, 227376), (6, 227562), (9, 227034), (2, 227119), (3, 227953), (7, 227426), (8, 227781), (10, 228158))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:33552 |                  |    false    | executing\r\n  1560a143d9da68480000000000000003 |       3 | root      | 2018-10-24 19:13:05.350421+00:00 | UPDATE ""order"" SET o_carrier_id = 8 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227954), (4, 226640), (5, 227377), (10, 228159), (1, 228444), (2, 227120), (6, 227563), (7, 227427), (8, 227782), (9, 227035))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:35344 |                  |    false    | executing\r\n  1560a144845952500000000000000004 |       4 | root      | 2018-10-24 19:13:08.210875+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228444), (3, 227954), (5, 227377), (6, 227563), (8, 227782), (9, 227035), (2, 227120), (4, 226640), (7, 227427), (10, 228159))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56474 |                  |    false    | executing\r\n  1560a1468ddd2cf00000000000000006 |       6 | root      | 2018-10-24 19:13:16.960368+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227565), (8, 227784), (9, 227037), (1, 228446), (5, 227379), (4, 226642), (7, 227429), (10, 228161), (2, 227122), (3, 227956))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57294 |                  |    false    | executing\r\n  1560a146958370b50000000000000006 |       6 | root      | 2018-10-24 19:13:17.088795+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227956), (4, 226642), (6, 227565), (9, 227037), (10, 228161), (1, 228446), (2, 227122), (5, 227379), (7, 227429), (8, 227784))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57280 |                  |    false    | executing\r\n  1560a146c6193c7e0000000000000002 |       2 | root      | 2018-10-24 19:13:17.90368+00:00  | UPDATE customer SET (c_balance, c_ytd_payment, c_payment_cnt, c_data) = (c_balance - 370.680000, c_ytd_payment + 370.680000, c_payment_cnt + 1, CASE c_credit WHEN 'BC' THEN left((((((c_id::STRING || c_d_id::STRING) || c_w_id::STRING) || 10::STRING) || 2::STRING) || 370.680000::STRING) || c_data, 500) ELSE c_data END) WHERE ((c_w_id = 9) AND (c_d_id = 1)) AND (c_id = 2400) RETURNING c_first, c_middle, c_last, c_street_1, c_street_2, c_city, c_state, c_zip, c_phone, c_since, c_credit, c_credit_lim, c_discount, c_balance, CASE c_credit WHEN 'BC' THEN left(c_data, 200) ELSE '' END | 127.0.0.1:56588 |                  |    false    | executing\r\n  1560a146cd5774f00000000000000003 |       3 | root      | 2018-10-24 19:13:18.02542+00:00  | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227565), (7, 227429), (8, 227784), (9, 227037), (10, 228161), (2, 227122), (3, 227956), (5, 227379), (1, 228446), (4, 226642))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:35352 |                  |    false    | executing\r\n  1560a146d84fa2de0000000000000001 |       1 | root      | 2018-10-24 19:13:18.209458+00:00 | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:33550 |                  |    false    | executing\r\n  1560a146db875da70000000000000002 |       2 | root      | 2018-10-24 19:13:18.263428+00:00 | UPDATE warehouse SET w_ytd = w_ytd + 1308.050000 WHERE w_id = 2 RETURNING w_name, w_street_1, w_street_2, w_city, w_state, w_zip                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 127.0.0.1:56596 |                  |    false    | executing\r\n  1560a146de14faff0000000000000003 |       3 | root      | 2018-10-24 19:13:18.306123+00:00 | UPDATE warehouse SET w_ytd = w_ytd + 3573.310000 WHERE w_id = 2 RETURNING w_name, w_street_1, w_street_2, w_city, w_state, w_zip                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 127.0.0.1:35342 |                  |    false    | executing\r\n  1560a146e4192aec0000000000000006 |       6 | root      | 2018-10-24 19:13:18.407267+00:00 | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:57290 |                  |    false    | executing\r\n  1560a146fc7d137a0000000000000006 |       6 | root      | 2018-10-24 19:13:18.79744+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:57298 |                  |    false    | executing\r\n  1560a147021c0d8a0000000000000004 |       4 | root      | 2018-10-24 19:13:18.910703+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((5, 227380), (7, 227430), (2, 227123), (3, 227957), (4, 226643), (9, 227038), (10, 228162), (1, 228447), (6, 227566), (8, 227785))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56456 |                  |    false    | executing\r\n  1560a1470a249c140000000000000005 |       5 | root      | 2018-10-24 19:13:19.04557+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:34916 |                  |    false    | executing\r\n  1560a1470cae87240000000000000001 |       1 | root      | 2018-10-24 19:13:19.08817+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:33526 |                  |    false    | executing\r\n  1560a1470fcaaa8a0000000000000005 |       5 | root      | 2018-10-24 19:13:19.140293+00:00 | SELECT d_next_o_id FROM district WHERE (d_w_id = 2) AND (d_id = 6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 127.0.0.1:34904 |                  |    false    | executing\r\n(25 rows)\r\n\r\nTime: 21.42194ms\r\n```\r\n\r\nThere are only two ""problem ranges"", and they're both just non-active leases from a node's previous liveness epoch; nothing too suspicious.\r\n\r\nI'm continuing to investigate.","sql\r\nroot@:26257/defaultdb> select * from [show queries] order by start limit 10;\r\n              query_id             | node_id | user_name |              start               |                                                                                                                    query                                                                                                                     | client_address  | application_name | distributed |   phase\r\n+----------------------------------+---------+-----------+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+-------------+-----------+\r\n  1560a13726f6cf9a0000000000000006 |       6 | root      | 2018-10-24 19:12:10.809581+00:00 | UPDATE ""order"" SET o_carrier_id = 6 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227105), (3, 227939), (5, 227362), (8, 227767), (9, 227020), (1, 228429), (4, 226625), (6, 227548), (7, 227412), (10, 228144))) RETURNING o_d_id, o_c_id  | 127.0.0.1:57286 |                  |    false    | executing\r\n  1560a137446e3dbc0000000000000006 |       6 | root      | 2018-10-24 19:12:11.301989+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((7, 227412), (8, 227767), (10, 228144), (1, 228429), (3, 227939), (4, 226625), (6, 227548), (2, 227105), (5, 227362), (9, 227020))) RETURNING o_d_id, o_c_id  | 127.0.0.1:57268 |                  |    false    | executing\r\n  1560a137d084fc280000000000000004 |       4 | root      | 2018-10-24 19:12:13.654228+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 7)) AND (ol_o_id BETWEEN (227432 - 20) AND (227432 - 1))) AND (s_quantity < 16))   | 127.0.0.1:56480 |                  |    true     | executing\r\n  1560a137f99ce9840000000000000005 |       5 | root      | 2018-10-24 19:12:14.343641+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((8, 227768), (9, 227021), (1, 228430), (3, 227940), (5, 227363), (6, 227549), (7, 227413), (10, 228145), (2, 227106), (4, 226626))) RETURNING o_d_id, o_c_id  | 127.0.0.1:34926 |                  |    false    | executing\r\n  1560a137feac302e0000000000000004 |       4 | root      | 2018-10-24 19:12:14.428539+00:00 | UPDATE ""order"" SET o_carrier_id = 3 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227940), (4, 226626), (5, 227363), (10, 228145), (1, 228430), (2, 227106), (6, 227549), (7, 227413), (8, 227768), (9, 227021))) RETURNING o_d_id, o_c_id  | 127.0.0.1:56486 |                  |    false    | executing\r\n  1560a138c42aca6a0000000000000003 |       3 | root      | 2018-10-24 19:12:17.741935+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 5)) AND (ol_o_id BETWEEN (227380 - 20) AND (227380 - 1))) AND (s_quantity < 14))   | 127.0.0.1:35326 |                  |    true     | executing\r\n  1560a13f74022c600000000000000006 |       6 | root      | 2018-10-24 19:12:46.461861+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227115), (3, 227949), (5, 227372), (7, 227422), (8, 227777), (9, 227030), (10, 228154), (1, 228439), (4, 226635), (6, 227558))) RETURNING o_d_id, o_c_id | 127.0.0.1:57284 |                  |    false    | executing\r\n  1560a13fb978cf620000000000000006 |       6 | root      | 2018-10-24 19:12:47.627277+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227559), (8, 227778), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (3, 227950), (4, 226636), (5, 227373), (7, 227423))) RETURNING o_d_id, o_c_id | 127.0.0.1:57288 |                  |    false    | executing\r\n  1560a1405ca733470000000000000004 |       4 | root      | 2018-10-24 19:12:50.365056+00:00 | UPDATE ""order"" SET o_carrier_id = 5 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227950), (4, 226636), (5, 227373), (7, 227423), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (6, 227559), (8, 227778))) RETURNING o_d_id, o_c_id  | 127.0.0.1:56482 |                  |    false    | executing\r\n  1560a1437024a1180000000000000001 |       1 | root      | 2018-10-24 19:13:03.576917+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228443), (4, 226639), (5, 227376), (6, 227562), (9, 227034), (2, 227119), (3, 227953), (7, 227426), (8, 227781), (10, 228158))) RETURNING o_d_id, o_c_id | 127.0.0.1:33552 |                  |    false    | executing\r\n(10 rows)\r\n\r\nTime: 8.425971ms\r\n\r\nroot@:26257/defaultdb> select * from [show queries] order by start limit 25;\r\n              query_id             | node_id | user_name |              start               |                                                                                                                                                                                                                                                                                                  query                                                                                                                                                                                                                                                                                                  | client_address  | application_name | distributed |   phase\r\n+----------------------------------+---------+-----------+----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+-------------+-----------+\r\n  1560a13726f6cf9a0000000000000006 |       6 | root      | 2018-10-24 19:12:10.809581+00:00 | UPDATE ""order"" SET o_carrier_id = 6 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227105), (3, 227939), (5, 227362), (8, 227767), (9, 227020), (1, 228429), (4, 226625), (6, 227548), (7, 227412), (10, 228144))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57286 |                  |    false    | executing\r\n  1560a137446e3dbc0000000000000006 |       6 | root      | 2018-10-24 19:12:11.301989+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((7, 227412), (8, 227767), (10, 228144), (1, 228429), (3, 227939), (4, 226625), (6, 227548), (2, 227105), (5, 227362), (9, 227020))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57268 |                  |    false    | executing\r\n  1560a137d084fc280000000000000004 |       4 | root      | 2018-10-24 19:12:13.654228+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 7)) AND (ol_o_id BETWEEN (227432 - 20) AND (227432 - 1))) AND (s_quantity < 16))                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:56480 |                  |    true     | executing\r\n  1560a137f99ce9840000000000000005 |       5 | root      | 2018-10-24 19:12:14.343641+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((8, 227768), (9, 227021), (1, 228430), (3, 227940), (5, 227363), (6, 227549), (7, 227413), (10, 228145), (2, 227106), (4, 226626))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:34926 |                  |    false    | executing\r\n  1560a137feac302e0000000000000004 |       4 | root      | 2018-10-24 19:12:14.428539+00:00 | UPDATE ""order"" SET o_carrier_id = 3 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227940), (4, 226626), (5, 227363), (10, 228145), (1, 228430), (2, 227106), (6, 227549), (7, 227413), (8, 227768), (9, 227021))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56486 |                  |    false    | executing\r\n  1560a138c42aca6a0000000000000003 |       3 | root      | 2018-10-24 19:12:17.741935+00:00 | SELECT count(*) FROM (SELECT DISTINCT s_i_id FROM order_line JOIN stock ON (s_i_id = ol_i_id) AND (s_w_id = ol_w_id) WHERE (((ol_w_id = 9) AND (ol_d_id = 5)) AND (ol_o_id BETWEEN (227380 - 20) AND (227380 - 1))) AND (s_quantity < 14))                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:35326 |                  |    true     | executing\r\n  1560a13f74022c600000000000000006 |       6 | root      | 2018-10-24 19:12:46.461861+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((2, 227115), (3, 227949), (5, 227372), (7, 227422), (8, 227777), (9, 227030), (10, 228154), (1, 228439), (4, 226635), (6, 227558))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57284 |                  |    false    | executing\r\n  1560a13fb978cf620000000000000006 |       6 | root      | 2018-10-24 19:12:47.627277+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227559), (8, 227778), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (3, 227950), (4, 226636), (5, 227373), (7, 227423))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57288 |                  |    false    | executing\r\n  1560a1405ca733470000000000000004 |       4 | root      | 2018-10-24 19:12:50.365056+00:00 | UPDATE ""order"" SET o_carrier_id = 5 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227950), (4, 226636), (5, 227373), (7, 227423), (9, 227031), (10, 228155), (1, 228440), (2, 227116), (6, 227559), (8, 227778))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56482 |                  |    false    | executing\r\n  1560a1437024a1180000000000000001 |       1 | root      | 2018-10-24 19:13:03.576917+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228443), (4, 226639), (5, 227376), (6, 227562), (9, 227034), (2, 227119), (3, 227953), (7, 227426), (8, 227781), (10, 228158))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:33552 |                  |    false    | executing\r\n  1560a143d9da68480000000000000003 |       3 | root      | 2018-10-24 19:13:05.350421+00:00 | UPDATE ""order"" SET o_carrier_id = 8 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227954), (4, 226640), (5, 227377), (10, 228159), (1, 228444), (2, 227120), (6, 227563), (7, 227427), (8, 227782), (9, 227035))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:35344 |                  |    false    | executing\r\n  1560a144845952500000000000000004 |       4 | root      | 2018-10-24 19:13:08.210875+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((1, 228444), (3, 227954), (5, 227377), (6, 227563), (8, 227782), (9, 227035), (2, 227120), (4, 226640), (7, 227427), (10, 228159))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56474 |                  |    false    | executing\r\n  1560a1468ddd2cf00000000000000006 |       6 | root      | 2018-10-24 19:13:16.960368+00:00 | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227565), (8, 227784), (9, 227037), (1, 228446), (5, 227379), (4, 226642), (7, 227429), (10, 228161), (2, 227122), (3, 227956))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:57294 |                  |    false    | executing\r\n  1560a146958370b50000000000000006 |       6 | root      | 2018-10-24 19:13:17.088795+00:00 | UPDATE ""order"" SET o_carrier_id = 10 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((3, 227956), (4, 226642), (6, 227565), (9, 227037), (10, 228161), (1, 228446), (2, 227122), (5, 227379), (7, 227429), (8, 227784))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                            | 127.0.0.1:57280 |                  |    false    | executing\r\n  1560a146c6193c7e0000000000000002 |       2 | root      | 2018-10-24 19:13:17.90368+00:00  | UPDATE customer SET (c_balance, c_ytd_payment, c_payment_cnt, c_data) = (c_balance - 370.680000, c_ytd_payment + 370.680000, c_payment_cnt + 1, CASE c_credit WHEN 'BC' THEN left((((((c_id::STRING || c_d_id::STRING) || c_w_id::STRING) || 10::STRING) || 2::STRING) || 370.680000::STRING) || c_data, 500) ELSE c_data END) WHERE ((c_w_id = 9) AND (c_d_id = 1)) AND (c_id = 2400) RETURNING c_first, c_middle, c_last, c_street_1, c_street_2, c_city, c_state, c_zip, c_phone, c_since, c_credit, c_credit_lim, c_discount, c_balance, CASE c_credit WHEN 'BC' THEN left(c_data, 200) ELSE '' END | 127.0.0.1:56588 |                  |    false    | executing\r\n  1560a146cd5774f00000000000000003 |       3 | root      | 2018-10-24 19:13:18.02542+00:00  | UPDATE ""order"" SET o_carrier_id = 7 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((6, 227565), (7, 227429), (8, 227784), (9, 227037), (10, 228161), (2, 227122), (3, 227956), (5, 227379), (1, 228446), (4, 226642))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:35352 |                  |    false    | executing\r\n  1560a146d84fa2de0000000000000001 |       1 | root      | 2018-10-24 19:13:18.209458+00:00 | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:33550 |                  |    false    | executing\r\n  1560a146db875da70000000000000002 |       2 | root      | 2018-10-24 19:13:18.263428+00:00 | UPDATE warehouse SET w_ytd = w_ytd + 1308.050000 WHERE w_id = 2 RETURNING w_name, w_street_1, w_street_2, w_city, w_state, w_zip                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 127.0.0.1:56596 |                  |    false    | executing\r\n  1560a146de14faff0000000000000003 |       3 | root      | 2018-10-24 19:13:18.306123+00:00 | UPDATE warehouse SET w_ytd = w_ytd + 3573.310000 WHERE w_id = 2 RETURNING w_name, w_street_1, w_street_2, w_city, w_state, w_zip                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 127.0.0.1:35342 |                  |    false    | executing\r\n  1560a146e4192aec0000000000000006 |       6 | root      | 2018-10-24 19:13:18.407267+00:00 | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:57290 |                  |    false    | executing\r\n  1560a146fc7d137a0000000000000006 |       6 | root      | 2018-10-24 19:13:18.79744+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:57298 |                  |    false    | executing\r\n  1560a147021c0d8a0000000000000004 |       4 | root      | 2018-10-24 19:13:18.910703+00:00 | UPDATE ""order"" SET o_carrier_id = 4 WHERE (o_w_id = 9) AND ((o_d_id, o_id) IN ((5, 227380), (7, 227430), (2, 227123), (3, 227957), (4, 226643), (9, 227038), (10, 228162), (1, 228447), (6, 227566), (8, 227785))) RETURNING o_d_id, o_c_id                                                                                                                                                                                                                                                                                                                                                             | 127.0.0.1:56456 |                  |    false    | executing\r\n  1560a1470a249c140000000000000005 |       5 | root      | 2018-10-24 19:13:19.04557+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:34916 |                  |    false    | executing\r\n  1560a1470cae87240000000000000001 |       1 | root      | 2018-10-24 19:13:19.08817+00:00  | SELECT w_tax FROM warehouse WHERE w_id = 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 127.0.0.1:33526 |                  |    false    | executing\r\n  1560a1470fcaaa8a0000000000000005 |       5 | root      | 2018-10-24 19:13:19.140293+00:00 | SELECT d_next_o_id FROM district WHERE (d_w_id = 2) AND (d_id = 6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 127.0.0.1:34904 |                  |    false    | executing\r\n(25 rows)\r\n\r\nTime: 21.42194ms\r\n"
31673,sql: internal error distsql physical planning **Describe the problem**\r\n\r\nFound via the Hibernate test org.hibernate.test.subselect.SubselectTest.testCustomColumnReadAndWrite\r\n\r\nQuery fails with CodeInternalError\r\n\r\n```\r\nconflicting ColumnTypes: semantic_type:INT width:0 precision:0 visible_type:NONE  and semantic_type:INT width:64 precision:0 visible_type:BIGINT\r\n```\r\n\r\n**To Reproduce**\r\n\r\nExtracting the statements from a SQL statement log:\r\n\r\n,C-bug|A-sql-execution|A-tools-hibernate|S-3,jordanlewis,"**Describe the problem**\r\n\r\nFound via the Hibernate test org.hibernate.test.subselect.SubselectTest.testCustomColumnReadAndWrite\r\n\r\nQuery fails with CodeInternalError\r\n\r\n```\r\nconflicting ColumnTypes: semantic_type:INT width:0 precision:0 visible_type:NONE  and semantic_type:INT width:64 precision:0 visible_type:BIGINT\r\n```\r\n\r\n**To Reproduce**\r\n\r\nExtracting the statements from a SQL statement log:\r\n\r\n```sql\r\nCREATE TABLE humans (bid INT8 NOT NULL, name VARCHAR(255) NOT NULL, sex CHAR NOT NULL, address VARCHAR(255), height_centimeters FLOAT8 NOT NULL, PRIMARY KEY (bid));\r\nCREATE TABLE aliens (bid INT8 NOT NULL, ident VARCHAR(255) NOT NULL, planet VARCHAR(255), species VARCHAR(255) NOT NULL, height_centimeters FLOAT8 NOT NULL, PRIMARY KEY (bid));\r\n\r\n        select\r\n            being0_.height_centimeters / 2.54E0 as col_0_0_\r\n        from\r\n            (     select\r\n                bid,\r\n                name as ident,\r\n                address as loc,\r\n                'human' as species,\r\n                height_centimeters\r\n            from\r\n                humans\r\n            union\r\n            select\r\n                bid,\r\n                ident,\r\n                planet as loc,\r\n                species,\r\n                height_centimeters\r\n            from\r\n                aliens\r\n        ) being0_\r\n    where\r\n        being0_.ident='gavin';\r\n```","sql\r\nCREATE TABLE humans (bid INT8 NOT NULL, name VARCHAR(255) NOT NULL, sex CHAR NOT NULL, address VARCHAR(255), height_centimeters FLOAT8 NOT NULL, PRIMARY KEY (bid));\r\nCREATE TABLE aliens (bid INT8 NOT NULL, ident VARCHAR(255) NOT NULL, planet VARCHAR(255), species VARCHAR(255) NOT NULL, height_centimeters FLOAT8 NOT NULL, PRIMARY KEY (bid));\r\n\r\n        select\r\n            being0_.height_centimeters / 2.54E0 as col_0_0_\r\n        from\r\n            (     select\r\n                bid,\r\n                name as ident,\r\n                address as loc,\r\n                'human' as species,\r\n                height_centimeters\r\n            from\r\n                humans\r\n            union\r\n            select\r\n                bid,\r\n                ident,\r\n                planet as loc,\r\n                species,\r\n                height_centimeters\r\n            from\r\n                aliens\r\n        ) being0_\r\n    where\r\n        being0_.ident='gavin';\r\n"
31628,"opt: lax key info is lost on projections\r\n\r\nNote that the distinct is not elided; we don't know that `(a,b)` forms a key.\r\n\r\nThe problem is that we are projecting away the implicit pk and from that point we lose the information that `(a,b)` was a lax key.",C-bug|A-sql-optimizer,RaduBerinde,"```sql\r\nCREATE TABLE t (a INT, b INT, UNIQUE (a, b))\r\nSELECT DISTINCT a, b FROM t WHERE (a, b) IN ((1, 1), (2, 2))\r\n  distinct-on\r\n   \u251c\u2500\u2500 columns: a:1(int!null) b:2(int!null)\r\n   \u251c\u2500\u2500 grouping columns: a:1(int!null) b:2(int!null)\r\n   \u251c\u2500\u2500 key: (1,2)\r\n   \u2514\u2500\u2500 scan t@secondary\r\n        \u251c\u2500\u2500 columns: a:1(int!null) b:2(int!null)\r\n        \u2514\u2500\u2500 constraint: /1/2: [/1/1 - /1/1] [/2/2 - /2/2]\r\n```\r\n\r\nNote that the distinct is not elided; we don't know that `(a,b)` forms a key.\r\n\r\nThe problem is that we are projecting away the implicit pk and from that point we lose the information that `(a,b)` was a lax key.","sql\r\nCREATE TABLE t (a INT, b INT, UNIQUE (a, b))\r\nSELECT DISTINCT a, b FROM t WHERE (a, b) IN ((1, 1), (2, 2))\r\n  distinct-on\r\n   \u251c\u2500\u2500 columns: a:1(int!null) b:2(int!null)\r\n   \u251c\u2500\u2500 grouping columns: a:1(int!null) b:2(int!null)\r\n   \u251c\u2500\u2500 key: (1,2)\r\n   \u2514\u2500\u2500 scan t@secondary\r\n        \u251c\u2500\u2500 columns: a:1(int!null) b:2(int!null)\r\n        \u2514\u2500\u2500 constraint: /1/2: [/1/1 - /1/1] [/2/2 - /2/2]\r\n"
31523,"sql: DROP TABLE in transaction is ignoredIf you drop a table in a transaction, you can't create a new one of the same name, in the same transaction.\r\n\r\n\r\n\r\nHere is what it looks like in the console:\r\n\r\n\r\nIf you commit the transaction before adding in the new table, it works as expected.\r\n\r\nThis was found via the hibernate test failure investigation.",C-bug|A-tools-hibernate,vivekmenezes,"If you drop a table in a transaction, you can't create a new one of the same name, in the same transaction.\r\n\r\n```sql\r\nCREATE TABLE test (id int primary key);\r\n\r\nBEGIN;\r\nDROP TABLE test;\r\nCREATE TABLE test (id int primary key);\r\n```\r\n\r\nHere is what it looks like in the console:\r\n```sql\r\nroot@:26257/test> CREATE TABLE test (id int primary key);\r\nCREATE TABLE\r\n\r\nTime: 7.454ms\r\n\r\nroot@:26257/test> begin;\r\nNow adding input for a multi-line SQL transaction client-side.\r\nPress Enter two times to send the SQL text collected so far to the server, or Ctrl+C to cancel.\r\nYou can also use \\show to display the statements entered so far.\r\n               ->\r\nBEGIN\r\n\r\nTime: 181\xb5s\r\n\r\nroot@:26257/test  OPEN> drop table test;\r\nDROP TABLE\r\n\r\nTime: 12.045ms\r\n\r\nroot@:26257/test  OPEN> create table test (id int primary key);\r\npq: relation ""test"" already exists\r\nroot@:26257/? ERROR>\r\n```\r\n\r\nIf you commit the transaction before adding in the new table, it works as expected.\r\n\r\nThis was found via the hibernate test failure investigation.",sql\r\nCREATE TABLE test (id int primary key);\r\n\r\nBEGIN;\r\nDROP TABLE test;\r\nCREATE TABLE test (id int primary key);\r\n
31255,"sql: `INSERT ON CONFLICT DO UPDATE` can reference computed columns\r\n\r\nThe SET is allowed on computed columns, the assignment is internally processed but then discarded (overwritten) by  the computed column computation.\r\n\r\ncc @jordanlewis ",C-bug|A-sql-mutations,knz,"```sql\r\n> create table tc(x int primary key, y int as (x+1) stored);\r\n> insert into tc(x) values(1) on conflict(x) do update set y = 456; -- woops?\r\n> select * from tc;\r\n x | y\r\n 1 | 2\r\n> insert into tc(x) values(1) on conflict(x) do update set y = 123; -- woops?\r\n> select * from tc;\r\n x | y\r\n 1 | 2\r\n```\r\n\r\nThe SET is allowed on computed columns, the assignment is internally processed but then discarded (overwritten) by  the computed column computation.\r\n\r\ncc @jordanlewis ","sql\r\n> create table tc(x int primary key, y int as (x+1) stored);\r\n> insert into tc(x) values(1) on conflict(x) do update set y = 456; -- woops?\r\n> select * from tc;\r\n x | y\r\n 1 | 2\r\n> insert into tc(x) values(1) on conflict(x) do update set y = 123; -- woops?\r\n> select * from tc;\r\n x | y\r\n 1 | 2\r\n"
30976,sql: Timestamp math is inconsistent with PostgreSQLHere it fails to work with days:\r\n\r\n\r\n\r\nHere's it working with seconds:\r\n\r\n\r\n\r\nThere are also no tests for this flavour of generate_series that I was able to find.,C-bug|A-sql-pgcompat|A-sql-builtins,bobvawter,"Here it fails to work with days:\r\n\r\n```sql\r\nselect i::date from generate_series('2012-06-29'::timestamp, '2012-07-03'::timestamp, '1 day'::interval) i;\r\npq: step cannot be 0\r\n```\r\n\r\nHere's it working with seconds:\r\n\r\n```sql\r\nselect i::date from generate_series('2012-06-29'::timestamp, '2012-07-03'::timestamp, '86400 seconds'::interval) i;\r\n              i\r\n+---------------------------+\r\n  2012-06-29 00:00:00+00:00\r\n  2012-06-30 00:00:00+00:00\r\n  2012-07-01 00:00:00+00:00\r\n  2012-07-02 00:00:00+00:00\r\n  2012-07-03 00:00:00+00:00\r\n(5 rows)\r\n\r\nTime: 485\xb5s\r\n```\r\n\r\nThere are also no tests for this flavour of generate_series that I was able to find.","sql\r\nselect i::date from generate_series('2012-06-29'::timestamp, '2012-07-03'::timestamp, '1 day'::interval) i;\r\npq: step cannot be 0\r\n"
30786,"ON CONFLICT RETURNING incorrect data returned on v2.1.0-beta.20180924**BUG REPORT**\r\n\r\nBug is observed on docker image `cockroachdb/cockroach-unstable:v2.1.0-beta.20180924`\r\n\r\nSQL:\r\n\r\n\r\n\r\nResult:\r\n```\r\nDROP DATABASE\r\nCREATE DATABASE\r\nSET\r\nDROP TABLE\r\nCREATE TABLE\r\nid      a       b\r\n1       1       387103546467024897\r\n1       2       387103546467057665\r\n```\r\n\r\nAs you can see CDB messes up the columns, reporting ""id"" on ""b"". Previously it worked fine, for example on `cockroachdb/cockroach:v2.0.5`:\r\n\r\n```\r\nDROP DATABASE\r\nCREATE DATABASE\r\nSET\r\nDROP TABLE\r\nCREATE TABLE\r\nid      a       b\r\n387103986403115009      1       1\r\n387103986403147777      1       2\r\n```\r\n\r\nIt is related to ON CONFLICT (a, b) DO NOTHING part of the SQL statement, works fine without it.",C-bug|O-community|A-sql-execution|A-sql-mutations|S-3-erroneous-edge-case,BramGruneir,"**BUG REPORT**\r\n\r\nBug is observed on docker image `cockroachdb/cockroach-unstable:v2.1.0-beta.20180924`\r\n\r\nSQL:\r\n\r\n```sql\r\nDROP DATABASE IF EXISTS foobar;\r\nCREATE DATABASE foobar;\r\n\r\nSET database = foobar;\r\n\r\nDROP TABLE IF EXISTS foobar;\r\nCREATE TABLE foobar (\r\n\tid INT NOT NULL DEFAULT unique_rowid(),\r\n\ta int not null,\r\n\tb int not null,\r\n\tCONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX foobar_uniq (a, b)\r\n);\r\n\r\nINSERT INTO foobar (a, b) values (1, 1), (1, 2) ON CONFLICT (a, b) DO NOTHING RETURNING id, a, b;\r\n```\r\n\r\nResult:\r\n```\r\nDROP DATABASE\r\nCREATE DATABASE\r\nSET\r\nDROP TABLE\r\nCREATE TABLE\r\nid      a       b\r\n1       1       387103546467024897\r\n1       2       387103546467057665\r\n```\r\n\r\nAs you can see CDB messes up the columns, reporting ""id"" on ""b"". Previously it worked fine, for example on `cockroachdb/cockroach:v2.0.5`:\r\n\r\n```\r\nDROP DATABASE\r\nCREATE DATABASE\r\nSET\r\nDROP TABLE\r\nCREATE TABLE\r\nid      a       b\r\n387103986403115009      1       1\r\n387103986403147777      1       2\r\n```\r\n\r\nIt is related to ON CONFLICT (a, b) DO NOTHING part of the SQL statement, works fine without it.","sql\r\nDROP DATABASE IF EXISTS foobar;\r\nCREATE DATABASE foobar;\r\n\r\nSET database = foobar;\r\n\r\nDROP TABLE IF EXISTS foobar;\r\nCREATE TABLE foobar (\r\n\tid INT NOT NULL DEFAULT unique_rowid(),\r\n\ta int not null,\r\n\tb int not null,\r\n\tCONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX foobar_uniq (a, b)\r\n);\r\n\r\nINSERT INTO foobar (a, b) values (1, 1), (1, 2) ON CONFLICT (a, b) DO NOTHING RETURNING id, a, b;\r\n"
30586,"sql: aggregation in window does not use index automaticallyReported by @bladefist on Gitter.\r\nForked from  #24415.\r\n\r\n**BUG REPORT**\r\n\r\nPlease describe the issue you observed, and any steps we can take to reproduce it:\r\n\r\n- Which version of CockroachDB are you using?\r\n\r\n2.1 beta / master\r\n\r\n- What did you do?\r\n\r\n\r\n\r\n- What did you expect to see?\r\n\r\nQuery uses index foo\r\n\r\n- What did you see instead?\r\n\r\nUses full table scan\r\n\r\n- What was the impact?\r\n\r\nPerformance\r\n\r\ncc @andy-kimball @RaduBerinde \r\n\r\n",C-performance|O-community|A-sql-optimizer,justinj|andy-kimball,"Reported by @bladefist on Gitter.\r\nForked from  #24415.\r\n\r\n**BUG REPORT**\r\n\r\nPlease describe the issue you observed, and any steps we can take to reproduce it:\r\n\r\n- Which version of CockroachDB are you using?\r\n\r\n2.1 beta / master\r\n\r\n- What did you do?\r\n\r\n```sql\r\nCREATE TABLE data (\r\n  id UUID NULL,\r\n  value INT NULL,\r\n  col1 INT NULL,   \r\n  col2 INT NULL,   \r\n  col3 INT NULL,   \r\n  col4 INT NULL,   \r\n  col5 INT NULL,   \r\n  col6 INT NULL,   \r\n  col7 INT NULL,   \r\n  col8 INT NULL,   \r\n  col9 INT NULL,   \r\n  col10 INT NULL,   \r\n  INDEX foo (id ASC) STORING (value)\r\n);\r\n\r\nSELECT id, rank() over(order by sum(value) desc) FROM data GROUP BY id;\r\n```\r\n\r\n- What did you expect to see?\r\n\r\nQuery uses index foo\r\n\r\n- What did you see instead?\r\n\r\nUses full table scan\r\n\r\n- What was the impact?\r\n\r\nPerformance\r\n\r\ncc @andy-kimball @RaduBerinde \r\n\r\n","sql\r\nCREATE TABLE data (\r\n  id UUID NULL,\r\n  value INT NULL,\r\n  col1 INT NULL,   \r\n  col2 INT NULL,   \r\n  col3 INT NULL,   \r\n  col4 INT NULL,   \r\n  col5 INT NULL,   \r\n  col6 INT NULL,   \r\n  col7 INT NULL,   \r\n  col8 INT NULL,   \r\n  col9 INT NULL,   \r\n  col10 INT NULL,   \r\n  INDEX foo (id ASC) STORING (value)\r\n);\r\n\r\nSELECT id, rank() over(order by sum(value) desc) FROM data GROUP BY id;\r\n"
30026,"sql: MATCH FULL is incorrectly implementedThis goes along with #20305.  Which is about implementing MATCH SIMPLE.\r\n\r\nWe don't implement MATCH FULL correctly.\r\n\r\nHere's the definition of MATCH FULL:\r\n\r\n> MATCH FULL considers a foreign key as valid if every foreign key column is\r\nnull or if all foreign key columns are not null and match the referenced\r\ntable.\r\n\r\nWe take this one step too far. \r\n\r\n* If all the values are NULL, it is considered valid (and shouldn't cascade updates or deletes). __We do this correctly.__\r\n* If no values are NULL, they all need to exist in the referenced table. __We do this correctly.__\r\n* If some values are NULL but not all, the reference is incorrect. __We incorrectly deal with this case.__ Instead check for values in the referenced table turning all the NULLs into a value that can be matched against.  \r\n\r\n\r\n\r\n\r\nWe even outright call this out in our docs:\r\nhttps://www.cockroachlabs.com/docs/stable/foreign-key.html#null-values\r\n\r\n> Multiple-column foreign keys only accept NULL values in these scenarios:\r\n> * The row you're ultimately referencing\u2014determined by the statement's other values\u2014contains NULL as the value of the referenced column (i.e., NULL is valid from the perspective of referential integrity)\r\n> * The write contains NULL values for all foreign key columns\r\n",C-bug|A-sql-pgcompat|A-sql-mutations|S-3-erroneous-edge-case,BramGruneir,"This goes along with #20305.  Which is about implementing MATCH SIMPLE.\r\n\r\nWe don't implement MATCH FULL correctly.\r\n\r\nHere's the definition of MATCH FULL:\r\n\r\n> MATCH FULL considers a foreign key as valid if every foreign key column is\r\nnull or if all foreign key columns are not null and match the referenced\r\ntable.\r\n\r\nWe take this one step too far. \r\n\r\n* If all the values are NULL, it is considered valid (and shouldn't cascade updates or deletes). __We do this correctly.__\r\n* If no values are NULL, they all need to exist in the referenced table. __We do this correctly.__\r\n* If some values are NULL but not all, the reference is incorrect. __We incorrectly deal with this case.__ Instead check for values in the referenced table turning all the NULLs into a value that can be matched against.  \r\n\r\n\r\n```sql\r\nCREATE TABLE a (\r\n  x INT\r\n ,y INT\r\n ,UNIQUE (x, y)\r\n);\r\nCREATE TABLE b (\r\n  x INT\r\n ,y INT\r\n ,INDEX (x, y)\r\n ,FOREIGN KEY (x, y) REFERENCES a (x, y) ON DELETE CASCADE ON UPDATE CASCADE\r\n);\r\n\r\nINSERT INTO b VALUES (NULL,NULL);\r\n# This should work. (and does)\r\n\r\nINSERT INTO b VALUES (1, NULL);\r\n# This should fail, due to it having a NULL. \r\n# But instead we get: foreign key violation: value [1 NULL] not found in a@a_x_y_key [x y]\r\n# Postgres reports this correctly (note that I had to include MATCH FULL in the FK reference).\r\n# ERROR:  insert or update on table ""b"" violates foreign key constraint ""b_x_fkey""\r\n# DETAIL:  MATCH FULL does not allow mixing of null and nonnull key values.\r\n\r\nINSERT INTO a VALUE (1, NULL);\r\n\r\nINSERT INTO b VALUE (1, NULL);\r\n# This should fail, due to it having a NULL. But passes now that (1, NULL) exists in a.\r\n# On postgres, it still fails due to the null/non-null mixing:\r\n# ERROR:  insert or update on table ""b"" violates foreign key constraint ""b_x_fkey""\r\n# DETAIL:  MATCH FULL does not allow mixing of null and nonnull key values.\r\n```\r\n\r\nWe even outright call this out in our docs:\r\nhttps://www.cockroachlabs.com/docs/stable/foreign-key.html#null-values\r\n\r\n> Multiple-column foreign keys only accept NULL values in these scenarios:\r\n> * The row you're ultimately referencing\u2014determined by the statement's other values\u2014contains NULL as the value of the referenced column (i.e., NULL is valid from the perspective of referential integrity)\r\n> * The write contains NULL values for all foreign key columns\r\n","sql\r\nCREATE TABLE a (\r\n  x INT\r\n ,y INT\r\n ,UNIQUE (x, y)\r\n);\r\nCREATE TABLE b (\r\n  x INT\r\n ,y INT\r\n ,INDEX (x, y)\r\n ,FOREIGN KEY (x, y) REFERENCES a (x, y) ON DELETE CASCADE ON UPDATE CASCADE\r\n);\r\n\r\nINSERT INTO b VALUES (NULL,NULL);\r\n# This should work. (and does)\r\n\r\nINSERT INTO b VALUES (1, NULL);\r\n# This should fail, due to it having a NULL. \r\n# But instead we get: foreign key violation: value [1 NULL] not found in a@a_x_y_key [x y]\r\n# Postgres reports this correctly (note that I had to include MATCH FULL in the FK reference).\r\n# ERROR:  insert or update on table ""b"" violates foreign key constraint ""b_x_fkey""\r\n# DETAIL:  MATCH FULL does not allow mixing of null and nonnull key values.\r\n\r\nINSERT INTO a VALUE (1, NULL);\r\n\r\nINSERT INTO b VALUE (1, NULL);\r\n# This should fail, due to it having a NULL. But passes now that (1, NULL) exists in a.\r\n# On postgres, it still fails due to the null/non-null mixing:\r\n# ERROR:  insert or update on table ""b"" violates foreign key constraint ""b_x_fkey""\r\n# DETAIL:  MATCH FULL does not allow mixing of null and nonnull key values.\r\n"
29625,"importccl: IMPORT MYSQLDUMP fails on fulltext index definitionI tried importing the data from the [CorpWatch API](http://api.corpwatch.org)'s MySQL database dumps. Most worked fine, but two failed with the error message `Error: pq: could not read definition for table ""company_locations"" (possible unsupported type?)`. The offending statement in both table definitions ended up being the FULLTEXT index definition at the end - when I removed those indexes, the tables imported happily.\r\n\r\nHere's one of the two schemas that wouldn't import because of this issue - the other was company_names.sql. (A third table in this dataset, filings.sql, couldn't be imported because it contained zero dates; see #29298 for that issue).\r\n\r\n",C-bug|A-disaster-recovery|good first issue,miretskiy,"I tried importing the data from the [CorpWatch API](http://api.corpwatch.org)'s MySQL database dumps. Most worked fine, but two failed with the error message `Error: pq: could not read definition for table ""company_locations"" (possible unsupported type?)`. The offending statement in both table definitions ended up being the FULLTEXT index definition at the end - when I removed those indexes, the tables imported happily.\r\n\r\nHere's one of the two schemas that wouldn't import because of this issue - the other was company_names.sql. (A third table in this dataset, filings.sql, couldn't be imported because it contained zero dates; see #29298 for that issue).\r\n\r\n```sql\r\nCREATE TABLE `company_locations` (\r\n  `location_id` int(11) NOT NULL AUTO_INCREMENT,\r\n  `cw_id` int(11) DEFAULT NULL,\r\n  `date` date DEFAULT NULL,\r\n  `type` varchar(15) DEFAULT NULL,\r\n  `raw_address` varchar(500) DEFAULT NULL,\r\n  `street_1` varchar(300) DEFAULT NULL,\r\n  `street_2` varchar(300) DEFAULT NULL,\r\n  `city` varchar(100) DEFAULT NULL,\r\n  `state` varchar(40) DEFAULT NULL,\r\n  `postal_code` varchar(11) DEFAULT NULL,\r\n  `country` varchar(100) DEFAULT NULL,\r\n  `country_code` char(2) DEFAULT NULL,\r\n  `subdiv_code` char(3) DEFAULT NULL,\r\n  `min_year` int(11) DEFAULT NULL,\r\n  `max_year` int(11) DEFAULT NULL,\r\n  `most_recent` tinyint(1) NOT NULL DEFAULT '0',\r\n  PRIMARY KEY (`location_id`),\r\n  KEY `country_code` (`country_code`,`cw_id`) USING BTREE,\r\n  KEY `subdiv_code` (`subdiv_code`,`cw_id`) USING BTREE,\r\n  KEY `year` (`min_year`,`max_year`),\r\n  KEY `cwindex` (`cw_id`,`min_year`,`max_year`) USING BTREE,\r\n  KEY `most_recent` (`most_recent`),\r\n  KEY `postal_code` (`postal_code`),\r\n  FULLTEXT KEY `raw_address` (`raw_address`)\r\n) ENGINE=MyISAM AUTO_INCREMENT=1789464 DEFAULT CHARSET=utf8 COMMENT='allows each company to have multiple locations associated wi';\r\n```","sql\r\nCREATE TABLE `company_locations` (\r\n  `location_id` int(11) NOT NULL AUTO_INCREMENT,\r\n  `cw_id` int(11) DEFAULT NULL,\r\n  `date` date DEFAULT NULL,\r\n  `type` varchar(15) DEFAULT NULL,\r\n  `raw_address` varchar(500) DEFAULT NULL,\r\n  `street_1` varchar(300) DEFAULT NULL,\r\n  `street_2` varchar(300) DEFAULT NULL,\r\n  `city` varchar(100) DEFAULT NULL,\r\n  `state` varchar(40) DEFAULT NULL,\r\n  `postal_code` varchar(11) DEFAULT NULL,\r\n  `country` varchar(100) DEFAULT NULL,\r\n  `country_code` char(2) DEFAULT NULL,\r\n  `subdiv_code` char(3) DEFAULT NULL,\r\n  `min_year` int(11) DEFAULT NULL,\r\n  `max_year` int(11) DEFAULT NULL,\r\n  `most_recent` tinyint(1) NOT NULL DEFAULT '0',\r\n  PRIMARY KEY (`location_id`),\r\n  KEY `country_code` (`country_code`,`cw_id`) USING BTREE,\r\n  KEY `subdiv_code` (`subdiv_code`,`cw_id`) USING BTREE,\r\n  KEY `year` (`min_year`,`max_year`),\r\n  KEY `cwindex` (`cw_id`,`min_year`,`max_year`) USING BTREE,\r\n  KEY `most_recent` (`most_recent`),\r\n  KEY `postal_code` (`postal_code`),\r\n  FULLTEXT KEY `raw_address` (`raw_address`)\r\n) ENGINE=MyISAM AUTO_INCREMENT=1789464 DEFAULT CHARSET=utf8 COMMENT='allows each company to have multiple locations associated wi';\r\n"
29522,"importccl: IMPORT PGDUMP should ignore sequence permissionsIn the OpenTrials dataset I tried to import in #29518, when I comment out the triggers, CockroachDB later hits the following error:\r\n\r\n```\r\nroot@:26257/defaultdb> import pgdump ('nodelocal:/pgdump/opentrials_schema.sql');\r\npq: postgres parse error: syntax error at or near ""knex_migrations_id_seq"": (\r\n\r\n\r\n--\r\n-- Name: knex_migrations_id_seq; Type: ACL; Schema: public; Owner: database\r\n--\r\n\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM PUBLIC;)\r\n```\r\n\r\nIt looks like the database has custom permissions granted on a specific sequence. We correctly ignore all the other grants in the file, so we just need to ignore these as well. Here's the offending bit of SQL; when I comment it out, the rest of the schema import completes successfully:\r\n\r\n",C-bug|A-disaster-recovery,mjibson,"In the OpenTrials dataset I tried to import in #29518, when I comment out the triggers, CockroachDB later hits the following error:\r\n\r\n```\r\nroot@:26257/defaultdb> import pgdump ('nodelocal:/pgdump/opentrials_schema.sql');\r\npq: postgres parse error: syntax error at or near ""knex_migrations_id_seq"": (\r\n\r\n\r\n--\r\n-- Name: knex_migrations_id_seq; Type: ACL; Schema: public; Owner: database\r\n--\r\n\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM PUBLIC;)\r\n```\r\n\r\nIt looks like the database has custom permissions granted on a specific sequence. We correctly ignore all the other grants in the file, so we just need to ignore these as well. Here's the offending bit of SQL; when I comment it out, the rest of the schema import completes successfully:\r\n\r\n```sql\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM PUBLIC;\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM database;\r\nGRANT ALL ON SEQUENCE knex_migrations_id_seq TO database;\r\nGRANT SELECT ON SEQUENCE knex_migrations_id_seq TO opentrials_readonly;\r\n```",sql\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM PUBLIC;\r\nREVOKE ALL ON SEQUENCE knex_migrations_id_seq FROM database;\r\nGRANT ALL ON SEQUENCE knex_migrations_id_seq TO database;\r\nGRANT SELECT ON SEQUENCE knex_migrations_id_seq TO opentrials_readonly;\r\n
29495,"sql: panic in INSERT ... ON CONFLICT```\r\nroot@:26257/backboard> *\r\n* ERROR: [n1,client=[::1]:50098,user=root] a SQL panic has occurred while executing ""INSERT INTO repos(github_owner, github_repo) VALUES ($1, $2) ON CONFLICT (github_owner, github_repo) DO UPDATE SET github_owner = excluded.github_owner RETURNING id"": invalid datum type given: string, expected int\r\n*\r\n*\r\n* ERROR: [n1,client=[::1]:50098,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: INSERT INTO _(_, _) VALUES ($1, $2) ON CONFLICT (_, _) DO UPDATE SET _ = _._ RETURNING _; caused by invalid datum type given: string, expected int\r\n\r\ngoroutine 177967 [running]:\r\nruntime/debug.Stack(0x6a27780, 0xc42e554bc0, 0x3)\r\n\t/usr/local/Cellar/go/1.10.3/libexec/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x6a27780, 0xc42e554bc0, 0xc42062b980, 0x63736c0, 0xc427d27770, 0x1)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:213 +0xa5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc429716000, 0x6a27780, 0xc42e554bc0, 0x609e3a0, 0xc4262291a0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:668 +0x2df\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc429716000, 0x6a27780, 0xc42e554bc0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:414 +0x61\r\npanic(0x609e3a0, 0xc4262291a0)\r\n\t/usr/local/Cellar/go/1.10.3/libexec/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.DatumToEncDatum(0x1, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/encoded_datum.go:161 +0x281\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Next(0xc425cfa900, 0x0, 0xc426623ee8, 0x5145530, 0xc420343180)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:207 +0x102\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.Run(0x6a27840, 0xc425083080, 0x6a30f80, 0xc425cfa900, 0x6a05640, 0xc424ece380)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/base.go:170 +0x35\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Run(0xc425cfa900, 0x6a27840, 0xc425083080, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:264 +0x96\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).StartSync(0xc42ce96c40, 0x6a27840, 0xc425083080, 0x662f978, 0xc42676e0e0, 0x6a05240)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:594 +0x191\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc420aa8200, 0xc4268d0720, 0xc4264e0320, 0xc426624808, 0xc425e8a780, 0xc4297164d0, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:245 +0x879\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc420aa8200, 0x6a27840, 0xc425082b10, 0xc4297164d0, 0xc4268d0720, 0xc4264e0320, 0x6a1ac80, 0xc42836eb00, 0xc425e8a780)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:735 +0x24c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc429716000, 0x6a27840, 0xc425082b10, 0xc429716438, 0x3, 0xc200460, 0xc4259b4360, 0xc425e8da00, 0x1, 0x1)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:966 +0x2d8\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:808 +0x688\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:401 +0xb82\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:95 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc429716000, 0x6a27780, 0xc42e554bc0, 0xc42d6d2960, 0x0, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1135 +0x13cd\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc420972360, 0x6a27780, 0xc42e554bc0, 0xc4251a7022, 0x9, 0xc4251a7031, 0x4, 0x0, 0x0, 0x6a09180, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:416 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc420972360, 0x6a27780, 0xc42e554bc0, 0xc42783a1c0, 0x5400, 0x15000, 0xc4206c1ec0, 0xc42d6d2960, 0xc42d6d2950, 0xc42726c380, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:267 +0x122\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:266 +0xf04\r\n\r\n*\r\n* ERROR: [n1,client=[::1]:50098,user=root] Reported as error eaaf81189bfe4ab496040203c15e1b9b\r\n*\r\n\r\nError: interrupted\r\nFailed running ""sql""\r\n```\r\n\r\nThe full schema/query are:\r\n\r\n\r\n\r\n@knz for triage",C-bug|S-2-temp-unavailability|A-sql-execution|A-sql-mutations,knz|BramGruneir,"```\r\nroot@:26257/backboard> *\r\n* ERROR: [n1,client=[::1]:50098,user=root] a SQL panic has occurred while executing ""INSERT INTO repos(github_owner, github_repo) VALUES ($1, $2) ON CONFLICT (github_owner, github_repo) DO UPDATE SET github_owner = excluded.github_owner RETURNING id"": invalid datum type given: string, expected int\r\n*\r\n*\r\n* ERROR: [n1,client=[::1]:50098,user=root] a panic has occurred!\r\n*\r\npanic while executing 1 statements: INSERT INTO _(_, _) VALUES ($1, $2) ON CONFLICT (_, _) DO UPDATE SET _ = _._ RETURNING _; caused by invalid datum type given: string, expected int\r\n\r\ngoroutine 177967 [running]:\r\nruntime/debug.Stack(0x6a27780, 0xc42e554bc0, 0x3)\r\n\t/usr/local/Cellar/go/1.10.3/libexec/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x6a27780, 0xc42e554bc0, 0xc42062b980, 0x63736c0, 0xc427d27770, 0x1)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:213 +0xa5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc429716000, 0x6a27780, 0xc42e554bc0, 0x609e3a0, 0xc4262291a0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:668 +0x2df\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc429716000, 0x6a27780, 0xc42e554bc0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:414 +0x61\r\npanic(0x609e3a0, 0xc4262291a0)\r\n\t/usr/local/Cellar/go/1.10.3/libexec/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.DatumToEncDatum(0x1, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/encoded_datum.go:161 +0x281\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Next(0xc425cfa900, 0x0, 0xc426623ee8, 0x5145530, 0xc420343180)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:207 +0x102\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.Run(0x6a27840, 0xc425083080, 0x6a30f80, 0xc425cfa900, 0x6a05640, 0xc424ece380)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/base.go:170 +0x35\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planNodeToRowSource).Run(0xc425cfa900, 0x6a27840, 0xc425083080, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan_node_to_row_source.go:264 +0x96\r\ngithub.com/cockroachdb/cockroach/pkg/sql/distsqlrun.(*Flow).StartSync(0xc42ce96c40, 0x6a27840, 0xc425083080, 0x662f978, 0xc42676e0e0, 0x6a05240)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsqlrun/flow.go:594 +0x191\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).Run(0xc420aa8200, 0xc4268d0720, 0xc4264e0320, 0xc426624808, 0xc425e8a780, 0xc4297164d0, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:245 +0x879\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*DistSQLPlanner).PlanAndRun(0xc420aa8200, 0x6a27840, 0xc425082b10, 0xc4297164d0, 0xc4268d0720, 0xc4264e0320, 0x6a1ac80, 0xc42836eb00, 0xc425e8a780)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/distsql_running.go:735 +0x24c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execWithDistSQLEngine(0xc429716000, 0x6a27840, 0xc425082b10, 0xc429716438, 0x3, 0xc200460, 0xc4259b4360, 0xc425e8da00, 0x1, 0x1)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:966 +0x2d8\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:808 +0x688\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:401 +0xb82\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc429716000, 0x6a27840, 0xc425082b10, 0x6a2a880, 0xc4264e4320, 0xc425e8da40, 0x1, 0x1, 0xc4260d7a20, 0xa4, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:95 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc429716000, 0x6a27780, 0xc42e554bc0, 0xc42d6d2960, 0x0, 0x0)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1135 +0x13cd\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc420972360, 0x6a27780, 0xc42e554bc0, 0xc4251a7022, 0x9, 0xc4251a7031, 0x4, 0x0, 0x0, 0x6a09180, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:416 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc420972360, 0x6a27780, 0xc42e554bc0, 0xc42783a1c0, 0x5400, 0x15000, 0xc4206c1ec0, 0xc42d6d2960, 0xc42d6d2950, 0xc42726c380, ...)\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:267 +0x122\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/Users/benesch/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:266 +0xf04\r\n\r\n*\r\n* ERROR: [n1,client=[::1]:50098,user=root] Reported as error eaaf81189bfe4ab496040203c15e1b9b\r\n*\r\n\r\nError: interrupted\r\nFailed running ""sql""\r\n```\r\n\r\nThe full schema/query are:\r\n\r\n```sql\r\nCREATE TABLE IF NOT EXISTS repos (\r\n\tid serial PRIMARY KEY,\r\n\tgithub_owner string NOT NULL,\r\n\tgithub_repo string NOT NULL,\r\n\tUNIQUE (github_owner, github_repo)\r\n);\r\n\r\nINSERT INTO repos (github_owner, github_repo)\r\nVALUES ($1, $2)\r\nON CONFLICT (github_owner, github_repo) DO UPDATE SET github_owner = excluded.github_owner\t\t\tRETURNING id\r\n```\r\n\r\n@knz for triage","sql\r\nCREATE TABLE IF NOT EXISTS repos (\r\n\tid serial PRIMARY KEY,\r\n\tgithub_owner string NOT NULL,\r\n\tgithub_repo string NOT NULL,\r\n\tUNIQUE (github_owner, github_repo)\r\n);\r\n\r\nINSERT INTO repos (github_owner, github_repo)\r\nVALUES ($1, $2)\r\nON CONFLICT (github_owner, github_repo) DO UPDATE SET github_owner = excluded.github_owner\t\t\tRETURNING id\r\n"
29309,"importccl: IMPORT MYSQLDUMP fails with ""unsupported default expression""Testing with [this MySQL dump file](https://raw.githubusercontent.com/cchou-git/DTCC_JavaAcademy/5cda05fd8001e0e0224c043078f572c4ffe3f2dd/appDataModel/src/database/sql/dtcc_db_script.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/cchou-git/DTCC_JavaAcademy/5cda05fd8001e0e0224c043078f572c4ffe3f2dd/appDataModel/src/database/sql/dtcc_db_script.sql');\r\npq: unsupported default expression for column ""address1"": syntax error at or near "")""\r\n```\r\n\r\nThere are two table definitions with an ""address1"" column in this dump, both of which are defined like this example:\r\n\r\n",C-bug|A-disaster-recovery,dt,"Testing with [this MySQL dump file](https://raw.githubusercontent.com/cchou-git/DTCC_JavaAcademy/5cda05fd8001e0e0224c043078f572c4ffe3f2dd/appDataModel/src/database/sql/dtcc_db_script.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/cchou-git/DTCC_JavaAcademy/5cda05fd8001e0e0224c043078f572c4ffe3f2dd/appDataModel/src/database/sql/dtcc_db_script.sql');\r\npq: unsupported default expression for column ""address1"": syntax error at or near "")""\r\n```\r\n\r\nThere are two table definitions with an ""address1"" column in this dump, both of which are defined like this example:\r\n\r\n```sql\r\n-- TABLE STRUCTURE FOR TABLE `RS_CLIENT`\r\n--\r\n\r\nDROP TABLE IF EXISTS `RS_CLIENT`;\r\n/*!40101 SET @SAVED_CS_CLIENT     = @@CHARACTER_SET_CLIENT */;\r\n/*!40101 SET CHARACTER_SET_CLIENT = UTF8 */;\r\nCREATE TABLE `RS_CLIENT` (\r\n  `NAME` VARCHAR(32) NOT NULL,\r\n  `DISPLAY_NAME` VARCHAR(256) NOT NULL,\r\n  `IS_DELETED` VARCHAR(1) DEFAULT 'N',\r\n  `ADDRESS1` VARCHAR(256) DEFAULT '',\r\n  `ADDRESS2` VARCHAR(256) DEFAULT '',\r\n  `CITY` VARCHAR(256) DEFAULT '',\r\n  `STATE` VARCHAR(256) DEFAULT '',\r\n  `ZIP` VARCHAR(32) DEFAULT '',\r\n  `COUNTRY` VARCHAR(256) DEFAULT '',\r\n  `ID` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,\r\n  `VERSION` INT(10) UNSIGNED DEFAULT NULL,\r\n  `SHOW_KEYFIND_NOTES` VARCHAR(1) DEFAULT 'N',\r\n  `IS_LOGOUT_BTN_SHOWN` VARCHAR(1) DEFAULT 'Y',\r\n  PRIMARY KEY (`ID`),\r\n  UNIQUE KEY `ID` (`ID`),\r\n  UNIQUE KEY `CLIENT_NAME` (`NAME`)\r\n) ENGINE=INNODB AUTO_INCREMENT=6 DEFAULT CHARSET=UTF8;\r\n/*!40101 SET CHARACTER_SET_CLIENT = @SAVED_CS_CLIENT */;\r\n```","sql\r\n-- TABLE STRUCTURE FOR TABLE `RS_CLIENT`\r\n--\r\n\r\nDROP TABLE IF EXISTS `RS_CLIENT`;\r\n/*!40101 SET @SAVED_CS_CLIENT     = @@CHARACTER_SET_CLIENT */;\r\n/*!40101 SET CHARACTER_SET_CLIENT = UTF8 */;\r\nCREATE TABLE `RS_CLIENT` (\r\n  `NAME` VARCHAR(32) NOT NULL,\r\n  `DISPLAY_NAME` VARCHAR(256) NOT NULL,\r\n  `IS_DELETED` VARCHAR(1) DEFAULT 'N',\r\n  `ADDRESS1` VARCHAR(256) DEFAULT '',\r\n  `ADDRESS2` VARCHAR(256) DEFAULT '',\r\n  `CITY` VARCHAR(256) DEFAULT '',\r\n  `STATE` VARCHAR(256) DEFAULT '',\r\n  `ZIP` VARCHAR(32) DEFAULT '',\r\n  `COUNTRY` VARCHAR(256) DEFAULT '',\r\n  `ID` BIGINT(20) UNSIGNED NOT NULL AUTO_INCREMENT,\r\n  `VERSION` INT(10) UNSIGNED DEFAULT NULL,\r\n  `SHOW_KEYFIND_NOTES` VARCHAR(1) DEFAULT 'N',\r\n  `IS_LOGOUT_BTN_SHOWN` VARCHAR(1) DEFAULT 'Y',\r\n  PRIMARY KEY (`ID`),\r\n  UNIQUE KEY `ID` (`ID`),\r\n  UNIQUE KEY `CLIENT_NAME` (`NAME`)\r\n) ENGINE=INNODB AUTO_INCREMENT=6 DEFAULT CHARSET=UTF8;\r\n/*!40101 SET CHARACTER_SET_CLIENT = @SAVED_CS_CLIENT */;\r\n"
29307,"importccl: IMPORT MYSQLDUMP fails with parse errorTesting with [this MySQL dump file](https://raw.githubusercontent.com/ejbiva1/joblink/2fe2ccf91eb5307992d2c31931faabde9a9d695c/database/Dump20150621.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/ejbiva1/joblink/2fe2ccf91eb5307992d2c31931faabde9a9d695c/database/Dump20150621.sql');\r\npq: mysql parse error: syntax error at position 2533 near 'status'\r\n```\r\n\r\nI'm not sure what the issue is - position 2533 in the file is the space after ""status"" in this table definition, which otherwise looks fine and happily loads into MySQL:\r\n\r\n",C-bug|A-disaster-recovery,dt,"Testing with [this MySQL dump file](https://raw.githubusercontent.com/ejbiva1/joblink/2fe2ccf91eb5307992d2c31931faabde9a9d695c/database/Dump20150621.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/ejbiva1/joblink/2fe2ccf91eb5307992d2c31931faabde9a9d695c/database/Dump20150621.sql');\r\npq: mysql parse error: syntax error at position 2533 near 'status'\r\n```\r\n\r\nI'm not sure what the issue is - position 2533 in the file is the space after ""status"" in this table definition, which otherwise looks fine and happily loads into MySQL:\r\n\r\n```sql\r\n/*Table structure for table \u652f\u4ed8\u8868*/\r\nDROP TABLE IF EXISTS TB_PAYMENT;\r\n\r\nCREATE TABLE TB_PAYMENT (\r\n  ID VARCHAR(50) NOT NULL,\r\n  PAYMENT_TYPE INT(11) NOT NULL COMMENT '\u652f\u4ed8\u7c7b\u578b: 1 \u9762\u8bd5; 2 \u57f9\u8bad; 3 \u4e0b\u8f7d\u7b80\u5386',\r\n  REFERENCE_ID VARCHAR(50) NOT NULL COMMENT '\u5173\u8054ID',\r\n  PAYMENT_CHANNEL ENUM('wechat', 'alipay') NOT NULL COMMENT '\u652f\u4ed8\u6e20\u9053',\r\n  PAYMENT_AMOUNT DECIMAL(5,2) NOT NULL COMMENT '\u91d1\u989d',\r\n  PAYMENT_USERID VARCHAR(50) NOT NULL COMMENT '\u652f\u4ed8\u4ebaID',\r\n  SUBMIT_DATE DATETIME DEFAULT NULL COMMENT '\u63d0\u4ea4\u8ba2\u5355\u65e5\u671f\u65f6\u95f4',\r\n  PAYMENT_DATE DATETIME DEFAULT NULL COMMENT '\u652f\u4ed8\u65e5\u671f\u65f6\u95f4',\r\n  STATUS INT(11) DEFAULT 0 COMMENT '\u72b6\u6001  0 \u672a\u652f\u4ed8  1 \u5df2\u652f\u4ed8',\r\n  PRIMARY KEY (ID),\r\n  UNIQUE KEY (PAYMENT_TYPE, REFERENCE_ID)\r\n) ENGINE=INNODB DEFAULT CHARSET=UTF8 COMMENT='\u652f\u4ed8\u8868';\r\n```","sql\r\n/*Table structure for table \u652f\u4ed8\u8868*/\r\nDROP TABLE IF EXISTS TB_PAYMENT;\r\n\r\nCREATE TABLE TB_PAYMENT (\r\n  ID VARCHAR(50) NOT NULL,\r\n  PAYMENT_TYPE INT(11) NOT NULL COMMENT '\u652f\u4ed8\u7c7b\u578b: 1 \u9762\u8bd5; 2 \u57f9\u8bad; 3 \u4e0b\u8f7d\u7b80\u5386',\r\n  REFERENCE_ID VARCHAR(50) NOT NULL COMMENT '\u5173\u8054ID',\r\n  PAYMENT_CHANNEL ENUM('wechat', 'alipay') NOT NULL COMMENT '\u652f\u4ed8\u6e20\u9053',\r\n  PAYMENT_AMOUNT DECIMAL(5,2) NOT NULL COMMENT '\u91d1\u989d',\r\n  PAYMENT_USERID VARCHAR(50) NOT NULL COMMENT '\u652f\u4ed8\u4ebaID',\r\n  SUBMIT_DATE DATETIME DEFAULT NULL COMMENT '\u63d0\u4ea4\u8ba2\u5355\u65e5\u671f\u65f6\u95f4',\r\n  PAYMENT_DATE DATETIME DEFAULT NULL COMMENT '\u652f\u4ed8\u65e5\u671f\u65f6\u95f4',\r\n  STATUS INT(11) DEFAULT 0 COMMENT '\u72b6\u6001  0 \u672a\u652f\u4ed8  1 \u5df2\u652f\u4ed8',\r\n  PRIMARY KEY (ID),\r\n  UNIQUE KEY (PAYMENT_TYPE, REFERENCE_ID)\r\n) ENGINE=INNODB DEFAULT CHARSET=UTF8 COMMENT='\u652f\u4ed8\u8868';\r\n"
29300,"importccl: IMPORT MYSQLDUMP fails when AUTO_INCREMENT isn't set in table optionsTesting with [this MySQL dump file](https://raw.githubusercontent.com/jethrocarr/amberphplib/40bbad29f8104c79506a802b569c4eb2559e1ab4/sql/amberphplib_template.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/jethrocarr/amberphplib/40bbad29f8104c79506a802b569c4eb2559e1ab4/sql/amberphplib_template.sql');\r\npq: column ""id"" specifies AUTO_INCREMENT but table options did not include it\r\n```\r\n\r\nThe dump file includes a few table definitions that have an AUTO_INCREMENT `id` column; some of them have an AUTO_INCREMENT=xxx option in the table options, but others do not, and those are presumably the source of this error:\r\n\r\n",C-bug|A-disaster-recovery,dt,"Testing with [this MySQL dump file](https://raw.githubusercontent.com/jethrocarr/amberphplib/40bbad29f8104c79506a802b569c4eb2559e1ab4/sql/amberphplib_template.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/jethrocarr/amberphplib/40bbad29f8104c79506a802b569c4eb2559e1ab4/sql/amberphplib_template.sql');\r\npq: column ""id"" specifies AUTO_INCREMENT but table options did not include it\r\n```\r\n\r\nThe dump file includes a few table definitions that have an AUTO_INCREMENT `id` column; some of them have an AUTO_INCREMENT=xxx option in the table options, but others do not, and those are presumably the source of this error:\r\n\r\n```sql\r\nDROP TABLE IF EXISTS `file_upload_data`;\r\n/*!40101 SET @saved_cs_client     = @@character_set_client */;\r\n/*!40101 SET character_set_client = utf8 */;\r\nCREATE TABLE `file_upload_data` (\r\n  `id` int(11) NOT NULL AUTO_INCREMENT,\r\n  `fileid` int(11) NOT NULL DEFAULT '0',\r\n  `data` blob NOT NULL,\r\n  PRIMARY KEY (`id`)\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Table for use as database-backed file storage system';\r\n/*!40101 SET character_set_client = @saved_cs_client */;\r\n```","sql\r\nDROP TABLE IF EXISTS `file_upload_data`;\r\n/*!40101 SET @saved_cs_client     = @@character_set_client */;\r\n/*!40101 SET character_set_client = utf8 */;\r\nCREATE TABLE `file_upload_data` (\r\n  `id` int(11) NOT NULL AUTO_INCREMENT,\r\n  `fileid` int(11) NOT NULL DEFAULT '0',\r\n  `data` blob NOT NULL,\r\n  PRIMARY KEY (`id`)\r\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='Table for use as database-backed file storage system';\r\n/*!40101 SET character_set_client = @saved_cs_client */;\r\n"
29298,"importccl: IMPORT MYSQLDUMP failure for timestamp with default valueTesting with [this MySQL dump file](https://raw.githubusercontent.com/bartektomas/mojanowa/a2076d8d4c52448c2274df4d7ffab771b4b57cba/backup.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/bartektomas/mojanowa/a2076d8d4c52448c2274df4d7ffab771b4b57cba/backup.sql');\r\npq: unsupported default expression for column ""created_at"": syntax error at or near ""0""\r\n```\r\n\r\nThe error message doesn't specify which table definition this occurred in (see #29297), but the file contains multiple table definitions that have a ""created_at"" column with a ""0"" in them, like this one:\r\n\r\n",C-bug|A-disaster-recovery,dt,"Testing with [this MySQL dump file](https://raw.githubusercontent.com/bartektomas/mojanowa/a2076d8d4c52448c2274df4d7ffab771b4b57cba/backup.sql) I found on GitHub, CockroachDB gave me this error:\r\n\r\n```\r\nroot@:26257/defaultdb> import mysqldump ('https://raw.githubusercontent.com/bartektomas/mojanowa/a2076d8d4c52448c2274df4d7ffab771b4b57cba/backup.sql');\r\npq: unsupported default expression for column ""created_at"": syntax error at or near ""0""\r\n```\r\n\r\nThe error message doesn't specify which table definition this occurred in (see #29297), but the file contains multiple table definitions that have a ""created_at"" column with a ""0"" in them, like this one:\r\n\r\n```sql\r\nDROP TABLE IF EXISTS `blogs`;\r\n/*!40101 SET @saved_cs_client     = @@character_set_client */;\r\n/*!40101 SET character_set_client = utf8 */;\r\nCREATE TABLE `blogs` (\r\n  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,\r\n  `user_id` int(10) unsigned NOT NULL,\r\n  `name` varchar(255) COLLATE utf8_unicode_ci NOT NULL,\r\n  `config` text COLLATE utf8_unicode_ci NOT NULL,\r\n  `domain` varchar(255) COLLATE utf8_unicode_ci NOT NULL,\r\n  `created_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\r\n  `updated_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\r\n  PRIMARY KEY (`id`),\r\n  KEY `blogs_user_id_foreign` (`user_id`),\r\n  CONSTRAINT `blogs_user_id_foreign` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`)\r\n) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci;\r\n/*!40101 SET character_set_client = @saved_cs_client */;\r\n```","sql\r\nDROP TABLE IF EXISTS `blogs`;\r\n/*!40101 SET @saved_cs_client     = @@character_set_client */;\r\n/*!40101 SET character_set_client = utf8 */;\r\nCREATE TABLE `blogs` (\r\n  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,\r\n  `user_id` int(10) unsigned NOT NULL,\r\n  `name` varchar(255) COLLATE utf8_unicode_ci NOT NULL,\r\n  `config` text COLLATE utf8_unicode_ci NOT NULL,\r\n  `domain` varchar(255) COLLATE utf8_unicode_ci NOT NULL,\r\n  `created_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\r\n  `updated_at` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',\r\n  PRIMARY KEY (`id`),\r\n  KEY `blogs_user_id_foreign` (`user_id`),\r\n  CONSTRAINT `blogs_user_id_foreign` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`)\r\n) ENGINE=InnoDB AUTO_INCREMENT=16 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci;\r\n/*!40101 SET character_set_client = @saved_cs_client */;\r\n"
29128,"storage: clear abort span on non-poisoning, aborting EndTransaction requestForked from https://github.com/cockroachdb/cockroach/pull/28799#issuecomment-414090586.\r\n\r\n> Why isn't\r\n>\r\n> https://github.com/cockroachdb/cockroach/blob/f435f61bec59768ad54fe4ff5ae0028059b9bf08/pkg/storage/batcheval/cmd_end_transaction.go#L492-L497\r\n>\r\n> written as:\r\n> \r\n> \r\n> \r\n> The effect of this is that we currently never remove abort span entries if they exist when the `TxnCoordSender` sends an `EndTransactionRequest(Commit=false)`. My understanding of `Poison` and its interaction with the `AbortSpan` is that it is safe to remove the entry when `Poison=false` because that implies an acknowlegement from the `TxnCoordSender` of the transaction's aborted disposition. I wonder if this has any implications on https://github.com/cockroachdb/cockroach/issues/25233.\r\n\r\nResponse from @tschottdorf:\r\n\r\n> I think this comes (came?) down to a lack of trust in that TxnCoordSender was doing the right thing with respect to its ""async aborts"". Could a txn be aborted (unbeknownst to it) and dispatch a read (which needs to hit the abort span for correctness) that would be ""passed"" by an async abort but would still return its value to the client? If so, clearing the abort span from the async abort would let the read return something incorrect (i.e. not read what you wrote).\r\n>\r\n> I agree that we should revisit this and that it has a likely connection to #25233!\r\n\r\nWe should explore this further.\r\n\r\n@tschottdorf for triage.",C-enhancement|A-kv-transactions|S-3-erroneous-edge-case,nvanbenschoten,"Forked from https://github.com/cockroachdb/cockroach/pull/28799#issuecomment-414090586.\r\n\r\n> Why isn't\r\n>\r\n> https://github.com/cockroachdb/cockroach/blob/f435f61bec59768ad54fe4ff5ae0028059b9bf08/pkg/storage/batcheval/cmd_end_transaction.go#L492-L497\r\n>\r\n> written as:\r\n> \r\n> ```go\r\n> if txn.Status == roachpb.ABORTED {\r\n> \tif err := SetAbortSpan(ctx, evalCtx, batch, ms, txn.TxnMeta, args.Poison); err != nil {\r\n> \t\treturn nil, err\r\n> \t}\r\n> }\r\n> ```\r\n> \r\n> The effect of this is that we currently never remove abort span entries if they exist when the `TxnCoordSender` sends an `EndTransactionRequest(Commit=false)`. My understanding of `Poison` and its interaction with the `AbortSpan` is that it is safe to remove the entry when `Poison=false` because that implies an acknowlegement from the `TxnCoordSender` of the transaction's aborted disposition. I wonder if this has any implications on https://github.com/cockroachdb/cockroach/issues/25233.\r\n\r\nResponse from @tschottdorf:\r\n\r\n> I think this comes (came?) down to a lack of trust in that TxnCoordSender was doing the right thing with respect to its ""async aborts"". Could a txn be aborted (unbeknownst to it) and dispatch a read (which needs to hit the abort span for correctness) that would be ""passed"" by an async abort but would still return its value to the client? If so, clearing the abort span from the async abort would let the read return something incorrect (i.e. not read what you wrote).\r\n>\r\n> I agree that we should revisit this and that it has a likely connection to #25233!\r\n\r\nWe should explore this further.\r\n\r\n@tschottdorf for triage.","go\r\n> if txn.Status == roachpb.ABORTED {\r\n> \tif err := SetAbortSpan(ctx, evalCtx, batch, ms, txn.TxnMeta, args.Poison); err != nil {\r\n> \t\treturn nil, err\r\n> \t}\r\n> }\r\n> "
29021,"sql: views do not track fine-grained dependencies\r\n\r\nsays \r\n\r\n```\r\npq: cannot drop column ""b"" because view ""v"" depends on it\r\n```\r\n\r\nThis is a result of #17280 because our previous attempt at being fine-grained was erroneous. However the result errs on the other side of the UX inconvenience.\r\n\r\nThere should be a middle ground that looks just as those columns actually mentioned in the view query.",C-bug|A-sql-semantics|O-support|A-schema-descriptors,RichardJCai,"```sql\r\nCREATE TABLE t (a INT PRIMARY KEY, b BOOLEAN DEFAULT FALSE);\r\nCREATE VIEW v AS SELECT a from t;\r\nALTER TABLE t DROP COLUMN b;\r\n```\r\n\r\nsays \r\n\r\n```\r\npq: cannot drop column ""b"" because view ""v"" depends on it\r\n```\r\n\r\nThis is a result of #17280 because our previous attempt at being fine-grained was erroneous. However the result errs on the other side of the UX inconvenience.\r\n\r\nThere should be a middle ground that looks just as those columns actually mentioned in the view query.","sql\r\nCREATE TABLE t (a INT PRIMARY KEY, b BOOLEAN DEFAULT FALSE);\r\nCREATE VIEW v AS SELECT a from t;\r\nALTER TABLE t DROP COLUMN b;\r\n"
29004,"storage: single node merge experiment gets stuckWhile trying to repro https://github.com/cockroachdb/cockroach/issues/28995 in a smaller setting.\r\n\r\n\r\n\r\nexpected: all ranges get merged away\r\nreality: there's a flurry of activity, but that stops abruptly. I still have 9.3k replicas.\r\n\r\nI also think we should have some rapid testing like this in our acceptance tests to really put merges through its paces.",C-enhancement|A-kv-client,benesch,"While trying to repro https://github.com/cockroachdb/cockroach/issues/28995 in a smaller setting.\r\n\r\n```go\r\n# shell 1\r\n./cockroach start --insecure --logtostderr\r\n# shell 2\r\n./bin/workload run kv --init --splits 10000 --max-ops 1\r\necho 'SET CLUSTER SETTING kv.range_merge.queue_enabled = true;' | ./cockroach sql --insecure\r\n```\r\n\r\nexpected: all ranges get merged away\r\nreality: there's a flurry of activity, but that stops abruptly. I still have 9.3k replicas.\r\n\r\nI also think we should have some rapid testing like this in our acceptance tests to really put merges through its paces.",go\r\n# shell 1\r\n./cockroach start --insecure --logtostderr\r\n# shell 2\r\n./bin/workload run kv --init --splits 10000 --max-ops 1\r\necho 'SET CLUSTER SETTING kv.range_merge.queue_enabled = true;' | ./cockroach sql --insecure\r\n
28827,"panic: cannot inline references within correlated subqueriesI was investigating #28816, trying to pare down the query and ran into a panic in `InlineProjections`.\r\n\r\nquery:\r\n\r\n\r\n```\r\npanic: cannot inline references within correlated subqueries [recovered]\r\n\tpanic: cannot inline references within correlated subqueries [recovered]\r\n\tpanic: panic while executing 1 statements: SELECT _._ AS _ FROM (SELECT _ AS _, _ AS _) AS _ WHERE _ >= CASE WHEN _._ IS NOT _ THEN extract(CAST(CASE WHEN (EXISTS (SELECT _._ AS _, _._ AS _, _._ AS _, _._ AS _ FROM _._ AS _ WHERE _._ IS NOT _ LIMIT _)) THEN version() ELSE version() END AS TEXT), CAST(current_date() AS DATE)) ELSE _ END LIMIT _; caused by cannot inline references within correlated subqueries\r\n\r\ngoroutine 405 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc4201c5900, 0x2777b20, 0xc420dc0780, 0x1f399c0, 0x274a230)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:675 +0x36f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc4201c5900, 0x2777b20, 0xc420dc0780)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:414 +0x61\r\npanic(0x1f399c0, 0x274a230)\r\n\t/usr/local/go/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc420c6a998)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:130 +0x99\r\npanic(0x1f399c0, 0x274a230)\r\n\t/usr/local/go/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc400000010, 0xc420c68d60)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:120 +0x5cd\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1738, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff003200000015, 0xc4201c5dd0, 0xc420c69f50, 0xc42044d598, 0xc420c68e78)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1a00000015, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1878, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006500000017, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff002b00000012)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x17, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1918, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006400000018, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6ed30, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc400000018, 0xc420d6e000)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e19b8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006300000019, 0xc4201c5dd0, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x19, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1af8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00670000001b, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff004a00000020)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1b, 0x20)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1b98, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00650000001c, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff002b00000012)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1c, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1c38, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00640000001d, 0xc4201c5dd0, 0xc420c69f50, 0x3, 0x1d0000003a)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1d, 0x1d0000003a)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1cd8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff003a0000001e, 0xc4201c5dd0, 0xc420c69f50, 0xffff003200000015, 0xffff003200650017)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1e, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1d78, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00330000001f, 0xc4201c5dd0, 0xc420c69f50, 0xc420c69e78, 0x16db6fa)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc40000001f, 0x176d097)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections(0xc4201c6888, 0x50000001f, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:168 +0x155\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructSelect(0xc4201c6878, 0x1f00000006, 0x1f)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:850 +0x1c55\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildFrom(0xc420336540, 0xc42086e120, 0xc4202ad840, 0xc421150400, 0xc4211505d0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:527 +0x592\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelectClause(0xc420336540, 0xc420c6c480, 0x0, 0x0, 0x0, 0xc421150400, 0x30)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:429 +0x56\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelect(0xc420336540, 0xc420442dc0, 0xc421150400, 0x21fd580)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:387 +0x1a6\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildStmt(0xc420336540, 0x277aea0, 0xc420442dc0, 0xc421150400, 0x4)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:177 +0xf1\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build(0xc420336540, 0xc400000000, 0x0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:135 +0x128\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makeOptimizerPlan(0xc4201c5d38, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:378 +0x287\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).optionallyUseOptimizer(0xc4201c5d38, 0x2777be0, 0xc42086e510, 0x0, 0x0, 0xc4207de011, 0x4, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/planner.go:534 +0xdb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:751 +0x192\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:401 +0xb82\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:95 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc4201c5900, 0x2777b20, 0xc420dc0780, 0xc4202c0e10, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1079 +0x2186\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc4201c0000, 0x2777b20, 0xc420dc0780, 0xc4207de011, 0x4, 0xc4207de030, 0x4, 0x0, 0x0, 0x275dea0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:416 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc4201c0000, 0x2777b20, 0xc420dc0780, 0xc4205ea700, 0x5400, 0x15000, 0xc420612180, 0xc4202c0e10, 0xc4202c0e00, 0xc420471180, ...)\r\n```",C-bug|A-sql-optimizer,rytaft,"I was investigating #28816, trying to pare down the query and ran into a panic in `InlineProjections`.\r\n\r\nquery:\r\n```sql\r\nquery I\r\nSELECT\r\n  subq_0.c0 AS c0\r\nFROM (SELECT 1 AS c0, 2 as c1) AS subq_0\r\nWHERE\r\n  1\r\n  >= CASE\r\n    WHEN subq_0.c1 IS NOT NULL\r\n    THEN pg_catalog.extract(\r\n      CAST(\r\n        CASE\r\n        WHEN\r\n        (\r\n            EXISTS(\r\n              SELECT\r\n                ref_1.config_yaml AS c0,\r\n                ref_1.config_yaml AS c1,\r\n                subq_0.c0 AS c2,\r\n                ref_1.config_yaml AS c3\r\n              FROM\r\n                crdb_internal.zones AS ref_1\r\n              WHERE\r\n                subq_0.c0 IS NOT NULL\r\n              LIMIT\r\n                52\r\n            )\r\n          )\r\n        THEN pg_catalog.version()\r\n        ELSE pg_catalog.version()\r\n        END\r\n          AS TEXT\r\n      ),\r\n      CAST(pg_catalog.current_date() AS DATE)\r\n    )\r\n    ELSE 1\r\n    END\r\nLIMIT\r\n  107\r\n----\r\n```\r\n\r\n```\r\npanic: cannot inline references within correlated subqueries [recovered]\r\n\tpanic: cannot inline references within correlated subqueries [recovered]\r\n\tpanic: panic while executing 1 statements: SELECT _._ AS _ FROM (SELECT _ AS _, _ AS _) AS _ WHERE _ >= CASE WHEN _._ IS NOT _ THEN extract(CAST(CASE WHEN (EXISTS (SELECT _._ AS _, _._ AS _, _._ AS _, _._ AS _ FROM _._ AS _ WHERE _._ IS NOT _ LIMIT _)) THEN version() ELSE version() END AS TEXT), CAST(current_date() AS DATE)) ELSE _ END LIMIT _; caused by cannot inline references within correlated subqueries\r\n\r\ngoroutine 405 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc4201c5900, 0x2777b20, 0xc420dc0780, 0x1f399c0, 0x274a230)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:675 +0x36f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc4201c5900, 0x2777b20, 0xc420dc0780)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:414 +0x61\r\npanic(0x1f399c0, 0x274a230)\r\n\t/usr/local/go/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc420c6a998)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:130 +0x99\r\npanic(0x1f399c0, 0x274a230)\r\n\t/usr/local/go/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc400000010, 0xc420c68d60)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:120 +0x5cd\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1738, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff003200000015, 0xc4201c5dd0, 0xc420c69f50, 0xc42044d598, 0xc420c68e78)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1a00000015, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1878, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006500000017, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff002b00000012)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x17, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1918, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006400000018, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6ed30, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc400000018, 0xc420d6e000)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e19b8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff006300000019, 0xc4201c5dd0, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x19, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1af8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00670000001b, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff004a00000020)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1b, 0x20)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1b98, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00650000001c, 0xc4201c5dd0, 0xc420c69f50, 0xc420d6e000, 0xffff002b00000012)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1c, 0x2)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1c38, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00640000001d, 0xc4201c5dd0, 0xc420c69f50, 0x3, 0x1d0000003a)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1d, 0x1d0000003a)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1cd8, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:181 +0x9f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff003a0000001e, 0xc4201c5dd0, 0xc420c69f50, 0xffff003200000015, 0xffff003200650017)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0x1e, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Expr).Replace(0xc4201e1d78, 0xc420d6e000, 0xc420c69f50, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr.go:191 +0x21f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.ExprView.Replace(0xc420d6e000, 0xffff00330000001f, 0xc4201c5dd0, 0xc420c69f50, 0xc420c69e78, 0x16db6fa)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/expr_view.go:201 +0x6f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections.func1(0xc40000001f, 0x176d097)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:165 +0x49f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*CustomFuncs).InlineProjections(0xc4201c6888, 0x50000001f, 0x1)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/inline.go:168 +0x155\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructSelect(0xc4201c6878, 0x1f00000006, 0x1f)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:850 +0x1c55\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildFrom(0xc420336540, 0xc42086e120, 0xc4202ad840, 0xc421150400, 0xc4211505d0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:527 +0x592\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelectClause(0xc420336540, 0xc420c6c480, 0x0, 0x0, 0x0, 0xc421150400, 0x30)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:429 +0x56\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelect(0xc420336540, 0xc420442dc0, 0xc421150400, 0x21fd580)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:387 +0x1a6\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildStmt(0xc420336540, 0x277aea0, 0xc420442dc0, 0xc421150400, 0x4)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:177 +0xf1\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build(0xc420336540, 0xc400000000, 0x0, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:135 +0x128\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makeOptimizerPlan(0xc4201c5d38, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:378 +0x287\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).optionallyUseOptimizer(0xc4201c5d38, 0x2777be0, 0xc42086e510, 0x0, 0x0, 0xc4207de011, 0x4, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/planner.go:534 +0xdb\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:751 +0x192\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:401 +0xb82\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc4201c5900, 0x2777be0, 0xc42086e510, 0x277aea0, 0xc420442dc0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:95 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc4201c5900, 0x2777b20, 0xc420dc0780, 0xc4202c0e10, 0x0, 0x0)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1079 +0x2186\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc4201c0000, 0x2777b20, 0xc420dc0780, 0xc4207de011, 0x4, 0xc4207de030, 0x4, 0x0, 0x0, 0x275dea0, ...)\r\n\t/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:416 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc4201c0000, 0x2777b20, 0xc420dc0780, 0xc4205ea700, 0x5400, 0x15000, 0xc420612180, 0xc4202c0e10, 0xc4202c0e00, 0xc420471180, ...)\r\n```","sql\r\nquery I\r\nSELECT\r\n  subq_0.c0 AS c0\r\nFROM (SELECT 1 AS c0, 2 as c1) AS subq_0\r\nWHERE\r\n  1\r\n  >= CASE\r\n    WHEN subq_0.c1 IS NOT NULL\r\n    THEN pg_catalog.extract(\r\n      CAST(\r\n        CASE\r\n        WHEN\r\n        (\r\n            EXISTS(\r\n              SELECT\r\n                ref_1.config_yaml AS c0,\r\n                ref_1.config_yaml AS c1,\r\n                subq_0.c0 AS c2,\r\n                ref_1.config_yaml AS c3\r\n              FROM\r\n                crdb_internal.zones AS ref_1\r\n              WHERE\r\n                subq_0.c0 IS NOT NULL\r\n              LIMIT\r\n                52\r\n            )\r\n          )\r\n        THEN pg_catalog.version()\r\n        ELSE pg_catalog.version()\r\n        END\r\n          AS TEXT\r\n      ),\r\n      CAST(pg_catalog.current_date() AS DATE)\r\n    )\r\n    ELSE 1\r\n    END\r\nLIMIT\r\n  107\r\n----\r\n"
28309,"sql: DBeaver Metadata Quries# DBeaver Compatibility\r\n\r\nThis issues contains a list of all known DBeaver metadata queries and what we do and do not support.\r\n\r\n## Database List\r\n\r\n\r\n\r\n## Role list\r\n\r\n\r\n\r\n## Role members\r\n\r\n\r\n\r\n## Access methods\r\n\r\n\r\n\r\n## Op classes\r\n\r\n\r\n\r\n## Encodings\r\n\r\n\r\n\r\n## Languages\r\n\r\n\r\n\r\n## Collations\r\n\r\n\r\n\r\n## Extensions\r\n\r\n\r\n\r\n## Foreign servers\r\n\r\n\r\n\r\n## Foreign data wrappers\r\n\r\n\r\n\r\n## Foreign tables info\r\n\r\n\r\n\r\n## Tablespaces\r\n\r\n\r\n\r\n## Schemas\r\n\r\n\r\n\r\n## Sequences\r\n\r\n\r\n\r\n## Enums\r\n\r\n\r\n\r\n## Classes (tables, views, etc)\r\n\r\n\r\n\r\n## Attributes (columns)\r\n\r\n\r\n\r\n\r\n## Constraints (check, fk, etc):\r\n\r\n\r\n\r\n## Indexes\r\n\r\n\r\n\r\n## Procedures\r\n\r\n\r\n\r\n## Triggers\r\n\r\n\r\n\r\n## Data types\r\n\r\n\r\n\r\n## Inheritance\r\n\r\n\r\n\r\n## Activity\r\n\r\n\r\n\r\n## Transactions locks\r\n\r\n\r\n\r\n## Object comments\r\n\r\n\r\n\r\n## View definition\r\n\r\n\r\n\r\n## Procedure definition\r\n\r\n\r\n",C-investigation|A-sql-pgcompat,BramGruneir,"# DBeaver Compatibility\r\n\r\nThis issues contains a list of all known DBeaver metadata queries and what we do and do not support.\r\n\r\n## Database List\r\n\r\n```sql\r\nSELECT db.datname FROM pg_catalog.pg_database AS db WHERE datallowconn;\r\n-- supported by cockroach\r\n-- pg_database has a number of limitations\r\n--   datdba, datlastsysoid, datfrozenxid, datminmxid and datacl are always NULL\r\n--   encoding is always set to 6\r\n--   datcollate and datctype are always set to 'en_US.utf8'\r\n--   datistemplate is always set to FALSE\r\n--   datallowconn is always set to TRUE\r\n--   datconnlimit is always set to -1\r\n--   dattablespace is always set to an OID of 0\r\n-- but this is more idiomatic of cockroach\r\nSELECT database_name AS datname FROM [SHOW DATABASES];\r\n```\r\n\r\n## Role list\r\n\r\n```sql\r\nSELECT a.oid, a.* FROM pg_catalog.pg_roles AS a;\r\n-- supported by cockroach\r\n-- pg_roles exists but has a number of preset values\r\n--   rolsuper, rolcreaterole and rolecreatedb are always set to TRUE or FALSE based on if the user is an admin\r\n--   rolcatupdate, rolreplication and rolbypassrls is always FALSE\r\n--   rolpassword is always set to '********'\r\n--   rolconnlimit is always -1\r\n--   rolvaliduntil and rolconfig are always NULL\r\n-- DBeaver seems to use the Postgres OID values to identify roles, and this is not stable in Cockroach if roles are renamed.\r\n-- In order to ensure a join (or client-side correspondence) is possible with the list of role members (see next section), \r\n-- DBeaver may need to use the role name as key to identify the role. See the info on Role Member just below.\r\n```\r\n\r\n## Role members\r\n\r\n```sql\r\nSELECT * FROM pg_catalog.pg_auth_members;\r\n-- supported by cockroach:\r\n+-----------+------------+---------+--------------+\r\n|  roleid   |   member   | grantor | admin_option |\r\n+-----------+------------+---------+--------------+\r\n| 823966177 | 2901009604 | NULL    |     true     |\r\n+-----------+------------+---------+--------------+\r\n-- grantor is always null, so all you're getting is admin_option\r\n-- Instead, this would give the proper values\r\nSELECT * FROM information_schema.applicable_roles;\r\n+---------+-----------+--------------+\r\n| grantee | role_name | is_grantable |\r\n+---------+-----------+--------------+\r\n| root    | admin     | YES          |\r\n+---------+-----------+--------------+\r\n-- A join with the OID from pg_roles is possible with pg_auth_members; however to join with\r\ninformation_schema.applicable_roles one should use the role name.\r\n```\r\n\r\n## Access methods\r\n\r\n```sql\r\nSELECT am.oid, am.* FROM pg_catalog.pg_am AS am;\r\n-- supported in cockroach, but this is always a single fixed returned row.\r\n+------------+------------+--------+-----------+--------+\r\n|    oid     |    oid     | amname | amhandler | amtype |\r\n+------------+------------+--------+-----------+--------+\r\n| 2631952481 | 2631952481 | prefix | NULL      | i      |\r\n+------------+------------+--------+-----------+--------+\r\n```\r\n\r\n## Op classes\r\n\r\n```sql\r\nSELECT oc.oid, oc.* FROM pg_catalog.pg_opclass AS oc;\r\n-- there are no concepts within cockroach for opclasses as we only have one access method.\r\n-- pg_opclass does not exist\r\n-- I'm sure we can provide some defaults if needed.\r\n```\r\n\r\n## Encodings\r\n\r\n```sql\r\nSELECT * FROM pg_catalog.pg_conversion c;\r\n-- not supported by cockroach\r\n-- pg_conversion does not exist\r\n```\r\n\r\n## Languages\r\n\r\n```sql\r\nSELECT l.oid, l.* FROM pg_catalog.pg_language AS l;\r\n-- languages are not supported by cockroach\r\n-- pg_language returns an empty table\r\n```\r\n\r\n## Collations\r\n\r\n```sql\r\nSELECT c.oid, c.* FROM pg_catalog.pg_collation AS c;\r\n-- supported by cockroach\r\n+------------+------------+---------------------+---------------+-----------+--------------+-------------+-----------+\r\n|    oid     |    oid     |      collname       | collnamespace | collowner | collencoding | collcollate | collctype |\r\n+------------+------------+---------------------+---------------+-----------+--------------+-------------+-----------+\r\n| 1342659818 | 1342659818 | und                 |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  563257977 |  563257977 | aa                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  563257982 |  563257982 | af                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  563257962 |  563257962 | ar                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  563257963 |  563257963 | as                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  563257954 |  563257954 | az                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  613590708 |  613590708 | be                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  613590710 |  613590710 | bg                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  613590719 |  613590719 | bn                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  613590690 |  613590690 | bs                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  496222161 |  496222161 | bs-Cyrl             |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  596813151 |  596813151 | ca                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n|  596813133 |  596813133 | cs                  |    1130344414 | NULL      |            6 | NULL        | NULL      |\r\n+------------+------------+---------------------+---------------+-----------+--------------+-------------+-----------+\r\n-- But most of these values are preset\r\n--   collowner, collcollate and collctype are always NULL\r\n--   collencodign is always 0\r\n--   collnamespace is always set to 1130344414,\r\n--   so the only real info is the collname column\r\n```\r\n\r\n## Extensions\r\n\r\n```sql\r\nSELECT e.oid, e.* FROM pg_catalog.pg_extension AS e;\r\n-- extensions are not supported in cockroach\r\n-- pg_extension does not exist\r\n```\r\n\r\n## Foreign servers\r\n\r\n```sql\r\nSELECT l.oid, l.* FROM pg_catalog.pg_foreign_server AS l;\r\n-- foreign server are not supported in cockroach\r\n-- pg_foreign_server returns an empty table\r\n```\r\n\r\n## Foreign data wrappers\r\n\r\n```sql\r\nSELECT\r\n  l.oid, l.*, p.pronamespace AS handler_schema_id\r\nFROM\r\n  pg_catalog.pg_foreign_data_wrapper AS l\r\n  LEFT JOIN pg_catalog.pg_proc AS p ON p.oid = l.fdwhandler\r\nORDER BY\r\n  l.fdwname;\r\n-- Foreign data wrappers are not supported\r\n-- pg_catalog.pg_foreign_data_wrapper is just an empty table\r\n-- pg_proc is supported but again with a lot of pre-set columns\r\n  -- proowner, procost, prorows, protransform, provolatile, proparallel, proallargtypes, proargnames, proargdefaults, protrftypes, probin, proconfig and proacl are always NULL\r\n  -- pronargdefaults is always 0\r\n```\r\n\r\n## Foreign tables info\r\n\r\n```sql\r\nSELECT * FROM pg_catalog.pg_foreign_table WHERE ftrelid = ?;\r\n-- foreign tables are not supported\r\n-- pg_foreign_table is just an empty table\r\n```\r\n\r\n## Tablespaces\r\n\r\n```sql\r\nSELECT t.oid, t.* FROM pg_catalog.pg_tablespace AS t;\r\n-- This is supported but Cockroach only has a single tablespace\r\n+-----+-----+------------+----------+-------------+--------+------------+\r\n| oid | oid |  spcname   | spcowner | spclocation | spcacl | spcoptions |\r\n+-----+-----+------------+----------+-------------+--------+------------+\r\n|   0 |   0 | pg_default | NULL     | NULL        | NULL   | NULL       |\r\n+-----+-----+------------+----------+-------------+--------+------------+\r\n-- all values are preset, including spcname's 'pg_default'\r\n```\r\n\r\n## Schemas\r\n\r\n```sql\r\nSELECT\r\n  n.oid, n.*, d.description\r\nFROM\r\n  pg_catalog.pg_namespace AS n\r\n  LEFT JOIN pg_catalog.pg_description AS d ON d.objoid = n.oid;\r\n-- This is supported by cockroach\r\n-- pg_namespace returns the nspname only, nspowner and nspacl are always set to NULL\r\n-- pg_description is an empty table and descriptions are not supported\r\n-- The idiomatic way to get the list of schemas is to use information schema:\r\nSELECT * FROM information_schema.schemata;\r\n-- And with that, we don't allow new schemas to be added.\r\n-- So it's really just a list of the 4 for all databases.\r\n+--------------+--------------------+----------------------------+----------+\r\n| catalog_name |    schema_name     | default_character_set_name | sql_path |\r\n+--------------+--------------------+----------------------------+----------+\r\n| system       | crdb_internal      | NULL                       | NULL     |\r\n| system       | information_schema | NULL                       | NULL     |\r\n| system       | pg_catalog         | NULL                       | NULL     |\r\n| system       | public             | NULL                       | NULL     |\r\n+--------------+--------------------+----------------------------+----------+\r\n-- Note that information_schema.schemata only returns schemas for the current database. \r\n-- To return the schemas for an arbitrary database, use SHOW SCHEMAS FOR <dbname>.\r\n```\r\n\r\n## Sequences\r\n\r\n```sql\r\nSELECT * FROM pg_catalog.pg_sequences WHERE schemaname = ? AND sequencename = ?;\r\n-- Sequences are supported\r\n-- However, there are two tables in postgres for them\r\n-- pg_sequences is not yet supported and the table does not exist\r\n-- pg_sequence exists but has a number of pre-set columns\r\n--   seqcache is always set to 1\r\n--   seqcycle is always FALSE, as cycles are not yet supported\r\n-- However, the best way to get them for a specific db is:\r\nSELECT * FROM information_schema.sequences;\r\n```\r\n\r\n## Enums\r\n\r\n```sql\r\nSELECT e.enumlabel FROM pg_catalog.pg_enum AS e WHERE e.enumtypid = ?;\r\n-- enums are not supported Cockroach yet\r\n-- pg_enum is just an empty table\r\n```\r\n\r\n## Classes (tables, views, etc)\r\n\r\n```sql\r\nSELECT\r\n  c.oid, c.*, d.description\r\nFROM\r\n  pg_catalog.pg_class AS c\r\n  LEFT JOIN pg_catalog.pg_description AS d\r\n  ON d.objoid = c.oid AND d.objsubid = 0\r\nWHERE\r\n  c.relnamespace = ? AND c.relkind NOT IN ('i', 'c');\r\n-- descriptions (comments) are not supported\r\n-- pg_description is always an empty table\r\n-- pg_class is supported, but most if it's values pre-set\r\n--   reltype, relfilenode, reltablespace and reltoastrelid are all set to an OID of 0\r\n--   relowner, relam, relpages, reltuples, relacl and reloptions are all set to NULL\r\n--   relallvisible and relfrozenxid are both set to 0\r\n--   relisshared, relistemp, relhasoids, relhasrules, relhastriggers and relhassubclass are all set to FALSE\r\n--   relPersistencePermanent is always set to 'p'\r\n-- The more idiomatic way of looking these up should be using the information_schema\r\nSELECT * FROM information_schema.tables;\r\n+---------------+--------------------+-----------------------------------+-------------+--------------------+---------+\r\n| table_catalog |    table_schema    |            table_name             | table_type  | is_insertable_into | version |\r\n+---------------+--------------------+-----------------------------------+-------------+--------------------+---------+\r\n| defaultdb     | crdb_internal      | backward_dependencies             | SYSTEM VIEW | NO                 |       1 |\r\n| defaultdb     | crdb_internal      | builtin_functions                 | SYSTEM VIEW | NO                 |       1 |\r\n| defaultdb     | crdb_internal      | cluster_queries                   | SYSTEM VIEW | NO                 |       1 |\r\n| defaultdb     | crdb_internal      | cluster_sessions                  | SYSTEM VIEW | NO                 |       1 |\r\n...\r\n```\r\n\r\n## Attributes (columns)\r\n\r\n```sql\r\nSELECT\r\n  c.relname,\r\n  a.*,\r\n  ad.oid AS attr_id,\r\n  pg_catalog.pg_get_expr(ad.adbin, ad.adrelid, true) AS def_value,\r\n  dsc.description\r\nFROM\r\n  pg_catalog.pg_attribute AS a\r\n  INNER JOIN pg_catalog.pg_class AS c ON a.attrelid = c.oid\r\n  LEFT JOIN pg_catalog.pg_attrdef AS ad\r\n  ON a.attrelid = ad.adrelid AND a.attnum = ad.adnum\r\n  LEFT JOIN pg_catalog.pg_description AS dsc\r\n  ON c.oid = dsc.objoid AND a.attnum = dsc.objsubid\r\nWHERE\r\n  a.attnum > 0 AND NOT a.attisdropped AND c.oid = ?\r\nORDER BY\r\n  a.attnum;\r\n-- This is supported, but with a number of caveats:\r\n--   pg_class is limited, see the previous comment in the Classes section\r\n--   pg_description is just an empty table\r\n--   pg_attrdef is fully supported\r\n--   pg_attribute is supported but with a bunch of preset values\r\n--     attstattarget, attndims and attinhcount are always 0\r\n--     attcacheoff and atttypmod are always -1\r\n--     attbyval, attstorage, attalign, attacl, attoptions and attfdwoptions are always NULL\r\n--     attisdropped is always false\r\n--     attislocal is always true\r\n-- The more idiomatic way to lookup columns (including their default values) is using the information_schema\r\nSELECT * FROM information_schema.columns;\r\n+---------------+---------------+-----------------------+-----------------+------------------+----------------+-------------+-----------+--------------------------+------------------------+-------------------+---------------+--------------------+-----------------------+----------------------+--------------------+-----------------------+\r\n| table_catalog | table_schema  |      table_name       |   column_name   | ordinal_position | column_default | is_nullable | data_type | character_maximum_length | character_octet_length | numeric_precision | numeric_scale | datetime_precision | character_set_catalog | character_set_schema | character_set_name | generation_expression |\r\n+---------------+---------------+-----------------------+-----------------+------------------+----------------+-------------+-----------+--------------------------+------------------------+-------------------+---------------+--------------------+-----------------------+----------------------+--------------------+-----------------------+\r\n| defaultdb     | crdb_internal | backward_dependencies | descriptor_id   |                1 | NULL           | YES         | INT       |                     NULL |                   NULL |                64 |             0 |               NULL | NULL                  | NULL                 | NULL               |                       |\r\n| defaultdb     | crdb_internal | backward_dependencies | descriptor_name |                2 | NULL           | NO          | STRING    |                     NULL |                   NULL |              NULL |          NULL |               NULL | NULL                  | NULL                 | NULL               |                       |\r\n| defaultdb     | crdb_internal | backward_dependencies | index_id        |                3 | NULL           | YES         | INT       |                     NULL |                   NULL |                64 |             0 |               NULL | NULL                  | NULL                 | NULL               |                       |\r\n| defaultdb     | crdb_internal | backward_dependencies | column_id       |                4 | NULL           | YES         | INT       |                     NULL |                   NULL |                64 |             0 |               NULL | NULL                  | NULL                 | NULL               |                       |\r\n| defaultdb     | crdb_internal | backward_dependencies | dependson_id    |                5 | NULL           | NO          | INT       |                     NULL |                   NULL |                64 |             0 |               NULL | NULL                  | NULL                 | NULL               |                       |\r\n...\r\n```\r\n\r\n\r\n## Constraints (check, fk, etc):\r\n\r\n```sql\r\nSELECT\r\n  c.oid,\r\n  c.*,\r\n  t.relname AS tabrelname,\r\n  rt.relnamespace AS refnamespace,\r\n  d.description\r\nFROM\r\n  pg_catalog.pg_constraint AS c\r\n  INNER JOIN pg_catalog.pg_class AS t ON t.oid = c.conrelid\r\n  LEFT JOIN pg_catalog.pg_class AS rt ON rt.oid = c.confrelid\r\n  LEFT JOIN pg_catalog.pg_description AS d\r\n  ON d.objoid = c.oid AND d.objsubid = 0;\r\n-- constraints are supported in cockroach\r\n-- pg_class is limited, see the previous comment in the Classes section\r\n-- pg_descriptions is always just an empty table as comments are not supported\r\n-- pg_constraint is supported, with a number of preset values\r\n--   condeferrable and condeferred are always set to FALSE\r\n--   contypid is always set to an OID of 0\r\n--   conislocal and connoinherit are always set to TRUE\r\n--   coninhcount is always set to 0\r\n--   conpfeqop, conppeqop, conffeqop and conexclop are always set to NULL\r\n-- The more idiomatic way to get constraints is using the information schema:\r\nSELECT * FROM information_schema.table_constraints;\r\n+--------------------+-------------------+-----------------+---------------+--------------+------------------+-----------------+---------------+--------------------+\r\n| constraint_catalog | constraint_schema | constraint_name | table_catalog | table_schema |    table_name    | constraint_type | is_deferrable | initially_deferred |\r\n+--------------------+-------------------+-----------------+---------------+--------------+------------------+-----------------+---------------+--------------------+\r\n| system             | public            | primary         | system        | public       | namespace        | PRIMARY KEY     | NO            | NO                 |\r\n| system             | public            | primary         | system        | public       | descriptor       | PRIMARY KEY     | NO            | NO                 |\r\n| system             | public            | primary         | system        | public       | users            | PRIMARY KEY     | NO            | NO                 |\r\n| system             | public            | primary         | system        | public       | zones            | PRIMARY KEY     | NO            | NO                 |\r\n...\r\n-- That being said, the information_schame.table_constraints wont show check expressions, while the SHOW CONSTRAINTS statement will:\r\nSHOW CONSTRAINTS FROM orders;\r\n+--------+------------------------+-------------+---------------+--------------------------------------------------------+\r\n| Table  |          Name          |    Type     |   Column(s)   |                        Details                         |\r\n+--------+------------------------+-------------+---------------+--------------------------------------------------------+\r\n| orders |                        | CHECK       | NULL          | status IN ('open', 'in progress', 'done', 'cancelled') |\r\n| orders |                        | CHECK       | NULL          | priority BETWEEN 1 AND 5                               |\r\n| orders | orders_customer_id_key | UNIQUE      | [customer_id] | NULL                                                   |\r\n| orders | primary                | PRIMARY KEY | [id]          | NULL                                                   |\r\n+--------+------------------------+-------------+---------------+--------------------------------------------------------+\r\n```\r\n\r\n## Indexes\r\n\r\n```sql\r\nSELECT\r\n  *\r\nFROM\r\n  pg_catalog.pg_index AS i\r\n  INNER JOIN pg_catalog.pg_class AS c ON c.oid = i.indexrelid\r\n  INNER JOIN pg_catalog.pg_class AS tc ON tc.oid = i.indrelid\r\n  LEFT JOIN pg_catalog.pg_description AS dsc ON i.indexrelid = dsc.objoid;\r\n-- pg_index is supported, but with some preset values\r\n--   indisexclusion, indisclustered, indcheckxmin and indisreplident are always FALSE\r\n--   indislive is always TRUE\r\n--   indexprs and indpred are always set to NULL\r\n-- pg_class is limited, see the previous comment in the Classes section\r\n-- The more idiomatic way to get indexes is to use information schema\r\nSELECT * FROM information_schema.indexes;\r\n+---------------+--------------+------------------+------------+--------------+----------------------------+--------------+---------------+-----------+-------------+-----------+---------+----------+\r\n| table_catalog | table_schema |    table_name    | non_unique | index_schema |         index_name         | seq_in_index |  column_name  | COLLATION | cardinality | direction | storing | implicit |\r\n+---------------+--------------+------------------+------------+--------------+----------------------------+--------------+---------------+-----------+-------------+-----------+---------+----------+\r\n| system        | public       | namespace        | NO         | public       | primary                    |            1 | parentID      | NULL      |        NULL | ASC       | NO      | NO       |\r\n| system        | public       | namespace        | NO         | public       | primary                    |            2 | name          | NULL      |        NULL | ASC       | NO      | NO       |\r\n| system        | public       | descriptor       | NO         | public       | primary                    |            1 | id            | NULL      |        NULL | ASC       | NO      | NO       |\r\n| system        | public       | users            | NO         | public       | primary                    |            1 | username      | NULL      |        NULL | ASC       | NO      | NO       |\r\n...\r\n```\r\n\r\n## Procedures\r\n\r\n```sql\r\nSELECT\r\n  p.oid,\r\n  p.*,\r\n  pg_catalog.pg_get_expr(p.proargdefaults, 0) AS arg_defaults,\r\n  d.description\r\nFROM\r\n  pg_catalog.pg_proc AS p\r\n  LEFT JOIN pg_catalog.pg_description AS d ON d.objoid = p.oid\r\nWHERE\r\n  p.pronamespace = ?;\r\n-- cockroach does not support stored procedures yet\r\n-- but we do support pg_proc is supported, and it is filled with all active builtin functions, with limitations\r\n--   proowner, procost, prorows, protransform, provolatile, proparallel, proallargtypes, proargnames, proargdefaults, protrftypes, probin, proconfig and proacl are always null\r\n--   prolang is always set to an OID of 0\r\n--   prosecdef and proisstrict are always set to false\r\n--   pronargdefaults is always set to 0\r\n-- cockroach does not support comments, so pg_description is always empty.\r\n```\r\n\r\n## Triggers\r\n\r\n```sql\r\nSELECT\r\n  x.oid, x.*, p.pronamespace AS func_schema_id\r\nFROM\r\n  pg_catalog.pg_trigger AS x\r\n  LEFT JOIN pg_catalog.pg_proc AS p ON p.oid = x.tgfoid;\r\n-- cockroach does not yet support triggers or stored procedures\r\n-- pg_trigger returns an empty table\r\n-- pg_proc is supported with a number of limitations and hard coded values, see ""procedures""\r\n```\r\n\r\n## Data types\r\n\r\n```sql\r\nSELECT\r\n  t.oid, t.*\r\nFROM\r\n  pg_catalog.pg_type AS t\r\nWHERE\r\n  typnamespace = ? AND typcategory NOT IN ('A', 'C');\r\n-- cockroach does not allow user defined types, so this is pretty static, but we do add types\r\n-- pg_type is supported, but a lot of the columns are preset\r\n--   typowner, typalign, typstorage, typdefaultbin, typdefault and typacl are all set to null\r\n--   typispreferred and typnotnull are always false\r\n--   typisdefined is always true\r\n--   typrelid, typmodin, typmodout, typanalyze and typbasetype are always set to and OID of 0\r\n--   typndims is always set to 0\r\n--   typtypmod is always set to -1\r\n--   typdlim is always ','\r\n```\r\n\r\n## Inheritance\r\n\r\n```sql\r\nSELECT\r\n  i.*, c.relnamespace\r\nFROM\r\n  pg_catalog.pg_inherits AS i, pg_catalog.pg_class AS c\r\nWHERE\r\n  i.inhrelid = ? AND c.oid = i.inhparent\r\nORDER BY\r\n  i.inhseqno;\r\n-- cockroach does not support table inheritance\r\n-- pg_inherits returns an empty table\r\n-- pg_class is limited, see the previous comment in the Classes section\r\n```\r\n\r\n## Activity\r\n\r\n```sql\r\nSELECT sa.* FROM pg_catalog.pg_stat_activity AS sa;\r\n-- cockroach does not currently populate postgres stats\r\n-- pg_stat_activity is just an empty table\r\n-- We do of course have a large collection of stats, but not the kind that are relevant.\r\n```\r\n\r\n## Transactions locks\r\n\r\n```sql\r\nSELECT\r\n  COALESCE(db.datname, '') AS datname,\r\n  COALESCE(lock.locktype, '') AS locktype,\r\n  COALESCE(lock.relation::REGCLASS::VARCHAR, '') AS relation,\r\n  COALESCE(lock.mode, '') AS mode,\r\n  COALESCE(lock.transactionid::VARCHAR, '') AS tid,\r\n  lock.page AS page,\r\n  lock.tuple AS tuple,\r\n  lock.pid AS pid,\r\n  lock.granted\r\nFROM\r\n  pg_catalog.pg_locks AS lock\r\n  LEFT JOIN pg_catalog.pg_database AS db ON db.oid = lock.database\r\nWHERE\r\n  lock.pid = ?;\r\n-- locks are not supported by cockroach\r\n-- pg_locks table does not exist\r\n```\r\n\r\n## Object comments\r\n\r\n```sql\r\nSELECT\r\n  description\r\nFROM\r\n  pg_catalog.pg_description\r\n  JOIN pg_catalog.pg_class ON pg_description.objoid = pg_class.oid\r\n  JOIN pg_catalog.pg_namespace ON pg_class.relnamespace = pg_namespace.oid\r\nWHERE\r\n  pg_class.relname = ? AND pg_namespace.nspname = ?;\r\n-- Comments are not supported in cockroach at this time\r\n-- pg_class is limited, see the previous comment in the Classes section\r\n-- pg_namespace is limited, see the previous comment in the Schemas section\r\n-- pg_description is always an empty table\r\n```\r\n\r\n## View definition\r\n\r\n```sql\r\nSELECT pg_get_viewdef(?, true);\r\n-- This function is fully supported.\r\n```\r\n\r\n## Procedure definition\r\n\r\n```sql\r\nSELECT pg_get_functiondef(?);\r\n-- Cockroach does not support user defined functions\r\n-- this function is not supported by cockroach and does not exist yet.\r\n```\r\n","sql\r\nSELECT db.datname FROM pg_catalog.pg_database AS db WHERE datallowconn;\r\n-- supported by cockroach\r\n-- pg_database has a number of limitations\r\n--   datdba, datlastsysoid, datfrozenxid, datminmxid and datacl are always NULL\r\n--   encoding is always set to 6\r\n--   datcollate and datctype are always set to 'en_US.utf8'\r\n--   datistemplate is always set to FALSE\r\n--   datallowconn is always set to TRUE\r\n--   datconnlimit is always set to -1\r\n--   dattablespace is always set to an OID of 0\r\n-- but this is more idiomatic of cockroach\r\nSELECT database_name AS datname FROM [SHOW DATABASES];\r\n"
28134,pgwire: DInterval conversion missing on binary protocol in 2.0Reported by @mostov on gitter\r\n\r\n- Which version of CockroachDB are you using?\r\n\r\ncockroach-v2.0.4.windows-6.2-amd64\r\n\r\n- What did you do?\r\n\r\n\r\n\r\n- What did you expect to see?\r\n\r\nsuccessful result\r\n\r\n- What did you see instead?\r\n\r\ncockroachdb crashes\r\n,C-bug|O-community|S-2-temp-unavailability|A-sql-pgwire,knz,"Reported by @mostov on gitter\r\n\r\n- Which version of CockroachDB are you using?\r\n\r\ncockroach-v2.0.4.windows-6.2-amd64\r\n\r\n- What did you do?\r\n\r\n```sql\r\nCREATE TABLE tbl_time_gift (\r\nuser_id STRING(128) NOT NULL,\r\nnext_gift_time TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now(),\r\nFAMILY ""primary"" (user_id, next_gift_time, rowid)\r\n);\r\n-- ...\r\nselect age(next_gift_time ) from tbl_time_gift;\r\n```\r\n\r\n- What did you expect to see?\r\n\r\nsuccessful result\r\n\r\n- What did you see instead?\r\n\r\ncockroachdb crashes\r\n","sql\r\nCREATE TABLE tbl_time_gift (\r\nuser_id STRING(128) NOT NULL,\r\nnext_gift_time TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now(),\r\nFAMILY ""primary"" (user_id, next_gift_time, rowid)\r\n);\r\n-- ...\r\nselect age(next_gift_time ) from tbl_time_gift;\r\n"
28059,"sql: make GROUP BY recognize projection aliasesReported by @clanstyles on Gitter.\r\n\r\nThis is a subtle extension to the SQL standard recognized by PostgreSQL **which currently causes CockroachDB to silently return different results from pg in common cases**. So the severity is high.\r\n\r\nTo understand the problem, let's start with the ""easy"" and innocuous form of the bug:\r\n\r\n\r\n\r\nThis is recognized by PostgreSQL (groups by the projected expression `x+1`), but not by CockroachDB currently (reports an error `z must be grouped by`).\r\n\r\nThe difference between pg and crdb here is that GROUP BY will recognize simple/naked projection aliases **before** resolving the names using the regular algorithm (like ORDER BY already does.) This is a behavior specific to ""simple identifiers"" (like for ORDER BY).\r\n\r\nWhy this matters is that CockroachDB fails to report an error in pretty common cases, and instead returns, silently, very different results from PostgreSQL:\r\n\r\n\r\n\r\nThis currently groups by `t.x` in CockroachDB, whereas it should group by `(x%10)` like in PostgreSQL! \r\n\r\nBoth the heuristic planner and the opt code must be extended to support this!",C-bug|A-sql-semantics|docs-done|A-sql-pgcompat|docs-known-limitation|A-sql-optimizer|S-3-erroneous-edge-case,RaduBerinde,"Reported by @clanstyles on Gitter.\r\n\r\nThis is a subtle extension to the SQL standard recognized by PostgreSQL **which currently causes CockroachDB to silently return different results from pg in common cases**. So the severity is high.\r\n\r\nTo understand the problem, let's start with the ""easy"" and innocuous form of the bug:\r\n\r\n```sql\r\n> create table t(x int);\r\n> select x+1 as z from t group by z;\r\n```\r\n\r\nThis is recognized by PostgreSQL (groups by the projected expression `x+1`), but not by CockroachDB currently (reports an error `z must be grouped by`).\r\n\r\nThe difference between pg and crdb here is that GROUP BY will recognize simple/naked projection aliases **before** resolving the names using the regular algorithm (like ORDER BY already does.) This is a behavior specific to ""simple identifiers"" (like for ORDER BY).\r\n\r\nWhy this matters is that CockroachDB fails to report an error in pretty common cases, and instead returns, silently, very different results from PostgreSQL:\r\n\r\n```sql\r\n>  select (x % 10) as x\r\n     from t\r\n group by x;\r\n```\r\n\r\nThis currently groups by `t.x` in CockroachDB, whereas it should group by `(x%10)` like in PostgreSQL! \r\n\r\nBoth the heuristic planner and the opt code must be extended to support this!",sql\r\n> create table t(x int);\r\n> select x+1 as z from t group by z;\r\n
27732,sql: a new foreign key column cannot be added in a single transaction**Bug report/Feature request**\r\n\r\nA foreign key constraint cannot be added in the same transaction as the column:\r\n\r\n\r\n\r\nWorkaround: Don't use a transaction.\r\nLimitations: Invalid values could be inserted into the table before the constraint can be applied.\r\n\r\nVersion: 2.0.1\r\n\r\n\r\n,C-enhancement|S-3-ux-surprise,vivekmenezes,"**Bug report/Feature request**\r\n\r\nA foreign key constraint cannot be added in the same transaction as the column:\r\n\r\n```sql\r\nCREATE TABLE a(id INTEGER PRIMARY KEY);\r\nCREATE TABLE b(id INTEGER PRIMARY KEY);\r\n\r\nBEGIN TRANSACTION;\r\nALTER TABLE a ADD COLUMN ab INTEGER;\r\nCREATE INDEX ""x"" ON a(ab);\r\nALTER TABLE a ADD CONSTRAINT fk_ab_refs_b FOREIGN KEY (ab) REFERENCES b(id);\r\nCOMMIT;\r\n\r\n-- pq: column ""ab"" does not exist\r\n```\r\n\r\nWorkaround: Don't use a transaction.\r\nLimitations: Invalid values could be inserted into the table before the constraint can be applied.\r\n\r\nVersion: 2.0.1\r\n\r\n\r\n","sql\r\nCREATE TABLE a(id INTEGER PRIMARY KEY);\r\nCREATE TABLE b(id INTEGER PRIMARY KEY);\r\n\r\nBEGIN TRANSACTION;\r\nALTER TABLE a ADD COLUMN ab INTEGER;\r\nCREATE INDEX ""x"" ON a(ab);\r\nALTER TABLE a ADD CONSTRAINT fk_ab_refs_b FOREIGN KEY (ab) REFERENCES b(id);\r\nCOMMIT;\r\n\r\n-- pq: column ""ab"" does not exist\r\n"
27032,sql: joinreader should ignore NULLs in input keyThe JoinReader will happily look up a NULL value which might return results. Example:\r\n\r\n\r\n\r\nWe should shortcircuit the lookup if any of the key columns is NULL.,C-bug|A-sql-execution|S-3-erroneous-edge-case,solongordon,"The JoinReader will happily look up a NULL value which might return results. Example:\r\n\r\n```sql\r\nCREATE TABLE t (a INT);\r\nINSERT INTO t VALUES (NULL);\r\nCREATE TABLE u (x INT PRIMARY KEY, y INT, z INT, UNIQUE INDEX (y));\r\nINSERT INTO u VALUES (1, NULL, NULL);\r\n\r\nSELECT y FROM t, u@u_y_key WHERE a=y;\r\n+---+\r\n| y |\r\n+---+\r\n+---+\r\n\r\n> set experimental_force_lookup_join=true;\r\n\r\nSELECT y FROM t, u@u_y_key WHERE a=y;\r\n+------+\r\n|  y   |\r\n+------+\r\n| NULL |\r\n+------+\r\n```\r\n\r\nWe should shortcircuit the lookup if any of the key columns is NULL.","sql\r\nCREATE TABLE t (a INT);\r\nINSERT INTO t VALUES (NULL);\r\nCREATE TABLE u (x INT PRIMARY KEY, y INT, z INT, UNIQUE INDEX (y));\r\nINSERT INTO u VALUES (1, NULL, NULL);\r\n\r\nSELECT y FROM t, u@u_y_key WHERE a=y;\r\n+---+\r\n| y |\r\n+---+\r\n+---+\r\n\r\n> set experimental_force_lookup_join=true;\r\n\r\nSELECT y FROM t, u@u_y_key WHERE a=y;\r\n+------+\r\n|  y   |\r\n+------+\r\n| NULL |\r\n+------+\r\n"
27016,"opt: crash found while running hibernate test suite - index out of rangeHibernate test org.hibernate.test.joinedsubclass.JoinedSubclassTest.testJoinedSubclass crashes the new optimizer.\r\n\r\nHere's the relevant logs with `--vmodule=conn_executor=2` enabled.\r\n\r\nLet me know if there's anything else you need.\r\n\r\n```\r\nI180627 14:36:53.589573 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:198] executing PrepareStmt: SELECT person0_.person_id AS person_i1_2_, person0_.version AS version2_2_, person0_.name AS name3_2_, person0_.sex AS sex4_2_, person0_.height_centimeters / 2.54E0 AS height_c5_2_, person0_.address AS address6_2_, person0_.zip AS zip7_2_, person0_.country AS country8_2_, person0_1_.title AS title2_0_, person0_1_.salary AS salary3_0_, person0_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_, person0_1_.manager AS manager5_0_, person0_2_.comments AS comments2_1_, person0_2_.salesperson AS salesper3_1_, CASE WHEN person0_1_.person_id IS NOT NULL THEN 1 WHEN person0_2_.person_id IS NOT NULL THEN 2 WHEN person0_.person_id IS NOT NULL THEN 0 END AS clazz_ FROM jperson AS person0_ LEFT JOIN jemployee AS person0_1_ ON person0_.person_id = person0_1_.person_id LEFT JOIN jmanager AS person0_2_ ON person0_.person_id = person0_2_.person_id WHERE person0_.zip = '30306'\r\nI180627 14:36:53.590202 154 storage/replica_proposal.go:202  [n1,s1,r1092/1:/Table/110{7-8}] new range lease repl=(n1,s1):1 seq=7 start=1530110146.359716559,0 epo=6 pro=1530110213.588780787,0 following repl=(n1,s1):1 seq=6 start=1530031975.037054445,0 epo=5 pro=1530033413.067933499,0\r\nI180627 14:36:53.590475 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:199] executing BindStmt: """"->""""\r\nI180627 14:36:53.590487 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:200] executing Describe: """"\r\nI180627 14:36:53.590505 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:201] executing ExecPortal name: """"\r\nI180627 14:36:53.590512 3555 sql/conn_executor.go:1093  [n1,client=127.0.0.1:59536,user=root] portal resolved to: SELECT person0_.person_id AS person_i1_2_, person0_.version AS version2_2_, person0_.name AS name3_2_, person0_.sex AS sex4_2_, person0_.height_centimeters / 2.54E0 AS height_c5_2_, person0_.address AS address6_2_, person0_.zip AS zip7_2_, person0_.country AS country8_2_, person0_1_.title AS title2_0_, person0_1_.salary AS salary3_0_, person0_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_, person0_1_.manager AS manager5_0_, person0_2_.comments AS comments2_1_, person0_2_.salesperson AS salesper3_1_, CASE WHEN person0_1_.person_id IS NOT NULL THEN 1 WHEN person0_2_.person_id IS NOT NULL THEN 2 WHEN person0_.person_id IS NOT NULL THEN 0 END AS clazz_ FROM jperson AS person0_ LEFT JOIN jemployee AS person0_1_ ON person0_.person_id = person0_1_.person_id LEFT JOIN jmanager AS person0_2_ ON person0_.person_id = person0_2_.person_id WHERE person0_.zip = '30306'\r\nI180627 14:36:53.591571 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:202] executing Sync\r\nI180627 14:36:53.608640 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:203] executing PrepareStmt: SELECT this_.person_id AS person_i1_2_0_, this_.version AS version2_2_0_, this_.name AS name3_2_0_, this_.sex AS sex4_2_0_, this_.height_centimeters / 2.54E0 AS height_c5_2_0_, this_.address AS address6_2_0_, this_.zip AS zip7_2_0_, this_.country AS country8_2_0_, this_1_.title AS title2_0_0_, this_1_.salary AS salary3_0_0_, this_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_, this_1_.manager AS manager5_0_0_, this_2_.comments AS comments2_1_0_, this_2_.salesperson AS salesper3_1_0_, CASE WHEN this_1_.person_id IS NOT NULL THEN 1 WHEN this_2_.person_id IS NOT NULL THEN 2 WHEN this_.person_id IS NOT NULL THEN 0 END AS clazz_0_ FROM jperson AS this_ LEFT JOIN jemployee AS this_1_ ON this_.person_id = this_1_.person_id LEFT JOIN jmanager AS this_2_ ON this_.person_id = this_2_.person_id WHERE (this_.address, this_.zip, this_.country) IN (($1, $2, $3), ($4, $5, $6))\r\nE180627 14:36:53.609534 3555 sql/conn_executor.go:655  [n1,client=127.0.0.1:59536,user=root] a SQL panic has occurred while executing ""SELECT this_.person_id AS person_i1_2_0_, this_.version AS version2_2_0_, this_.name AS name3_2_0_, this_.sex AS sex4_2_0_, this_.height_centimeters / 2.54E0 AS height_c5_2_0_, this_.address AS address6_2_0_, this_.zip AS zip7_2_0_, this_.country AS country8_2_0_, this_1_.title AS title2_0_0_, this_1_.salary AS salary3_0_0_, this_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_, this_1_.manager AS manager5_0_0_, this_2_.comments AS comments2_1_0_, this_2_.salesperson AS salesper3_1_0_, CASE WHEN th [...]"": runtime error: index out of range\r\nI180627 14:36:53.609560 3555 sql/conn_executor.go:675  [n1,client=127.0.0.1:59536,user=root] finishing connExecutor\r\nE180627 14:36:53.609770 3555 util/log/crash_reporting.go:203  [n1,client=127.0.0.1:59536,user=root] a panic has occurred!\r\nE180627 14:36:53.627041 3555 util/log/crash_reporting.go:477  [n1,client=127.0.0.1:59536,user=root] Reported as error 17d90a5dcb7d4679accb467cc6cabcaf\r\npanic: runtime error: index out of range [recovered]\r\n\tpanic: runtime error: index out of range [recovered]\r\n\tpanic: panic while executing 1 statements: SELECT _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ / _ AS _, _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ * _ AS _, _._ AS _, _._ AS _, _._ AS _, CASE WHEN _._ IS NOT _ THEN _ WHEN _._ IS NOT _ THEN _ WHEN _._ IS NOT _ THEN _ END AS _ FROM _ AS _ LEFT JOIN _ AS _ ON _._ = _._ LEFT JOIN _ AS _ ON _._ = _._ WHERE (_._, _._, _._) IN (($1, $2, $3), ($4, $5, $6)); caused by runtime error: index out of range\r\n\r\ngoroutine 3555 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc426024000, 0x66befa0, 0xc4259a5c40, 0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:669 +0x36f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc426024000, 0x66befa0, 0xc4259a5c40)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:411 +0x61\r\npanic(0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go1.10/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc425fceff8)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:124 +0x99\r\npanic(0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go1.10/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/constraint.(*Columns).Init(...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/constraint/columns.go:40\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraintForTupleIn(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff002900000019)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:225 +0xd3f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraints(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff002d0000001a)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:408 +0x4d0\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).getConstraints(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff00370000001a)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:351 +0xac\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraints(0xc425fce738, 0xc426f9c120, 0xffff002d0000001b, 0xc425105a40, 0x400b576)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:381 +0xef\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*logicalPropsBuilder).buildScalarProps(0xc425fce878, 0xc426f9c120, 0xffff002d0000001b, 0xc426f9c120, 0xffff002d0000001b)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:933 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*logicalPropsBuilder).buildProps(0xc425fce878, 0xc426f9c120, 0xffff002d0000001b, 0xffff002d0000001b, 0x20)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:48 +0xbe\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Memo).MemoizeNormExpr(0xc426f9c120, 0xc4260244c0, 0xe0000002d, 0x1, 0x5cd38f0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/memo.go:275 +0x2e3\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).onConstruct(0xc426c42360, 0xe0000002d, 0x1, 0x7a519a0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.go:158 +0x45\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructFilters(0xc426c42360, 0x10000000e, 0x1)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:4117 +0x959\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildFrom(0xc427a72690, 0xc426c36120, 0xc4296cd380, 0xc4261ce0c0, 0xd0891e0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:356 +0x511\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelectClause(0xc427a72690, 0xc42892d050, 0x0, 0x0, 0x0, 0xc4261ce0c0, 0xc4261cc101)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:275 +0x53\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelect(0xc427a72690, 0xc428925540, 0xc4261ce0c0, 0xc427a72690)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:233 +0x19e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildStmt(0xc427a72690, 0x66c1820, 0xc428925540, 0xc4261ce0c0, 0x1)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:172 +0xf4\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build(0xc427a72690, 0xc400000000, 0x0, 0x0, 0x0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:129 +0xab\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makeOptimizerPlan(0xc426024430, 0x66bf060, 0xc426775e90, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:365 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).prepare.func1(0xc426024000, 0x66bf060, 0xc426775e90, 0xc4261ca000, 0xc426c36000, 0xc426c42090, 0x66c1820, 0xc428925540, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:213 +0x5f9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).prepare(0xc426024000, 0x66bf060, 0xc426775e90, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:239 +0x310\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).addPreparedStmt(0xc426024000, 0x66bf060, 0xc426775e90, 0xc427e936c2, 0x0, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:123 +0xe1\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execPrepare(0xc426024000, 0x66bf060, 0xc426775e90, 0xc427e936c2, 0x0, 0x66c1820, 0xc428925540, 0xc426c36000, 0xc4278853e0, 0x6, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:56 +0x15e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc426024000, 0x66befa0, 0xc4259a5c40, 0xc4294dea30, 0x0, 0x0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1138 +0x2a15\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc420a5b600, 0x66befa0, 0xc4259a5c40, 0xc42a615017, 0x12, 0xc42a615009, 0x4, 0x0, 0x0, 0x66a3be0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:413 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc420a5b600, 0x66befa0, 0xc4259a5c40, 0xc425dc8000, 0x5400, 0x15000, 0xc420757800, 0xc4294dea30, 0xc4294dea20, 0xc426bee1a0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:267 +0x122\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:266 +0xf27\r\n```\r\n\r\nIt looks like the query is \r\n\r\n\r\n\r\nAnd the tables are \r\n\r\n\r\n\r\ncc: @awoods187 ",C-bug|C-test-failure|A-sql-optimizer|A-tools-hibernate,justinj|andy-kimball,"Hibernate test org.hibernate.test.joinedsubclass.JoinedSubclassTest.testJoinedSubclass crashes the new optimizer.\r\n\r\nHere's the relevant logs with `--vmodule=conn_executor=2` enabled.\r\n\r\nLet me know if there's anything else you need.\r\n\r\n```\r\nI180627 14:36:53.589573 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:198] executing PrepareStmt: SELECT person0_.person_id AS person_i1_2_, person0_.version AS version2_2_, person0_.name AS name3_2_, person0_.sex AS sex4_2_, person0_.height_centimeters / 2.54E0 AS height_c5_2_, person0_.address AS address6_2_, person0_.zip AS zip7_2_, person0_.country AS country8_2_, person0_1_.title AS title2_0_, person0_1_.salary AS salary3_0_, person0_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_, person0_1_.manager AS manager5_0_, person0_2_.comments AS comments2_1_, person0_2_.salesperson AS salesper3_1_, CASE WHEN person0_1_.person_id IS NOT NULL THEN 1 WHEN person0_2_.person_id IS NOT NULL THEN 2 WHEN person0_.person_id IS NOT NULL THEN 0 END AS clazz_ FROM jperson AS person0_ LEFT JOIN jemployee AS person0_1_ ON person0_.person_id = person0_1_.person_id LEFT JOIN jmanager AS person0_2_ ON person0_.person_id = person0_2_.person_id WHERE person0_.zip = '30306'\r\nI180627 14:36:53.590202 154 storage/replica_proposal.go:202  [n1,s1,r1092/1:/Table/110{7-8}] new range lease repl=(n1,s1):1 seq=7 start=1530110146.359716559,0 epo=6 pro=1530110213.588780787,0 following repl=(n1,s1):1 seq=6 start=1530031975.037054445,0 epo=5 pro=1530033413.067933499,0\r\nI180627 14:36:53.590475 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:199] executing BindStmt: """"->""""\r\nI180627 14:36:53.590487 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:200] executing Describe: """"\r\nI180627 14:36:53.590505 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:201] executing ExecPortal name: """"\r\nI180627 14:36:53.590512 3555 sql/conn_executor.go:1093  [n1,client=127.0.0.1:59536,user=root] portal resolved to: SELECT person0_.person_id AS person_i1_2_, person0_.version AS version2_2_, person0_.name AS name3_2_, person0_.sex AS sex4_2_, person0_.height_centimeters / 2.54E0 AS height_c5_2_, person0_.address AS address6_2_, person0_.zip AS zip7_2_, person0_.country AS country8_2_, person0_1_.title AS title2_0_, person0_1_.salary AS salary3_0_, person0_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_, person0_1_.manager AS manager5_0_, person0_2_.comments AS comments2_1_, person0_2_.salesperson AS salesper3_1_, CASE WHEN person0_1_.person_id IS NOT NULL THEN 1 WHEN person0_2_.person_id IS NOT NULL THEN 2 WHEN person0_.person_id IS NOT NULL THEN 0 END AS clazz_ FROM jperson AS person0_ LEFT JOIN jemployee AS person0_1_ ON person0_.person_id = person0_1_.person_id LEFT JOIN jmanager AS person0_2_ ON person0_.person_id = person0_2_.person_id WHERE person0_.zip = '30306'\r\nI180627 14:36:53.591571 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:202] executing Sync\r\nI180627 14:36:53.608640 3555 sql/conn_executor.go:1050  [n1,client=127.0.0.1:59536,user=root] [Open pos:203] executing PrepareStmt: SELECT this_.person_id AS person_i1_2_0_, this_.version AS version2_2_0_, this_.name AS name3_2_0_, this_.sex AS sex4_2_0_, this_.height_centimeters / 2.54E0 AS height_c5_2_0_, this_.address AS address6_2_0_, this_.zip AS zip7_2_0_, this_.country AS country8_2_0_, this_1_.title AS title2_0_0_, this_1_.salary AS salary3_0_0_, this_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_, this_1_.manager AS manager5_0_0_, this_2_.comments AS comments2_1_0_, this_2_.salesperson AS salesper3_1_0_, CASE WHEN this_1_.person_id IS NOT NULL THEN 1 WHEN this_2_.person_id IS NOT NULL THEN 2 WHEN this_.person_id IS NOT NULL THEN 0 END AS clazz_0_ FROM jperson AS this_ LEFT JOIN jemployee AS this_1_ ON this_.person_id = this_1_.person_id LEFT JOIN jmanager AS this_2_ ON this_.person_id = this_2_.person_id WHERE (this_.address, this_.zip, this_.country) IN (($1, $2, $3), ($4, $5, $6))\r\nE180627 14:36:53.609534 3555 sql/conn_executor.go:655  [n1,client=127.0.0.1:59536,user=root] a SQL panic has occurred while executing ""SELECT this_.person_id AS person_i1_2_0_, this_.version AS version2_2_0_, this_.name AS name3_2_0_, this_.sex AS sex4_2_0_, this_.height_centimeters / 2.54E0 AS height_c5_2_0_, this_.address AS address6_2_0_, this_.zip AS zip7_2_0_, this_.country AS country8_2_0_, this_1_.title AS title2_0_0_, this_1_.salary AS salary3_0_0_, this_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_, this_1_.manager AS manager5_0_0_, this_2_.comments AS comments2_1_0_, this_2_.salesperson AS salesper3_1_0_, CASE WHEN th [...]"": runtime error: index out of range\r\nI180627 14:36:53.609560 3555 sql/conn_executor.go:675  [n1,client=127.0.0.1:59536,user=root] finishing connExecutor\r\nE180627 14:36:53.609770 3555 util/log/crash_reporting.go:203  [n1,client=127.0.0.1:59536,user=root] a panic has occurred!\r\nE180627 14:36:53.627041 3555 util/log/crash_reporting.go:477  [n1,client=127.0.0.1:59536,user=root] Reported as error 17d90a5dcb7d4679accb467cc6cabcaf\r\npanic: runtime error: index out of range [recovered]\r\n\tpanic: runtime error: index out of range [recovered]\r\n\tpanic: panic while executing 1 statements: SELECT _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ / _ AS _, _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ AS _, _._ * _ AS _, _._ AS _, _._ AS _, _._ AS _, CASE WHEN _._ IS NOT _ THEN _ WHEN _._ IS NOT _ THEN _ WHEN _._ IS NOT _ THEN _ END AS _ FROM _ AS _ LEFT JOIN _ AS _ ON _._ = _._ LEFT JOIN _ AS _ ON _._ = _._ WHERE (_._, _._, _._) IN (($1, $2, $3), ($4, $5, $6)); caused by runtime error: index out of range\r\n\r\ngoroutine 3555 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc426024000, 0x66befa0, 0xc4259a5c40, 0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:669 +0x36f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func1(0xc426024000, 0x66befa0, 0xc4259a5c40)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:411 +0x61\r\npanic(0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go1.10/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build.func1(0xc425fceff8)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:124 +0x99\r\npanic(0x5ee1080, 0x79b41c0)\r\n\t/Users/bram/go1.10/src/runtime/panic.go:502 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/constraint.(*Columns).Init(...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/constraint/columns.go:40\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraintForTupleIn(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff002900000019)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:225 +0xd3f\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraints(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff002d0000001a)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:408 +0x4d0\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).getConstraints(0xc425fce738, 0xc426f9c120, 0xffff00370000001a, 0xc426f9c120, 0xffff00370000001a)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:351 +0xac\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*constraintsBuilder).buildConstraints(0xc425fce738, 0xc426f9c120, 0xffff002d0000001b, 0xc425105a40, 0x400b576)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/constraint_builder.go:381 +0xef\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*logicalPropsBuilder).buildScalarProps(0xc425fce878, 0xc426f9c120, 0xffff002d0000001b, 0xc426f9c120, 0xffff002d0000001b)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:933 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*logicalPropsBuilder).buildProps(0xc425fce878, 0xc426f9c120, 0xffff002d0000001b, 0xffff002d0000001b, 0x20)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/logical_props_builder.go:48 +0xbe\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/memo.(*Memo).MemoizeNormExpr(0xc426f9c120, 0xc4260244c0, 0xe0000002d, 0x1, 0x5cd38f0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/memo/memo.go:275 +0x2e3\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).onConstruct(0xc426c42360, 0xe0000002d, 0x1, 0x7a519a0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.go:158 +0x45\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/norm.(*Factory).ConstructFilters(0xc426c42360, 0x10000000e, 0x1)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/norm/factory.og.go:4117 +0x959\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildFrom(0xc427a72690, 0xc426c36120, 0xc4296cd380, 0xc4261ce0c0, 0xd0891e0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:356 +0x511\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelectClause(0xc427a72690, 0xc42892d050, 0x0, 0x0, 0x0, 0xc4261ce0c0, 0xc4261cc101)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:275 +0x53\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildSelect(0xc427a72690, 0xc428925540, 0xc4261ce0c0, 0xc427a72690)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/select.go:233 +0x19e\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).buildStmt(0xc427a72690, 0x66c1820, 0xc428925540, 0xc4261ce0c0, 0x1)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:172 +0xf4\r\ngithub.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder.(*Builder).Build(0xc427a72690, 0xc400000000, 0x0, 0x0, 0x0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/opt/optbuilder/builder.go:129 +0xab\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makeOptimizerPlan(0xc426024430, 0x66bf060, 0xc426775e90, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:365 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).prepare.func1(0xc426024000, 0x66bf060, 0xc426775e90, 0xc4261ca000, 0xc426c36000, 0xc426c42090, 0x66c1820, 0xc428925540, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:213 +0x5f9\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).prepare(0xc426024000, 0x66bf060, 0xc426775e90, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:239 +0x310\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).addPreparedStmt(0xc426024000, 0x66bf060, 0xc426775e90, 0xc427e936c2, 0x0, 0x66c1820, 0xc428925540, 0x0, 0x0, 0x0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:123 +0xe1\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execPrepare(0xc426024000, 0x66bf060, 0xc426775e90, 0xc427e936c2, 0x0, 0x66c1820, 0xc428925540, 0xc426c36000, 0xc4278853e0, 0x6, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_prepare.go:56 +0x15e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc426024000, 0x66befa0, 0xc4259a5c40, 0xc4294dea30, 0x0, 0x0)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:1138 +0x2a15\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc420a5b600, 0x66befa0, 0xc4259a5c40, 0xc42a615017, 0x12, 0xc42a615009, 0x4, 0x0, 0x0, 0x66a3be0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:413 +0x1bb\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc420a5b600, 0x66befa0, 0xc4259a5c40, 0xc425dc8000, 0x5400, 0x15000, 0xc420757800, 0xc4294dea30, 0xc4294dea20, 0xc426bee1a0, ...)\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:267 +0x122\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n\t/Users/bram/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:266 +0xf27\r\n```\r\n\r\nIt looks like the query is \r\n\r\n```sql\r\nSELECT\r\n\tthis_.person_id AS person_i1_2_0_,\r\n\tthis_.version AS version2_2_0_,\r\n\tthis_.name AS name3_2_0_,\r\n\tthis_.sex AS sex4_2_0_,\r\n\tthis_.height_centimeters / 2.54E0 AS height_c5_2_0_,\r\n\tthis_.address AS address6_2_0_,\r\n\tthis_.zip AS zip7_2_0_,\r\n\tthis_.country AS country8_2_0_,\r\n\tthis_1_.title AS title2_0_0_,\r\n\tthis_1_.salary AS salary3_0_0_,\r\n\tthis_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_,\r\n\tthis_1_.manager AS manager5_0_0_,\r\n\tthis_2_.comments AS comments2_1_0_,\r\n\tthis_2_.salesperson AS salesper3_1_0_,\r\n\tCASE WHEN this_1_.person_id IS NOT NULL THEN 1 WHEN this_2_.person_id IS NOT NULL THEN 2 WHEN this_.person_id IS NOT NULL THEN 0 END\r\n\t\tAS clazz_0_\r\nFROM\r\n\tjperson AS this_ LEFT JOIN jemployee AS this_1_ ON this_.person_id = this_1_.person_id\r\n\t\tLEFT JOIN jmanager AS this_2_ ON this_.person_id = this_2_.person_id\r\nWHERE (this_.address, this_.zip, this_.country) IN (($1, $2, $3), ($4, $5, $6));\r\n```\r\n\r\nAnd the tables are \r\n\r\n```sql\r\ncreate table JEmployee (\r\n       person_id int8 not null,\r\n        ""title"" varchar(20) not null,\r\n        salary numeric(19, 2),\r\n        pwd_expiry_weeks float8 not null,\r\n        manager int8,\r\n        primary key (person_id)\r\n    );\r\n\r\ncreate table JManager (\r\n       person_id int8 not null,\r\n        comments varchar(255),\r\n        salesperson int8,\r\n        primary key (person_id)\r\n    );\r\n\r\ncreate table JPerson (\r\n       person_id int8 not null,\r\n        version int4 not null,\r\n        name varchar(80) not null,\r\n        sex char(1) not null,\r\n        height_centimeters float8 not null,\r\n        address varchar(255),\r\n        zip varchar(255),\r\n        country varchar(255),\r\n        primary key (person_id)\r\n    );\r\n```\r\n\r\ncc: @awoods187 ","sql\r\nSELECT\r\n\tthis_.person_id AS person_i1_2_0_,\r\n\tthis_.version AS version2_2_0_,\r\n\tthis_.name AS name3_2_0_,\r\n\tthis_.sex AS sex4_2_0_,\r\n\tthis_.height_centimeters / 2.54E0 AS height_c5_2_0_,\r\n\tthis_.address AS address6_2_0_,\r\n\tthis_.zip AS zip7_2_0_,\r\n\tthis_.country AS country8_2_0_,\r\n\tthis_1_.title AS title2_0_0_,\r\n\tthis_1_.salary AS salary3_0_0_,\r\n\tthis_1_.pwd_expiry_weeks * 7.0E0 AS pwd_expi4_0_0_,\r\n\tthis_1_.manager AS manager5_0_0_,\r\n\tthis_2_.comments AS comments2_1_0_,\r\n\tthis_2_.salesperson AS salesper3_1_0_,\r\n\tCASE WHEN this_1_.person_id IS NOT NULL THEN 1 WHEN this_2_.person_id IS NOT NULL THEN 2 WHEN this_.person_id IS NOT NULL THEN 0 END\r\n\t\tAS clazz_0_\r\nFROM\r\n\tjperson AS this_ LEFT JOIN jemployee AS this_1_ ON this_.person_id = this_1_.person_id\r\n\t\tLEFT JOIN jmanager AS this_2_ ON this_.person_id = this_2_.person_id\r\nWHERE (this_.address, this_.zip, this_.country) IN (($1, $2, $3), ($4, $5, $6));\r\n"
26852,server: version upgrade predicate is wrongin `server_update.go` the following code:\r\n\r\n\r\n\r\ncc @tschottdorf ,C-bug|S-0-corruption-or-data-loss,knz,"in `server_update.go` the following code:\r\n\r\n```go\r\nfunc (s *Server) upgradeStatus(ctx context.Context) (bool, error) {\r\n  // ...\r\n  statusMap := s.nodeLiveness.GetLivenessStatusMap(false /*includeRemovedNodes*/)\r\n  var newVersion string\r\n  for _, row := range datums {\r\n    id := roachpb.NodeID(int32(tree.MustBeDInt(row[0])))\r\n    version := string(tree.MustBeDString(row[1]))\r\n\r\n    // problem -- this incorrectly skips temporarily dead nodes\r\n    if statusMap[id] == storage.NodeLivenessStatus_LIVE {\r\n      if newVersion == """" {\r\n        newVersion = version\r\n      } else if version != newVersion {\r\n        return false, errors.New(""not all nodes are running the latest version yet"")\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\ncc @tschottdorf ","go\r\nfunc (s *Server) upgradeStatus(ctx context.Context) (bool, error) {\r\n  // ...\r\n  statusMap := s.nodeLiveness.GetLivenessStatusMap(false /*includeRemovedNodes*/)\r\n  var newVersion string\r\n  for _, row := range datums {\r\n    id := roachpb.NodeID(int32(tree.MustBeDInt(row[0])))\r\n    version := string(tree.MustBeDString(row[1]))\r\n\r\n    // problem -- this incorrectly skips temporarily dead nodes\r\n    if statusMap[id] == storage.NodeLivenessStatus_LIVE {\r\n      if newVersion == """" {\r\n        newVersion = version\r\n      } else if version != newVersion {\r\n        return false, errors.New(""not all nodes are running the latest version yet"")\r\n      }\r\n    }\r\n  }\r\n"
26709,"sql: group by does not conform with postgresorg.hibernate.userguide.hql.HQLTest test_hql_group_by_example_3 fails\r\n\r\n\r\n\r\nThis should parse and execute, but instead it produces\r\n`ERROR: column ""address"" must appear in the GROUP BY clause or be used in an aggregate function`\r\n\r\n\r\nFrom the postgres docs: \r\n> When GROUP BY is present, or any aggregate functions are present, it is not valid for the SELECT list expressions to refer to ungrouped columns except within aggregate functions or when the ungrouped column is functionally dependent on the grouped columns, since there would otherwise be more than one possible value to return for an ungrouped column. A functional dependency exists if the grouped columns (or a subset thereof) are the primary key of the table containing the ungrouped column.\r\n",C-bug|docs-done|A-sql-pgcompat|docs-known-limitation|good first issue|A-sql-optimizer|A-tools-hibernate,RaduBerinde,"org.hibernate.userguide.hql.HQLTest test_hql_group_by_example_3 fails\r\n\r\n```sql\r\ncreate table Phone (\r\n   id int8 not null,\r\n    phone_number varchar(255),\r\n    phone_type varchar(255),\r\n    person_id int8,\r\n    order_id int4,\r\n    primary key (id)\r\n);\r\ncreate table phone_call (\r\n   id int8 not null,\r\n    duration int4 not null,\r\n    call_timestamp timestamp,\r\n    phone_id int8,\r\n    primary key (id)\r\n);\r\ncreate table Person (\r\n   id int8 not null,\r\n    address varchar(255),\r\n    createdOn timestamp,\r\n    name varchar(255),\r\n    nickName varchar(255),\r\n    version int4 not null,\r\n    primary key (id)\r\n);\r\nselect\r\n    person2_.id as col_0_0_,\r\n    sum(call0_.duration) as col_1_0_,\r\n    person2_.id as id1_2_,\r\n    person2_.address as address2_2_,\r\n    person2_.createdOn as createdO3_2_,\r\n    person2_.name as name4_2_,\r\n    person2_.nickName as nickName5_2_,\r\n    person2_.version as version6_2_ \r\nfrom\r\n    phone_call call0_ \r\ninner join\r\n    Phone phone1_ \r\n        on call0_.phone_id=phone1_.id \r\ninner join\r\n    Person person2_ \r\n        on phone1_.person_id=person2_.id \r\ngroup by\r\n    person2_.id;\r\n```\r\n\r\nThis should parse and execute, but instead it produces\r\n`ERROR: column ""address"" must appear in the GROUP BY clause or be used in an aggregate function`\r\n\r\n\r\nFrom the postgres docs: \r\n> When GROUP BY is present, or any aggregate functions are present, it is not valid for the SELECT list expressions to refer to ungrouped columns except within aggregate functions or when the ungrouped column is functionally dependent on the grouped columns, since there would otherwise be more than one possible value to return for an ungrouped column. A functional dependency exists if the grouped columns (or a subset thereof) are the primary key of the table containing the ungrouped column.\r\n","sql\r\ncreate table Phone (\r\n   id int8 not null,\r\n    phone_number varchar(255),\r\n    phone_type varchar(255),\r\n    person_id int8,\r\n    order_id int4,\r\n    primary key (id)\r\n);\r\ncreate table phone_call (\r\n   id int8 not null,\r\n    duration int4 not null,\r\n    call_timestamp timestamp,\r\n    phone_id int8,\r\n    primary key (id)\r\n);\r\ncreate table Person (\r\n   id int8 not null,\r\n    address varchar(255),\r\n    createdOn timestamp,\r\n    name varchar(255),\r\n    nickName varchar(255),\r\n    version int4 not null,\r\n    primary key (id)\r\n);\r\nselect\r\n    person2_.id as col_0_0_,\r\n    sum(call0_.duration) as col_1_0_,\r\n    person2_.id as id1_2_,\r\n    person2_.address as address2_2_,\r\n    person2_.createdOn as createdO3_2_,\r\n    person2_.name as name4_2_,\r\n    person2_.nickName as nickName5_2_,\r\n    person2_.version as version6_2_ \r\nfrom\r\n    phone_call call0_ \r\ninner join\r\n    Phone phone1_ \r\n        on call0_.phone_id=phone1_.id \r\ninner join\r\n    Person person2_ \r\n        on phone1_.person_id=person2_.id \r\ngroup by\r\n    person2_.id;\r\n"
26658,sql: Hibernate tests - correlated subqueriesThis issue is just a dumping ground and my notes from running the hibernate test suite and how it pertains to correlated subqueries.\r\n\r\nThe overall goal for this issue is to \r\n1) list common queries that current fail due to their correlatedness\r\n2) integrate these queries or ones like them into our own testing\r\n3) have one place to discuss which ones are working and are not\r\n\r\norg.hibernate.userguide.collections.UnidirectionalMapTest testLifecycle\r\nFails due to the following correlated subquery.\r\n\r\n,C-investigation|A-sql-pgcompat|A-sql-optimizer|meta-issue|A-testing|A-tools-hibernate,solongordon|andy-kimball,"This issue is just a dumping ground and my notes from running the hibernate test suite and how it pertains to correlated subqueries.\r\n\r\nThe overall goal for this issue is to \r\n1) list common queries that current fail due to their correlatedness\r\n2) integrate these queries or ones like them into our own testing\r\n3) have one place to discuss which ones are working and are not\r\n\r\norg.hibernate.userguide.collections.UnidirectionalMapTest testLifecycle\r\nFails due to the following correlated subquery.\r\n\r\n```sql\r\ncreate table Person (\r\n  id int8 not null,\r\n  primary key (id)\r\n)\r\ncreate table Phone (\r\n  id int8 not null,\r\n  ""number"" varchar(255),\r\n  since timestamp,\r\n  type int4,\r\n  primary key (id)\r\n)\r\ncreate table phone_register (\r\n  phone_id int8 not null,\r\n  person_id int8 not null,\r\n  primary key (phone_id, person_id)\r\n)\r\nalter table if exists phone_register \r\n  add constraint UK_tk8a6rlawn3vkedaekw4mnfbl unique (person_id)\r\nalter table if exists phone_register \r\n  add constraint FKc3jajlx41lw6clbygbw8wm65w \r\n  foreign key (person_id) \r\n  references Phone\r\nalter table if exists phone_register \r\n  add constraint FK6npoomh1rp660o1b55py9ndw4 \r\n  foreign key (phone_id) \r\n  references Person\r\n\r\nselect\r\n  phoneregis0_.phone_id as phone_id1_2_0_,\r\n  phoneregis0_.person_id as person_i2_2_0_,\r\n  (\r\n    select a10.since\r\n    from Phone a10\r\n    where a10.id=phoneregis0_.person_id\r\n  ) as formula159_0_,\r\n  unidirecti1_.id as id1_1_1_,\r\n  unidirecti1_.""number"" as number2_1_1_,\r\n  unidirecti1_.since as since3_1_1_,\r\n  unidirecti1_.type as type4_1_1_\r\nfrom\r\n  phone_register phoneregis0_\r\ninner join Phone unidirecti1_\r\n  on phoneregis0_.person_id=unidirecti1_.id\r\nwhere phoneregis0_.phone_id=1;\r\n```","sql\r\ncreate table Person (\r\n  id int8 not null,\r\n  primary key (id)\r\n)\r\ncreate table Phone (\r\n  id int8 not null,\r\n  ""number"" varchar(255),\r\n  since timestamp,\r\n  type int4,\r\n  primary key (id)\r\n)\r\ncreate table phone_register (\r\n  phone_id int8 not null,\r\n  person_id int8 not null,\r\n  primary key (phone_id, person_id)\r\n)\r\nalter table if exists phone_register \r\n  add constraint UK_tk8a6rlawn3vkedaekw4mnfbl unique (person_id)\r\nalter table if exists phone_register \r\n  add constraint FKc3jajlx41lw6clbygbw8wm65w \r\n  foreign key (person_id) \r\n  references Phone\r\nalter table if exists phone_register \r\n  add constraint FK6npoomh1rp660o1b55py9ndw4 \r\n  foreign key (phone_id) \r\n  references Person\r\n\r\nselect\r\n  phoneregis0_.phone_id as phone_id1_2_0_,\r\n  phoneregis0_.person_id as person_i2_2_0_,\r\n  (\r\n    select a10.since\r\n    from Phone a10\r\n    where a10.id=phoneregis0_.person_id\r\n  ) as formula159_0_,\r\n  unidirecti1_.id as id1_1_1_,\r\n  unidirecti1_.""number"" as number2_1_1_,\r\n  unidirecti1_.since as since3_1_1_,\r\n  unidirecti1_.type as type4_1_1_\r\nfrom\r\n  phone_register phoneregis0_\r\ninner join Phone unidirecti1_\r\n  on phoneregis0_.person_id=unidirecti1_.id\r\nwhere phoneregis0_.phone_id=1;\r\n"
26629,"sql: support pg_get_indexdef with 3 argumentsNeeded for  #16971.\r\n\r\nUsed in this query, already listed in the `srfs` logic test file.\r\n\r\n\r\n",C-enhancement|A-sql-pgcompat|A-sql-builtins,BramGruneir,"Needed for  #16971.\r\n\r\nUsed in this query, already listed in the `srfs` logic test file.\r\n\r\n```sql\r\nSELECT NULL AS table_cat,\r\n       n.nspname AS table_schem,\r\n       ct.relname AS TABLE_NAME,\r\n       NOT i.indisunique AS non_unique,\r\n       NULL AS index_qualifier,\r\n       ci.relname AS index_name,\r\n       CASE i.indisclustered\r\n         WHEN TRUE THEN 1\r\n         ELSE CASE am.amname\r\n           WHEN 'hash' THEN 2\r\n           ELSE 3\r\n         END\r\n       END AS TYPE,\r\n       (i.KEYS).n AS ordinal_position,\r\n       trim(BOTH '""' FROM pg_catalog.pg_get_indexdef(ci.oid, (i.KEYS).n, FALSE)) AS COLUMN_NAME,\r\n       CASE am.amcanorder\r\n         WHEN TRUE THEN CASE i.indoption[(i.keys).n - 1] & 1\r\n           WHEN 1 THEN 'D'\r\n           ELSE 'A'\r\n         END\r\n         ELSE NULL\r\n       END AS asc_or_desc,\r\n       ci.reltuples AS CARDINALITY,\r\n       ci.relpages AS pages,\r\n       pg_catalog.pg_get_expr(i.indpred, i.indrelid) AS filter_condition\r\nFROM pg_catalog.pg_class ct\r\nJOIN pg_catalog.pg_namespace n ON (ct.relnamespace = n.oid)\r\nJOIN (\r\n  SELECT i.indexrelid,\r\n         i.indrelid,\r\n         i.indoption,\r\n         i.indisunique,\r\n         i.indisclustered,\r\n         i.indpred,\r\n         i.indexprs,\r\n         information_schema._pg_expandarray(i.indkey) AS KEYS\r\n  FROM pg_catalog.pg_index i\r\n) i\r\n  ON (ct.oid = i.indrelid)\r\nJOIN pg_catalog.pg_class ci ON (ci.oid = i.indexrelid)\r\nJOIN pg_catalog.pg_am am ON (ci.relam = am.oid)\r\nWHERE TRUE\r\n  AND n.nspname = 'public'\r\n  AND ct.relname = 'j'\r\nORDER BY non_unique,\r\n         TYPE,\r\n         index_name,\r\n         ordinal_position\r\n```\r\n","sql\r\nSELECT NULL AS table_cat,\r\n       n.nspname AS table_schem,\r\n       ct.relname AS TABLE_NAME,\r\n       NOT i.indisunique AS non_unique,\r\n       NULL AS index_qualifier,\r\n       ci.relname AS index_name,\r\n       CASE i.indisclustered\r\n         WHEN TRUE THEN 1\r\n         ELSE CASE am.amname\r\n           WHEN 'hash' THEN 2\r\n           ELSE 3\r\n         END\r\n       END AS TYPE,\r\n       (i.KEYS).n AS ordinal_position,\r\n       trim(BOTH '""' FROM pg_catalog.pg_get_indexdef(ci.oid, (i.KEYS).n, FALSE)) AS COLUMN_NAME,\r\n       CASE am.amcanorder\r\n         WHEN TRUE THEN CASE i.indoption[(i.keys).n - 1] & 1\r\n           WHEN 1 THEN 'D'\r\n           ELSE 'A'\r\n         END\r\n         ELSE NULL\r\n       END AS asc_or_desc,\r\n       ci.reltuples AS CARDINALITY,\r\n       ci.relpages AS pages,\r\n       pg_catalog.pg_get_expr(i.indpred, i.indrelid) AS filter_condition\r\nFROM pg_catalog.pg_class ct\r\nJOIN pg_catalog.pg_namespace n ON (ct.relnamespace = n.oid)\r\nJOIN (\r\n  SELECT i.indexrelid,\r\n         i.indrelid,\r\n         i.indoption,\r\n         i.indisunique,\r\n         i.indisclustered,\r\n         i.indpred,\r\n         i.indexprs,\r\n         information_schema._pg_expandarray(i.indkey) AS KEYS\r\n  FROM pg_catalog.pg_index i\r\n) i\r\n  ON (ct.oid = i.indrelid)\r\nJOIN pg_catalog.pg_class ci ON (ci.oid = i.indexrelid)\r\nJOIN pg_catalog.pg_am am ON (ci.relam = am.oid)\r\nWHERE TRUE\r\n  AND n.nspname = 'public'\r\n  AND ct.relname = 'j'\r\nORDER BY non_unique,\r\n         TYPE,\r\n         index_name,\r\n         ordinal_position\r\n"
26627,"sql: distsql does not support labeled tuplesFound while investigating #26621. Ultimately needed for #16971.\r\n\r\n\r\n\r\nFails with `setting up flow: (""$0"").a: type tuple{int, string} is not composite` with distribution enabled, succeeds with local execution.\r\n\r\nThis is related to, but not identical to, #26624. The two issues must be addressed concurrently to ensure labeled tuples are fully supported.\r\n\r\nThere is a logic test introduced by #26621/#26628 in the `tuples` logic test that currently force-disables distsql to work. When this issue is addressed, the test must be reenabled (and checked).\r\n",C-bug|A-sql-execution,rjnn,"Found while investigating #26621. Ultimately needed for #16971.\r\n\r\n```sql\r\n> CREATE TABLE t (a INT, b STRING)\r\n> SELECT (x).a, (x).b\r\n    FROM (SELECT (ROW(a, b) AS a, b) AS x FROM t)\r\nORDER BY 1\r\n   LIMIT 1\r\n```\r\n\r\nFails with `setting up flow: (""$0"").a: type tuple{int, string} is not composite` with distribution enabled, succeeds with local execution.\r\n\r\nThis is related to, but not identical to, #26624. The two issues must be addressed concurrently to ensure labeled tuples are fully supported.\r\n\r\nThere is a logic test introduced by #26621/#26628 in the `tuples` logic test that currently force-disables distsql to work. When this issue is addressed, the test must be reenabled (and checked).\r\n","sql\r\n> CREATE TABLE t (a INT, b STRING)\r\n> SELECT (x).a, (x).b\r\n    FROM (SELECT (ROW(a, b) AS a, b) AS x FROM t)\r\nORDER BY 1\r\n   LIMIT 1\r\n"
26585,sql: support concatenating more types togetherFound in #26584:\r\n\r\n\r\n\r\nwhere `rolconnlimit` is an integer.\r\n\r\nArguably since `||` is a string-only operator we could support defining `||` over `types.Any` and then force-coerce (`PerformCast`) both operands to string.\r\n,C-enhancement|A-sql-pgcompat|A-sql-builtins|no-issue-activity,BramGruneir,Found in #26584:\r\n\r\n```sql\r\n> SELECT -- ... some stuff\r\n     CASE WHEN rolconnlimit > 0\r\n               THEN e'\\\\n  CONNECTION LIMIT ' || rolconnlimit\r\n               ELSE ''\r\n     END\r\n   FROM pg_roles\r\n```\r\n\r\nwhere `rolconnlimit` is an integer.\r\n\r\nArguably since `||` is a string-only operator we could support defining `||` over `types.Any` and then force-coerce (`PerformCast`) both operands to string.\r\n,sql\r\n> SELECT -- ... some stuff\r\n     CASE WHEN rolconnlimit > 0\r\n               THEN e'\\\\n  CONNECTION LIMIT ' || rolconnlimit\r\n               ELSE ''\r\n     END\r\n   FROM pg_roles\r\n
26409,sql: provide the session variable `default_tablespace`https://www.postgresql.org/docs/10/static/runtime-config-client.html\r\n\r\nThe default value is the empty string.\r\n\r\nNote: table spaces are a different concept from database schemas.\r\n\r\nUsed by PGAdmin in the following query:\r\n\r\n,C-enhancement|A-sql-pgcompat,jordanlewis,"https://www.postgresql.org/docs/10/static/runtime-config-client.html\r\n\r\nThe default value is the empty string.\r\n\r\nNote: table spaces are a different concept from database schemas.\r\n\r\nUsed by PGAdmin in the following query:\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26400,sql: add pg_stat_database_conflictsPGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails here https://www.postgresql.org/docs/current/static/monitoring-stats.html\r\n\r\nEpic CRDB-2462,C-enhancement|A-sql-pgcompat,mnovelodou,"PGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT db.datname AS \\""Database\\"", \r\n       numbackends AS \\""Backends\\"", \r\n       xact_commit AS \\""Xact committed\\"", \r\n       xact_rollback AS \\""Xact rolled back\\"", \r\n       blks_read AS \\""Blocks read\\"", \r\n       blks_hit AS \\""Blocks hit\\"", \r\n       tup_returned AS \\""Tuples returned\\"", \r\n       tup_fetched AS \\""Tuples fetched\\"", \r\n       tup_inserted AS \\""Tuples inserted\\"", \r\n       tup_updated AS \\""Tuples updated\\"", \r\n       tup_deleted AS \\""Tuples deleted\\"", \r\n       stats_reset AS \\""Last statistics reset\\"", \r\n       slave.confl_tablespace AS \\""Tablespace conflicts\\"", \r\n       slave.confl_lock AS \\""Lock conflicts\\"", \r\n       slave.confl_snapshot AS \\""Snapshot conflicts\\"", \r\n       slave.confl_bufferpin AS \\""Bufferpin conflicts\\"", \r\n       slave.confl_deadlock AS \\""Deadlock conflicts\\"", \r\n       temp_files AS \\""Temporary files\\"", \r\n       temp_bytes AS \\""Size of temporary files\\"", \r\n       deadlocks AS \\""Deadlocks\\"", \r\n       blk_read_time AS \\""Block read time\\"", \r\n       blk_write_time AS \\""Block write time\\"", \r\n       pg_database_size(db.datid) AS \\""Size\\"" \r\nFROM pg_stat_database AS db \r\nLEFT JOIN pg_stat_database_conflicts AS slave \r\n  ON db.datid = slave.datid \r\nWHERE db.datid > NULL::OID \r\nORDER BY db.datname\r\n```\r\n\r\nDetails here https://www.postgresql.org/docs/current/static/monitoring-stats.html\r\n\r\nEpic CRDB-2462","sql\r\nSELECT db.datname AS \\""Database\\"", \r\n       numbackends AS \\""Backends\\"", \r\n       xact_commit AS \\""Xact committed\\"", \r\n       xact_rollback AS \\""Xact rolled back\\"", \r\n       blks_read AS \\""Blocks read\\"", \r\n       blks_hit AS \\""Blocks hit\\"", \r\n       tup_returned AS \\""Tuples returned\\"", \r\n       tup_fetched AS \\""Tuples fetched\\"", \r\n       tup_inserted AS \\""Tuples inserted\\"", \r\n       tup_updated AS \\""Tuples updated\\"", \r\n       tup_deleted AS \\""Tuples deleted\\"", \r\n       stats_reset AS \\""Last statistics reset\\"", \r\n       slave.confl_tablespace AS \\""Tablespace conflicts\\"", \r\n       slave.confl_lock AS \\""Lock conflicts\\"", \r\n       slave.confl_snapshot AS \\""Snapshot conflicts\\"", \r\n       slave.confl_bufferpin AS \\""Bufferpin conflicts\\"", \r\n       slave.confl_deadlock AS \\""Deadlock conflicts\\"", \r\n       temp_files AS \\""Temporary files\\"", \r\n       temp_bytes AS \\""Size of temporary files\\"", \r\n       deadlocks AS \\""Deadlocks\\"", \r\n       blk_read_time AS \\""Block read time\\"", \r\n       blk_write_time AS \\""Block write time\\"", \r\n       pg_database_size(db.datid) AS \\""Size\\"" \r\nFROM pg_stat_database AS db \r\nLEFT JOIN pg_stat_database_conflicts AS slave \r\n  ON db.datid = slave.datid \r\nWHERE db.datid > NULL::OID \r\nORDER BY db.datname\r\n"
26399,sql: add pg_stat_databasehttps://www.postgresql.org/docs/9.2/monitoring-stats.html#PG-STAT-DATABASE-VIEW\r\n\r\nPGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails here https://www.postgresql.org/docs/current/static/monitoring-stats.html\n\nEpic CRDB-2462,C-enhancement|A-sql-pgcompat|T-sql-foundations,mnovelodou,"https://www.postgresql.org/docs/9.2/monitoring-stats.html#PG-STAT-DATABASE-VIEW\r\n\r\nPGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT db.datname AS \\""Database\\"", \r\n       numbackends AS \\""Backends\\"", \r\n       xact_commit AS \\""Xact committed\\"", \r\n       xact_rollback AS \\""Xact rolled back\\"", \r\n       blks_read AS \\""Blocks read\\"", \r\n       blks_hit AS \\""Blocks hit\\"", \r\n       tup_returned AS \\""Tuples returned\\"", \r\n       tup_fetched AS \\""Tuples fetched\\"", \r\n       tup_inserted AS \\""Tuples inserted\\"", \r\n       tup_updated AS \\""Tuples updated\\"", \r\n       tup_deleted AS \\""Tuples deleted\\"", \r\n       stats_reset AS \\""Last statistics reset\\"", \r\n       slave.confl_tablespace AS \\""Tablespace conflicts\\"", \r\n       slave.confl_lock AS \\""Lock conflicts\\"", \r\n       slave.confl_snapshot AS \\""Snapshot conflicts\\"", \r\n       slave.confl_bufferpin AS \\""Bufferpin conflicts\\"", \r\n       slave.confl_deadlock AS \\""Deadlock conflicts\\"", \r\n       temp_files AS \\""Temporary files\\"", \r\n       temp_bytes AS \\""Size of temporary files\\"", \r\n       deadlocks AS \\""Deadlocks\\"", \r\n       blk_read_time AS \\""Block read time\\"", \r\n       blk_write_time AS \\""Block write time\\"", \r\n       pg_database_size(db.datid) AS \\""Size\\"" \r\nFROM pg_stat_database AS db \r\nLEFT JOIN pg_stat_database_conflicts AS slave \r\n  ON db.datid = slave.datid \r\nWHERE db.datid > NULL::OID \r\nORDER BY db.datname\r\n```\r\n\r\nDetails here https://www.postgresql.org/docs/current/static/monitoring-stats.html\n\nEpic CRDB-2462","sql\r\nSELECT db.datname AS \\""Database\\"", \r\n       numbackends AS \\""Backends\\"", \r\n       xact_commit AS \\""Xact committed\\"", \r\n       xact_rollback AS \\""Xact rolled back\\"", \r\n       blks_read AS \\""Blocks read\\"", \r\n       blks_hit AS \\""Blocks hit\\"", \r\n       tup_returned AS \\""Tuples returned\\"", \r\n       tup_fetched AS \\""Tuples fetched\\"", \r\n       tup_inserted AS \\""Tuples inserted\\"", \r\n       tup_updated AS \\""Tuples updated\\"", \r\n       tup_deleted AS \\""Tuples deleted\\"", \r\n       stats_reset AS \\""Last statistics reset\\"", \r\n       slave.confl_tablespace AS \\""Tablespace conflicts\\"", \r\n       slave.confl_lock AS \\""Lock conflicts\\"", \r\n       slave.confl_snapshot AS \\""Snapshot conflicts\\"", \r\n       slave.confl_bufferpin AS \\""Bufferpin conflicts\\"", \r\n       slave.confl_deadlock AS \\""Deadlock conflicts\\"", \r\n       temp_files AS \\""Temporary files\\"", \r\n       temp_bytes AS \\""Size of temporary files\\"", \r\n       deadlocks AS \\""Deadlocks\\"", \r\n       blk_read_time AS \\""Block read time\\"", \r\n       blk_write_time AS \\""Block write time\\"", \r\n       pg_database_size(db.datid) AS \\""Size\\"" \r\nFROM pg_stat_database AS db \r\nLEFT JOIN pg_stat_database_conflicts AS slave \r\n  ON db.datid = slave.datid \r\nWHERE db.datid > NULL::OID \r\nORDER BY db.datname\r\n"
26392,sql: add pg_tablespace_size()PGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails here: https://www.postgresql.org/docs/current/static/functions-admin.html\n\nJira issue: CRDB-5676,C-enhancement|A-sql-pgcompat|X-stale|A-sql-pgcatalog|no-issue-activity,BramGruneir,"PGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT ts.spcname AS \\""Name\\"", \r\n       pg_tablespace_size(ts.oid) AS \\""Size\\"" \r\nFROM pg_catalog.pg_tablespace AS ts\r\n```\r\n\r\nDetails here: https://www.postgresql.org/docs/current/static/functions-admin.html\n\nJira issue: CRDB-5676","sql\r\nSELECT ts.spcname AS \\""Name\\"", \r\n       pg_tablespace_size(ts.oid) AS \\""Size\\"" \r\nFROM pg_catalog.pg_tablespace AS ts\r\n"
26390,sql: add pg_conversionPGAdmin issues the following query:\r\n\r\n\r\n\r\nSee https://www.postgresql.org/docs/current/static/catalog-pg-conversion.html for details.,C-enhancement|A-sql-pgcompat,BramGruneir,"PGAdmin issues the following query:\r\n\r\n```sql\r\n  SELECT cl.relkind, \r\n         COALESCE(cin.nspname, cln.nspname) AS nspname, \r\n         COALESCE(ci.relname, cl.relname) AS relname, \r\n         cl.relname AS indname \r\n  FROM pg_class AS cl \r\n  JOIN pg_namespace AS cln \r\n    ON cl.relnamespace = cln.oid \r\n  LEFT JOIN pg_index AS ind \r\n    ON ind.indexrelid = cl.oid \r\n  LEFT JOIN pg_class AS ci \r\n    ON ind.indrelid = ci.oid \r\n  LEFT JOIN pg_namespace AS cin \r\n    ON ci.relnamespace = cin.oid \r\n  WHERE (cl.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (cl.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'n', \r\n         NULL, \r\n         nspname, \r\n         NULL \r\n  FROM pg_namespace AS nsp \r\n  WHERE (nsp.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (nsp.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN typtype = 'd' \r\n           THEN 'd' \r\n           ELSE 'y' \r\n        END, \r\n        NULL, \r\n        typname, \r\n        NULL \r\n  FROM pg_type AS ty \r\n  WHERE (ty.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (ty.oid > none::OID) \r\nUNION ALL\r\n  SELECT 'C', \r\n         NULL, \r\n         conname, \r\n         NULL \r\n  FROM pg_conversion AS co \r\n  WHERE (co.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (co.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN prorettype = 2279 \r\n           THEN 'T' \r\n           ELSE 'p' \r\n        END, \r\n        NULL, \r\n        proname, \r\n        NULL \r\n  FROM pg_proc AS pr \r\n  WHERE (pr.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (pr.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'o', \r\n         NULL, \r\n         (\r\n           (\r\n             (\r\n               (oprname || '('::TEXT) \r\n                 || COALESCE(tl.typname, ''::TEXT)\r\n             ) || CASE WHEN (tl.oid IS NOT NULL) AND (tr.oid IS NOT NULL) \r\n               THEN ','::TEXT END\r\n           ) || COALESCE(tr.typname, ''::TEXT)\r\n          ) || ')'::TEXT, \r\n          NULL \r\n  FROM pg_operator AS op \r\n  LEFT JOIN pg_type AS tl \r\n    ON tl.oid = op.oprleft \r\n  LEFT JOIN pg_type AS tr \r\n    ON tr.oid = op.oprright \r\n  WHERE (op.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (op.oid > none::OID) \r\n  ORDER BY 1, 2, 3\r\n```\r\n\r\nSee https://www.postgresql.org/docs/current/static/catalog-pg-conversion.html for details.","sql\r\n  SELECT cl.relkind, \r\n         COALESCE(cin.nspname, cln.nspname) AS nspname, \r\n         COALESCE(ci.relname, cl.relname) AS relname, \r\n         cl.relname AS indname \r\n  FROM pg_class AS cl \r\n  JOIN pg_namespace AS cln \r\n    ON cl.relnamespace = cln.oid \r\n  LEFT JOIN pg_index AS ind \r\n    ON ind.indexrelid = cl.oid \r\n  LEFT JOIN pg_class AS ci \r\n    ON ind.indrelid = ci.oid \r\n  LEFT JOIN pg_namespace AS cin \r\n    ON ci.relnamespace = cin.oid \r\n  WHERE (cl.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (cl.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'n', \r\n         NULL, \r\n         nspname, \r\n         NULL \r\n  FROM pg_namespace AS nsp \r\n  WHERE (nsp.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (nsp.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN typtype = 'd' \r\n           THEN 'd' \r\n           ELSE 'y' \r\n        END, \r\n        NULL, \r\n        typname, \r\n        NULL \r\n  FROM pg_type AS ty \r\n  WHERE (ty.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (ty.oid > none::OID) \r\nUNION ALL\r\n  SELECT 'C', \r\n         NULL, \r\n         conname, \r\n         NULL \r\n  FROM pg_conversion AS co \r\n  WHERE (co.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (co.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN prorettype = 2279 \r\n           THEN 'T' \r\n           ELSE 'p' \r\n        END, \r\n        NULL, \r\n        proname, \r\n        NULL \r\n  FROM pg_proc AS pr \r\n  WHERE (pr.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (pr.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'o', \r\n         NULL, \r\n         (\r\n           (\r\n             (\r\n               (oprname || '('::TEXT) \r\n                 || COALESCE(tl.typname, ''::TEXT)\r\n             ) || CASE WHEN (tl.oid IS NOT NULL) AND (tr.oid IS NOT NULL) \r\n               THEN ','::TEXT END\r\n           ) || COALESCE(tr.typname, ''::TEXT)\r\n          ) || ')'::TEXT, \r\n          NULL \r\n  FROM pg_operator AS op \r\n  LEFT JOIN pg_type AS tl \r\n    ON tl.oid = op.oprleft \r\n  LEFT JOIN pg_type AS tr \r\n    ON tr.oid = op.oprright \r\n  WHERE (op.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (op.oid > none::OID) \r\n  ORDER BY 1, 2, 3\r\n"
26389,sql: add pg_catalog.pg_shdependPGAdmin issues the following query:\r\n\r\n\r\n\r\nSee https://www.postgresql.org/docs/current/static/catalog-pg-shdepend.html for details.,C-enhancement|A-sql-pgcompat,BramGruneir,"PGAdmin issues the following query:\r\n\r\n```sql\r\n  SELECT cl.relkind, \r\n         COALESCE(cin.nspname, cln.nspname) AS nspname, \r\n         COALESCE(ci.relname, cl.relname) AS relname, \r\n         cl.relname AS indname \r\n  FROM pg_class AS cl \r\n  JOIN pg_namespace AS cln \r\n    ON cl.relnamespace = cln.oid \r\n  LEFT JOIN pg_index AS ind \r\n    ON ind.indexrelid = cl.oid \r\n  LEFT JOIN pg_class AS ci \r\n    ON ind.indrelid = ci.oid \r\n  LEFT JOIN pg_namespace AS cin \r\n    ON ci.relnamespace = cin.oid \r\n  WHERE (cl.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (cl.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'n', \r\n         NULL, \r\n         nspname, \r\n         NULL \r\n  FROM pg_namespace AS nsp \r\n  WHERE (nsp.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (nsp.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN typtype = 'd' \r\n           THEN 'd' \r\n           ELSE 'y' \r\n        END, \r\n        NULL, \r\n        typname, \r\n        NULL \r\n  FROM pg_type AS ty \r\n  WHERE (ty.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (ty.oid > none::OID) \r\nUNION ALL\r\n  SELECT 'C', \r\n         NULL, \r\n         conname, \r\n         NULL \r\n  FROM pg_conversion AS co \r\n  WHERE (co.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (co.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN prorettype = 2279 \r\n           THEN 'T' \r\n           ELSE 'p' \r\n        END, \r\n        NULL, \r\n        proname, \r\n        NULL \r\n  FROM pg_proc AS pr \r\n  WHERE (pr.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (pr.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'o', \r\n         NULL, \r\n         (\r\n           (\r\n             (\r\n               (oprname || '('::TEXT) \r\n                 || COALESCE(tl.typname, ''::TEXT)\r\n             ) || CASE WHEN (tl.oid IS NOT NULL) AND (tr.oid IS NOT NULL) \r\n               THEN ','::TEXT END\r\n           ) || COALESCE(tr.typname, ''::TEXT)\r\n          ) || ')'::TEXT, \r\n          NULL \r\n  FROM pg_operator AS op \r\n  LEFT JOIN pg_type AS tl \r\n    ON tl.oid = op.oprleft \r\n  LEFT JOIN pg_type AS tr \r\n    ON tr.oid = op.oprright \r\n  WHERE (op.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (op.oid > none::OID) \r\n  ORDER BY 1, 2, 3\r\n```\r\n\r\nSee https://www.postgresql.org/docs/current/static/catalog-pg-shdepend.html for details.","sql\r\n  SELECT cl.relkind, \r\n         COALESCE(cin.nspname, cln.nspname) AS nspname, \r\n         COALESCE(ci.relname, cl.relname) AS relname, \r\n         cl.relname AS indname \r\n  FROM pg_class AS cl \r\n  JOIN pg_namespace AS cln \r\n    ON cl.relnamespace = cln.oid \r\n  LEFT JOIN pg_index AS ind \r\n    ON ind.indexrelid = cl.oid \r\n  LEFT JOIN pg_class AS ci \r\n    ON ind.indrelid = ci.oid \r\n  LEFT JOIN pg_namespace AS cin \r\n    ON ci.relnamespace = cin.oid \r\n  WHERE (cl.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (cl.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'n', \r\n         NULL, \r\n         nspname, \r\n         NULL \r\n  FROM pg_namespace AS nsp \r\n  WHERE (nsp.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (nsp.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN typtype = 'd' \r\n           THEN 'd' \r\n           ELSE 'y' \r\n        END, \r\n        NULL, \r\n        typname, \r\n        NULL \r\n  FROM pg_type AS ty \r\n  WHERE (ty.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (ty.oid > none::OID) \r\nUNION ALL\r\n  SELECT 'C', \r\n         NULL, \r\n         conname, \r\n         NULL \r\n  FROM pg_conversion AS co \r\n  WHERE (co.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (co.oid > none::OID) \r\nUNION ALL \r\n  SELECT CASE WHEN prorettype = 2279 \r\n           THEN 'T' \r\n           ELSE 'p' \r\n        END, \r\n        NULL, \r\n        proname, \r\n        NULL \r\n  FROM pg_proc AS pr \r\n  WHERE (pr.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (pr.oid > none::OID) \r\nUNION ALL \r\n  SELECT 'o', \r\n         NULL, \r\n         (\r\n           (\r\n             (\r\n               (oprname || '('::TEXT) \r\n                 || COALESCE(tl.typname, ''::TEXT)\r\n             ) || CASE WHEN (tl.oid IS NOT NULL) AND (tr.oid IS NOT NULL) \r\n               THEN ','::TEXT END\r\n           ) || COALESCE(tr.typname, ''::TEXT)\r\n          ) || ')'::TEXT, \r\n          NULL \r\n  FROM pg_operator AS op \r\n  LEFT JOIN pg_type AS tl \r\n    ON tl.oid = op.oprleft \r\n  LEFT JOIN pg_type AS tr \r\n    ON tr.oid = op.oprright \r\n  WHERE (op.oid IN (\r\n    SELECT objid \r\n    FROM pg_shdepend \r\n    WHERE refobjid = 2901009604::OID\r\n  )) \r\n  AND (op.oid > none::OID) \r\n  ORDER BY 1, 2, 3\r\n"
26386,sql: add current_setting()`current_setting()` retrieves the current value of a session variable.\r\nhttps://www.postgresql.org/docs/10/static/functions-admin.html\r\n\r\n\r\nPGAdmin issues the follow query:\r\n\r\n\r\n\r\nDetails here:  https://www.postgresql.org/docs/current/static/functions-admin.html,C-enhancement|A-sql-pgcompat,BramGruneir,"`current_setting()` retrieves the current value of a session variable.\r\nhttps://www.postgresql.org/docs/10/static/functions-admin.html\r\n\r\n\r\nPGAdmin issues the follow query:\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```\r\n\r\nDetails here:  https://www.postgresql.org/docs/current/static/functions-admin.html","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26384,"sql: add pg_is_xlog_replay_paused()PGAdmin issues the following query:\r\n\r\n\r\n\r\nSee https://www.postgresql.org/docs/current/static/functions-admin.html\r\n\r\nDue to the fact that we don't have a recovery mode, this should just always return false.",C-enhancement|A-sql-pgcompat,BramGruneir,"PGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT CASE WHEN usesuper\r\n         THEN pg_is_in_recovery()\r\n         ELSE false\r\n       END AS inrecovery,\r\n       CASE WHEN usesuper AND pg_is_in_recovery()\r\n         THEN pg_is_xlog_replay_paused()\r\n         ELSE false\r\n       END AS isreplaypaused\r\nFROM pg_user\r\nWHERE usename = current_user()\r\n```\r\n\r\nSee https://www.postgresql.org/docs/current/static/functions-admin.html\r\n\r\nDue to the fact that we don't have a recovery mode, this should just always return false.","sql\r\nSELECT CASE WHEN usesuper\r\n         THEN pg_is_in_recovery()\r\n         ELSE false\r\n       END AS inrecovery,\r\n       CASE WHEN usesuper AND pg_is_in_recovery()\r\n         THEN pg_is_xlog_replay_paused()\r\n         ELSE false\r\n       END AS isreplaypaused\r\nFROM pg_user\r\nWHERE usename = current_user()\r\n"
26380,"sql: add pg_encoding_to_char()`pg_encoding_to_char()` translates the value in the `pg_database.encoding` column into a string that represents the encoding. See https://www.postgresql.org/docs/10/static/catalog-pg-database.html\r\n\r\nSince CockroachDB uses UTF-8 everywhere this should be straightforward, but the function should assert that its argument is the value presented in `pg_database.encoding`.\r\n\r\nPGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/libpq-control.html",C-enhancement|A-sql-pgcompat,BramGruneir,"`pg_encoding_to_char()` translates the value in the `pg_database.encoding` column into a string that represents the encoding. See https://www.postgresql.org/docs/10/static/catalog-pg-database.html\r\n\r\nSince CockroachDB uses UTF-8 everywhere this should be straightforward, but the function should assert that its argument is the value presented in `pg_database.encoding`.\r\n\r\nPGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/libpq-control.html","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26379,sql: add pg_catalog.pg_shseclabelNote: this vtable is empty on pg by default.\r\n\r\nPGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/catalog-pg-shseclabel.html,A-sql-pgcompat,BramGruneir,"Note: this vtable is empty on pg by default.\r\n\r\nPGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/catalog-pg-shseclabel.html","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26378,sql: add pg_catalog.pg_default_aclNote: this vtable is empty on pg by default.\r\n\r\nPGAdmin issues the following query:\r\n\r\n\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/catalog-pg-default-acl.html,C-enhancement|A-sql-pgcompat,BramGruneir,"Note: this vtable is empty on pg by default.\r\n\r\nPGAdmin issues the following query:\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```\r\n\r\nDetails can be found here: https://www.postgresql.org/docs/current/static/catalog-pg-default-acl.html","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26376,sql: add pg_catalog.pg_shdescriptionFrom the PGAdmin query\r\n\r\n\r\n\r\n\r\nMore details here: https://www.postgresql.org/docs/current/static/catalog-pg-shdescription.html,C-enhancement|A-sql-pgcompat,BramGruneir,"From the PGAdmin query\r\n\r\n```sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n```\r\n\r\n\r\nMore details here: https://www.postgresql.org/docs/current/static/catalog-pg-shdescription.html","sql\r\nSELECT db.oid AS did,\r\n       db.datname AS name,\r\n       db.dattablespace AS spcoid,\r\n       spcname,\r\n       datallowconn,\r\n       pg_encoding_to_char(encoding) AS encoding,\r\n       pg_get_userbyid(datdba) AS datowner,\r\n       datcollate,\r\n       datctype,\r\n       datconnlimit,\r\n       has_database_privilege(db.oid, 'CREATE') AS cancreate,\r\n       current_setting('default_tablespace') AS default_tablespace,\r\n       descr.description AS comments,\r\n       db.datistemplate AS is_template,\r\n       (\r\n         SELECT array_to_string(ARRAY (\r\n          SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n          FROM pg_default_acl\r\n          WHERE (defaclobjtype = 'r')\r\n            AND (defaclnamespace = 0::OID)\r\n         ), ', ')\r\n        ) AS tblacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'S')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS seqacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'f')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS funcacl,\r\n        (\r\n          SELECT array_to_string(ARRAY (\r\n            SELECT array_to_string(defaclacl::TEXT[], ', ')\r\n            FROM pg_default_acl\r\n            WHERE (defaclobjtype = 'T')\r\n              AND (defaclnamespace = 0::OID)\r\n          ), ', ')\r\n        ) AS typeacl,\r\n        (\r\n          SELECT array_agg((provider || '=') || label)\r\n          FROM pg_shseclabel AS sl1\r\n          WHERE sl1.objoid = db.oid\r\n        ) AS seclabels,\r\n        array_to_string(datacl::TEXT[], ', ') AS acl\r\nFROM pg_database AS db\r\nLEFT JOIN pg_tablespace AS ta\r\n  ON db.dattablespace = ta.oid\r\nLEFT JOIN pg_shdescription AS descr\r\n  ON ((db.oid = descr.objoid) AND (descr.classoid = 'pg_database'::REGCLASS))\r\nWHERE db.oid > NULL::OID\r\nORDER BY datname\r\n"
26374,"sql: add pg_is_in_recovery()It is needed for this PGAdmin's query:\r\n\r\n\r\n\r\nDetails on it can be found here: https://www.postgresql.org/docs/current/static/functions-admin.html\r\nBut since we are never in recovery, this should just always return false.",C-enhancement|A-sql-pgcompat,BramGruneir,"It is needed for this PGAdmin's query:\r\n\r\n```sql\r\nSELECT CASE WHEN usesuper\r\n         THEN pg_is_in_recovery()\r\n         ELSE false\r\n       END AS inrecovery,\r\n       CASE WHEN usesuper AND pg_is_in_recovery()\r\n         THEN pg_is_xlog_replay_paused()\r\n         ELSE false\r\n       END AS isreplaypaused\r\nFROM pg_user\r\nWHERE usename = current_user()\r\n```\r\n\r\nDetails on it can be found here: https://www.postgresql.org/docs/current/static/functions-admin.html\r\nBut since we are never in recovery, this should just always return false.","sql\r\nSELECT CASE WHEN usesuper\r\n         THEN pg_is_in_recovery()\r\n         ELSE false\r\n       END AS inrecovery,\r\n       CASE WHEN usesuper AND pg_is_in_recovery()\r\n         THEN pg_is_xlog_replay_paused()\r\n         ELSE false\r\n       END AS isreplaypaused\r\nFROM pg_user\r\nWHERE usename = current_user()\r\n"
26367,"sql: the UPSERT/INSERT ON CONFLICT index lookup allocates excessivelyThis code in `sql/upsert.go`\r\n\r\n\r\n\r\nforces a heap copy of the item from `tableDesc.Indexes`\r\n\r\nInstead, a direct reference to that item could be returned.\r\n\r\n",C-cleanup|A-sql-mutations,emsal0,"This code in `sql/upsert.go`\r\n\r\n```go\r\n       for _, index := range tableDesc.Indexes {\r\n               if indexMatch(index) {\r\n                       return onConflict.Exprs, &index, nil\r\n```\r\n\r\nforces a heap copy of the item from `tableDesc.Indexes`\r\n\r\nInstead, a direct reference to that item could be returned.\r\n\r\n","go\r\n       for _, index := range tableDesc.Indexes {\r\n               if indexMatch(index) {\r\n                       return onConflict.Exprs, &index, nil\r\n"
25838,"sql: Joining Interleaved table with parent table over UUID typed column fails.Is this a question, feature request, or bug report?\r\n> BUG\r\n\r\n**BUG REPORT**\r\n\r\n1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. \r\n\r\n```\r\n80523 10:34:14.588013 1 util/log/clog.go:1134  [config] file created at: 2018/05/23 10:34:14\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] running on machine: derkan\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] binary: CockroachDB CCL v2.0.2 (x86_64-unknown-linux-gnu, built 2018/05/21 14:55:20, go1.10)\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] arguments: [/usr/local/bin/cockroach start --insecure --host=localhost --store=./db/data,attrs=ssd,size=2GiB --log-dir=./db/log --http-port=8088]\r\nI180523 10:34:19.348514 15 cli/start.go:595  [config] clusterID: fdae8641-2305-40e0-b2e5-f8501d4477d5\r\n```\r\n2. Please describe the issue you observed:\r\n\r\nThis code returns no rows although rows are existing(please look at result of last sql statement):\r\n\r\n\r\nBut if I change primary key columns' types to `SERIAL`, it works:\r\n\r\n",C-investigation|O-community|A-sql-execution,jordanlewis,"Is this a question, feature request, or bug report?\r\n> BUG\r\n\r\n**BUG REPORT**\r\n\r\n1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. \r\n\r\n```\r\n80523 10:34:14.588013 1 util/log/clog.go:1134  [config] file created at: 2018/05/23 10:34:14\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] running on machine: derkan\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] binary: CockroachDB CCL v2.0.2 (x86_64-unknown-linux-gnu, built 2018/05/21 14:55:20, go1.10)\r\nI180523 10:34:14.588013 1 util/log/clog.go:1134  [config] arguments: [/usr/local/bin/cockroach start --insecure --host=localhost --store=./db/data,attrs=ssd,size=2GiB --log-dir=./db/log --http-port=8088]\r\nI180523 10:34:19.348514 15 cli/start.go:595  [config] clusterID: fdae8641-2305-40e0-b2e5-f8501d4477d5\r\n```\r\n2. Please describe the issue you observed:\r\n\r\nThis code returns no rows although rows are existing(please look at result of last sql statement):\r\n```sql\r\nCREATE TABLE accounts (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\t""name"" STRING(128) NOT NULL,\r\n\tCONSTRAINT accounts_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX accounts_name_key (""name"" ASC)\r\n);\r\n\r\nCREATE TABLE users (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\t""name"" STRING(128) NOT NULL,\r\n\taccount_id UUID NOT NULL,\r\n\tusername STRING(128) NOT NULL,\r\n\tCONSTRAINT users_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX users_name_key (""name"" ASC),\r\n\tUNIQUE INDEX users_username_key (username ASC),\r\n\tCONSTRAINT users_account_id_fkey FOREIGN KEY (account_id) REFERENCES accounts (id),\r\n\tINDEX users_auto_index_users_account_id_fkey (account_id ASC)\r\n);\r\n\r\nCREATE TABLE documents (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\tuser_id UUID NOT NULL,\r\n\t""name"" STRING(256) NOT NULL,\r\n\tCONSTRAINT documents_pkey PRIMARY KEY (id ASC),\r\n\tCONSTRAINT documents_user_id_fkey FOREIGN KEY (user_id) REFERENCES users (id),\r\n\tUNIQUE INDEX documents_user_id_name_key (user_id ASC, ""name"" ASC)\r\n) INTERLEAVE IN PARENT users (id);\r\n\r\nINSERT INTO accounts(name) VALUES('Test Account');\r\nINSERT INTO users(name,username,account_id) SELECT 'Test User','test',id FROM accounts;\r\nINSERT INTO documents(name,user_id) SELECT 'Document',id FROM users;\r\n\r\nSELECT * FROM documents;\r\n+--------------------------------------+--------------------------------------+----------+\r\n|                  id                  |               user_id                |   name   |\r\n+--------------------------------------+--------------------------------------+----------+\r\n| 0c1de31d-2af7-4051-9ecc-e941ec0a33f4 | f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Document |\r\n+--------------------------------------+--------------------------------------+----------+\r\n(1 row)\r\n\r\nselect id,name from users where id='f0a20d89-19cc-432d-9c8e-a69dd796f7cd';\r\n+--------------------------------------+-----------+\r\n|                  id                  |   name    |\r\n+--------------------------------------+-----------+\r\n| f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Test User |\r\n+--------------------------------------+-----------+\r\n(1 row)\r\n\r\nSELECT documents.name,users.account_id FROM documents JOIN users  ON documents.user_id = users.id;\r\n+----------+--------------------------------------+\r\n|   name   |              account_id              |\r\n+----------+--------------------------------------+\r\n| Document | 726a2e47-b225-40b4-a67b-e8859e6ae771 |\r\n+----------+--------------------------------------+\r\n(1 row)\r\n\r\nselect * from documents where id='0c1de31d-2af7-4051-9ecc-e941ec0a33f4';\r\n+--------------------------------------+--------------------------------------+----------+\r\n|                  id                  |               user_id                |   name   |\r\n+--------------------------------------+--------------------------------------+----------+\r\n| 0c1de31d-2af7-4051-9ecc-e941ec0a33f4 | f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Document |\r\n+--------------------------------------+--------------------------------------+----------+\r\n(1 row)\r\n\r\n\r\nSELECT documents.name,users.account_id FROM documents JOIN users ON documents.user_id = users.id \r\nWHERE documents.id = '0c1de31d-2af7-4051-9ecc-e941ec0a33f4';\r\n+------+------------+\r\n| name | account_id |\r\n+------+------------+\r\n+------+------------+\r\n(0 rows)\r\n```\r\n\r\nBut if I change primary key columns' types to `SERIAL`, it works:\r\n\r\n```sql\r\nCREATE TABLE accounts_b (\r\n\tid SERIAL NOT NULL,\r\n\t""name"" STRING(128) NOT NULL,\r\n\tCONSTRAINT accounts_b_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX accounts_b_name_key (""name"" ASC)\r\n);\r\n\r\nCREATE TABLE users_b (\r\n\tid SERIAL NOT NULL,\r\n\t""name"" STRING(128) NOT NULL,\r\n\taccount_id INT NOT NULL,\r\n\tusername STRING(128) NOT NULL,\r\n\tCONSTRAINT users_b_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX users_b_name_key (""name"" ASC),\r\n\tUNIQUE INDEX users_b_username_key (username ASC),\r\n\tCONSTRAINT users_b_account_id_fkey FOREIGN KEY (account_id) REFERENCES accounts_b (id),\r\n\tINDEX users_b_auto_index_users_b_account_id_fkey (account_id ASC)\r\n);\r\n\r\nCREATE TABLE documents_b (\r\n\tid SERIAL NOT NULL,\r\n\tuser_id INT NOT NULL,\r\n\t""name"" STRING(256) NOT NULL,\r\n\tCONSTRAINT documents_b_pkey PRIMARY KEY (id ASC),\r\n\tCONSTRAINT documents_b_user_id_fkey FOREIGN KEY (user_id) REFERENCES users_b (id),\r\n\tUNIQUE INDEX documents_b_user_id_name_key (user_id ASC, ""name"" ASC)\r\n) INTERLEAVE IN PARENT users_b (id);\r\n\r\nINSERT INTO accounts_b(name) VALUES('Test Account');\r\nINSERT INTO users_b(name,username,account_id) SELECT 'Test User','test',id FROM accounts_b;\r\nINSERT INTO documents_b(name,user_id) SELECT 'Document',id FROM users_b;\r\n\r\nSELECT * FROM documents_b;\r\n+--------------------+--------------------+----------+\r\n|         id         |      user_id       |   name   |\r\n+--------------------+--------------------+----------+\r\n| 350626680253906945 | 350626680226086913 | Document |\r\n+--------------------+--------------------+----------+\r\n(1 row)\r\n\r\n\r\nselect id,name from users_b where id=350626680226086913;\r\n+--------------------+-----------+\r\n|         id         |   name    |\r\n+--------------------+-----------+\r\n| 350626680226086913 | Test User |\r\n+--------------------+-----------+\r\n(1 row)\r\n\r\nSELECT documents_b.name,users_b.account_id FROM documents_b JOIN users_b  ON documents_b.user_id = users_b.id;\r\n+----------+--------------------+\r\n|   name   |     account_id     |\r\n+----------+--------------------+\r\n| Document | 350626680201871361 |\r\n+----------+--------------------+\r\n(1 row)\r\n\r\n\r\n\r\nSELECT documents_b.name,users_b.account_id FROM documents_b JOIN users_b ON documents_b.user_id = users_b.id \r\nWHERE documents_b.id = 350626680253906945;\r\n+----------+--------------------+\r\n|   name   |     account_id     |\r\n+----------+--------------------+\r\n| Document | 350626680201871361 |\r\n+----------+--------------------+\r\n(1 row)\r\n```","sql\r\nCREATE TABLE accounts (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\t""name"" STRING(128) NOT NULL,\r\n\tCONSTRAINT accounts_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX accounts_name_key (""name"" ASC)\r\n);\r\n\r\nCREATE TABLE users (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\t""name"" STRING(128) NOT NULL,\r\n\taccount_id UUID NOT NULL,\r\n\tusername STRING(128) NOT NULL,\r\n\tCONSTRAINT users_pkey PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX users_name_key (""name"" ASC),\r\n\tUNIQUE INDEX users_username_key (username ASC),\r\n\tCONSTRAINT users_account_id_fkey FOREIGN KEY (account_id) REFERENCES accounts (id),\r\n\tINDEX users_auto_index_users_account_id_fkey (account_id ASC)\r\n);\r\n\r\nCREATE TABLE documents (\r\n\tid UUID NOT NULL DEFAULT gen_random_uuid(),\r\n\tuser_id UUID NOT NULL,\r\n\t""name"" STRING(256) NOT NULL,\r\n\tCONSTRAINT documents_pkey PRIMARY KEY (id ASC),\r\n\tCONSTRAINT documents_user_id_fkey FOREIGN KEY (user_id) REFERENCES users (id),\r\n\tUNIQUE INDEX documents_user_id_name_key (user_id ASC, ""name"" ASC)\r\n) INTERLEAVE IN PARENT users (id);\r\n\r\nINSERT INTO accounts(name) VALUES('Test Account');\r\nINSERT INTO users(name,username,account_id) SELECT 'Test User','test',id FROM accounts;\r\nINSERT INTO documents(name,user_id) SELECT 'Document',id FROM users;\r\n\r\nSELECT * FROM documents;\r\n+--------------------------------------+--------------------------------------+----------+\r\n|                  id                  |               user_id                |   name   |\r\n+--------------------------------------+--------------------------------------+----------+\r\n| 0c1de31d-2af7-4051-9ecc-e941ec0a33f4 | f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Document |\r\n+--------------------------------------+--------------------------------------+----------+\r\n(1 row)\r\n\r\nselect id,name from users where id='f0a20d89-19cc-432d-9c8e-a69dd796f7cd';\r\n+--------------------------------------+-----------+\r\n|                  id                  |   name    |\r\n+--------------------------------------+-----------+\r\n| f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Test User |\r\n+--------------------------------------+-----------+\r\n(1 row)\r\n\r\nSELECT documents.name,users.account_id FROM documents JOIN users  ON documents.user_id = users.id;\r\n+----------+--------------------------------------+\r\n|   name   |              account_id              |\r\n+----------+--------------------------------------+\r\n| Document | 726a2e47-b225-40b4-a67b-e8859e6ae771 |\r\n+----------+--------------------------------------+\r\n(1 row)\r\n\r\nselect * from documents where id='0c1de31d-2af7-4051-9ecc-e941ec0a33f4';\r\n+--------------------------------------+--------------------------------------+----------+\r\n|                  id                  |               user_id                |   name   |\r\n+--------------------------------------+--------------------------------------+----------+\r\n| 0c1de31d-2af7-4051-9ecc-e941ec0a33f4 | f0a20d89-19cc-432d-9c8e-a69dd796f7cd | Document |\r\n+--------------------------------------+--------------------------------------+----------+\r\n(1 row)\r\n\r\n\r\nSELECT documents.name,users.account_id FROM documents JOIN users ON documents.user_id = users.id \r\nWHERE documents.id = '0c1de31d-2af7-4051-9ecc-e941ec0a33f4';\r\n+------+------------+\r\n| name | account_id |\r\n+------+------------+\r\n+------+------------+\r\n(0 rows)\r\n"
25726,"sql: correctness bug with UPSERT and FAMILYOn master:\r\n\r\nreturns\r\n```\r\n+---+------+---+\r\n| a |  b   | c |\r\n+---+------+---+\r\n| 1 | NULL | 3 |\r\n+---+------+---+\r\n```\r\nbut should return `(1, NULL, NULL)`. This reproduces without the `b` column, but I thought the example was more obvious with it.\r\n\r\nLooks like the all-NULL column family isn't actually getting a tombstone written. The equivalent UPDATE works fine.  @knz you've touched UPSERT last, mind looking into this?",C-bug|S-0-visible-logical-error|A-sql-mutations,knz,"On master:\r\n```sql\r\nCREATE TABLE t (a INT PRIMARY KEY, b INT, c INT, FAMILY (a, b), FAMILY (c));\r\nINSERT INTO t VALUES (1, 2, 3);\r\nUPSERT INTO t VALUES (1, NULL, NULL);\r\nSELECT * FROM t;\r\n```\r\nreturns\r\n```\r\n+---+------+---+\r\n| a |  b   | c |\r\n+---+------+---+\r\n| 1 | NULL | 3 |\r\n+---+------+---+\r\n```\r\nbut should return `(1, NULL, NULL)`. This reproduces without the `b` column, but I thought the example was more obvious with it.\r\n\r\nLooks like the all-NULL column family isn't actually getting a tombstone written. The equivalent UPDATE works fine.  @knz you've touched UPSERT last, mind looking into this?","sql\r\nCREATE TABLE t (a INT PRIMARY KEY, b INT, c INT, FAMILY (a, b), FAMILY (c));\r\nINSERT INTO t VALUES (1, 2, 3);\r\nUPSERT INTO t VALUES (1, NULL, NULL);\r\nSELECT * FROM t;\r\n"
25665,sql: support json - textThis is related but not equivalent to #23293.\r\n\r\nReported in https://github.com/cockroachdb/cockroach/issues/18442#issuecomment-390100705:\r\n\r\n\r\n\r\n,C-enhancement|E-easy|A-sql-pgcompat|good first issue|A-sql-json,yuzefovich,"This is related but not equivalent to #23293.\r\n\r\nReported in https://github.com/cockroachdb/cockroach/issues/18442#issuecomment-390100705:\r\n\r\n```sql\r\ncreate table catalog(sku text, properties jsonb);\r\ninsert into catalog values\r\n('a', '[\r\n    ""one"",\r\n    ""two"",\r\n    ""three""\r\n]')\r\nreturning *;  \r\n\r\nupdate catalog\r\nset properties = properties - 'two'   --  <== THIS PART DOESN'T WORK\r\nreturning *;\r\n```\r\n\r\n","sql\r\ncreate table catalog(sku text, properties jsonb);\r\ninsert into catalog values\r\n('a', '[\r\n    ""one"",\r\n    ""two"",\r\n    ""three""\r\n]')\r\nreturning *;  \r\n\r\nupdate catalog\r\nset properties = properties - 'two'   --  <== THIS PART DOESN'T WORK\r\nreturning *;\r\n"
25658,"sql: implement ""project set"" in distsqlThis issue describes the proposed semantics of a ProjectSet operator, along with a discussion of how it would be used on some sample queries.\r\n\r\n### ProjectSet\r\n\r\nThe ProjectSet operator has:\r\n - an input data source\r\n - a list of SRFs\r\n - a list of projected columns\r\n\r\nFor each row received from the source, all SRFs are applied. Each SRF\r\napplication results in a tuple, which we can think of as a bunch of rows with\r\none column. For each input row, ProjectSet emits as many rows as the largest SRF\r\napplication result (others are padded with NULLs as necessary). Each row has one\r\ncolumn per SRF and one column per projected column; the rows emitted for a\r\nsingle input row have the same values on the projected columns.\r\n\r\nThese semantics mirror the PostgreSQL 10 semantics when multiple SRFs are used\r\nin a SELECT (e.g. `SELECT unnest(x), unnest(y), ...`). Note that earlier\r\nversions had much crazier semantics for these cases; see the note at the end of\r\nthis section:\r\nhttps://www.postgresql.org/docs/devel/static/xfunc-sql.html#XFUNC-SQL-FUNCTIONS-RETURNING-SET\r\n\r\nExample (with pg 10.4):\r\n```\r\nSELECT a, unnest(b), unnest(c) FROM (VALUES (1, ARRAY[1,2,3], ARRAY[4,5])) AS t(a,b,c);\r\n a | unnest | unnest \r\n---+--------+--------\r\n 1 |      1 |      4\r\n 1 |      2 |      5\r\n 1 |      3 |       \r\n```\r\n\r\nThis operator seems relatively straightforward to implement as either a\r\n`planNode` or a distsql processor (preferrably both).\r\n\r\n### Basic use\r\n\r\nAn example of the simplest case for an SRF is part of a Hibernate query:\r\n\r\n```\r\nSELECT information_schema._pg_expandarray(indkey) AS keys FROM pg_index\r\n```\r\n\r\nThis would be a straight-forward application of ApplySRF (with the input being\r\nthe table scan).\r\n\r\nA slightly more complicated example is:\r\n\r\n```\r\nSELECT\r\n  indexrelid,\r\n  (information_schema._pg_expandarray(indclass)).x AS operator_argument_type_oid,\r\n  (information_schema._pg_expandarray(indclass)).n AS operator_argument_position\r\nFROM\r\n  pg_index\r\n```\r\n\r\nHere we are just selecting two record fields from the same SRF, but in general\r\nthese functions need not be the same. Even if we treat them as separate SRFs,\r\nthe results are as expected because of the semantics of ProjectSet (so detecting\r\nthat we only need to apply the SRF once would be just an optimization).\r\n\r\n### Correlated subqueries\r\n\r\nConsider this sample query (posted on the forum):\r\n\r\n\r\n\r\nThe query looks complicated but it's just trying to emit a `(group_name, member_name)`\r\nrow for every member in a group.\r\n\r\nThis cannot be easily decorrelated today; we can get very close with a join, but\r\nthe query behaves differently if there are multiple rows with the same\r\n`data->>'name'`) - the original query errors out.\r\n\r\n\r\n\r\nThis transformation would be valid if the equality column was a unique\r\nkey, but this does not apply to a JSON field (at least not today).\r\n\r\nThis problem could be solved with a special `MAX_1_ROW` aggregation function\r\nwhich causes the query to error out if any group receives more than one non-NULL\r\nvalue. The rewritten query would be something like:\r\n\r\n\r\n\r\nThis is now similar to the basic queries: ProjectSet would be used on the result\r\nof the join.",C-enhancement|A-sql-pgcompat|A-sql-optimizer|A-sql-execution,solongordon,"This issue describes the proposed semantics of a ProjectSet operator, along with a discussion of how it would be used on some sample queries.\r\n\r\n### ProjectSet\r\n\r\nThe ProjectSet operator has:\r\n - an input data source\r\n - a list of SRFs\r\n - a list of projected columns\r\n\r\nFor each row received from the source, all SRFs are applied. Each SRF\r\napplication results in a tuple, which we can think of as a bunch of rows with\r\none column. For each input row, ProjectSet emits as many rows as the largest SRF\r\napplication result (others are padded with NULLs as necessary). Each row has one\r\ncolumn per SRF and one column per projected column; the rows emitted for a\r\nsingle input row have the same values on the projected columns.\r\n\r\nThese semantics mirror the PostgreSQL 10 semantics when multiple SRFs are used\r\nin a SELECT (e.g. `SELECT unnest(x), unnest(y), ...`). Note that earlier\r\nversions had much crazier semantics for these cases; see the note at the end of\r\nthis section:\r\nhttps://www.postgresql.org/docs/devel/static/xfunc-sql.html#XFUNC-SQL-FUNCTIONS-RETURNING-SET\r\n\r\nExample (with pg 10.4):\r\n```\r\nSELECT a, unnest(b), unnest(c) FROM (VALUES (1, ARRAY[1,2,3], ARRAY[4,5])) AS t(a,b,c);\r\n a | unnest | unnest \r\n---+--------+--------\r\n 1 |      1 |      4\r\n 1 |      2 |      5\r\n 1 |      3 |       \r\n```\r\n\r\nThis operator seems relatively straightforward to implement as either a\r\n`planNode` or a distsql processor (preferrably both).\r\n\r\n### Basic use\r\n\r\nAn example of the simplest case for an SRF is part of a Hibernate query:\r\n\r\n```\r\nSELECT information_schema._pg_expandarray(indkey) AS keys FROM pg_index\r\n```\r\n\r\nThis would be a straight-forward application of ApplySRF (with the input being\r\nthe table scan).\r\n\r\nA slightly more complicated example is:\r\n\r\n```\r\nSELECT\r\n  indexrelid,\r\n  (information_schema._pg_expandarray(indclass)).x AS operator_argument_type_oid,\r\n  (information_schema._pg_expandarray(indclass)).n AS operator_argument_position\r\nFROM\r\n  pg_index\r\n```\r\n\r\nHere we are just selecting two record fields from the same SRF, but in general\r\nthese functions need not be the same. Even if we treat them as separate SRFs,\r\nthe results are as expected because of the semantics of ProjectSet (so detecting\r\nthat we only need to apply the SRF once would be just an optimization).\r\n\r\n### Correlated subqueries\r\n\r\nConsider this sample query (posted on the forum):\r\n\r\n```sql\r\nSELECT\r\n  g.data->>'name' AS group_name,\r\n  jsonb_array_elements( (SELECT gg.data->'members' FROM groups AS gg WHERE gg.data->>'name' = g.data->>'name') )\r\nFROM\r\n  groups AS g\r\n```\r\n\r\nThe query looks complicated but it's just trying to emit a `(group_name, member_name)`\r\nrow for every member in a group.\r\n\r\nThis cannot be easily decorrelated today; we can get very close with a join, but\r\nthe query behaves differently if there are multiple rows with the same\r\n`data->>'name'`) - the original query errors out.\r\n\r\n```sql\r\nSELECT\r\n  g.data->>'name',\r\n  json_array_elements((gg.data->>'members')::json)\r\nFROM\r\n  groups AS g JOIN groups AS gg ON gg.data->>'name' = g.data->>'name'\r\n```\r\n\r\nThis transformation would be valid if the equality column was a unique\r\nkey, but this does not apply to a JSON field (at least not today).\r\n\r\nThis problem could be solved with a special `MAX_1_ROW` aggregation function\r\nwhich causes the query to error out if any group receives more than one non-NULL\r\nvalue. The rewritten query would be something like:\r\n\r\n```sql\r\nSELECT\r\n  g.data->>'name',\r\n  json_array_elements(max_1_row(gg.data->>'members')::json)\r\nFROM\r\n  groups AS g JOIN groups AS gg ON gg.data->>'name' = g.data->>'name'\r\nGROUP BY g.data->'name'\r\n```\r\n\r\nThis is now similar to the basic queries: ProjectSet would be used on the result\r\nof the join.","sql\r\nSELECT\r\n  g.data->>'name' AS group_name,\r\n  jsonb_array_elements( (SELECT gg.data->'members' FROM groups AS gg WHERE gg.data->>'name' = g.data->>'name') )\r\nFROM\r\n  groups AS g\r\n"
25607,"sql: memory accounting leak (strongly suspected)Running v2.0.1.\r\n\r\nThe symptoms: \r\n\r\n- a given query running multiple times over constant data starts running into ""memory budget exceeded"" although it succeeded before. \r\n- Restarting the cluster alleviates the problem temporarily.\r\n\r\nWorking hypothesis, a bug: some memory gets billed against the root SQL monitor and not properly released (by closing accounts). \r\n\r\nTwo places where this can happen: \r\n\r\n- at the SQL gateway, with session monitors, since we have recently demoted accounting errors from fatal errors to mere warnings.\r\n- in distsql processors, where some memory account does not get closed properly.\r\n\r\nUser @rjinski on Gitter was helpful to present exactly the symptoms described above:\r\n\r\n- SQL max mem configured to 2GB\r\n- a single query issued performing a clean inner join over 2 tables with 31K rows each (clean = good match between keys, so no cross product amplification)\r\n- query runs into aforementioned error after succeeding for a while, note the values: `memory budget exceeded: 327680 bytes requested, 2016333949 currently allocated, 2016399360 bytes in budget` (2GB in budget!)\r\n- **at the time the error occurs, less than 1GB is allocated by Go as per the web UI graph.** ![https://files.gitter.im/cockroachdb/cockroach/4Xma/image.png](https://files.gitter.im/cockroachdb/cockroach/4Xma/image.png)\r\n- cluster restart alleviates.\r\n\r\nThe fact that the SQL memory error is encountered for an account of 2GB while the overall Go memory is less than 1GB alone is for me strong indication of a memory accounting error.\r\n\r\n**However** at the same time the `sql_mem` metrics in the `/_status/vars` endpoint remain at zero, which in turn is a counter-signal against a memory accounting problem. This counter-signal would disappear if we found out somehow that the metrics are not collected properly.\r\n\r\nThe user reports that **the problem tends to disappear if recreating the cluster from scratch** which in turn suggests the error is dependent on data layout, which in turn somewhat orients the investigation towards a failure in distsql memory accounting.\r\n\r\nThe schema of that use case:\r\n\r\n\r\n\r\nThe query that encounters the error:\r\n\r\n\r\n\r\nThe specific distsql physical plan in the user environment: [See here](https://cockroachdb.github.io/distsqlplan/decode.html?eJzkk0-LnEAQxe_5FEuddqECljr5IwTqmM1hNyy5BQm9dmVscG3T3YYMg989qIGJRs0MyW2PdvevXr3H8wi11XKnnsRD9hkIEBLIERpnC_Heuv54fHSrf0AWIZi6aUN_nCMU1glkRwgmVAIZfFKPlTyI0uIAQUtQphoGN848KXfgb60N8sXJd-ONrT0g3Lchu2JCjpET5BR5h_wK8g7BtuGX1Enh8XBVKl9OpzNB3uUIPqi9QBZ3uLLyaU5bW6fFiZ5Mynvyb08WfL9XvvxgTT23XcnXcM3xzTtn9mW4ZrpZdYz8GvkN8ltkipCJkClGpgSZUmTaIdM8lpPl5B8sL_i5sy9tM09mUTidCNP_qMd6KyYZXViReFIRWl37GVZkYdsH8Y2tvZzVgaj3K3ovY37etq6Qj84Wg8z4eT9wQ9RafBhv0_Hjth6uht_2fJi2YZrD0e9wPIFpDseb8G4CR3M4ucDzH8rbMG3D6UWe8-7FzwAAAP__OSnsOw==)\r\n\r\n\r\n\r\n\r\n",C-bug|C-investigation|O-community|S-2-temp-unavailability|A-sql-execution|A-sql-memmon,asubiotto,"Running v2.0.1.\r\n\r\nThe symptoms: \r\n\r\n- a given query running multiple times over constant data starts running into ""memory budget exceeded"" although it succeeded before. \r\n- Restarting the cluster alleviates the problem temporarily.\r\n\r\nWorking hypothesis, a bug: some memory gets billed against the root SQL monitor and not properly released (by closing accounts). \r\n\r\nTwo places where this can happen: \r\n\r\n- at the SQL gateway, with session monitors, since we have recently demoted accounting errors from fatal errors to mere warnings.\r\n- in distsql processors, where some memory account does not get closed properly.\r\n\r\nUser @rjinski on Gitter was helpful to present exactly the symptoms described above:\r\n\r\n- SQL max mem configured to 2GB\r\n- a single query issued performing a clean inner join over 2 tables with 31K rows each (clean = good match between keys, so no cross product amplification)\r\n- query runs into aforementioned error after succeeding for a while, note the values: `memory budget exceeded: 327680 bytes requested, 2016333949 currently allocated, 2016399360 bytes in budget` (2GB in budget!)\r\n- **at the time the error occurs, less than 1GB is allocated by Go as per the web UI graph.** ![https://files.gitter.im/cockroachdb/cockroach/4Xma/image.png](https://files.gitter.im/cockroachdb/cockroach/4Xma/image.png)\r\n- cluster restart alleviates.\r\n\r\nThe fact that the SQL memory error is encountered for an account of 2GB while the overall Go memory is less than 1GB alone is for me strong indication of a memory accounting error.\r\n\r\n**However** at the same time the `sql_mem` metrics in the `/_status/vars` endpoint remain at zero, which in turn is a counter-signal against a memory accounting problem. This counter-signal would disappear if we found out somehow that the metrics are not collected properly.\r\n\r\nThe user reports that **the problem tends to disappear if recreating the cluster from scratch** which in turn suggests the error is dependent on data layout, which in turn somewhat orients the investigation towards a failure in distsql memory accounting.\r\n\r\nThe schema of that use case:\r\n\r\n```sql\r\nCREATE TABLE quotes (\r\n    id UUID NOT NULL,\r\n    revision_id UUID NOT NULL,\r\n    rfq_id UUID NOT NULL,\r\n    market_id UUID NOT NULL,\r\n    client_id UUID NOT NULL,\r\n    provider_id UUID NOT NULL,\r\n    payload STRING NOT NULL,\r\n    lifespan INT NOT NULL,\r\n    created_on TIMESTAMP WITH TIME ZONE NULL DEFAULT current_timestamp(),\r\n    on_behalf_of STRING NULL,\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    UNIQUE INDEX quotes_revision_id_key (revision_id ASC),\r\n    FAMILY ""primary"" (id, revision_id, rfq_id, market_id, client_id, provider_id, payload, lifespan, created_on, on_behalf_of)\r\n);\r\nCREATE TABLE quote_revisions (\r\n    id UUID NOT NULL,\r\n    quote_id UUID NOT NULL,\r\n    status STRING NOT NULL,\r\n    acceptance STRING NOT NULL,\r\n    completion STRING NOT NULL,\r\n    updated_on TIMESTAMP WITH TIME ZONE NULL DEFAULT current_timestamp(),\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    FAMILY ""primary"" (id, quote_id, status, acceptance, completion, updated_on)\r\n);\r\n```\r\n\r\nThe query that encounters the error:\r\n\r\n```sql\r\nselect * from quotes \r\ninner join quote_revisions ON quotes.revision_id = quote_revisions.id;\r\n```\r\n\r\nThe specific distsql physical plan in the user environment: [See here](https://cockroachdb.github.io/distsqlplan/decode.html?eJzkk0-LnEAQxe_5FEuddqECljr5IwTqmM1hNyy5BQm9dmVscG3T3YYMg989qIGJRs0MyW2PdvevXr3H8wi11XKnnsRD9hkIEBLIERpnC_Heuv54fHSrf0AWIZi6aUN_nCMU1glkRwgmVAIZfFKPlTyI0uIAQUtQphoGN848KXfgb60N8sXJd-ONrT0g3Lchu2JCjpET5BR5h_wK8g7BtuGX1Enh8XBVKl9OpzNB3uUIPqi9QBZ3uLLyaU5bW6fFiZ5Mynvyb08WfL9XvvxgTT23XcnXcM3xzTtn9mW4ZrpZdYz8GvkN8ltkipCJkClGpgSZUmTaIdM8lpPl5B8sL_i5sy9tM09mUTidCNP_qMd6KyYZXViReFIRWl37GVZkYdsH8Y2tvZzVgaj3K3ovY37etq6Qj84Wg8z4eT9wQ9RafBhv0_Hjth6uht_2fJi2YZrD0e9wPIFpDseb8G4CR3M4ucDzH8rbMG3D6UWe8-7FzwAAAP__OSnsOw==)\r\n\r\n\r\n\r\n\r\n","sql\r\nCREATE TABLE quotes (\r\n    id UUID NOT NULL,\r\n    revision_id UUID NOT NULL,\r\n    rfq_id UUID NOT NULL,\r\n    market_id UUID NOT NULL,\r\n    client_id UUID NOT NULL,\r\n    provider_id UUID NOT NULL,\r\n    payload STRING NOT NULL,\r\n    lifespan INT NOT NULL,\r\n    created_on TIMESTAMP WITH TIME ZONE NULL DEFAULT current_timestamp(),\r\n    on_behalf_of STRING NULL,\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    UNIQUE INDEX quotes_revision_id_key (revision_id ASC),\r\n    FAMILY ""primary"" (id, revision_id, rfq_id, market_id, client_id, provider_id, payload, lifespan, created_on, on_behalf_of)\r\n);\r\nCREATE TABLE quote_revisions (\r\n    id UUID NOT NULL,\r\n    quote_id UUID NOT NULL,\r\n    status STRING NOT NULL,\r\n    acceptance STRING NOT NULL,\r\n    completion STRING NOT NULL,\r\n    updated_on TIMESTAMP WITH TIME ZONE NULL DEFAULT current_timestamp(),\r\n    CONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n    FAMILY ""primary"" (id, quote_id, status, acceptance, completion, updated_on)\r\n);\r\n"
25098,"sql: INT4 column type not reported as INT4 when inspecting column types (SHOW CREATE, errors etc)According to https://www.cockroachlabs.com/docs/stable/int.html#names-and-aliases, INT4 is treated as a 32-bit type that can't simply be aliased to a full 64-bit `INTEGER`. However, the actual code does convert `INT4` columns to `INTEGER` on both v1.1.4 and v2.0.1:\r\n\r\n\r\n\r\nWhich is wrong -- the code or the docs?",C-bug|A-sql-semantics|S-3-ux-surprise|O-community,knz,"According to https://www.cockroachlabs.com/docs/stable/int.html#names-and-aliases, INT4 is treated as a 32-bit type that can't simply be aliased to a full 64-bit `INTEGER`. However, the actual code does convert `INT4` columns to `INTEGER` on both v1.1.4 and v2.0.1:\r\n\r\n```sql\r\nroot@:26257/> create database foo;                                                                                                                                                                          CREATE DATABASE\r\n\r\nTime: 5.524673ms\r\n\r\nroot@:26257/> create table foo.bar (x INT4, y INT, z INT2);\r\nCREATE TABLE\r\n\r\nTime: 5.551593ms\r\n\r\nroot@:26257/> show create table foo.bar;\r\n+---------+---------------------------------------+\r\n|  Table  |              CreateTable              |\r\n+---------+---------------------------------------+\r\n| foo.bar | CREATE TABLE bar (                    |\r\n|         |                                       |\r\n|         |     x INTEGER NULL,                   |\r\n|         |                                       |\r\n|         |     y INT NULL,                       |\r\n|         |                                       |\r\n|         |     z SMALLINT NULL,                  |\r\n|         |                                       |\r\n|         |     FAMILY ""primary"" (x, y, z, rowid) |\r\n|         |                                       |\r\n|         | )                                     |\r\n+---------+---------------------------------------+\r\n(1 row)\r\n\r\nTime: 7.215492ms\r\n```\r\n\r\nWhich is wrong -- the code or the docs?","sql\r\nroot@:26257/> create database foo;                                                                                                                                                                          CREATE DATABASE\r\n\r\nTime: 5.524673ms\r\n\r\nroot@:26257/> create table foo.bar (x INT4, y INT, z INT2);\r\nCREATE TABLE\r\n\r\nTime: 5.551593ms\r\n\r\nroot@:26257/> show create table foo.bar;\r\n+---------+---------------------------------------+\r\n|  Table  |              CreateTable              |\r\n+---------+---------------------------------------+\r\n| foo.bar | CREATE TABLE bar (                    |\r\n|         |                                       |\r\n|         |     x INTEGER NULL,                   |\r\n|         |                                       |\r\n|         |     y INT NULL,                       |\r\n|         |                                       |\r\n|         |     z SMALLINT NULL,                  |\r\n|         |                                       |\r\n|         |     FAMILY ""primary"" (x, y, z, rowid) |\r\n|         |                                       |\r\n|         | )                                     |\r\n+---------+---------------------------------------+\r\n(1 row)\r\n\r\nTime: 7.215492ms\r\n"
24774,"sql: panic while attempting to rename primary indexFound while investigating #24475.\r\n\r\n\r\n\r\ncrashes with\r\n\r\n```\r\npanic while executing 1 statements: ALTER INDEX _@_ RENAME TO _; caused by index with id = 1 does not exist\r\n\r\ngoroutine 453 [running]:\r\nruntime/debug.Stack(0x292b660, 0xc4207ad540, 0x3)\r\n        /usr/local/go/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x292b660, 0xc4207ad540, 0xc420784000, 0x24b2820, 0xc42721d050, 0x1)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:212 +0xa5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc425381000, 0x292b660, 0xc4207ad540, 0x224ba60, 0xc426791380)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:521 +0x2d5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func2(0xc425381000, 0x292b660, 0xc4207ad540)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:489 +0x61\r\npanic(0x224ba60, 0xc426791380)\r\n        /usr/local/go/src/runtime/panic.go:505 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*TableDescriptor).RenameIndexDescriptor(0xc426a75500, 0xc42673b960, 0x7, 0x100000001, 0xc4267912f0, 0x1, 0x1, 0xc42673b978, 0x1, 0x2, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/structured.go:1913 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).RenameIndex(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0xc4269d59a0, 0xc4269d59a0, 0x0, 0x0, 0x0)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/rename_index.go:80 +0x46f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).newPlan(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:698 +0x709\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makePlan(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:296 +0x100\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:551 +0xb1b\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:368 +0xa8c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:83 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc425381000, 0x292b660, 0xc4207ad540, 0xc4202f6b30, 0x0, 0x0)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:917 +0x1d7a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc4208e8a00, 0x292b660, 0xc4207ad540, 0x0, 0x0, 0xc425b01028, 0x4, 0xc425b01015, 0xd, 0x2913860, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:491 +0xfae\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc4208e8a00, 0x292b660, 0xc4207ad540, 0xc42522c1c0, 0x5400, 0x15000, 0xc42090c3d0, 0xc4202f6b30, 0xc4202f6b20, 0xc4202e9010, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:260 +0x10c\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:259 +0xf27\r\n```",C-bug,knz,"Found while investigating #24475.\r\n\r\n```sql\r\nALTER INDEX kv@""primary"" RENAME TO pk\r\n```\r\n\r\ncrashes with\r\n\r\n```\r\npanic while executing 1 statements: ALTER INDEX _@_ RENAME TO _; caused by index with id = 1 does not exist\r\n\r\ngoroutine 453 [running]:\r\nruntime/debug.Stack(0x292b660, 0xc4207ad540, 0x3)\r\n        /usr/local/go/src/runtime/debug/stack.go:24 +0xa7\r\ngithub.com/cockroachdb/cockroach/pkg/util/log.ReportPanic(0x292b660, 0xc4207ad540, 0xc420784000, 0x24b2820, 0xc42721d050, 0x1)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/util/log/crash_reporting.go:212 +0xa5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).closeWrapper(0xc425381000, 0x292b660, 0xc4207ad540, 0x224ba60, 0xc426791380)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:521 +0x2d5\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn.func2(0xc425381000, 0x292b660, 0xc4207ad540)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:489 +0x61\r\npanic(0x224ba60, 0xc426791380)\r\n        /usr/local/go/src/runtime/panic.go:505 +0x229\r\ngithub.com/cockroachdb/cockroach/pkg/sql/sqlbase.(*TableDescriptor).RenameIndexDescriptor(0xc426a75500, 0xc42673b960, 0x7, 0x100000001, 0xc4267912f0, 0x1, 0x1, 0xc42673b978, 0x1, 0x2, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/sqlbase/structured.go:1913 +0x24e\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).RenameIndex(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0xc4269d59a0, 0xc4269d59a0, 0x0, 0x0, 0x0)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/rename_index.go:80 +0x46f\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).newPlan(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:698 +0x709\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*planner).makePlan(0xc4253813f0, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/plan.go:296 +0x100\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).dispatchToExecutionEngine(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:551 +0xb1b\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmtInOpenState(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:368 +0xa8c\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).execStmt(0xc425381000, 0x292b720, 0xc4266f1e00, 0x292daa0, 0xc4269d59a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor_exec.go:83 +0x358\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*connExecutor).run(0xc425381000, 0x292b660, 0xc4207ad540, 0xc4202f6b30, 0x0, 0x0)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:917 +0x1d7a\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Server).ServeConn(0xc4208e8a00, 0x292b660, 0xc4207ad540, 0x0, 0x0, 0xc425b01028, 0x4, 0xc425b01015, 0xd, 0x2913860, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/conn_executor.go:491 +0xfae\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl.func3(0xc4208e8a00, 0x292b660, 0xc4207ad540, 0xc42522c1c0, 0x5400, 0x15000, 0xc42090c3d0, 0xc4202f6b30, 0xc4202f6b20, 0xc4202e9010, ...)\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:260 +0x10c\r\ncreated by github.com/cockroachdb/cockroach/pkg/sql/pgwire.(*conn).serveImpl\r\n        /data/home/kena/src/go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/conn.go:259 +0xf27\r\n```","sql\r\nALTER INDEX kv@""primary"" RENAME TO pk\r\n"
24759,"sql: support vtables information_schema.{routines,parameters}requested by @awoods187 , Navicat needs this\r\n\r\n\r\n\r\nThe vtables should be visible to clients (and contain no data initially) even though #17511 is not solved.",C-enhancement|E-easy|A-sql-pgcompat|good first issue,emsal0,"requested by @awoods187 , Navicat needs this\r\n\r\n```sql\r\nFROM information_schema.routines AS r \r\nLEFT JOIN information_schema.parameters AS p ON r.specific_name = p.specific_name GROUP BY p.specific_name, r.routine_schema, r.routine_name \r\nORDER BY r.routine_schema\r\n```\r\n\r\nThe vtables should be visible to clients (and contain no data initially) even though #17511 is not solved.","sql\r\nFROM information_schema.routines AS r \r\nLEFT JOIN information_schema.parameters AS p ON r.specific_name = p.specific_name GROUP BY p.specific_name, r.routine_schema, r.routine_name \r\nORDER BY r.routine_schema\r\n"
24681,"sql: crash in (*FuncExpr).Eval() in CockroachDB v1.1.xFound on Sentry\r\nhttps://sentry.io/cockroach-labs/cockroachdb/issues/392500093/\r\n\r\n```\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser/eval.go in Eval at line 2800\r\ngithub.com/cockroachdb/cockroach/pkg/sql/render.go in renderRow at line 586\r\ngithub.com/cockroachdb/cockroach/pkg/sql/render.go in Next at line 113\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in forEachRow at line 2045\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in execClassic at line 2029\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in execStmt at line 2176\r\n```\r\n\r\nContext in eval.go: `res, err := expr.fn.fn(ctx, args.D)`\r\n\r\n\r\n\r\nThis in turn suggests a function that's not been memoized properly. Probably an operator overload error.\r\n\r\n\r\n\r\n",C-bug|C-investigation,knz,"Found on Sentry\r\nhttps://sentry.io/cockroach-labs/cockroachdb/issues/392500093/\r\n\r\n```\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser/eval.go in Eval at line 2800\r\ngithub.com/cockroachdb/cockroach/pkg/sql/render.go in renderRow at line 586\r\ngithub.com/cockroachdb/cockroach/pkg/sql/render.go in Next at line 113\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in forEachRow at line 2045\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in execClassic at line 2029\r\ngithub.com/cockroachdb/cockroach/pkg/sql/executor.go in execStmt at line 2176\r\n```\r\n\r\nContext in eval.go: `res, err := expr.fn.fn(ctx, args.D)`\r\n\r\n```go\r\n// Eval implements the TypedExpr interface.\r\nfunc (expr *FuncExpr) Eval(ctx *EvalContext) (Datum, error) {\r\n        args := NewDTupleWithCap(len(expr.Exprs))\r\n        for _, e := range expr.Exprs {\r\n                arg, err := e.(TypedExpr).Eval(ctx)\r\n                if err != nil {\r\n                        return nil, err\r\n                }\r\n                if arg == DNull && !expr.fn.nullableArgs {\r\n                        return DNull, nil\r\n                }\r\n                args.D = append(args.D, arg)\r\n        }\r\n\r\n        res, err := expr.fn.fn(ctx, args.D)  // <- HERE\r\n        if err != nil {\r\n                // If we are facing a retry error, in particular those generated\r\n                // by crdb_internal.force_retry(), propagate it unchanged, so that\r\n                // the executor can see it with the right type.\r\n                if _, ok := err.(*roachpb.HandledRetryableTxnError); ok {\r\n                        return nil, err\r\n                }\r\n                // If we are facing an explicit error, propagate it unchanged.\r\n                fName := expr.Func.String()\r\n                if fName == `crdb_internal.force_error` {\r\n                        return nil, err\r\n                }\r\n                return nil, errors.Wrapf(err, ""%s()"", fName)\r\n        }\r\n        return res, nil\r\n}\r\n\r\n```\r\n\r\nThis in turn suggests a function that's not been memoized properly. Probably an operator overload error.\r\n\r\n\r\n\r\n","go\r\n// Eval implements the TypedExpr interface.\r\nfunc (expr *FuncExpr) Eval(ctx *EvalContext) (Datum, error) {\r\n        args := NewDTupleWithCap(len(expr.Exprs))\r\n        for _, e := range expr.Exprs {\r\n                arg, err := e.(TypedExpr).Eval(ctx)\r\n                if err != nil {\r\n                        return nil, err\r\n                }\r\n                if arg == DNull && !expr.fn.nullableArgs {\r\n                        return DNull, nil\r\n                }\r\n                args.D = append(args.D, arg)\r\n        }\r\n\r\n        res, err := expr.fn.fn(ctx, args.D)  // <- HERE\r\n        if err != nil {\r\n                // If we are facing a retry error, in particular those generated\r\n                // by crdb_internal.force_retry(), propagate it unchanged, so that\r\n                // the executor can see it with the right type.\r\n                if _, ok := err.(*roachpb.HandledRetryableTxnError); ok {\r\n                        return nil, err\r\n                }\r\n                // If we are facing an explicit error, propagate it unchanged.\r\n                fName := expr.Func.String()\r\n                if fName == `crdb_internal.force_error` {\r\n                        return nil, err\r\n                }\r\n                return nil, errors.Wrapf(err, ""%s()"", fName)\r\n        }\r\n        return res, nil\r\n}\r\n\r\n"
24611,"build: cmake 3.11.0 does not work with our makefileThis is on OSX 10.13.3 using brew.\r\n\r\nBrew now installs and updates cmake to 3.11.0 and this is not compatible with out Makefile.\r\n\r\n`make` will build librocksdb correctly, but then stop with the following (without an error):\r\n\r\n\r\nIf you run `make` again:\r\n\r\n\r\nTo downgrade back to cmake 3.10.3, run the following commands:\r\n1. `brew unlink cmake`\r\n1. `brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/a169609a5dcd5bf0cb83c2d8ffc7da934b3d81fe/Formula/cmake.rb`\r\n1. `brew pin cmake`\r\n\r\nOnce this is fixed, you should be able to run `brew unpin cmake` and allow it to upgrade like normal.\r\n\r\n",C-bug|B-os-macos|A-build-system,benesch,"This is on OSX 10.13.3 using brew.\r\n\r\nBrew now installs and updates cmake to 3.11.0 and this is not compatible with out Makefile.\r\n\r\n`make` will build librocksdb correctly, but then stop with the following (without an error):\r\n```bash\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_impl_debug.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_bench_tool.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(rocks_lua_compaction_filter.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_impl_debug.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_bench_tool.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(rocks_lua_compaction_filter.cc.o) has no symbols\r\n```\r\n\r\nIf you run `make` again:\r\n```bash\r\nGOPATH set to /Users/bram/go\r\nRunning make with -j8\r\ngo run pkg/cmd/docgen/{main,funcs}.go functions docs/generated/sql --quiet\r\nmake[1]: /usr/local/Cellar/cmake/3.10.3/bin/cmake: No such file or directory\r\nmake[1]: *** [cmake_check_build_system] Error 1\r\nmake: *** [libroachccl] Error 2\r\nmake: *** Waiting for unfinished jobs....\r\ntouch bin/.docgen_functions\r\n```\r\n\r\nTo downgrade back to cmake 3.10.3, run the following commands:\r\n1. `brew unlink cmake`\r\n1. `brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/a169609a5dcd5bf0cb83c2d8ffc7da934b3d81fe/Formula/cmake.rb`\r\n1. `brew pin cmake`\r\n\r\nOnce this is fixed, you should be able to run `brew unpin cmake` and allow it to upgrade like normal.\r\n\r\n",bash\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_impl_debug.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_bench_tool.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(rocks_lua_compaction_filter.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_impl_debug.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(db_bench_tool.cc.o) has no symbols\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: librocksdb.a(rocks_lua_compaction_filter.cc.o) has no symbols\r\n
24556,"sql: views do not track string-based dependencies properly\r\n\r\nFails with:\r\n\r\n```\r\npq: nextval(): relation ""s"" does not exist\r\n```\r\n\r\nThis is because CockroachDB fails to peek into strings that designate object names.\r\n\r\nOther possible failing cases: `'tablename'::regclass` and related constructs.\r\n\r\nThe reason why pg does not suffer from this is that object name strings are converted to OIDs internally, and those OIDs don't change across renames.\r\n\r\ncc @jordanlewis \r\n",C-bug|A-schema-descriptors|A-schema-changes,RichardJCai,"```sql\r\nCREATE SEQUENCE s;\r\nCREATE VIEW v AS SELECT nextval('s');\r\nALTER SEQUENCE s RENAME TO u;\r\nSELECT * FROM v;\r\n```\r\n\r\nFails with:\r\n\r\n```\r\npq: nextval(): relation ""s"" does not exist\r\n```\r\n\r\nThis is because CockroachDB fails to peek into strings that designate object names.\r\n\r\nOther possible failing cases: `'tablename'::regclass` and related constructs.\r\n\r\nThe reason why pg does not suffer from this is that object name strings are converted to OIDs internally, and those OIDs don't change across renames.\r\n\r\ncc @jordanlewis \r\n",sql\r\nCREATE SEQUENCE s;\r\nCREATE VIEW v AS SELECT nextval('s');\r\nALTER SEQUENCE s RENAME TO u;\r\nSELECT * FROM v;\r\n
24297,"sql: suboptimal query plan improved by manual index selectionThis user got a slow query, and was able to speed it up by forcing an index selection with the `table@index` syntax: https://forum.cockroachlabs.com/t/how-to-improve-cockroachdb-query-performance/1461\r\n\r\nThis may be a known issue, but they (and I) would be interested in some explanation of why the manually-chosen index was faster.\r\n\r\n### Setup:\r\n- 8M rows\r\n- 10 nodes\r\n- CockroachDB V2.0 20180326\r\n\r\n### Schema: (copy pasted / cleaned up from the forum)\r\n\r\n\r\n### Original (slow) query and plan:\r\n\r\n~6sec\r\n\r\n```\r\nSELECT *\r\nFROM t_sync_data\r\nWHERE last_sync_id>0\r\n  AND user_id='11cd19e4-837d-4bff-4a76-aefa0ddbec64'\r\n  AND data_type='a01'\r\nORDER BY last_sync_id ASC\r\nLIMIT 50;\r\n\xb1------------------------\xb1-------\xb1---------------------------------------------------------------+\r\n| Tree                   | Field | Description                                                   |\r\n\xb1------------------------\xb1-------\xb1---------------------------------------------------------------+\r\n| limit                  |       |                                                               |\r\n| \u2514\u2500\u2500 render             |       |                                                               |\r\n| \u2514\u2500\u2500 index-join         |       |                                                               |\r\n| \u251c\u2500\u2500 scan               |       |                                                               |\r\n| \u2502                      | table | t_sync_data@t_sync_data_last_sync_id_user_id_data_type_idx    |\r\n| \u2502                      | spans | /1/""!\\xcd\\x19\\xe4\\x83}K\\xffJv\\xae\\xfa\\r\\xdb\\xecd""/\u201chighlight\u201d-|\r\n| \u2514\u2500\u2500 scan               |       |                                                               |\r\n|                        | table | t_sync_data@primary                                           |\r\n\xb1------------------------\xb1------\xb1----------------------------------------------------------------+\r\n(8 rows)\r\n```\r\n\r\n### Faster query:\r\n\r\n~50ms\r\n\r\n```\r\nSELECT *\r\nFROM t_sync_data@t_sync_data_user_id_data_type_idx\r\nWHERE last_sync_id>0\r\n  AND user_id='11cd19e4-837d-4bff-4a76-aefa0ddbec64'\r\n  AND data_type='a01'\r\nORDER BY last_sync_id ASC\r\nLIMIT 50;\r\n```",A-sql-optimizer|C-question,justinj,"This user got a slow query, and was able to speed it up by forcing an index selection with the `table@index` syntax: https://forum.cockroachlabs.com/t/how-to-improve-cockroachdb-query-performance/1461\r\n\r\nThis may be a known issue, but they (and I) would be interested in some explanation of why the manually-chosen index was faster.\r\n\r\n### Setup:\r\n- 8M rows\r\n- 10 nodes\r\n- CockroachDB V2.0 20180326\r\n\r\n### Schema: (copy pasted / cleaned up from the forum)\r\n```sql\r\nCREATE TABLE t_sync_data (\r\n    remote_id UUID NOT NULL,\r\n    conflict_remote_id UUID NULL,\r\n    user_id UUID NOT NULL,\r\n    last_sync_id INT NOT NULL,\r\n    data_type STRING NOT NULL,\r\n    sync_data STRING NULL,\r\n    deleted BOOL NOT NULL,\r\n    create_device_id STRING NOT NULL,\r\n    original_data_id STRING NOT NULL,\r\n    create_time INT NOT NULL,\r\n    last_update_time INT NOT NULL,\r\n    CONSTRAINT \u201cprimary\u201d PRIMARY KEY (remote_id ASC),\r\n    INDEX t_sync_data_user_id_data_type_idx (user_id ASC, data_type ASC),\r\n    INDEX t_sync_data_last_sync_id_idx (last_sync_id ASC),\r\n    FAMILY f_meta (remote_id, conflict_remote_id, user_id, last_sync_id, data_type, deleted, create_device_id, original_data_id, create_time, last_update_time),\r\n    FAMILY f_data (sync_data)\r\n)\r\n\r\nCREATE INDEX ON t_sync_data (last_sync_id, user_id, data_type);\r\nCREATE INDEX ON t_sync_data (user_id, data_type);\r\nCREATE INDEX ON t_sync_data (last_sync_id);\r\n```\r\n\r\n### Original (slow) query and plan:\r\n\r\n~6sec\r\n\r\n```\r\nSELECT *\r\nFROM t_sync_data\r\nWHERE last_sync_id>0\r\n  AND user_id='11cd19e4-837d-4bff-4a76-aefa0ddbec64'\r\n  AND data_type='a01'\r\nORDER BY last_sync_id ASC\r\nLIMIT 50;\r\n\xb1------------------------\xb1-------\xb1---------------------------------------------------------------+\r\n| Tree                   | Field | Description                                                   |\r\n\xb1------------------------\xb1-------\xb1---------------------------------------------------------------+\r\n| limit                  |       |                                                               |\r\n| \u2514\u2500\u2500 render             |       |                                                               |\r\n| \u2514\u2500\u2500 index-join         |       |                                                               |\r\n| \u251c\u2500\u2500 scan               |       |                                                               |\r\n| \u2502                      | table | t_sync_data@t_sync_data_last_sync_id_user_id_data_type_idx    |\r\n| \u2502                      | spans | /1/""!\\xcd\\x19\\xe4\\x83}K\\xffJv\\xae\\xfa\\r\\xdb\\xecd""/\u201chighlight\u201d-|\r\n| \u2514\u2500\u2500 scan               |       |                                                               |\r\n|                        | table | t_sync_data@primary                                           |\r\n\xb1------------------------\xb1------\xb1----------------------------------------------------------------+\r\n(8 rows)\r\n```\r\n\r\n### Faster query:\r\n\r\n~50ms\r\n\r\n```\r\nSELECT *\r\nFROM t_sync_data@t_sync_data_user_id_data_type_idx\r\nWHERE last_sync_id>0\r\n  AND user_id='11cd19e4-837d-4bff-4a76-aefa0ddbec64'\r\n  AND data_type='a01'\r\nORDER BY last_sync_id ASC\r\nLIMIT 50;\r\n```","sql\r\nCREATE TABLE t_sync_data (\r\n    remote_id UUID NOT NULL,\r\n    conflict_remote_id UUID NULL,\r\n    user_id UUID NOT NULL,\r\n    last_sync_id INT NOT NULL,\r\n    data_type STRING NOT NULL,\r\n    sync_data STRING NULL,\r\n    deleted BOOL NOT NULL,\r\n    create_device_id STRING NOT NULL,\r\n    original_data_id STRING NOT NULL,\r\n    create_time INT NOT NULL,\r\n    last_update_time INT NOT NULL,\r\n    CONSTRAINT \u201cprimary\u201d PRIMARY KEY (remote_id ASC),\r\n    INDEX t_sync_data_user_id_data_type_idx (user_id ASC, data_type ASC),\r\n    INDEX t_sync_data_last_sync_id_idx (last_sync_id ASC),\r\n    FAMILY f_meta (remote_id, conflict_remote_id, user_id, last_sync_id, data_type, deleted, create_device_id, original_data_id, create_time, last_update_time),\r\n    FAMILY f_data (sync_data)\r\n)\r\n\r\nCREATE INDEX ON t_sync_data (last_sync_id, user_id, data_type);\r\nCREATE INDEX ON t_sync_data (user_id, data_type);\r\nCREATE INDEX ON t_sync_data (last_sync_id);\r\n"
24175,"pq: unsupported comparison operator: <string> = ANY <uuid[]>Is this a question, feature request, or bug report?\r\n\r\n> This is a bug report.\r\n\r\n**BUG REPORT**\r\n     grep -F '[config]' cockroach-data/logs/cockroach.log\r\n```\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] file created at: 2018/03/20 16:52:22\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] running on machine: db1\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] binary: CockroachDB CCL v1.1.6 (linux amd64, built 2018/03/12 17:58:05, go1.8.3)\r\n```\r\n\r\n2. Please describe the issue you observed:\r\n\r\nThis SQL works as expected:\r\n\r\nBut when checking against table column with type:\r\n\r\nwith this sql, it fails:\r\n\r\n\r\nA workaround is to wrap `UUID` value in a inner `SELECT`:\r\n\r\n\r\nI think there is a problem while checking type matching for `ANY` operator on arrays.",C-bug|A-sql-semantics|A-sql-optimizer,justinj,"Is this a question, feature request, or bug report?\r\n\r\n> This is a bug report.\r\n\r\n**BUG REPORT**\r\n     grep -F '[config]' cockroach-data/logs/cockroach.log\r\n```\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] file created at: 2018/03/20 16:52:22\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] running on machine: db1\r\nI180320 16:52:22.923088 518 util/log/clog.go:938  [config] binary: CockroachDB CCL v1.1.6 (linux amd64, built 2018/03/12 17:58:05, go1.8.3)\r\n```\r\n\r\n2. Please describe the issue you observed:\r\n\r\nThis SQL works as expected:\r\n```sql\r\nSELECT '3ae3560e-d771-4b63-affb-47e8d7853680'::UUID = ANY ('{3ae3560e-d771-4b63-affb-47e8d7853680}'::UUID[])as a;\r\n+------+\r\n|  a   |\r\n+------+\r\n| true |\r\n+------+\r\n(1 row)\r\n\r\n```\r\nBut when checking against table column with type:\r\n```sql\r\nshared_users      UUID[]\r\n```\r\nwith this sql, it fails:\r\n```sql\r\nSELECT * FROM documents WHERE '3ae3560e-d771-4b63-affb-47e8d7853680'::UUID = ANY (documents.shared_users);\r\npq: unsupported comparison operator: <string> = ANY <uuid[]>\r\n```\r\n\r\nA workaround is to wrap `UUID` value in a inner `SELECT`:\r\n```sql\r\nSELECT shared_users FROM documents WHERE (SELECT '3ae3560e-d771-4b63-affb-47e8d7853680'::UUID) = ANY (documents.shared_users);\r\n+--------------+\r\n| shared_users |\r\n+--------------+\r\n+--------------+\r\n(0 rows)\r\n\r\n\r\n```\r\n\r\nI think there is a problem while checking type matching for `ANY` operator on arrays.",sql\r\nSELECT '3ae3560e-d771-4b63-affb-47e8d7853680'::UUID = ANY ('{3ae3560e-d771-4b63-affb-47e8d7853680}'::UUID[])as a;\r\n+------+\r\n|  a   |\r\n+------+\r\n| true |\r\n+------+\r\n(1 row)\r\n\r\n
24169,"v2.0-beta.20180319: Regression in star queries**BUG REPORT**\r\n\r\n- What did you do?\r\nExecuted the following query\r\n\r\n\r\n- What did you expect to see?\r\nAll rows with all columns from the table.\r\n\r\n- What did you see instead?\r\nThe following error `[42703] ERROR: no data source named ""myblueprint.study_industry""`\r\n\r\nThis worked fine in v1.1.6 but is now broken in v2.0-beta.20180319\r\n",C-bug,knz,"**BUG REPORT**\r\n\r\n- What did you do?\r\nExecuted the following query\r\n```sql\r\nselect\r\n    ""myblueprint"".""study_industry"".*\r\nfrom ""myblueprint"".""study_industry""\r\n```\r\n\r\n- What did you expect to see?\r\nAll rows with all columns from the table.\r\n\r\n- What did you see instead?\r\nThe following error `[42703] ERROR: no data source named ""myblueprint.study_industry""`\r\n\r\nThis worked fine in v1.1.6 but is now broken in v2.0-beta.20180319\r\n","sql\r\nselect\r\n    ""myblueprint"".""study_industry"".*\r\nfrom ""myblueprint"".""study_industry""\r\n"
24034,"server: node status in TestCluster may be missing nodes(Discovered from #23861.)\r\n\r\nIn the PR linked above, I wrote a test that creates a 2-node cluster (using `serverutils.TestCluster`), creates a session on each node, and issues a `CANCEL SESSION` statement in one session to cancel the other session. I noticed that the test sometimes failed under stress because the cancel statement was unable to find the session's ID. I added the following log to `server/status.go` (see commented line):\r\n\r\n\r\nIn a failed test run, I saw the following in the logs:\r\n```\r\nI180319 19:53:52.417966 594 server/status.go:1231 [client=127.0.0.1:54767,user=root,n1,status] Nodes: 1\r\n```\r\n\r\nSo for some reason, `n1` doesn't know about `n2`. This error can be reproduced fairly quickly (within ~1 minute of stress testing). I'm not sure if this is a bug in `TestCluster` or a production bug, so the issue just mentions `TestCluster` as a starting point.",C-enhancement|A-testing,andreimatei,"(Discovered from #23861.)\r\n\r\nIn the PR linked above, I wrote a test that creates a 2-node cluster (using `serverutils.TestCluster`), creates a session on each node, and issues a `CANCEL SESSION` statement in one session to cancel the other session. I noticed that the test sometimes failed under stress because the cancel statement was unable to find the session's ID. I added the following log to `server/status.go` (see commented line):\r\n```go\r\n// ListSessions returns a list of SQL sessions on all nodes in the cluster.\r\nfunc (s *statusServer) ListSessions(\r\n\tctx context.Context, req *serverpb.ListSessionsRequest,\r\n) (*serverpb.ListSessionsResponse, error) {\r\n\tctx = s.AnnotateCtx(ctx)\r\n\tnodes, err := s.Nodes(ctx, nil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n// ...\r\n\tnumNodes := len(nodes.Nodes)\r\n\tlog.Infof(ctx, ""Nodes: %d"", numNodes) // LOG ADDED HERE\r\n\r\n// ...\r\n```\r\n\r\nIn a failed test run, I saw the following in the logs:\r\n```\r\nI180319 19:53:52.417966 594 server/status.go:1231 [client=127.0.0.1:54767,user=root,n1,status] Nodes: 1\r\n```\r\n\r\nSo for some reason, `n1` doesn't know about `n2`. This error can be reproduced fairly quickly (within ~1 minute of stress testing). I'm not sure if this is a bug in `TestCluster` or a production bug, so the issue just mentions `TestCluster` as a starting point.","go\r\n// ListSessions returns a list of SQL sessions on all nodes in the cluster.\r\nfunc (s *statusServer) ListSessions(\r\n\tctx context.Context, req *serverpb.ListSessionsRequest,\r\n) (*serverpb.ListSessionsResponse, error) {\r\n\tctx = s.AnnotateCtx(ctx)\r\n\tnodes, err := s.Nodes(ctx, nil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n// ...\r\n\tnumNodes := len(nodes.Nodes)\r\n\tlog.Infof(ctx, ""Nodes: %d"", numNodes) // LOG ADDED HERE\r\n\r\n// ...\r\n"
24029,"storage: dropping a large table will brick a cluster due to compactions\r\n\r\nWait ~10m for stores to download. Then drop the 2TiB table:\r\n\r\n\r\n\r\nThe cluster explodes a few minutes later as RocksDB tombstones pile up. I can no longer execute any SQL queries that read from/write to disk.\r\n\r\nVery closely related to #21901, but thought I'd file a separate tracking issue. \r\n\r\n/cc @spencerkimball ",S-1-stability|A-storage,petermattis,"```shell\r\n$ roachprod create USER-FOO -n10\r\n$ roachprod run USER-FOO 'mkdir -p /mnt/data1/cockroach && gsutil -m -q cp -r gs://cockroach-fixtures/workload/bank/version=1.0.0,payload-bytes=10240,ranges=0,rows=65104166,seed=1/stores=10/$((10#$(hostname | grep -oE [0-9]+$)))/* /mnt/data1/cockroach'\r\n```\r\n\r\nWait ~10m for stores to download. Then drop the 2TiB table:\r\n\r\n```SQL\r\nALTER TABLE bank.bank EXPERIMENTAL CONFIGURE ZONE 'gc: {ttlseconds: 30}';\r\nDROP TABLE bank.bank;\r\n```\r\n\r\nThe cluster explodes a few minutes later as RocksDB tombstones pile up. I can no longer execute any SQL queries that read from/write to disk.\r\n\r\nVery closely related to #21901, but thought I'd file a separate tracking issue. \r\n\r\n/cc @spencerkimball ","shell\r\n$ roachprod create USER-FOO -n10\r\n$ roachprod run USER-FOO 'mkdir -p /mnt/data1/cockroach && gsutil -m -q cp -r gs://cockroach-fixtures/workload/bank/version=1.0.0,payload-bytes=10240,ranges=0,rows=65104166,seed=1/stores=10/$((10#$(hostname | grep -oE [0-9]+$)))/* /mnt/data1/cockroach'\r\n"
23850,sql: INET columns are not formatted during pretty-printingThe following is the result of an `EXPLAIN` statement. It demonstrates that INET values are not properly formatted when pretty printing.\r\n\r\n,C-investigation|E-easy|S-3-ux-surprise|A-sql-execution,rohany,"The following is the result of an `EXPLAIN` statement. It demonstrates that INET values are not properly formatted when pretty printing.\r\n\r\n```sql\r\nroot@:26257/inettest> explain select * from users where addr = '2001:db8::68/0':::INET;\r\n+------+-------+--------------------------------------------------------------------------+\r\n| Tree | Field |                               Description                                |\r\n+------+-------+--------------------------------------------------------------------------+\r\n| scan |       |                                                                          |\r\n|      | table | users@users_addr_idx                                                     |\r\n|      | spans | /""\\x01\\x00                                                               |\r\n|      |       | \\x01\\r\\xb8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h""-/""\\x01\\x00      |\r\n|      |       | \\x01\\r\\xb8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h""/PrefixEnd       |\r\n+------+-------+--------------------------------------------------------------------------+\r\n(3 rows)\r\n```","sql\r\nroot@:26257/inettest> explain select * from users where addr = '2001:db8::68/0':::INET;\r\n+------+-------+--------------------------------------------------------------------------+\r\n| Tree | Field |                               Description                                |\r\n+------+-------+--------------------------------------------------------------------------+\r\n| scan |       |                                                                          |\r\n|      | table | users@users_addr_idx                                                     |\r\n|      | spans | /""\\x01\\x00                                                               |\r\n|      |       | \\x01\\r\\xb8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h""-/""\\x01\\x00      |\r\n|      |       | \\x01\\r\\xb8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00h""/PrefixEnd       |\r\n+------+-------+--------------------------------------------------------------------------+\r\n(3 rows)\r\n"
23774,sql: implement array_to_stringI am building an application with PHP Yii2 Framework which has its ORM module. Yii2 already have a good PostgreSQL support so that wanted to use CockroachDB with Yii2 and unfortunately I am getting below error:\r\n\r\n**ERROR:  unknown function: array_to_string()**\r\n\r\nAny help would be appreciated.\r\n\r\nQuery is being run by Yii2 is\r\n\r\n,C-enhancement|E-easy|A-sql-builtins,justinj,"I am building an application with PHP Yii2 Framework which has its ORM module. Yii2 already have a good PostgreSQL support so that wanted to use CockroachDB with Yii2 and unfortunately I am getting below error:\r\n\r\n**ERROR:  unknown function: array_to_string()**\r\n\r\nAny help would be appreciated.\r\n\r\nQuery is being run by Yii2 is\r\n\r\n```sql\r\nSELECT\u2028 d.nspname AS table_schema,\r\n\u2028c.relname AS table_name,\u2028\r\na.attname AS column_name,\u2028\r\nCOALESCE(td.typname, tb.typname, t.typname) AS data_type,\r\nCOALESCE(td.typtype, tb.typtype, t.typtype) AS type_type,\u2028\r\na.attlen AS character_maximum_length,\r\n\u2028pg_catalog.col_description(c.oid, a.attnum) AS column_comment,\u2028\r\na.atttypmod AS modifier,\u2028\r\na.attnotnull = false AS is_nullable,\u2028\r\nCAST(pg_get_expr(ad.adbin, ad.adrelid) AS varchar) AS column_default,\r\n\u2028coalesce(pg_get_expr(ad.adbin, ad.adrelid) ~ 'nextval',false) AS is_autoinc,\r\n\u2028CASE \r\nWHEN COALESCE(td.typtype, tb.typtype, t.typtype) = 'e'::char\u2028\r\nTHEN array_to_string((SELECT array_agg(enumlabel) \r\n   FROM pg_enum \r\n   WHERE enumtypid = COALESCE(td.oid, tb.oid, a.atttypid))::varchar[], ',')\u2028\r\nELSE NULL \r\n\u2028END AS enum_values,\r\n\u2028CASE atttypid\u2028 \r\nWHEN 21 /*int2*/ THEN 16\r\n\u2028WHEN 23 /*int4*/ THEN 32\u2028\r\nWHEN 20 /*int8*/ THEN 64\r\n\u2028WHEN 1700 /*numeric*/ THEN\u2028 CASE \r\n  WHEN atttypmod = -1\u2028THEN null\r\n  \u2028ELSE ((atttypmod - 4) >> 16) & 65535\u2028 END\r\n\u2028WHEN 700 /*float4*/ THEN 24 /*FLT_MANT_DIG*/\u2028\r\nWHEN 701 /*float8*/ THEN 53 /*DBL_MANT_DIG*/\u2028\r\nELSE null\u2028\r\nEND AS numeric_precision,\u2028\r\nCASE\u2028\r\nWHEN atttypid IN (21, 23, 20) THEN 0\r\n\u2028WHEN atttypid IN (1700) THEN\u2028 CASE\u2028\r\n   WHEN atttypmod = -1 THEN null\r\n   \u2028ELSE (atttypmod - 4) & 65535\r\n\u2028END\r\n\u2028ELSE null \u2028END AS numeric_scale,\u2028\r\nCAST(\u2028information_schema._pg_char_max_length(information_schema._pg_truetypid(a, t), information_schema._pg_truetypmod(a, t)) \u2028AS numeric\u2028) AS size,\u2028\r\na.attnum = any (ct.conkey) as is_pkey,\u2028\r\nCOALESCE(NULLIF(a.attndims, 0), NULLIF(t.typndims, 0), (t.typcategory='A')::int) AS dimension\u2028 \r\nFROM\u2028 pg_class c\u2028\r\nLEFT JOIN pg_attribute a ON a.attrelid = c.oid\u2028\r\nLEFT JOIN pg_attrdef ad ON a.attrelid = ad.adrelid AND a.attnum = ad.adnum\u2028LEFT JOIN pg_type t ON a.atttypid = t.oid\r\n\u2028LEFT JOIN pg_type tb ON (a.attndims > 0 OR t.typcategory='A') AND t.typelem > 0 AND t.typelem = tb.oid OR t.typbasetype > 0 AND t.typbasetype = tb.oid\u2028\r\nLEFT JOIN pg_type td ON t.typndims > 0 AND t.typbasetype > 0 AND tb.typelem = td.oid\u2028\r\nLEFT JOIN pg_namespace d ON d.oid = c.relnamespace\u2028\r\nLEFT JOIN pg_constraint ct ON ct.conrelid = c.oid AND ct.contype = 'p'\u2028\r\nWHERE \u2028a.attnum > 0 AND t.typname != ''\u2028AND c.relname = 'tbl_user'\u2028 AND d.nspname = 'public'\u2028\r\nORDER BY \u2028a.attnum;\r\n```","sql\r\nSELECT\u2028 d.nspname AS table_schema,\r\n\u2028c.relname AS table_name,\u2028\r\na.attname AS column_name,\u2028\r\nCOALESCE(td.typname, tb.typname, t.typname) AS data_type,\r\nCOALESCE(td.typtype, tb.typtype, t.typtype) AS type_type,\u2028\r\na.attlen AS character_maximum_length,\r\n\u2028pg_catalog.col_description(c.oid, a.attnum) AS column_comment,\u2028\r\na.atttypmod AS modifier,\u2028\r\na.attnotnull = false AS is_nullable,\u2028\r\nCAST(pg_get_expr(ad.adbin, ad.adrelid) AS varchar) AS column_default,\r\n\u2028coalesce(pg_get_expr(ad.adbin, ad.adrelid) ~ 'nextval',false) AS is_autoinc,\r\n\u2028CASE \r\nWHEN COALESCE(td.typtype, tb.typtype, t.typtype) = 'e'::char\u2028\r\nTHEN array_to_string((SELECT array_agg(enumlabel) \r\n   FROM pg_enum \r\n   WHERE enumtypid = COALESCE(td.oid, tb.oid, a.atttypid))::varchar[], ',')\u2028\r\nELSE NULL \r\n\u2028END AS enum_values,\r\n\u2028CASE atttypid\u2028 \r\nWHEN 21 /*int2*/ THEN 16\r\n\u2028WHEN 23 /*int4*/ THEN 32\u2028\r\nWHEN 20 /*int8*/ THEN 64\r\n\u2028WHEN 1700 /*numeric*/ THEN\u2028 CASE \r\n  WHEN atttypmod = -1\u2028THEN null\r\n  \u2028ELSE ((atttypmod - 4) >> 16) & 65535\u2028 END\r\n\u2028WHEN 700 /*float4*/ THEN 24 /*FLT_MANT_DIG*/\u2028\r\nWHEN 701 /*float8*/ THEN 53 /*DBL_MANT_DIG*/\u2028\r\nELSE null\u2028\r\nEND AS numeric_precision,\u2028\r\nCASE\u2028\r\nWHEN atttypid IN (21, 23, 20) THEN 0\r\n\u2028WHEN atttypid IN (1700) THEN\u2028 CASE\u2028\r\n   WHEN atttypmod = -1 THEN null\r\n   \u2028ELSE (atttypmod - 4) & 65535\r\n\u2028END\r\n\u2028ELSE null \u2028END AS numeric_scale,\u2028\r\nCAST(\u2028information_schema._pg_char_max_length(information_schema._pg_truetypid(a, t), information_schema._pg_truetypmod(a, t)) \u2028AS numeric\u2028) AS size,\u2028\r\na.attnum = any (ct.conkey) as is_pkey,\u2028\r\nCOALESCE(NULLIF(a.attndims, 0), NULLIF(t.typndims, 0), (t.typcategory='A')::int) AS dimension\u2028 \r\nFROM\u2028 pg_class c\u2028\r\nLEFT JOIN pg_attribute a ON a.attrelid = c.oid\u2028\r\nLEFT JOIN pg_attrdef ad ON a.attrelid = ad.adrelid AND a.attnum = ad.adnum\u2028LEFT JOIN pg_type t ON a.atttypid = t.oid\r\n\u2028LEFT JOIN pg_type tb ON (a.attndims > 0 OR t.typcategory='A') AND t.typelem > 0 AND t.typelem = tb.oid OR t.typbasetype > 0 AND t.typbasetype = tb.oid\u2028\r\nLEFT JOIN pg_type td ON t.typndims > 0 AND t.typbasetype > 0 AND tb.typelem = td.oid\u2028\r\nLEFT JOIN pg_namespace d ON d.oid = c.relnamespace\u2028\r\nLEFT JOIN pg_constraint ct ON ct.conrelid = c.oid AND ct.contype = 'p'\u2028\r\nWHERE \u2028a.attnum > 0 AND t.typname != ''\u2028AND c.relname = 'tbl_user'\u2028 AND d.nspname = 'public'\u2028\r\nORDER BY \u2028a.attnum;\r\n"
23718,"sql: table creation freeze on version 2.0-beta**Description**\r\nI'm using Ebean ORM with Ebean migration (Java / JDBC). Everything works on CockroachDB 1.1.5 but with 2.0-beta, the process hang (freeze) on the creation of the table ""db_migration"".\r\n\r\nDuring this ""freeze"", the instruction ""SHOW TABLES;"" hand too. If I kill the Java application, the currently blocked instruction ""SHOW TABLES;"" return results.\r\n\r\nEbean create the table with this SQL instruction:\r\n\r\n\r\n\r\n\r\n**CockroachDB version**\r\n```\r\nBuild Tag:    v2.0-beta.20180305\r\nBuild Time:   2018/03/05 15:32:39\r\nDistribution: CCL\r\nPlatform:     darwin amd64\r\nGo Version:   go1.9.4\r\nC Compiler:   4.2.1 Compatible Clang 3.8.0 (tags/RELEASE_380/final)\r\nBuild SHA-1:  59246c8375fcffeb24b87ff2f40ed81eb84c070d\r\nBuild Type:   release\r\n```\r\n\r\n\r\n**Output from Java application**\r\n```\r\n[info] c.z.h.HikariDataSource - HikariPool-1 - Starting...\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@76c5e806\r\n[info] c.z.h.HikariDataSource - HikariPool-1 - Start completed.\r\n[info] p.a.d.DefaultDBApi - Database [default] connected at jdbc:postgresql://127.0.0.1:26257/documentation?autoReconnect=true&ApplicationName=documentation\r\n[info] i.e.c.p.LoadContext - loaded properties from [ebean.properties]\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Before cleanup stats (total=1, active=0, idle=1, waiting=0)\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - After cleanup  stats (total=1, active=0, idle=1, waiting=0)\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@2793a2e8\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - After adding stats (total=2, active=0, idle=2, waiting=0)\r\n[info] i.e.EbeanVersion - ebean version: 11.14.3\r\n[debug] c.z.h.p.ProxyConnection - HikariPool-1 - Executed rollback on connection org.postgresql.jdbc.PgConnection@76c5e806 due to dirty commit state on close().\r\n[debug] i.e.s.d.BeanDescriptorManager - Entities[5]\r\n[debug] i.e.d.DbOffline - reset\r\n[debug] i.e.m.r.LocalMigrationResources - resources: [seeds/default-dev/R__1.sql]\r\n[info] io.ebean.DDL - Executing create migration table - 1 statements\r\n[debug] io.ebean.DDL - executing 1 of 1 create table db_migration ( id integer not null, mtype...\r\n// HANG / FREEZE\r\n```\r\n\r\n\r\n**Output from CockroachDB**\r\n```\r\nI180312 12:37:00.155872 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT version() AS version"" {} 0.597 1 """"\r\nI180312 12:37:00.160290 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT current_schema()"" {} 0.170 1 """"\r\nI180312 12:37:00.167679 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT NULL AS table_cat, n.nspname AS table_schem, c.relname AS table_name, CASE (n.nspname ~ '^pg_') OR (n.nspname = 'information_schema') WHEN true THEN CASE WHEN (n.nspname = 'pg_catalog') OR (n.nspname = 'information_schema') THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TABLE' WHEN 'v' THEN 'SYSTEM VIEW' WHEN 'i' THEN 'SYSTEM INDEX' ELSE NULL END WHEN n.nspname = 'pg_toast' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TOAST TABLE' WHEN 'i' THEN 'SYSTEM TOAST INDEX' ELSE NULL END ELSE CASE c.relkind WHEN 'r' THEN 'TEMPORARY TABLE' WHEN 'p' THEN 'TEMPORARY TABLE' WHEN 'i' THEN 'TEMPORARY INDEX' WHEN 'S' THEN 'TEMPORARY SEQUENCE' WHEN 'v' THEN 'TEMPORARY VIEW' ELSE NULL END END WHEN false THEN CASE c.relkind WHEN 'r' THEN 'TABLE' WHEN 'p' THEN 'TABLE' WHEN 'i' THEN 'INDEX' WHEN 'S' THEN 'SEQUENCE' WHEN 'v' THEN 'VIEW' WHEN 'c' THEN 'TYPE' WHEN 'f' THEN 'FOREIGN TABLE' WHEN 'm' THEN 'MATERIALIZED VIEW' ELSE NULL END ELSE NULL END AS table_type, d.description AS remarks FROM pg_catalog.pg_namespace AS n, pg_catalog.pg_class AS c LEFT JOIN pg_catalog.pg_description AS d ON ((c.oid = d.objoid) AND (d.objsubid = 0)) LEFT JOIN pg_catalog.pg_class AS dc ON ((d.classoid = dc.oid) AND (dc.relname = 'pg_class')) LEFT JOIN pg_catalog.pg_namespace AS dn ON ((dn.oid = dc.relnamespace) AND (dn.nspname = 'pg_catalog')) WHERE ((c.relnamespace = n.oid) AND (n.nspname LIKE 'public')) AND (c.relname LIKE 'db_migration') ORDER BY table_type, table_schem, table_name"" {} 5.444 0 """"\r\nI180312 12:37:00.180320 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] internal-exec """" {} ""INSERT INTO system.public.eventlog(\\""timestamp\\"", \\""eventType\\"", \\""targetID\\"", \\""reportingID\\"", info) VALUES (now(), $1, $2, $3, $4)"" {$1:""'create_table'"", $2:""69"", $3:""1"", $4:""'{\\""TableName\\"":\\""documentation.public.db_migration\\"",\\""Statement\\"":\\""CREATE TABLE db_migration (id INTEGER NOT NULL, mtype VARCHAR(1) NOT NULL, mstatus VARCHAR(10) NOT NULL, mversion VARCHAR(150) NOT NULL, mcomment VARCHAR(150) NOT NULL, mchecksum INTEGER NOT NULL, run_on TIMESTAMP NOT NULL, run_by VARCHAR(30) NOT NULL, run_time INTEGER NOT NULL, CONSTRAINT pk_db_migration PRIMARY KEY (id))\\"",\\""User\\"":\\""root\\""}'""} 2.577 1 """"\r\nI180312 12:37:00.180446 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""CREATE TABLE db_migration (id INTEGER NOT NULL, mtype VARCHAR(1) NOT NULL, mstatus VARCHAR(10) NOT NULL, mversion VARCHAR(150) NOT NULL, mcomment VARCHAR(150) NOT NULL, mchecksum INTEGER NOT NULL, run_on TIMESTAMP NOT NULL, run_by VARCHAR(30) NOT NULL, run_time INTEGER NOT NULL, CONSTRAINT pk_db_migration PRIMARY KEY (id))"" {} 6.210 0 """"\r\nI180312 12:37:00.491223 215 sql/exec_log.go:173  [n1] internal-exec """" {} ""SELECT count(version) FROM system.public.lease WHERE ((\\""descID\\"" = $1) AND (version = $2)) AND (expiration > $3)"" {$1:""68"", $2:""2"", $3:""'2018-03-12 12:37:00.490083+00:00'""} 1.086 1 """"\r\nI180312 12:37:09.212135 212 server/status/runtime.go:219  [n1] runtime stats: 68 MiB RSS, 157 goroutines, 74 MiB/31 MiB/122 MiB GO alloc/idle/total, 16 MiB/23 MiB CGO alloc/total, 122.70cgo/sec, 0.01/0.01 %(u/s)time, 0.00 %gc (1x)\r\nI180312 12:37:19.212465 212 server/status/runtime.go:219  [n1] runtime stats: 71 MiB RSS, 157 goroutines, 81 MiB/25 MiB/122 MiB GO alloc/idle/total, 16 MiB/23 MiB CGO alloc/total, 81.20cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:29.110967 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec """" {} ""SET extra_float_digits = 3"" {} 0.184 0 """"\r\nI180312 12:37:29.111435 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec ""documentation"" {} ""SET application_name = 'documentation'"" {} 0.119 0 """"\r\nI180312 12:37:29.112396 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec ""documentation"" {} ""SET timezone = 'UTC'"" {} 0.251 0 """"\r\nI180312 12:37:29.211535 210 gossip/gossip.go:487  [n1] gossip status (ok, 1 node)\r\ngossip client (0/3 cur/max conns)\r\ngossip server (0/3 cur/max conns, infos 0/0 sent/received, bytes 0B/0B sent/received)\r\nI180312 12:37:29.211642 212 server/status/runtime.go:219  [n1] runtime stats: 74 MiB RSS, 159 goroutines, 87 MiB/19 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 79.71cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:29.216558 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.775 0 """"\r\nI180312 12:37:39.211863 212 server/status/runtime.go:219  [n1] runtime stats: 79 MiB RSS, 159 goroutines, 94 MiB/13 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 79.30cgo/sec, 0.01/0.01 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:49.212229 212 server/status/runtime.go:219  [n1] runtime stats: 81 MiB RSS, 159 goroutines, 100 MiB/7.1 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 80.70cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:59.210760 212 server/status/runtime.go:219  [n1] runtime stats: 85 MiB RSS, 159 goroutines, 107 MiB/696 KiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 74.21cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:59.217284 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.020 0 """"\r\nI180312 12:38:09.211651 212 server/status/runtime.go:219  [n1] runtime stats: 90 MiB RSS, 159 goroutines, 113 MiB/384 KiB/128 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 83.29cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:38:19.211624 212 server/status/runtime.go:219  [n1] runtime stats: 96 MiB RSS, 159 goroutines, 120 MiB/96 KiB/134 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 73.00cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nW180312 12:38:26.914324 374 internal/client/txn.go:531  [n1,client=127.0.0.1:51280,user=root] failure aborting transaction: HandledRetryableTxnError: TransactionAbortedError: txn aborted ""sql txn"" id=40e2c4ae key=/Table/SystemConfigSpan/Start rw=true pri=0.01057810 iso=SERIALIZABLE stat=PENDING epo=0 ts=1520858220.155526399,0 orig=1520858220.155526399,0 max=1520858220.655526399,0 wto=false rop=false seq=13; abort caused by: context canceled\r\nI180312 12:38:29.210026 210 gossip/gossip.go:487  [n1] gossip status (ok, 1 node)\r\ngossip client (0/3 cur/max conns)\r\ngossip server (0/3 cur/max conns, infos 0/0 sent/received, bytes 0B/0B sent/received)\r\nI180312 12:38:29.210128 212 server/status/runtime.go:219  [n1] runtime stats: 101 MiB RSS, 152 goroutines, 126 MiB/872 KiB/142 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 86.61cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:38:29.218543 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.440 0 """"\r\n^CI180312 12:38:31.345681 1 cli/start.go:650  received signal 'interrupt'\r\nNote: a second interrupt will skip graceful shutdown and terminate forcefully\r\nI180312 12:38:31.345859 1 cli/start.go:701  initiating graceful shutdown of server\r\ninitiating graceful shutdown of server\r\nI180312 12:38:31.348153 474 sql/exec_log.go:173  internal-exec """" {} ""DELETE FROM system.public.lease WHERE (\\""descID\\"", version, \\""nodeID\\"", expiration) = ($1, $2, $3, $4)"" {$2:""1"", $3:""1"", $4:""'2018-03-12 12:43:14.26188+00:00'"", $1:""56""} 2.080 1 """"\r\nW180312 12:38:31.351204 22 vendor/google.golang.org/grpc/clientconn.go:1277  grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing\r\nI180312 12:38:31.351269 474 storage/engine/rocksdb.go:674  closing rocksdb instance at ""/Users/meyer_t/Downloads/cockroach-data/cockroach-temp010543778""\r\nW180312 12:38:31.351393 243 vendor/google.golang.org/grpc/clientconn.go:1158  grpc: addrConn.createTransport failed to connect to {anansi:26257 0  <nil>}. Err :connection error: desc = ""transport: Error while dialing cannot reuse client connection"". Reconnecting...\r\nW180312 12:38:31.351560 243 vendor/google.golang.org/grpc/clientconn.go:1277  grpc: addrConn.transportMonitor exits due to: context canceled\r\nI180312 12:38:31.352889 474 storage/engine/rocksdb.go:674  closing rocksdb instance at ""/Users/meyer_t/Downloads/cockroach-data""\r\nI180312 12:38:31.354248 1 cli/start.go:755  server drained and shutdown completed\r\nserver drained and shutdown completed\r\nI180312 12:38:31.354413 1 cli/error.go:109  interrupted\r\nError: interrupted\r\nFailed running ""start""\r\n```",C-bug|S-2-temp-unavailability,vivekmenezes,"**Description**\r\nI'm using Ebean ORM with Ebean migration (Java / JDBC). Everything works on CockroachDB 1.1.5 but with 2.0-beta, the process hang (freeze) on the creation of the table ""db_migration"".\r\n\r\nDuring this ""freeze"", the instruction ""SHOW TABLES;"" hand too. If I kill the Java application, the currently blocked instruction ""SHOW TABLES;"" return results.\r\n\r\nEbean create the table with this SQL instruction:\r\n\r\n```sql\r\nCREATE TABLE db_migration (\r\n    id INTEGER NOT NULL,\r\n    mtype VARCHAR(1) NOT NULL,\r\n    mstatus VARCHAR(10) NOT NULL,\r\n    mversion VARCHAR(150) NOT NULL,\r\n    mcomment VARCHAR(150) NOT NULL,\r\n    mchecksum INTEGER NOT NULL,\r\n    run_on TIMESTAMP NOT NULL,\r\n    run_by VARCHAR(30) NOT NULL,\r\n    run_time INTEGER NOT NULL,\r\n\r\n    CONSTRAINT pk_db_migration PRIMARY KEY (id)\r\n);\r\n```\r\n\r\n\r\n**CockroachDB version**\r\n```\r\nBuild Tag:    v2.0-beta.20180305\r\nBuild Time:   2018/03/05 15:32:39\r\nDistribution: CCL\r\nPlatform:     darwin amd64\r\nGo Version:   go1.9.4\r\nC Compiler:   4.2.1 Compatible Clang 3.8.0 (tags/RELEASE_380/final)\r\nBuild SHA-1:  59246c8375fcffeb24b87ff2f40ed81eb84c070d\r\nBuild Type:   release\r\n```\r\n\r\n\r\n**Output from Java application**\r\n```\r\n[info] c.z.h.HikariDataSource - HikariPool-1 - Starting...\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@76c5e806\r\n[info] c.z.h.HikariDataSource - HikariPool-1 - Start completed.\r\n[info] p.a.d.DefaultDBApi - Database [default] connected at jdbc:postgresql://127.0.0.1:26257/documentation?autoReconnect=true&ApplicationName=documentation\r\n[info] i.e.c.p.LoadContext - loaded properties from [ebean.properties]\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Before cleanup stats (total=1, active=0, idle=1, waiting=0)\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - After cleanup  stats (total=1, active=0, idle=1, waiting=0)\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@2793a2e8\r\n[debug] c.z.h.p.HikariPool - HikariPool-1 - After adding stats (total=2, active=0, idle=2, waiting=0)\r\n[info] i.e.EbeanVersion - ebean version: 11.14.3\r\n[debug] c.z.h.p.ProxyConnection - HikariPool-1 - Executed rollback on connection org.postgresql.jdbc.PgConnection@76c5e806 due to dirty commit state on close().\r\n[debug] i.e.s.d.BeanDescriptorManager - Entities[5]\r\n[debug] i.e.d.DbOffline - reset\r\n[debug] i.e.m.r.LocalMigrationResources - resources: [seeds/default-dev/R__1.sql]\r\n[info] io.ebean.DDL - Executing create migration table - 1 statements\r\n[debug] io.ebean.DDL - executing 1 of 1 create table db_migration ( id integer not null, mtype...\r\n// HANG / FREEZE\r\n```\r\n\r\n\r\n**Output from CockroachDB**\r\n```\r\nI180312 12:37:00.155872 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT version() AS version"" {} 0.597 1 """"\r\nI180312 12:37:00.160290 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT current_schema()"" {} 0.170 1 """"\r\nI180312 12:37:00.167679 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""SELECT NULL AS table_cat, n.nspname AS table_schem, c.relname AS table_name, CASE (n.nspname ~ '^pg_') OR (n.nspname = 'information_schema') WHEN true THEN CASE WHEN (n.nspname = 'pg_catalog') OR (n.nspname = 'information_schema') THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TABLE' WHEN 'v' THEN 'SYSTEM VIEW' WHEN 'i' THEN 'SYSTEM INDEX' ELSE NULL END WHEN n.nspname = 'pg_toast' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TOAST TABLE' WHEN 'i' THEN 'SYSTEM TOAST INDEX' ELSE NULL END ELSE CASE c.relkind WHEN 'r' THEN 'TEMPORARY TABLE' WHEN 'p' THEN 'TEMPORARY TABLE' WHEN 'i' THEN 'TEMPORARY INDEX' WHEN 'S' THEN 'TEMPORARY SEQUENCE' WHEN 'v' THEN 'TEMPORARY VIEW' ELSE NULL END END WHEN false THEN CASE c.relkind WHEN 'r' THEN 'TABLE' WHEN 'p' THEN 'TABLE' WHEN 'i' THEN 'INDEX' WHEN 'S' THEN 'SEQUENCE' WHEN 'v' THEN 'VIEW' WHEN 'c' THEN 'TYPE' WHEN 'f' THEN 'FOREIGN TABLE' WHEN 'm' THEN 'MATERIALIZED VIEW' ELSE NULL END ELSE NULL END AS table_type, d.description AS remarks FROM pg_catalog.pg_namespace AS n, pg_catalog.pg_class AS c LEFT JOIN pg_catalog.pg_description AS d ON ((c.oid = d.objoid) AND (d.objsubid = 0)) LEFT JOIN pg_catalog.pg_class AS dc ON ((d.classoid = dc.oid) AND (dc.relname = 'pg_class')) LEFT JOIN pg_catalog.pg_namespace AS dn ON ((dn.oid = dc.relnamespace) AND (dn.nspname = 'pg_catalog')) WHERE ((c.relnamespace = n.oid) AND (n.nspname LIKE 'public')) AND (c.relname LIKE 'db_migration') ORDER BY table_type, table_schem, table_name"" {} 5.444 0 """"\r\nI180312 12:37:00.180320 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] internal-exec """" {} ""INSERT INTO system.public.eventlog(\\""timestamp\\"", \\""eventType\\"", \\""targetID\\"", \\""reportingID\\"", info) VALUES (now(), $1, $2, $3, $4)"" {$1:""'create_table'"", $2:""69"", $3:""1"", $4:""'{\\""TableName\\"":\\""documentation.public.db_migration\\"",\\""Statement\\"":\\""CREATE TABLE db_migration (id INTEGER NOT NULL, mtype VARCHAR(1) NOT NULL, mstatus VARCHAR(10) NOT NULL, mversion VARCHAR(150) NOT NULL, mcomment VARCHAR(150) NOT NULL, mchecksum INTEGER NOT NULL, run_on TIMESTAMP NOT NULL, run_by VARCHAR(30) NOT NULL, run_time INTEGER NOT NULL, CONSTRAINT pk_db_migration PRIMARY KEY (id))\\"",\\""User\\"":\\""root\\""}'""} 2.577 1 """"\r\nI180312 12:37:00.180446 374 sql/exec_log.go:173  [n1,client=127.0.0.1:51280,user=root] exec ""documentation"" {} ""CREATE TABLE db_migration (id INTEGER NOT NULL, mtype VARCHAR(1) NOT NULL, mstatus VARCHAR(10) NOT NULL, mversion VARCHAR(150) NOT NULL, mcomment VARCHAR(150) NOT NULL, mchecksum INTEGER NOT NULL, run_on TIMESTAMP NOT NULL, run_by VARCHAR(30) NOT NULL, run_time INTEGER NOT NULL, CONSTRAINT pk_db_migration PRIMARY KEY (id))"" {} 6.210 0 """"\r\nI180312 12:37:00.491223 215 sql/exec_log.go:173  [n1] internal-exec """" {} ""SELECT count(version) FROM system.public.lease WHERE ((\\""descID\\"" = $1) AND (version = $2)) AND (expiration > $3)"" {$1:""68"", $2:""2"", $3:""'2018-03-12 12:37:00.490083+00:00'""} 1.086 1 """"\r\nI180312 12:37:09.212135 212 server/status/runtime.go:219  [n1] runtime stats: 68 MiB RSS, 157 goroutines, 74 MiB/31 MiB/122 MiB GO alloc/idle/total, 16 MiB/23 MiB CGO alloc/total, 122.70cgo/sec, 0.01/0.01 %(u/s)time, 0.00 %gc (1x)\r\nI180312 12:37:19.212465 212 server/status/runtime.go:219  [n1] runtime stats: 71 MiB RSS, 157 goroutines, 81 MiB/25 MiB/122 MiB GO alloc/idle/total, 16 MiB/23 MiB CGO alloc/total, 81.20cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:29.110967 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec """" {} ""SET extra_float_digits = 3"" {} 0.184 0 """"\r\nI180312 12:37:29.111435 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec ""documentation"" {} ""SET application_name = 'documentation'"" {} 0.119 0 """"\r\nI180312 12:37:29.112396 239 sql/exec_log.go:173  [n1,client=127.0.0.1:51284,user=root] exec ""documentation"" {} ""SET timezone = 'UTC'"" {} 0.251 0 """"\r\nI180312 12:37:29.211535 210 gossip/gossip.go:487  [n1] gossip status (ok, 1 node)\r\ngossip client (0/3 cur/max conns)\r\ngossip server (0/3 cur/max conns, infos 0/0 sent/received, bytes 0B/0B sent/received)\r\nI180312 12:37:29.211642 212 server/status/runtime.go:219  [n1] runtime stats: 74 MiB RSS, 159 goroutines, 87 MiB/19 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 79.71cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:29.216558 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.775 0 """"\r\nI180312 12:37:39.211863 212 server/status/runtime.go:219  [n1] runtime stats: 79 MiB RSS, 159 goroutines, 94 MiB/13 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 79.30cgo/sec, 0.01/0.01 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:49.212229 212 server/status/runtime.go:219  [n1] runtime stats: 81 MiB RSS, 159 goroutines, 100 MiB/7.1 MiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 80.70cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:59.210760 212 server/status/runtime.go:219  [n1] runtime stats: 85 MiB RSS, 159 goroutines, 107 MiB/696 KiB/122 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 74.21cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:37:59.217284 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.020 0 """"\r\nI180312 12:38:09.211651 212 server/status/runtime.go:219  [n1] runtime stats: 90 MiB RSS, 159 goroutines, 113 MiB/384 KiB/128 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 83.29cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:38:19.211624 212 server/status/runtime.go:219  [n1] runtime stats: 96 MiB RSS, 159 goroutines, 120 MiB/96 KiB/134 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 73.00cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nW180312 12:38:26.914324 374 internal/client/txn.go:531  [n1,client=127.0.0.1:51280,user=root] failure aborting transaction: HandledRetryableTxnError: TransactionAbortedError: txn aborted ""sql txn"" id=40e2c4ae key=/Table/SystemConfigSpan/Start rw=true pri=0.01057810 iso=SERIALIZABLE stat=PENDING epo=0 ts=1520858220.155526399,0 orig=1520858220.155526399,0 max=1520858220.655526399,0 wto=false rop=false seq=13; abort caused by: context canceled\r\nI180312 12:38:29.210026 210 gossip/gossip.go:487  [n1] gossip status (ok, 1 node)\r\ngossip client (0/3 cur/max conns)\r\ngossip server (0/3 cur/max conns, infos 0/0 sent/received, bytes 0B/0B sent/received)\r\nI180312 12:38:29.210128 212 server/status/runtime.go:219  [n1] runtime stats: 101 MiB RSS, 152 goroutines, 126 MiB/872 KiB/142 MiB GO alloc/idle/total, 16 MiB/24 MiB CGO alloc/total, 86.61cgo/sec, 0.01/0.00 %(u/s)time, 0.00 %gc (0x)\r\nI180312 12:38:29.218543 223 sql/exec_log.go:173  internal-exec """" {} ""SELECT id, payload FROM system.public.jobs WHERE status IN ($1, $2) ORDER BY created DESC"" {$1:""'pending'"", $2:""'running'""} 1.440 0 """"\r\n^CI180312 12:38:31.345681 1 cli/start.go:650  received signal 'interrupt'\r\nNote: a second interrupt will skip graceful shutdown and terminate forcefully\r\nI180312 12:38:31.345859 1 cli/start.go:701  initiating graceful shutdown of server\r\ninitiating graceful shutdown of server\r\nI180312 12:38:31.348153 474 sql/exec_log.go:173  internal-exec """" {} ""DELETE FROM system.public.lease WHERE (\\""descID\\"", version, \\""nodeID\\"", expiration) = ($1, $2, $3, $4)"" {$2:""1"", $3:""1"", $4:""'2018-03-12 12:43:14.26188+00:00'"", $1:""56""} 2.080 1 """"\r\nW180312 12:38:31.351204 22 vendor/google.golang.org/grpc/clientconn.go:1277  grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing\r\nI180312 12:38:31.351269 474 storage/engine/rocksdb.go:674  closing rocksdb instance at ""/Users/meyer_t/Downloads/cockroach-data/cockroach-temp010543778""\r\nW180312 12:38:31.351393 243 vendor/google.golang.org/grpc/clientconn.go:1158  grpc: addrConn.createTransport failed to connect to {anansi:26257 0  <nil>}. Err :connection error: desc = ""transport: Error while dialing cannot reuse client connection"". Reconnecting...\r\nW180312 12:38:31.351560 243 vendor/google.golang.org/grpc/clientconn.go:1277  grpc: addrConn.transportMonitor exits due to: context canceled\r\nI180312 12:38:31.352889 474 storage/engine/rocksdb.go:674  closing rocksdb instance at ""/Users/meyer_t/Downloads/cockroach-data""\r\nI180312 12:38:31.354248 1 cli/start.go:755  server drained and shutdown completed\r\nserver drained and shutdown completed\r\nI180312 12:38:31.354413 1 cli/error.go:109  interrupted\r\nError: interrupted\r\nFailed running ""start""\r\n```","sql\r\nCREATE TABLE db_migration (\r\n    id INTEGER NOT NULL,\r\n    mtype VARCHAR(1) NOT NULL,\r\n    mstatus VARCHAR(10) NOT NULL,\r\n    mversion VARCHAR(150) NOT NULL,\r\n    mcomment VARCHAR(150) NOT NULL,\r\n    mchecksum INTEGER NOT NULL,\r\n    run_on TIMESTAMP NOT NULL,\r\n    run_by VARCHAR(30) NOT NULL,\r\n    run_time INTEGER NOT NULL,\r\n\r\n    CONSTRAINT pk_db_migration PRIMARY KEY (id)\r\n);\r\n"
23700,"sql: an error on the 1PC code path for update, etc, may fail with an invalid txn state?Found the following code path:\r\n\r\n\r\n\r\nIs it guaranteed that  `CommitInBatch` never fails after the EndTxn batch command has been processed?\r\nOtherwise, `err` is not nil, but the txn is committed, and `ConvertBatchError` will fail with ""using txn in COMMITTED state"".\r\n\r\n@nvanbenschoten thoughts? This is low priority since I don't recall seeing this in the wild. Maybe my understanding is limited.",C-investigation,knz,"Found the following code path:\r\n\r\n```go\r\n  if autoCommit == autoCommitEnabled {\r\n    // An auto-txn can commit the transaction with the batch. This is an\r\n    // optimization to avoid an extra round-trip to the transaction\r\n    // coordinator.\r\n    err = tb.txn.CommitInBatch(ctx, tb.b)\r\n  } else {\r\n    err = tb.txn.Run(ctx, tb.b)\r\n  }\r\n\r\n  if err != nil {\r\n    return sqlbase.ConvertBatchError(ctx, tableDesc, tb.b)\r\n  }\r\n  return nil\r\n```\r\n\r\nIs it guaranteed that  `CommitInBatch` never fails after the EndTxn batch command has been processed?\r\nOtherwise, `err` is not nil, but the txn is committed, and `ConvertBatchError` will fail with ""using txn in COMMITTED state"".\r\n\r\n@nvanbenschoten thoughts? This is low priority since I don't recall seeing this in the wild. Maybe my understanding is limited.","go\r\n  if autoCommit == autoCommitEnabled {\r\n    // An auto-txn can commit the transaction with the batch. This is an\r\n    // optimization to avoid an extra round-trip to the transaction\r\n    // coordinator.\r\n    err = tb.txn.CommitInBatch(ctx, tb.b)\r\n  } else {\r\n    err = tb.txn.Run(ctx, tb.b)\r\n  }\r\n\r\n  if err != nil {\r\n    return sqlbase.ConvertBatchError(ctx, tableDesc, tb.b)\r\n  }\r\n  return nil\r\n"
23699,"sql: the updated values on ON CONFLICT do not get constraint-checked properlyConsider the following table:\r\n\r\n\r\n\r\nA simple INSERT statement fails, as it should:\r\n\r\n```\r\n> INSERT INTO ab(a,b) VALUES(1, 12312);\r\npq: failed to satisfy CHECK constraint (b < 1)\r\n```\r\n\r\nNow, the same with INSERT ... ON CONFLICT:\r\n\r\n\r\nThis succeeds!\r\nAnd results in a row that fails constraint validation:\r\n\r\n```\r\n> SELECT * FROM ab;\r\n+---+--------+\r\n| a |   b    |\r\n+---+--------+\r\n| 1 | 123132 |\r\n+---+--------+\r\n(1 row)\r\n```\r\n\r\nThis may or may not need to be fixed for 2.0, but certainly needs a note in docs as known limitation.\r\n",C-bug|docs-done|docs-known-limitation|backport-2.0.x|S-0-visible-logical-error|A-sql-mutations,emsal0,"Consider the following table:\r\n\r\n```sql\r\nCREATE TABLE ab(a INT PRIMARY KEY, b INT, CHECK (b < 1))\r\n```\r\n\r\nA simple INSERT statement fails, as it should:\r\n\r\n```\r\n> INSERT INTO ab(a,b) VALUES(1, 12312);\r\npq: failed to satisfy CHECK constraint (b < 1)\r\n```\r\n\r\nNow, the same with INSERT ... ON CONFLICT:\r\n```sql\r\n> INSERT INTO ab(a, b) VALUES (1,0); -- create some initial valid value\r\n> INSERT INTO ab(a, b) VALUES (1,0) ON CONFLICT(a) DO UPDATE SET b = 123132;\r\n```\r\n\r\nThis succeeds!\r\nAnd results in a row that fails constraint validation:\r\n\r\n```\r\n> SELECT * FROM ab;\r\n+---+--------+\r\n| a |   b    |\r\n+---+--------+\r\n| 1 | 123132 |\r\n+---+--------+\r\n(1 row)\r\n```\r\n\r\nThis may or may not need to be fixed for 2.0, but certainly needs a note in docs as known limitation.\r\n","sql\r\nCREATE TABLE ab(a INT PRIMARY KEY, b INT, CHECK (b < 1))\r\n"
23672,"sql: missing check on the unicity of SET LHS operands in ON CONFLICT DO UPDATE\r\n\r\nSucceeds, mistakenly, whereas `UPDATE ... SET x = 1, x = 1` properly fails saying the name `x` cannot be assigned twice.\r\n\r\nFound while working on #23373.",C-bug,knz,"```sql\r\nINSERT INTO ... ON CONFLICT ... DO UPDATE SET x = 1, x = 1\r\n```\r\n\r\nSucceeds, mistakenly, whereas `UPDATE ... SET x = 1, x = 1` properly fails saying the name `x` cannot be assigned twice.\r\n\r\nFound while working on #23373.","sql\r\nINSERT INTO ... ON CONFLICT ... DO UPDATE SET x = 1, x = 1\r\n"
23660,"sql: make UPSERT/INSERT ON CONFLICT DO UPDATE able to overwrite valuesThere's code currently in UPSERT/INSERT to refuse writing to the same PK twice, for example:\r\n\r\n(tablewriter.go)\r\n\r\n\r\nThis is, I believe,  `UPSERT INTO kv VALUES (1,1), (1,2)` should succeed and ensure the later value overwrites the earlier one.\r\n\r\nAlso, postgres accepts the following:\r\n```\r\ninsert into kv values (1,2), (1,3) on conflict(k) do update set k = excluded.k + (random()*100)::int;\r\n```\r\n\r\nbut cockroachdb refuses it with the error above. This is incorrect.\r\n",C-bug,knz,"There's code currently in UPSERT/INSERT to refuse writing to the same PK twice, for example:\r\n\r\n(tablewriter.go)\r\n```go\r\n  primaryKey, _, err := sqlbase.EncodeIndexKey(\r\n    tableDesc, &tableDesc.PrimaryIndex, tu.ri.InsertColIDtoRowIndex, row, tu.indexKeyPrefix)\r\n  if err != nil {\r\n    return nil, err\r\n  }\r\n  if _, ok := tu.fastPathKeys[string(primaryKey)]; ok {\r\n    return nil, fmt.Errorf(""UPSERT/ON CONFLICT DO UPDATE command cannot affect row a second time"")\r\n  }\r\n  tu.fastPathKeys[string(primaryKey)] = struct{}{}\r\n```\r\n\r\nThis is, I believe,  `UPSERT INTO kv VALUES (1,1), (1,2)` should succeed and ensure the later value overwrites the earlier one.\r\n\r\nAlso, postgres accepts the following:\r\n```\r\ninsert into kv values (1,2), (1,3) on conflict(k) do update set k = excluded.k + (random()*100)::int;\r\n```\r\n\r\nbut cockroachdb refuses it with the error above. This is incorrect.\r\n","go\r\n  primaryKey, _, err := sqlbase.EncodeIndexKey(\r\n    tableDesc, &tableDesc.PrimaryIndex, tu.ri.InsertColIDtoRowIndex, row, tu.indexKeyPrefix)\r\n  if err != nil {\r\n    return nil, err\r\n  }\r\n  if _, ok := tu.fastPathKeys[string(primaryKey)]; ok {\r\n    return nil, fmt.Errorf(""UPSERT/ON CONFLICT DO UPDATE command cannot affect row a second time"")\r\n  }\r\n  tu.fastPathKeys[string(primaryKey)] = struct{}{}\r\n"
23361,"sql: ON DELETE SET DEFAULT violates constraintsFound while investigating #22304.\r\n\r\n\r\n\r\nExpected, pg has this correct:\r\n```\r\npq: foreign key violation: value [123] not found in base@primary [x]\r\n```\r\n\r\nin CockroachDB, currently, incorrect: no error.\r\n\r\ncc @BramGruneir.\r\n\r\nI have a patch incoming.",C-bug,knz,"Found while investigating #22304.\r\n\r\n```sql\r\n create table base (x int primary key);\r\n create table derived(x int primary key, y int default 123 references base(x) on delete set default);\r\n insert into base values(456);\r\n insert into derived values (111, 456);\r\n\r\n --- bang:\r\n delete from base where true;\r\n```\r\n\r\nExpected, pg has this correct:\r\n```\r\npq: foreign key violation: value [123] not found in base@primary [x]\r\n```\r\n\r\nin CockroachDB, currently, incorrect: no error.\r\n\r\ncc @BramGruneir.\r\n\r\nI have a patch incoming.","sql\r\n create table base (x int primary key);\r\n create table derived(x int primary key, y int default 123 references base(x) on delete set default);\r\n insert into base values(456);\r\n insert into derived values (111, 456);\r\n\r\n --- bang:\r\n delete from base where true;\r\n"
23286,"roachtest: table creation stress testWe should put the investigation in https://github.com/cockroachdb/cockroach/issues/23254 into a roachtest. Here's the relevant bit of code courtesy of @andreimatei  (no need to transcribe this verbatim at all, should be enough to follow it in spirit):\r\n\r\n",C-enhancement|help wanted|E-easy|A-schema-changes,spaskob,"We should put the investigation in https://github.com/cockroachdb/cockroach/issues/23254 into a roachtest. Here's the relevant bit of code courtesy of @andreimatei  (no need to transcribe this verbatim at all, should be enough to follow it in spirit):\r\n\r\n```go\r\nfunc TestParallelDropCreateTables(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\r\n\t// This number has to be around 10 or else testrace will take too long to\r\n\t// finish.\r\n\tconst numberOfNodes = 3\r\n\tconst workers = 60\r\n\tconst repeat = 1000\r\n\r\n\ttc := testcluster.StartTestCluster(t, numberOfNodes, base.TestClusterArgs{})\r\n\tdefer tc.Stopper().Stop(context.TODO())\r\n\r\n\tif _, err := tc.ServerConn(0).Exec(`CREATE DATABASE ""test""`); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tvar wgStart sync.WaitGroup\r\n\tvar wgEnd sync.WaitGroup\r\n\twgStart.Add(workers)\r\n\twgEnd.Add(workers)\r\n\tsignal := make(chan struct{})\r\n\tfor i := 0; i < workers; i++ {\r\n\t\tdb := tc.ServerConn(i % numberOfNodes)\r\n\t\tgo createTestTable2(t, tc, i, repeat, db, &wgStart, &wgEnd, signal)\r\n\t}\r\n\r\n\t// Wait until all goroutines are ready.\r\n\twgStart.Wait()\r\n\t// Signal the create table goroutines to start.\r\n\tclose(signal)\r\n\t// Wait until all create tables are finished.\r\n\twgEnd.Wait()\r\n\r\n}\r\nfunc createTestTable2(\r\n\tt *testing.T,\r\n\ttc *testcluster.TestCluster,\r\n\tid int,\r\n\trepeat int,\r\n\tdb *gosql.DB,\r\n\twgStart *sync.WaitGroup,\r\n\twgEnd *sync.WaitGroup,\r\n\tsignal chan struct{},\r\n) {\r\n\tdefer wgEnd.Done()\r\n\r\n\twgStart.Done()\r\n\t<-signal\r\n\r\n\tfor i := 0; i < repeat; i++ {\r\n\t\tlog.Infof(context.TODO(), ""!!! worker %d loop %d"", id, i)\r\n\t\ttableName := fmt.Sprintf(""%d_%d"", id, i)\r\n\t\ttableSQL := fmt.Sprintf(`\r\n\t\tCREATE TABLE ""test"".""table_%s"" (\r\n\t\t\tid INT PRIMARY KEY,\r\n\t\t\tval INT\r\n\t\t)`, tableName)\r\n\t\t// if i%2 == 0 {\r\n\t\t//   txn, err := db.Begin()\r\n\t\t//   if err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created. begin err: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t//   if _, err := txn.Exec(tableSQL); err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t//   if err := txn.Commit(); err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created. commit err: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t// } else {\r\n\t\tif _, err := db.Exec(tableSQL); err != nil {\r\n\t\t\tt.Errorf(""table %d: could not be created: %s"", id, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// }\r\n\t}\r\n}\r\n\r\n```","go\r\nfunc TestParallelDropCreateTables(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\r\n\t// This number has to be around 10 or else testrace will take too long to\r\n\t// finish.\r\n\tconst numberOfNodes = 3\r\n\tconst workers = 60\r\n\tconst repeat = 1000\r\n\r\n\ttc := testcluster.StartTestCluster(t, numberOfNodes, base.TestClusterArgs{})\r\n\tdefer tc.Stopper().Stop(context.TODO())\r\n\r\n\tif _, err := tc.ServerConn(0).Exec(`CREATE DATABASE ""test""`); err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\r\n\tvar wgStart sync.WaitGroup\r\n\tvar wgEnd sync.WaitGroup\r\n\twgStart.Add(workers)\r\n\twgEnd.Add(workers)\r\n\tsignal := make(chan struct{})\r\n\tfor i := 0; i < workers; i++ {\r\n\t\tdb := tc.ServerConn(i % numberOfNodes)\r\n\t\tgo createTestTable2(t, tc, i, repeat, db, &wgStart, &wgEnd, signal)\r\n\t}\r\n\r\n\t// Wait until all goroutines are ready.\r\n\twgStart.Wait()\r\n\t// Signal the create table goroutines to start.\r\n\tclose(signal)\r\n\t// Wait until all create tables are finished.\r\n\twgEnd.Wait()\r\n\r\n}\r\nfunc createTestTable2(\r\n\tt *testing.T,\r\n\ttc *testcluster.TestCluster,\r\n\tid int,\r\n\trepeat int,\r\n\tdb *gosql.DB,\r\n\twgStart *sync.WaitGroup,\r\n\twgEnd *sync.WaitGroup,\r\n\tsignal chan struct{},\r\n) {\r\n\tdefer wgEnd.Done()\r\n\r\n\twgStart.Done()\r\n\t<-signal\r\n\r\n\tfor i := 0; i < repeat; i++ {\r\n\t\tlog.Infof(context.TODO(), ""!!! worker %d loop %d"", id, i)\r\n\t\ttableName := fmt.Sprintf(""%d_%d"", id, i)\r\n\t\ttableSQL := fmt.Sprintf(`\r\n\t\tCREATE TABLE ""test"".""table_%s"" (\r\n\t\t\tid INT PRIMARY KEY,\r\n\t\t\tval INT\r\n\t\t)`, tableName)\r\n\t\t// if i%2 == 0 {\r\n\t\t//   txn, err := db.Begin()\r\n\t\t//   if err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created. begin err: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t//   if _, err := txn.Exec(tableSQL); err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t//   if err := txn.Commit(); err != nil {\r\n\t\t//     t.Errorf(""table %d: could not be created. commit err: %s"", id, err)\r\n\t\t//     panic(""!!!"")\r\n\t\t//     return\r\n\t\t//   }\r\n\t\t// } else {\r\n\t\tif _, err := db.Exec(tableSQL); err != nil {\r\n\t\t\tt.Errorf(""table %d: could not be created: %s"", id, err)\r\n\t\t\treturn\r\n\t\t}\r\n\t\t// }\r\n\t}\r\n}\r\n\r\n"
22431,"sql: UNION ALL with NULL in first select clause.\r\n\r\nThis is the case even if you specify the type of the null with `::INT` or `:::INT`.\r\n\r\nThere are 2 main problems here (in @knz words):\r\n1) the union plan constructor must check that the column types on both sides of the union are compatible (proximate problem) \r\n2) (general protection) the executor must reject any plan that reports NULL for its top-level columns\r\n\r\nIf the order of the selects are swapped, we get this instead, which looks correct:\r\n```\r\nSELECT x, pg_typeof(y) FROM (                                                                                                                                          \r\n  SELECT 3, 4                                                                                                                                                                         UNION ALL                                                                                                                                                                               \r\n  SELECT 3, NULL                                                                                                                                                                      ) AS t(x,y);\r\n\r\n+---+-----------+\r\n| x | pg_typeof |\r\n+---+-----------+\r\n| 3 | int       |\r\n| 3 | NULL      |\r\n+---+-----------+\r\n(2 rows)\r\n```",C-bug,jordanlewis|nvanbenschoten,"```sql\r\nSELECT x, pg_typeof(y) FROM (\r\n  SELECT 3, NULL \r\nUNION ALL\r\n  SELECT 3, 4\r\n) AS t(x,y);\r\n\r\n\r\nReturns:\r\n+---+-----------+\r\n| x | pg_typeof |\r\n+---+-----------+\r\n| 3 | NULL      |\r\n| 3 | NULL      |\r\n+---+-----------+\r\n(2 rows)\r\n```\r\n\r\nThis is the case even if you specify the type of the null with `::INT` or `:::INT`.\r\n\r\nThere are 2 main problems here (in @knz words):\r\n1) the union plan constructor must check that the column types on both sides of the union are compatible (proximate problem) \r\n2) (general protection) the executor must reject any plan that reports NULL for its top-level columns\r\n\r\nIf the order of the selects are swapped, we get this instead, which looks correct:\r\n```\r\nSELECT x, pg_typeof(y) FROM (                                                                                                                                          \r\n  SELECT 3, 4                                                                                                                                                                         UNION ALL                                                                                                                                                                               \r\n  SELECT 3, NULL                                                                                                                                                                      ) AS t(x,y);\r\n\r\n+---+-----------+\r\n| x | pg_typeof |\r\n+---+-----------+\r\n| 3 | int       |\r\n| 3 | NULL      |\r\n+---+-----------+\r\n(2 rows)\r\n```","sql\r\nSELECT x, pg_typeof(y) FROM (\r\n  SELECT 3, NULL \r\nUNION ALL\r\n  SELECT 3, 4\r\n) AS t(x,y);\r\n\r\n\r\nReturns:\r\n+---+-----------+\r\n| x | pg_typeof |\r\n+---+-----------+\r\n| 3 | NULL      |\r\n| 3 | NULL      |\r\n+---+-----------+\r\n(2 rows)\r\n"
22208,"sql: bracket subquery results in wrong transaction status**BUG REPORT**\r\n\r\n## Reproduction\r\n\r\n\r\n\r\n*Expected result:* foo contains a single row with id 2.\r\n\r\n*Actual result:* the following error.\r\n\r\n```\r\npq: internal/client/txn.go:885: attempting to use transaction with wrong status or finalized: COMMITTED true\r\n```\r\n\r\n*Potentially useful note:* it seems like a bug in [bracket subqueries](https://www.cockroachlabs.com/docs/stable/table-expressions.html#using-the-output-of-other-statements), as the following simpler query has an incorrect result but no errors:\r\n\r\n\r\n\r\n## Configuration\r\n\r\n```\r\n$ grep -F '[config]' node0/logs/cockroach.log\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] file created at: 2018/01/26 15:25:11\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] running on machine: ip-192-168-1-157\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] arguments: [cockroach start --insecure --store node0]\r\nbenchmark master\r\n$ grep -F '[config]' node1/logs/cockroach.log\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] file created at: 2018/01/29 14:45:35\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] running on machine: ip-192-168-1-157\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] arguments: [cockroach start --insecure --store node1 --port 26258 --http-port 8081 --join localhost:26257]\r\nbenchmark master\r\n$ grep -F '[config]' node2/logs/cockroach.log\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] file created at: 2018/01/27 08:10:08\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] running on machine: ip-192-168-1-157\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] arguments: [cockroach start --insecure --store node2 --port 26259 --http-port 8082 --join localhost:26257]\r\n```",C-bug|S-3-ux-surprise,knz,"**BUG REPORT**\r\n\r\n## Reproduction\r\n\r\n```sql\r\nCREATE TABLE foo (id INT);\r\nUPDATE foo SET id = 2 WHERE id = (SELECT id FROM [INSERT INTO foo(id) VALUES (1) RETURNING id]);\r\n```\r\n\r\n*Expected result:* foo contains a single row with id 2.\r\n\r\n*Actual result:* the following error.\r\n\r\n```\r\npq: internal/client/txn.go:885: attempting to use transaction with wrong status or finalized: COMMITTED true\r\n```\r\n\r\n*Potentially useful note:* it seems like a bug in [bracket subqueries](https://www.cockroachlabs.com/docs/stable/table-expressions.html#using-the-output-of-other-statements), as the following simpler query has an incorrect result but no errors:\r\n\r\n```sql\r\nCREATE TABLE foo (id INT);\r\nSELECT id FROM [INSERT INTO foo(id) VALUES (1) RETURNING id] LIMIT 1;\r\n+----+\r\n| id |\r\n+----+\r\n|  1 |\r\n+----+\r\nSELECT * FROM foo;\r\n+----+\r\n| id |\r\n+----+\r\n+----+\r\n```\r\n\r\n## Configuration\r\n\r\n```\r\n$ grep -F '[config]' node0/logs/cockroach.log\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] file created at: 2018/01/26 15:25:11\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] running on machine: ip-192-168-1-157\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180126 15:25:11.916087 1 util/log/clog.go:1031  [config] arguments: [cockroach start --insecure --store node0]\r\nbenchmark master\r\n$ grep -F '[config]' node1/logs/cockroach.log\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] file created at: 2018/01/29 14:45:35\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] running on machine: ip-192-168-1-157\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180129 14:45:35.336207 32031545 util/log/clog.go:928  [config] arguments: [cockroach start --insecure --store node1 --port 26258 --http-port 8081 --join localhost:26257]\r\nbenchmark master\r\n$ grep -F '[config]' node2/logs/cockroach.log\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] file created at: 2018/01/27 08:10:08\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] running on machine: ip-192-168-1-157\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] binary: CockroachDB CCL v1.1.4 (darwin amd64, built 2018/01/08 18:25:09, go1.9.2)\r\nI180127 08:10:08.201495 49 util/log/clog.go:928  [config] arguments: [cockroach start --insecure --store node2 --port 26259 --http-port 8082 --join localhost:26257]\r\n```",sql\r\nCREATE TABLE foo (id INT);\r\nUPDATE foo SET id = 2 WHERE id = (SELECT id FROM [INSERT INTO foo(id) VALUES (1) RETURNING id]);\r\n
21300,"sql: checkResultType called for *each* result rowThis is unneeded:\r\n\r\n\r\nThe code for initStatementResult and prepare is simpler and should be applied above as well: **check the result column types, not each row of results**\r\n\r\n\r\n\r\n\r\n\r\ncc @jordanlewis ",C-performance,knz,"This is unneeded:\r\n```go\r\n  case tree.Rows:\r\n    err := forEachRow(params, plan, func(values tree.Datums) error {\r\n      for _, val := range values {\r\n        if err := checkResultType(val.ResolvedType()); err != nil {\r\n          return err\r\n        }\r\n      }\r\n      return rowResultWriter.AddRow(ctx, values)\r\n    })\r\n```\r\n\r\nThe code for initStatementResult and prepare is simpler and should be applied above as well: **check the result column types, not each row of results**\r\n\r\n```go\r\n  if err := planner.prepare(session.Ctx(), stmt.AST); err != nil {\r\n    return nil, err\r\n  }\r\n  // [...]\r\n  prepared.Columns = planColumns(plan)\r\n  for _, c := range prepared.Columns {\r\n    if err := checkResultType(c.Typ); err != nil {\r\n      return nil, err\r\n    }\r\n  }\r\n```\r\n\r\n```go\r\n// initStatementResult initializes res according to a query.\r\n//\r\n// cols represents the columns of the result rows. Should be nil if\r\n// stmt.AST.StatementType() != tree.Rows.\r\nfunc initStatementResult(res StatementResult, stmt Statement, cols sqlbase.ResultColumns) error {\r\n  stmtAst := stmt.AST\r\n  res.BeginResult(stmtAst)\r\n  res.SetColumns(cols)\r\n  for _, c := range cols {\r\n    if err := checkResultType(c.Typ); err != nil {\r\n      return err\r\n    }\r\n  }\r\n  return nil\r\n}\r\n\r\n```\r\n\r\ncc @jordanlewis ","go\r\n  case tree.Rows:\r\n    err := forEachRow(params, plan, func(values tree.Datums) error {\r\n      for _, val := range values {\r\n        if err := checkResultType(val.ResolvedType()); err != nil {\r\n          return err\r\n        }\r\n      }\r\n      return rowResultWriter.AddRow(ctx, values)\r\n    })\r\n"
21146,"storage: *roachpb.RaftGroupDeletedError for RHS in splitPostApply\n\nhttps://sentry.io/cockroach-labs/cockroachdb/issues/426530382/\n\n```\n*log.safeError: store.go:1820: *roachpb.RaftGroupDeletedError\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_proposal.go"", line 718, in handleReplicatedEvalResult\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_proposal.go"", line 981, in handleEvalResultRaftMuLocked\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 4480, in processRaftCommand\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 3481, in handleRaftReadyRaftMuLocked\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 3173, in handleRaftReady\n...\n(4 additional frame(s) were not displayed)\n\nstore.go:1820: *roachpb.RaftGroupDeletedError\n```",C-bug|S-2-temp-unavailability|A-kv-replication,tbg,"```go\nfunc splitPostApply(\n\tctx context.Context, deltaMS enginepb.MVCCStats, split *roachpb.SplitTrigger, r *Replica,\n) {\n\t// The right hand side of the split was already created (and its raftMu\n\t// acquired) in Replica.acquireSplitLock. It must be present here.\n\trightRng, err := r.store.GetReplica(split.RightDesc.RangeID)\n\tif err != nil {\n\t\tlog.Fatalf(ctx, ""unable to find RHS replica: %s"", err)\n\t}\n\t{\n\t\trightRng.mu.Lock()\n\t\t// Already holding raftMu, see above.\n\t\terr := rightRng.initRaftMuLockedReplicaMuLocked(&split.RightDesc, r.store.Clock(), 0)\n\t\trightRng.mu.Unlock()\n\t\tif err != nil {\n\t\t\tlog.Fatal(ctx, err) <--\n\t\t}\n\t}\n```\n\nhttps://sentry.io/cockroach-labs/cockroachdb/issues/426530382/\n\n```\n*log.safeError: store.go:1820: *roachpb.RaftGroupDeletedError\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_proposal.go"", line 718, in handleReplicatedEvalResult\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_proposal.go"", line 981, in handleEvalResultRaftMuLocked\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 4480, in processRaftCommand\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 3481, in handleRaftReadyRaftMuLocked\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 3173, in handleRaftReady\n...\n(4 additional frame(s) were not displayed)\n\nstore.go:1820: *roachpb.RaftGroupDeletedError\n```","go\nfunc splitPostApply(\n\tctx context.Context, deltaMS enginepb.MVCCStats, split *roachpb.SplitTrigger, r *Replica,\n) {\n\t// The right hand side of the split was already created (and its raftMu\n\t// acquired) in Replica.acquireSplitLock. It must be present here.\n\trightRng, err := r.store.GetReplica(split.RightDesc.RangeID)\n\tif err != nil {\n\t\tlog.Fatalf(ctx, ""unable to find RHS replica: %s"", err)\n\t}\n\t{\n\t\trightRng.mu.Lock()\n\t\t// Already holding raftMu, see above.\n\t\terr := rightRng.initRaftMuLockedReplicaMuLocked(&split.RightDesc, r.store.Clock(), 0)\n\t\trightRng.mu.Unlock()\n\t\tif err != nil {\n\t\t\tlog.Fatal(ctx, err) <--\n\t\t}\n\t}\n"
20875,"sql: sparse partitionings should use minimal number of rangesCurrently the following table\r\n\r\n\r\n\r\nwill create seven ranges, but it only needs to create three.",A-partitioning,danhhz,"Currently the following table\r\n\r\n```sql\r\nCREATE TABLE ... PARTITION BY LIST (pkey) (\r\n  PARTITION p VALUES IN (1, 3, 5)\r\n)\r\n```\r\n\r\nwill create seven ranges, but it only needs to create three.","sql\r\nCREATE TABLE ... PARTITION BY LIST (pkey) (\r\n  PARTITION p VALUES IN (1, 3, 5)\r\n)\r\n"
20870,"sql: collated string error messages are confusing\r\n\r\nThis error message is much nicer in other situations (like directly inserting a datum of the wrong collation), suggesting that the error message logic is too high in the stack and needs to be plumbed lower down.\r\n",A-partitioning,benesch,"```sql\r\n> create table a(k text collate en primary key) partition by range(k) (partition k values < 'foo' collate de);\r\npq: PARTITION k: value type collatedstring{de} doesn't match type COLLATEDSTRING of column ""k""\r\n```\r\n\r\nThis error message is much nicer in other situations (like directly inserting a datum of the wrong collation), suggesting that the error message logic is too high in the stack and needs to be plumbed lower down.\r\n","sql\r\n> create table a(k text collate en primary key) partition by range(k) (partition k values < 'foo' collate de);\r\npq: PARTITION k: value type collatedstring{de} doesn't match type COLLATEDSTRING of column ""k""\r\n"
20869,"sql: implement sparse range partitioningThe current range partitioning syntax\r\n\r\n\r\n\r\nallows exactly one (optional) gap in the partitioning scheme, between the last declared partition and the maximum possible value. This is arbitrary, and a result of the syntactical requirement of specifying partition boundaries in strictly increasing order. It's quite confusing when partitioning on a descending index, and nearly impossible to make sensible error messages as a result. \r\n\r\nWe took this syntax from Oracle/MySQL and there's already evidence that users of those systems are confused by the requirement that partitions be declared in sorted order.\r\n\r\nThe new syntax will look something like this (not finalized!):\r\n\r\n",A-partitioning,benesch,"The current range partitioning syntax\r\n\r\n```sql\r\nCREATE TABLE ... PARTITION BY RANGE (pkey) (\r\n    PARTITION p1 VALUES < 10,\r\n    PARTITION p2 VALUES < MAXVALUE\r\n)\r\n```\r\n\r\nallows exactly one (optional) gap in the partitioning scheme, between the last declared partition and the maximum possible value. This is arbitrary, and a result of the syntactical requirement of specifying partition boundaries in strictly increasing order. It's quite confusing when partitioning on a descending index, and nearly impossible to make sensible error messages as a result. \r\n\r\nWe took this syntax from Oracle/MySQL and there's already evidence that users of those systems are confused by the requirement that partitions be declared in sorted order.\r\n\r\nThe new syntax will look something like this (not finalized!):\r\n\r\n```sql\r\nCREATE TABLE ... PARTITION BY RANGE (pkey) (\r\n    PARTITION p1 VALUES BETWEEN 0 AND EXCLUSIVE 10,\r\n    PARTITION p2 VALUES BETWEEN 20 AND EXCLUSIVE 30\r\n)\r\n```","sql\r\nCREATE TABLE ... PARTITION BY RANGE (pkey) (\r\n    PARTITION p1 VALUES < 10,\r\n    PARTITION p2 VALUES < MAXVALUE\r\n)\r\n"
20488,"sql: DROP takes ~6min until async deletion begins@vivekmenezes pointed me in the general vicinity of this code:\r\n\r\n\r\n\r\nI'm not clear why that 360 second delay is introduced. Once a `DROP` is issued, the user expects data to be cleared out immediately. (and so did I, and it was confusing that nothing would happen).",C-bug,vivekmenezes,"@vivekmenezes pointed me in the general vicinity of this code:\r\n\r\n```go\r\nfunc (s *SchemaChangeManager) Start(stopper *stop.Stopper) {\r\n\tstopper.RunWorker(s.ambientCtx.AnnotateCtx(context.Background()), func(ctx context.Context) {\r\n\t\tdescKeyPrefix := keys.MakeTablePrefix(uint32(sqlbase.DescriptorTable.ID))\r\n\t\tgossipUpdateC := s.gossip.RegisterSystemConfigChannel()\r\n\t\ttimer := &time.Timer{}\r\n\t\tdelay := 360 * time.Second\r\n\t\tif s.testingKnobs.AsyncExecQuickly {\r\n\t\t\tdelay = 20 * time.Millisecond\r\n\t\t}\r\n```\r\n\r\nI'm not clear why that 360 second delay is introduced. Once a `DROP` is issued, the user expects data to be cleared out immediately. (and so did I, and it was confusing that nothing would happen).","go\r\nfunc (s *SchemaChangeManager) Start(stopper *stop.Stopper) {\r\n\tstopper.RunWorker(s.ambientCtx.AnnotateCtx(context.Background()), func(ctx context.Context) {\r\n\t\tdescKeyPrefix := keys.MakeTablePrefix(uint32(sqlbase.DescriptorTable.ID))\r\n\t\tgossipUpdateC := s.gossip.RegisterSystemConfigChannel()\r\n\t\ttimer := &time.Timer{}\r\n\t\tdelay := 360 * time.Second\r\n\t\tif s.testingKnobs.AsyncExecQuickly {\r\n\t\t\tdelay = 20 * time.Millisecond\r\n\t\t}\r\n"
20035,"sql: incorrect span generationTable schema:\r\n\r\n\r\n\r\nQuery:\r\n\r\n\r\n\r\nThis query produces the following plan:\r\n\r\n```\r\nLevel   Type    Field   Description\r\n0       sort\r\n0               order   -total\r\n1       group\r\n1               group by        @1-@1\r\n2       render\r\n3       scan\r\n3               table   favourites@favourites_glob_fav_idx\r\n3               spans   ""/""""GAME""""/""""web""""/""""MT""""/""""xxx""""/""""en_GB""""/""""ts3""""-/""""GAME""""/""""web""""/""""MT""""/""""xxx""""/""""en_GB""""/""""ts3\\x00""""""\r\n```\r\n\r\nThis is incorrect, because the spans here restrict the results to only `ts3`. There should be 3 spans instead.\r\n\r\n@RaduBerinde can you have a look?\r\n",C-bug,RaduBerinde,"Table schema:\r\n\r\n```sql\r\nCREATE TABLE favourites (\r\n\tid INT NOT NULL DEFAULT unique_rowid(),\r\n\tresource_type STRING(30) NOT NULL,\r\n\tresource_key STRING(255) NOT NULL,\r\n\tdevice_group STRING(30) NOT NULL,\r\n\tcustomerid INT NOT NULL,\r\n\tjurisdiction STRING(2) NOT NULL,\r\n\tbrand STRING(255) NOT NULL,\r\n\tcreated_ts TIMESTAMP NULL,\r\n\tguid_id STRING(100) NOT NULL,\r\n\tlocale STRING(10) NOT NULL DEFAULT NULL,\r\n\tCONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX favourites_idx (resource_type ASC, device_group ASC, resource_key ASC, customerid ASC),\r\n\tINDEX favourites_guid_idx (guid_id ASC),\r\n\tINDEX favourites_glob_fav_idx (resource_type ASC, device_group ASC, jurisdiction ASC, brand ASC, locale ASC, resource_key ASC),\r\n\tFAMILY ""primary"" (id, resource_type, resource_key, device_group, customerid, jurisdiction, brand, created_ts, guid_id, locale)\r\n);\r\n```\r\n\r\nQuery:\r\n\r\n```sql\r\nSELECT\r\n  resource_key,\r\n  count(resource_key) total\r\nFROM favourites f1\r\nWHERE f1.jurisdiction   = 'MT'\r\n      AND   f1.brand          = 'xxx'\r\n      AND   f1.resource_type  = 'GAME'\r\n      AND   f1.device_group   = 'web'\r\n      AND   f1.locale         = 'en_GB'\r\n      AND   f1.resource_key IN ('ts', 'ts2', 'ts3')\r\nGROUP BY resource_key\r\nORDER BY total DESC;\r\n```\r\n\r\nThis query produces the following plan:\r\n\r\n```\r\nLevel   Type    Field   Description\r\n0       sort\r\n0               order   -total\r\n1       group\r\n1               group by        @1-@1\r\n2       render\r\n3       scan\r\n3               table   favourites@favourites_glob_fav_idx\r\n3               spans   ""/""""GAME""""/""""web""""/""""MT""""/""""xxx""""/""""en_GB""""/""""ts3""""-/""""GAME""""/""""web""""/""""MT""""/""""xxx""""/""""en_GB""""/""""ts3\\x00""""""\r\n```\r\n\r\nThis is incorrect, because the spans here restrict the results to only `ts3`. There should be 3 spans instead.\r\n\r\n@RaduBerinde can you have a look?\r\n","sql\r\nCREATE TABLE favourites (\r\n\tid INT NOT NULL DEFAULT unique_rowid(),\r\n\tresource_type STRING(30) NOT NULL,\r\n\tresource_key STRING(255) NOT NULL,\r\n\tdevice_group STRING(30) NOT NULL,\r\n\tcustomerid INT NOT NULL,\r\n\tjurisdiction STRING(2) NOT NULL,\r\n\tbrand STRING(255) NOT NULL,\r\n\tcreated_ts TIMESTAMP NULL,\r\n\tguid_id STRING(100) NOT NULL,\r\n\tlocale STRING(10) NOT NULL DEFAULT NULL,\r\n\tCONSTRAINT ""primary"" PRIMARY KEY (id ASC),\r\n\tUNIQUE INDEX favourites_idx (resource_type ASC, device_group ASC, resource_key ASC, customerid ASC),\r\n\tINDEX favourites_guid_idx (guid_id ASC),\r\n\tINDEX favourites_glob_fav_idx (resource_type ASC, device_group ASC, jurisdiction ASC, brand ASC, locale ASC, resource_key ASC),\r\n\tFAMILY ""primary"" (id, resource_type, resource_key, device_group, customerid, jurisdiction, brand, created_ts, guid_id, locale)\r\n);\r\n"
19305,"Wrong pgwire bytes during COPYHello @jordanlewis,\r\n\r\nI ping you on this issue because this is a very low level issue and I saw that you did a refactoring of COPY in https://github.com/cockroachdb/cockroach/commit/c814889106bf7d35cd57e6016a684c540fe9dcc7#diff-a1b0d50005718d307271ed63d499e976 so you will probably be the one with the best knowledge of what I am talking about, and I hopefully think this will help solve 2 low-level bugs in the pgwire COPY implementation.\r\n\r\nI tried doing a COPY .. FROM STDIN with one of my clients but observed weird things. Drilling down and going back to the psql tool, I found 2 issues (2 differences between the pgwire protocol as implemented by psql/postgres and psql/cockroach).\r\n\r\n**issue n\xb01:**\r\nIn the CopyInResponse message that cr sends to the client, cr considers that there are 0 columns `4700 0000 0700 0000` : meaning G (CopyInResponse) TEXT 0 Columns. This is not correct and it does not correspond to what the code tries to do in https://github.com/cockroachdb/cockroach/blob/19f1914fdd72de5dc5f3f2790916d429f4fdeda9/pkg/sql/pgwire/v3.go#L1087\r\n\r\nAfter drilling down a bit more in the code (I may be wrong because I am new to cockroach), it seems that `c.streamingState.columns` is empty.\r\n\r\nThe best culprit I found by reading the code would be that `initStatementResult` in https://github.com/cockroachdb/cockroach/blob/master/pkg/sql/executor.go#L2115 does not do its job because of the test.\r\n\r\n``\r\nparser.Rows is tested but it may be necessary to also test parser.copyIn so that columns is also filled in this case. IMHO it would explain why `c.streamingState.columns` is empty.\r\n\r\n\r\n\r\n\r\n**issue n\xb02:**\r\nAfter the statement is closed, cr should be sending a `CommandComplete` message. This is what is coded in https://github.com/cockroachdb/cockroach/blob/19f1914fdd72de5dc5f3f2790916d429f4fdeda9/pkg/sql/pgwire/v3.go#L562\r\n\r\nNevertheless, This `C` message is not sent.\r\n\r\nHere is a comparison of what I receive in a psql->postgres versus a psql->cockroach test\r\n\r\npsql->postgres: 4300 0000 0c43 4f50 5920 3336 005a 0000 0005 49 C....COPY 36.Z....I\r\npsql->cockroach: 0000 0549  Z....I\r\n\r\nso the `C` message is not sent.\r\n\r\nAs of now I don't have a code flow explanation of why this happens. I just observe on the wire (with tcpflow) that it is not sent and it initially triggered my drilling down because my client expects the CommandComplete message.\r\n\r\nMaybe you will have an idea of why this is not sent. Maybe this is a flush issue and that this message is lost, beeing overwritten by the MessageReady (Z) message (?)\r\n\r\nmy tests are done against version 1.1.0\r\n\r\nthanks for your help, and I hope that it is not a problem to contact you directly on this.\r\n\r\n",C-bug|O-community|C-question,andreimatei,"Hello @jordanlewis,\r\n\r\nI ping you on this issue because this is a very low level issue and I saw that you did a refactoring of COPY in https://github.com/cockroachdb/cockroach/commit/c814889106bf7d35cd57e6016a684c540fe9dcc7#diff-a1b0d50005718d307271ed63d499e976 so you will probably be the one with the best knowledge of what I am talking about, and I hopefully think this will help solve 2 low-level bugs in the pgwire COPY implementation.\r\n\r\nI tried doing a COPY .. FROM STDIN with one of my clients but observed weird things. Drilling down and going back to the psql tool, I found 2 issues (2 differences between the pgwire protocol as implemented by psql/postgres and psql/cockroach).\r\n\r\n**issue n\xb01:**\r\nIn the CopyInResponse message that cr sends to the client, cr considers that there are 0 columns `4700 0000 0700 0000` : meaning G (CopyInResponse) TEXT 0 Columns. This is not correct and it does not correspond to what the code tries to do in https://github.com/cockroachdb/cockroach/blob/19f1914fdd72de5dc5f3f2790916d429f4fdeda9/pkg/sql/pgwire/v3.go#L1087\r\n\r\nAfter drilling down a bit more in the code (I may be wrong because I am new to cockroach), it seems that `c.streamingState.columns` is empty.\r\n\r\nThe best culprit I found by reading the code would be that `initStatementResult` in https://github.com/cockroachdb/cockroach/blob/master/pkg/sql/executor.go#L2115 does not do its job because of the test.\r\n\r\n````go\r\n    if stmtAst.StatementType() == parser.Rows {\r\n\tcolumns := planColumns(plan)\r\n\tres.SetColumns(columns)\r\n````\r\nparser.Rows is tested but it may be necessary to also test parser.copyIn so that columns is also filled in this case. IMHO it would explain why `c.streamingState.columns` is empty.\r\n\r\n\r\n\r\n\r\n**issue n\xb02:**\r\nAfter the statement is closed, cr should be sending a `CommandComplete` message. This is what is coded in https://github.com/cockroachdb/cockroach/blob/19f1914fdd72de5dc5f3f2790916d429f4fdeda9/pkg/sql/pgwire/v3.go#L562\r\n\r\nNevertheless, This `C` message is not sent.\r\n\r\nHere is a comparison of what I receive in a psql->postgres versus a psql->cockroach test\r\n\r\npsql->postgres: 4300 0000 0c43 4f50 5920 3336 005a 0000 0005 49 C....COPY 36.Z....I\r\npsql->cockroach: 0000 0549  Z....I\r\n\r\nso the `C` message is not sent.\r\n\r\nAs of now I don't have a code flow explanation of why this happens. I just observe on the wire (with tcpflow) that it is not sent and it initially triggered my drilling down because my client expects the CommandComplete message.\r\n\r\nMaybe you will have an idea of why this is not sent. Maybe this is a flush issue and that this message is lost, beeing overwritten by the MessageReady (Z) message (?)\r\n\r\nmy tests are done against version 1.1.0\r\n\r\nthanks for your help, and I hope that it is not a problem to contact you directly on this.\r\n\r\n",go\r\n    if stmtAst.StatementType() == parser.Rows {\r\n\tcolumns := planColumns(plan)\r\n\tres.SetColumns(columns)\r\n
19141,"sql: primary keys cannot be changedIt is not currently possible to change a table's primary key, e.g. via\r\n\r\n\r\n\r\nThis becomes more important in 1.2 because the upcoming partitioning feature (#18683) will only allow partitioning by prefixes of the primary key. For example, suppose you want to partition this table by country:\r\n\r\n\r\n\r\nThe table needs to have a composite primary key that lists country first:\r\n\r\n\r\n\r\nThis essentially requires that the schema be designed with partitioning in mind from the get-go until we can alter primary keys.",C-enhancement|A-sql-pgcompat|A-partitioning|A-schema-changes|X-anchored-telemetry,awoods187,"It is not currently possible to change a table's primary key, e.g. via\r\n\r\n```sql\r\nALTER TABLE foo DROP CONSTRAINT old_pkey;\r\nALTER TABLE foo ADD PRIMARY KEY (new, pkey, cols)\r\n```\r\n\r\nThis becomes more important in 1.2 because the upcoming partitioning feature (#18683) will only allow partitioning by prefixes of the primary key. For example, suppose you want to partition this table by country:\r\n\r\n```sql\r\nCREATE TABLE users (\r\n  id INT PRIMARY KEY,\r\n  name STRING,\r\n  country STRING,\r\n  ...\r\n)\r\n```\r\n\r\nThe table needs to have a composite primary key that lists country first:\r\n\r\n```sql\r\nCREATE TABLE users (\r\n  id INT,\r\n  name STRING,\r\n  country STRING,\r\n  ...,\r\n  PRIMARY KEY (int, country)\r\n) PARTITION BY LIST (country)...\r\n```\r\n\r\nThis essentially requires that the schema be designed with partitioning in mind from the get-go until we can alter primary keys.","sql\r\nALTER TABLE foo DROP CONSTRAINT old_pkey;\r\nALTER TABLE foo ADD PRIMARY KEY (new, pkey, cols)\r\n"
19087,"sql: spurious ""unique constraint violation"" error on non-unique indexTable schema:\r\n\r\n\r\n\r\nThe user encounters the following error:\r\n```\r\nERROR: duplicate key value (send_timestamp)=(285826096097689601) violates unique constraint ""transaction_event_send_timestamp_idx""\r\n```\r\n\r\nwhen issuing this statement:\r\n```\r\nupdate transaction_event set send_id=?, send_timestamp=? where (send_id is null) and (id in (? , ? , ? , ? , ? , ? , ? , ?))\r\n```\r\nwith parameters (e.g.)\r\n```\r\n(b79195a0-5bd7-4bc5-ab74-526ea0e5a10d,2017-10-06 15:46:22.785,285826096097689601,285826096657727489,285826096989339650,285826097310597121,285826097641521155,285826097964285954,285826098301337602,285826098678267906)\r\n```\r\n\r\nSince the index is not unique the error should not be generated. However the function `ConvertBatchError` which generates the error is a bit convoluted and it may be that the message really means something different.\r\n\r\n@danhhz @jordanlewis any opinion? Is there any chance this spurious message could appear as a result of #18533?",C-bug,jordanlewis,"Table schema:\r\n\r\n```sql\r\nCREATE TABLE transaction_event (\r\n    id INT NOT NULL DEFAULT unique_rowid(),\r\n    message BYTES NOT NULL,\r\n    received INT NOT NULL,\r\n    username STRING(255) NOT NULL,\r\n    classname STRING(255) NOT NULL,\r\n    send_id STRING(255) NULL,\r\n    send_timestamp TIMESTAMP NULL,\r\n    CONSTRAINT pk_transaction_event PRIMARY KEY (id ASC),\r\n    INDEX transaction_event_send_timestamp_idx (send_timestamp ASC),\r\n    FAMILY ""primary"" (id, message, received, username, classname, send_id, send_timestamp)\r\n);\r\n```\r\n\r\nThe user encounters the following error:\r\n```\r\nERROR: duplicate key value (send_timestamp)=(285826096097689601) violates unique constraint ""transaction_event_send_timestamp_idx""\r\n```\r\n\r\nwhen issuing this statement:\r\n```\r\nupdate transaction_event set send_id=?, send_timestamp=? where (send_id is null) and (id in (? , ? , ? , ? , ? , ? , ? , ?))\r\n```\r\nwith parameters (e.g.)\r\n```\r\n(b79195a0-5bd7-4bc5-ab74-526ea0e5a10d,2017-10-06 15:46:22.785,285826096097689601,285826096657727489,285826096989339650,285826097310597121,285826097641521155,285826097964285954,285826098301337602,285826098678267906)\r\n```\r\n\r\nSince the index is not unique the error should not be generated. However the function `ConvertBatchError` which generates the error is a bit convoluted and it may be that the message really means something different.\r\n\r\n@danhhz @jordanlewis any opinion? Is there any chance this spurious message could appear as a result of #18533?","sql\r\nCREATE TABLE transaction_event (\r\n    id INT NOT NULL DEFAULT unique_rowid(),\r\n    message BYTES NOT NULL,\r\n    received INT NOT NULL,\r\n    username STRING(255) NOT NULL,\r\n    classname STRING(255) NOT NULL,\r\n    send_id STRING(255) NULL,\r\n    send_timestamp TIMESTAMP NULL,\r\n    CONSTRAINT pk_transaction_event PRIMARY KEY (id ASC),\r\n    INDEX transaction_event_send_timestamp_idx (send_timestamp ASC),\r\n    FAMILY ""primary"" (id, message, received, username, classname, send_id, send_timestamp)\r\n);\r\n"
18806,sql: erroneous auto-commit for sub-queryFound while experimenting with @robert-s-lee \r\n\r\n\r\n\r\nthe INSERT fails with:\r\n\r\n```\r\npq: internal/client/txn.go:856: attempting to use transaction with wrong status or finalized: COMMITTED true\r\n```\r\n\r\ncc @andreimatei - can you point in which direction we should be looking at to solve this?,C-bug,andreimatei,"Found while experimenting with @robert-s-lee \r\n\r\n```sql\r\nCREATE TABLE x (id1 INT, id2 INT, id3 serial);\r\ncreate table y (x int, y int, z int, s string);\r\nINSERT INTO y SELECT * FROM [INSERT INTO x VALUES (1, 1), (2, 2), (3, 3) RETURNING id1, id2, id3] AS a, (values('a')) AS b;\r\n```\r\n\r\nthe INSERT fails with:\r\n\r\n```\r\npq: internal/client/txn.go:856: attempting to use transaction with wrong status or finalized: COMMITTED true\r\n```\r\n\r\ncc @andreimatei - can you point in which direction we should be looking at to solve this?","sql\r\nCREATE TABLE x (id1 INT, id2 INT, id3 serial);\r\ncreate table y (x int, y int, z int, s string);\r\nINSERT INTO y SELECT * FROM [INSERT INTO x VALUES (1, 1), (2, 2), (3, 3) RETURNING id1, id2, id3] AS a, (values('a')) AS b;\r\n"
18500,"sql+cli: dump should work with PKs named other than ""primary""It looks like the primary key constraint name has to be ""primary"" for sql dump to work, however schema migration tools often have their own naming conventions e.g. liquibase does ""pk_[tablename]"" so this could be a somewhat common situation.\r\n\r\nSteps to reproduce:\r\n\r\n\r\n\r\nRunning `./cockroach dump liquibase --insecure` will now error with:\r\n\r\n```\r\nError: pq: column name ""rowid"" not found\r\n```\r\n\r\nEven though `./cockroach dump standard --insecure` works as expected\r\n\r\n(client and server version: v1.1-beta.20170907)",C-bug,mjibson|dianasaur323,"It looks like the primary key constraint name has to be ""primary"" for sql dump to work, however schema migration tools often have their own naming conventions e.g. liquibase does ""pk_[tablename]"" so this could be a somewhat common situation.\r\n\r\nSteps to reproduce:\r\n\r\n```sql\r\nCREATE DATABASE liquibase;\r\nCREATE TABLE liquibase.episodes (id INT NOT NULL, season INT NULL, num INT NULL, title STRING NULL, stardate DECIMAL NULL, CONSTRAINT ""pk_episodes"" PRIMARY KEY (id), FAMILY ""primary"" (id, season, num), FAMILY fam_1_title (title), FAMILY fam_2_stardate (stardate));\r\n\r\nCREATE DATABASE standard;\r\nCREATE TABLE standard.episodes (id INT NOT NULL, season INT NULL, num INT NULL, title STRING NULL, stardate DECIMAL NULL, CONSTRAINT ""primary"" PRIMARY KEY (id), FAMILY ""primary"" (id, season, num), FAMILY fam_1_title (title), FAMILY fam_2_stardate (stardate));\r\n```\r\n\r\nRunning `./cockroach dump liquibase --insecure` will now error with:\r\n\r\n```\r\nError: pq: column name ""rowid"" not found\r\n```\r\n\r\nEven though `./cockroach dump standard --insecure` works as expected\r\n\r\n(client and server version: v1.1-beta.20170907)","sql\r\nCREATE DATABASE liquibase;\r\nCREATE TABLE liquibase.episodes (id INT NOT NULL, season INT NULL, num INT NULL, title STRING NULL, stardate DECIMAL NULL, CONSTRAINT ""pk_episodes"" PRIMARY KEY (id), FAMILY ""primary"" (id, season, num), FAMILY fam_1_title (title), FAMILY fam_2_stardate (stardate));\r\n\r\nCREATE DATABASE standard;\r\nCREATE TABLE standard.episodes (id INT NOT NULL, season INT NULL, num INT NULL, title STRING NULL, stardate DECIMAL NULL, CONSTRAINT ""primary"" PRIMARY KEY (id), FAMILY ""primary"" (id, season, num), FAMILY fam_1_title (title), FAMILY fam_2_stardate (stardate));\r\n"
17883,"Queries take >10s when updating same row concurrentlyI have a table like this:\r\n\r\n\r\n\r\nI have tens of concurrent processes running this in a loop:\r\n\r\n\r\n\r\nThis starts out relatively fast but queries can take 10s of seconds after a bit. After killing these processes, operations on this table (even as simple as `SELECT count(*) FROM testtable`) remain slow for a very long time.\r\n\r\nI'm running single-node but I've also observed similar behavior on clusters. Config header:\r\n\r\n```\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] file created at: 2017/08/23 15:46:49\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] running on machine: jethroft\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] binary: CockroachDB CCL v1.0.4 (linux amd64, built 2017/07/27 17:54:36, go1.8.3)\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] arguments: [./cockroach start --insecure]\r\n```\r\n\r\nTest program below. Run with:\r\n```\r\n./test-cockroach.py --server localhost init\r\n./test-cockroach.py --server localhost run --rows 1 --max-processes 60\r\n```\r\nIf it doesn't get slow enough, try increasing the `num_updates` variable in the code or the max-processes number on the command line. With `num_updates=1` it usually finishes in a couple of minutes. With `num_updates=10` it can take a very long time to finish.\r\n\r\n",C-performance|O-community|A-kv-transactions,nvanbenschoten,"I have a table like this:\r\n\r\n```sql\r\nCREATE TABLE testtable (\r\n    lookup BYTES PRIMARY KEY,\r\n    name STRING,\r\n    field1 BYTES,\r\n    field2 BYTES,\r\n    blob BYTES\r\n)\r\n```\r\n\r\nI have tens of concurrent processes running this in a loop:\r\n\r\n```sql\r\nUPDATE testtable SET blob = 'some random value' WHERE lookup = 'one value'\r\n```\r\n\r\nThis starts out relatively fast but queries can take 10s of seconds after a bit. After killing these processes, operations on this table (even as simple as `SELECT count(*) FROM testtable`) remain slow for a very long time.\r\n\r\nI'm running single-node but I've also observed similar behavior on clusters. Config header:\r\n\r\n```\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] file created at: 2017/08/23 15:46:49\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] running on machine: jethroft\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] binary: CockroachDB CCL v1.0.4 (linux amd64, built 2017/07/27 17:54:36, go1.8.3)\r\nI170823 15:46:49.746002 1 util/log/clog.go:1011  [config] arguments: [./cockroach start --insecure]\r\n```\r\n\r\nTest program below. Run with:\r\n```\r\n./test-cockroach.py --server localhost init\r\n./test-cockroach.py --server localhost run --rows 1 --max-processes 60\r\n```\r\nIf it doesn't get slow enough, try increasing the `num_updates` variable in the code or the max-processes number on the command line. With `num_updates=1` it usually finishes in a couple of minutes. With `num_updates=10` it can take a very long time to finish.\r\n\r\n```python\r\n#!/usr/bin/python\r\n\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nfrom multiprocessing import Array, Process\r\nimport psycopg2\r\nfrom psycopg2.extras import LoggingConnection, LoggingCursor\r\nimport random\r\nimport six\r\nimport string\r\nimport sys\r\nimport time\r\nimport uuid\r\n\r\nletters = list(string.ascii_letters)\r\ndbname = ""ftxtest""\r\ntable_name = ""testtable""\r\n\r\ndefault_db_rows = 5000\r\n# Number of different sizes to stop and gather performance data.\r\nnum_measurements = 50\r\n\r\nname_len = 16\r\nmin_blob_len = 128\r\nmax_blob_len = 256\r\nnum_updates = 2\r\n\r\n# Maximum number of processes for multi-process test. (We use Python\r\n# multiprocessing instead of threading because Python threads can't\r\n# run concurrently very much due to the global interpreter lock).\r\ndefault_processes = 10\r\n\r\ndef random_string(len=name_len):\r\n    output = """"\r\n    for i in range(name_len):\r\n        output += random.choice(letters)\r\n    return output\r\n\r\ndef random_blob_len():\r\n    return random.randrange(min_blob_len, max_blob_len)\r\n\r\ndef random_blob():\r\n    return random_bytearray(random_blob_len())\r\n\r\ndef random_bytes(length):\r\n    for _ in xrange(length):\r\n        yield random.getrandbits(8)\r\n\r\ndef random_bytearray(length):\r\n    return bytearray(random_bytes(length))\r\n\r\ndef create_database(conn):\r\n    try:\r\n        cur = conn.cursor()\r\n        cur.execute(""create database {}"".format(dbname));\r\n        conn.commit()\r\n    except psycopg2.ProgrammingError as e:\r\n        if e.pgcode == ""42P04"":\r\n            print(""Database already exists, continuing"")\r\n        else:\r\n            raise\r\n\r\ndef create_table(conn, mysql=False):\r\n    try:\r\n        cur = conn.cursor()\r\n        if mysql:\r\n            binary = ""bytea""\r\n            text = ""varchar""\r\n        else:\r\n            binary = ""bytes""\r\n            text = ""string""\r\n\r\n        schema = (""lookup {binary} primary key, name {text}, field1 {binary}, "" +\r\n                  ""field2 {binary}, blob {binary}"").format(binary=binary, text=text)\r\n        cur.execute(""create table {} ({})"".format(table_name, schema))\r\n        conn.commit()\r\n    except psycopg2.ProgrammingError as e:\r\n        if e.pgcode == ""42P07"":\r\n            print(""Table already exists, continuing"")\r\n        else:\r\n            raise\r\n\r\ndef create_rows(conn, num_rows):\r\n    cur = conn.cursor()\r\n    rows = []\r\n    for i in range(num_rows):\r\n        lookup = str(uuid.uuid4())\r\n        name = random_string()\r\n        field1 = str(uuid.uuid4())\r\n        field2 = str(uuid.uuid4())\r\n        blob = psycopg2.Binary(random_blob())\r\n\r\n        cur.execute(""insert into {} "".format(table_name) +\r\n                    ""(lookup, name, field1, field2, blob) VALUES "" +\r\n                    ""(%s, %s, %s, %s, %s);"",\r\n                    (lookup, name, field1, field2, blob))\r\n\r\n        rows.append(lookup)\r\n\r\n    return rows\r\n\r\ndef first_server(args):\r\n    return args.server.split("","")[0]\r\n\r\ndef connect(server, args):\r\n    # To have the client log what it""s doing, run with the following code:\r\n    #conn = psycopg2.connect(dbname=dbname, host=args.server, port=args.port,\r\n    #                        user=""root"", connection_factory=LoggingConnection,\r\n    #                        cursor_factory=LoggingCursor)\r\n    #conn.initialize(open(""/tmp/connection"", ""w""))\r\n    #return conn\r\n    return psycopg2.connect(dbname=dbname, host=server, port=args.port,\r\n                            user=args.user, password=args.password)\r\n        \r\ndef init(args):\r\n    create_database(connect(first_server(args), args))\r\n    create_table(connect(first_server(args), args), mysql=args.mysql)\r\n\r\ndef init_table(args):\r\n    create_table(connect(first_server(args), args), mysql=args.mysql)\r\n\r\ndef time_updates(cur, conn, rows, num_rows, num_updates):\r\n    total = 0.0\r\n    for i in range(num_updates):\r\n        row_idx = random.randrange(num_rows)\r\n\r\n        lookup = rows[row_idx]\r\n        # Leave everything else the same, but update the blob.\r\n        blob = random_blob()\r\n\r\n        start = time.time()\r\n        cur.execute(""update {} set blob = %s where lookup = %s;"".format(table_name),\r\n                    (blob, lookup))\r\n        end = time.time()\r\n        print(""."", end="""")\r\n        sys.stdout.flush()\r\n        total += end - start\r\n    return total / num_updates\r\n\r\ndef update_rows_worker(args, server, rows, num_rows, num_updates, index, output):\r\n    conn = connect(server, args)\r\n    conn.autocommit = True\r\n    cur = conn.cursor()\r\n\r\n    output[index] = time_updates(cur, conn, rows, num_rows, num_updates)\r\n\r\ndef run_multiprocess_test(args, rows, num_rows, num_processes):\r\n    servers = args.server.split("","")\r\n    output = Array(""f"", num_processes)\r\n    processes = []\r\n    for i in xrange(num_processes):\r\n        server = servers[i % len(servers)]\r\n        processes.append(Process(target=update_rows_worker,\r\n                                 args=(args, server, rows, num_rows,\r\n                                       num_updates, i, output)))\r\n    for process in processes:\r\n        process.start()\r\n\r\n    for process in processes:\r\n        process.join();\r\n\r\n    total = 0.0\r\n    for average in output:\r\n        total += average\r\n\r\n    print(""{},{}"".format(num_processes, total / num_processes))\r\n\r\n        \r\ndef run(args):\r\n    conn = connect(first_server(args), args)\r\n    conn.autocommit = True\r\n    cur = conn.cursor()\r\n\r\n    cur.execute(""delete from {};"".format(table_name))\r\n    \r\n    rows = []\r\n    num_rows = len(rows)\r\n\r\n    increment = max(1, args.rows / num_measurements)\r\n\r\n    while True:\r\n        if num_rows >= args.rows:\r\n            break\r\n        # Add the new rows to the end of the list.\r\n        rows.extend(create_rows(conn, increment))\r\n        num_rows = len(rows)\r\n\r\n        if not args.skip_scaling:\r\n            average = time_updates(cur, conn, rows,\r\n                                   num_rows, num_updates)\r\n\r\n            print(""{},{}"".format(num_rows, average))\r\n        else:\r\n            print(""{} rows created"".format(num_rows))\r\n\r\n    print(""Multiprocess latencies:"")\r\n    for i in [args.max_processes + 1]:\r\n        run_multiprocess_test(args, rows, num_rows, i)\r\n        \r\n        \r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(prog=""python-test-cockroach.py"")\r\n    parser.add_argument(""--server"", required=True,\r\n                        help=""Cockroach server to connect to"")\r\n    parser.add_argument(""--port"", type=int, default=26257,\r\n                        help=""Port that cockroach is listening on"")\r\n    parser.add_argument(""--user"", default=""root"",\r\n                        help=""User to use to log into postgresql"")\r\n    parser.add_argument(""--password"", default=None,\r\n                        help=""Password to use to log into postgresql"")\r\n    parser.add_argument(""--mysql"", action=""store_true"", default=False,\r\n                        help=""Run with mysql"")\r\n    subparsers = parser.add_subparsers()\r\n    init_parser = subparsers.add_parser(""init"", help=""initialize the database"")\r\n    init_parser.set_defaults(func=init)\r\n    init_table_parser = subparsers.add_parser(""inittable"",\r\n                                              help=""initialize the db table"")\r\n    init_table_parser.set_defaults(func=init_table)\r\n    run_parser = subparsers.add_parser(""run"", help=""run test"")\r\n    run_parser.set_defaults(func=run)\r\n    run_parser.add_argument(""--rows"", help=""Maximum number of rows to test"",\r\n                            type=int, default=default_db_rows)\r\n    run_parser.add_argument(""--skip-scaling"", action=""store_true"", default=False,\r\n                            help=""Skip scaling measurements, go straight to MP"")\r\n    run_parser.add_argument(""--max-processes"", default=default_processes,\r\n                            type=int,\r\n                            help=""Maximum number of processes to test"")\r\n\r\n    args = parser.parse_args()\r\n    args.func(args)\r\n```","sql\r\nCREATE TABLE testtable (\r\n    lookup BYTES PRIMARY KEY,\r\n    name STRING,\r\n    field1 BYTES,\r\n    field2 BYTES,\r\n    blob BYTES\r\n)\r\n"
17504,"sql: tracing of parallel statements is hard to readExample:\r\n\r\n\r\nOutputs:\r\n\r\n```\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n|      age       |                                  message                                   |  span  |\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n| 0s             | === SPAN START: sql txn implicit ===                                       | (0,0)  |\r\n| 137\xb5s609ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (0,0)  |\r\n|                | txn: ""sql txn implicit"" id=<nil> key=/Min                                  |        |\r\n| 152\xb5s217ns     | === SPAN START: sql txn ===                                                | (1,0)  |\r\n| 163\xb5s556ns     | executing 1/6: BEGIN TRANSACTION                                           | (1,0)  |\r\n| 187\xb5s838ns     | executing 2/6: UPDATE t.kv SET v = 123 WHERE (k % 2) = 0 RETURNING NOTHING | (1,0)  |\r\n| 220\xb5s350ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 231\xb5s827ns     | added table 't.kv' to table collection                                     | (1,0)  |\r\n| 284\xb5s78ns      | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 289\xb5s346ns     | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 482\xb5s51ns      | executing 3/6: UPDATE t.kv SET v = 431 WHERE (k % 2) = 1 RETURNING NOTHING | (1,0)  |\r\n| 508\xb5s389ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 513\xb5s743ns     | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 539\xb5s903ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 544\xb5s9ns       | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 705\xb5s222ns     | querying next range at /Table/76/1                                         | (1,0)  |\r\n| 734\xb5s353ns     | executing 4/6: COMMIT TRANSACTION                                          | (1,0)  |\r\n| 754\xb5s368ns     | r181: sending batch 1 Scan to (n1,s1):1                                    | (1,0)  |\r\n| 770\xb5s956ns     | sending request to localhost:26257                                         | (1,0)  |\r\n| 793\xb5s693ns     | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,1)  |\r\n| 1ms208\xb5s95ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,7)  |\r\n| 1ms267\xb5s682ns  | 1 Scan                                                                     | (1,7)  |\r\n| 1ms327\xb5s156ns  | executing 1 requests                                                       | (1,7)  |\r\n| 1ms341\xb5s663ns  | read-only path                                                             | (1,7)  |\r\n| 1ms356\xb5s175ns  | command queue                                                              | (1,7)  |\r\n| 1ms369\xb5s297ns  | waiting for read lock                                                      | (1,7)  |\r\n| 1ms520\xb5s529ns  | read completed                                                             | (1,7)  |\r\n| 1ms924\xb5s230ns  | querying next range at /Table/76/1/2/0                                     | (1,0)  |\r\n| 1ms961\xb5s689ns  | r181: sending batch 2 Put, 1 BeginTxn to (n1,s1):1                         | (1,0)  |\r\n| 1ms987\xb5s100ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 2ms1\xb5s270ns    | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,2)  |\r\n| 2ms256\xb5s156ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,8)  |\r\n| 2ms295\xb5s505ns  | 2 Put, 1 BeginTxn                                                          | (1,8)  |\r\n| 2ms310\xb5s911ns  | executing 3 requests                                                       | (1,8)  |\r\n| 2ms317\xb5s971ns  | read-write path                                                            | (1,8)  |\r\n| 2ms329\xb5s76ns   | command queue                                                              | (1,8)  |\r\n| 2ms378\xb5s26ns   | applied timestamp cache                                                    | (1,8)  |\r\n| 2ms583\xb5s77ns   | evaluated request                                                          | (1,8)  |\r\n| 2ms596\xb5s697ns  | acquired {raft,replica}mu                                                  | (1,8)  |\r\n| 4ms742\xb5s397ns  | applying command                                                           | (1,8)  |\r\n| 5ms227\xb5s736ns  | coordinator spawns                                                         | (1,0)  |\r\n| 5ms237\xb5s537ns  | kv.TxnCoordSender: heartbeat loop ===                                      | (1,3)  |\r\n| 5ms404\xb5s899ns  | querying next range at /Table/76/1                                         | (1,0)  |\r\n| 5ms444\xb5s706ns  | r181: sending batch 1 Scan to (n1,s1):1                                    | (1,0)  |\r\n| 5ms465\xb5s980ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 5ms480\xb5s156ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,4)  |\r\n| 5ms747\xb5s79ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,9)  |\r\n| 5ms776\xb5s283ns  | 1 Scan                                                                     | (1,9)  |\r\n| 5ms787\xb5s923ns  | executing 1 requests                                                       | (1,9)  |\r\n| 5ms794\xb5s445ns  | read-only path                                                             | (1,9)  |\r\n| 5ms801\xb5s145ns  | command queue                                                              | (1,9)  |\r\n| 5ms808\xb5s805ns  | waiting for read lock                                                      | (1,9)  |\r\n| 5ms910\xb5s485ns  | read completed                                                             | (1,9)  |\r\n| 6ms196\xb5s19ns   | querying next range at /Table/76/1/1/0                                     | (1,0)  |\r\n| 6ms224\xb5s750ns  | r181: sending batch 3 Put to (n1,s1):1                                     | (1,0)  |\r\n| 6ms242\xb5s299ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 6ms255\xb5s922ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,5)  |\r\n| 6ms535\xb5s859ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,10) |\r\n| 6ms565\xb5s86ns   | 3 Put                                                                      | (1,10) |\r\n| 6ms576\xb5s276ns  | executing 3 requests                                                       | (1,10) |\r\n| 6ms582\xb5s313ns  | read-write path                                                            | (1,10) |\r\n| 6ms589\xb5s205ns  | command queue                                                              | (1,10) |\r\n| 6ms620\xb5s876ns  | applied timestamp cache                                                    | (1,10) |\r\n| 6ms715\xb5s543ns  | evaluated request                                                          | (1,10) |\r\n| 6ms724\xb5s125ns  | acquired {raft,replica}mu                                                  | (1,10) |\r\n| 8ms544\xb5s886ns  | applying command                                                           | (1,10) |\r\n| 9ms28\xb5s742ns   | querying next range at /Table/76/1/2/0                                     | (1,0)  |\r\n| 9ms54\xb5s262ns   | r181: sending batch 1 EndTxn to (n1,s1):1                                  | (1,0)  |\r\n| 9ms76\xb5s865ns   | sending request to localhost:26257                                         | (1,0)  |\r\n| 9ms91\xb5s159ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,6)  |\r\n| 9ms322\xb5s159ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,11) |\r\n| 9ms351\xb5s56ns   | 1 EndTxn                                                                   | (1,11) |\r\n| 9ms369\xb5s757ns  | executing 1 requests                                                       | (1,11) |\r\n| 9ms376\xb5s128ns  | read-write path                                                            | (1,11) |\r\n| 9ms384\xb5s519ns  | command queue                                                              | (1,11) |\r\n| 9ms406\xb5s522ns  | applied timestamp cache                                                    | (1,11) |\r\n| 9ms604\xb5s457ns  | evaluated request                                                          | (1,11) |\r\n| 9ms613\xb5s313ns  | acquired {raft,replica}mu                                                  | (1,11) |\r\n| 11ms467\xb5s29ns  | applying command                                                           | (1,11) |\r\n| 11ms812\xb5s660ns | coordinator stops                                                          | (1,0)  |\r\n| 11ms894\xb5s771ns | === SPAN START: sql txn implicit ===                                       | (2,0)  |\r\n| 11ms923\xb5s919ns | executing 1/1: SET tracing = 'off'                                         | (2,0)  |\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n(81 rows)\r\n```\r\n\r\n\r\nI can see there are multiple spans for the execution of UPDATE after the COMMIT message has been received (as expected, COMMIT waits for the parallel statements to complete) however I don't know how to map which span belongs to which statement.\r\n\r\ncc @cockroachdb/sql-execution ",C-enhancement|A-sql-execution,andreimatei,"Example:\r\n```sql\r\nCREATE TABLE kv(k INT PRIMARY KEY, v INT);\r\nINSERT INTO kv VALUES (1,2),(3,4);\r\n\r\nSET TRACING=on;\r\nBEGIN;\r\nUPDATE t.kv SET v =123 WHERE k %2 = 0 RETURNING NOTHING; \r\nUPDATE t.kv SET v=431 WHERE k%2 = 1 RETURNING NOTHING; \r\nCOMMIT; \r\nSET TRACING=off; \r\n\r\nSELECT age,substring(message,1,80) AS message,span FROM [SHOW TRACE FOR SESSION];\r\n```\r\n\r\nOutputs:\r\n\r\n```\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n|      age       |                                  message                                   |  span  |\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n| 0s             | === SPAN START: sql txn implicit ===                                       | (0,0)  |\r\n| 137\xb5s609ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (0,0)  |\r\n|                | txn: ""sql txn implicit"" id=<nil> key=/Min                                  |        |\r\n| 152\xb5s217ns     | === SPAN START: sql txn ===                                                | (1,0)  |\r\n| 163\xb5s556ns     | executing 1/6: BEGIN TRANSACTION                                           | (1,0)  |\r\n| 187\xb5s838ns     | executing 2/6: UPDATE t.kv SET v = 123 WHERE (k % 2) = 0 RETURNING NOTHING | (1,0)  |\r\n| 220\xb5s350ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 231\xb5s827ns     | added table 't.kv' to table collection                                     | (1,0)  |\r\n| 284\xb5s78ns      | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 289\xb5s346ns     | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 482\xb5s51ns      | executing 3/6: UPDATE t.kv SET v = 431 WHERE (k % 2) = 1 RETURNING NOTHING | (1,0)  |\r\n| 508\xb5s389ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 513\xb5s743ns     | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 539\xb5s903ns     | client.Txn did AutoCommit. err: <nil>\u2424                                     | (1,0)  |\r\n|                | txn: ""unnamed"" id=<nil> key=/Min rw=false                                  |        |\r\n| 544\xb5s9ns       | found table in table collection for table 't.kv'                           | (1,0)  |\r\n| 705\xb5s222ns     | querying next range at /Table/76/1                                         | (1,0)  |\r\n| 734\xb5s353ns     | executing 4/6: COMMIT TRANSACTION                                          | (1,0)  |\r\n| 754\xb5s368ns     | r181: sending batch 1 Scan to (n1,s1):1                                    | (1,0)  |\r\n| 770\xb5s956ns     | sending request to localhost:26257                                         | (1,0)  |\r\n| 793\xb5s693ns     | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,1)  |\r\n| 1ms208\xb5s95ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,7)  |\r\n| 1ms267\xb5s682ns  | 1 Scan                                                                     | (1,7)  |\r\n| 1ms327\xb5s156ns  | executing 1 requests                                                       | (1,7)  |\r\n| 1ms341\xb5s663ns  | read-only path                                                             | (1,7)  |\r\n| 1ms356\xb5s175ns  | command queue                                                              | (1,7)  |\r\n| 1ms369\xb5s297ns  | waiting for read lock                                                      | (1,7)  |\r\n| 1ms520\xb5s529ns  | read completed                                                             | (1,7)  |\r\n| 1ms924\xb5s230ns  | querying next range at /Table/76/1/2/0                                     | (1,0)  |\r\n| 1ms961\xb5s689ns  | r181: sending batch 2 Put, 1 BeginTxn to (n1,s1):1                         | (1,0)  |\r\n| 1ms987\xb5s100ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 2ms1\xb5s270ns    | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,2)  |\r\n| 2ms256\xb5s156ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,8)  |\r\n| 2ms295\xb5s505ns  | 2 Put, 1 BeginTxn                                                          | (1,8)  |\r\n| 2ms310\xb5s911ns  | executing 3 requests                                                       | (1,8)  |\r\n| 2ms317\xb5s971ns  | read-write path                                                            | (1,8)  |\r\n| 2ms329\xb5s76ns   | command queue                                                              | (1,8)  |\r\n| 2ms378\xb5s26ns   | applied timestamp cache                                                    | (1,8)  |\r\n| 2ms583\xb5s77ns   | evaluated request                                                          | (1,8)  |\r\n| 2ms596\xb5s697ns  | acquired {raft,replica}mu                                                  | (1,8)  |\r\n| 4ms742\xb5s397ns  | applying command                                                           | (1,8)  |\r\n| 5ms227\xb5s736ns  | coordinator spawns                                                         | (1,0)  |\r\n| 5ms237\xb5s537ns  | kv.TxnCoordSender: heartbeat loop ===                                      | (1,3)  |\r\n| 5ms404\xb5s899ns  | querying next range at /Table/76/1                                         | (1,0)  |\r\n| 5ms444\xb5s706ns  | r181: sending batch 1 Scan to (n1,s1):1                                    | (1,0)  |\r\n| 5ms465\xb5s980ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 5ms480\xb5s156ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,4)  |\r\n| 5ms747\xb5s79ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,9)  |\r\n| 5ms776\xb5s283ns  | 1 Scan                                                                     | (1,9)  |\r\n| 5ms787\xb5s923ns  | executing 1 requests                                                       | (1,9)  |\r\n| 5ms794\xb5s445ns  | read-only path                                                             | (1,9)  |\r\n| 5ms801\xb5s145ns  | command queue                                                              | (1,9)  |\r\n| 5ms808\xb5s805ns  | waiting for read lock                                                      | (1,9)  |\r\n| 5ms910\xb5s485ns  | read completed                                                             | (1,9)  |\r\n| 6ms196\xb5s19ns   | querying next range at /Table/76/1/1/0                                     | (1,0)  |\r\n| 6ms224\xb5s750ns  | r181: sending batch 3 Put to (n1,s1):1                                     | (1,0)  |\r\n| 6ms242\xb5s299ns  | sending request to localhost:26257                                         | (1,0)  |\r\n| 6ms255\xb5s922ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,5)  |\r\n| 6ms535\xb5s859ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,10) |\r\n| 6ms565\xb5s86ns   | 3 Put                                                                      | (1,10) |\r\n| 6ms576\xb5s276ns  | executing 3 requests                                                       | (1,10) |\r\n| 6ms582\xb5s313ns  | read-write path                                                            | (1,10) |\r\n| 6ms589\xb5s205ns  | command queue                                                              | (1,10) |\r\n| 6ms620\xb5s876ns  | applied timestamp cache                                                    | (1,10) |\r\n| 6ms715\xb5s543ns  | evaluated request                                                          | (1,10) |\r\n| 6ms724\xb5s125ns  | acquired {raft,replica}mu                                                  | (1,10) |\r\n| 8ms544\xb5s886ns  | applying command                                                           | (1,10) |\r\n| 9ms28\xb5s742ns   | querying next range at /Table/76/1/2/0                                     | (1,0)  |\r\n| 9ms54\xb5s262ns   | r181: sending batch 1 EndTxn to (n1,s1):1                                  | (1,0)  |\r\n| 9ms76\xb5s865ns   | sending request to localhost:26257                                         | (1,0)  |\r\n| 9ms91\xb5s159ns   | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,6)  |\r\n| 9ms322\xb5s159ns  | === SPAN START: /cockroach.roachpb.Internal/Batch ===                      | (1,11) |\r\n| 9ms351\xb5s56ns   | 1 EndTxn                                                                   | (1,11) |\r\n| 9ms369\xb5s757ns  | executing 1 requests                                                       | (1,11) |\r\n| 9ms376\xb5s128ns  | read-write path                                                            | (1,11) |\r\n| 9ms384\xb5s519ns  | command queue                                                              | (1,11) |\r\n| 9ms406\xb5s522ns  | applied timestamp cache                                                    | (1,11) |\r\n| 9ms604\xb5s457ns  | evaluated request                                                          | (1,11) |\r\n| 9ms613\xb5s313ns  | acquired {raft,replica}mu                                                  | (1,11) |\r\n| 11ms467\xb5s29ns  | applying command                                                           | (1,11) |\r\n| 11ms812\xb5s660ns | coordinator stops                                                          | (1,0)  |\r\n| 11ms894\xb5s771ns | === SPAN START: sql txn implicit ===                                       | (2,0)  |\r\n| 11ms923\xb5s919ns | executing 1/1: SET tracing = 'off'                                         | (2,0)  |\r\n+----------------+----------------------------------------------------------------------------+--------+\r\n(81 rows)\r\n```\r\n\r\n\r\nI can see there are multiple spans for the execution of UPDATE after the COMMIT message has been received (as expected, COMMIT waits for the parallel statements to complete) however I don't know how to map which span belongs to which statement.\r\n\r\ncc @cockroachdb/sql-execution ","sql\r\nCREATE TABLE kv(k INT PRIMARY KEY, v INT);\r\nINSERT INTO kv VALUES (1,2),(3,4);\r\n\r\nSET TRACING=on;\r\nBEGIN;\r\nUPDATE t.kv SET v =123 WHERE k %2 = 0 RETURNING NOTHING; \r\nUPDATE t.kv SET v=431 WHERE k%2 = 1 RETURNING NOTHING; \r\nCOMMIT; \r\nSET TRACING=off; \r\n\r\nSELECT age,substring(message,1,80) AS message,span FROM [SHOW TRACE FOR SESSION];\r\n"
17306,"sql: views don't track their dependencies correctly (again...)\r\n\r\nThe problem is again that dependency analysis looks at the query plan after optimization (and thus filter elimination), not the tables actually named.\r\n\r\n\r\n",C-bug,knz,"```sql\r\nCREATE TABLE t(x INT);\r\nCREATE VIEW v AS SELECT x FROM t WHERE false;\r\nDROP TABLE t; -- succeeds, it shouldn't\r\nSELECT * FROM v; -- broken\r\n```\r\n\r\nThe problem is again that dependency analysis looks at the query plan after optimization (and thus filter elimination), not the tables actually named.\r\n\r\n\r\n","sql\r\nCREATE TABLE t(x INT);\r\nCREATE VIEW v AS SELECT x FROM t WHERE false;\r\nDROP TABLE t; -- succeeds, it shouldn't\r\nSELECT * FROM v; -- broken\r\n"
17269,"sql: views do not track their dependencies properlyIt is possible to break a view by removing a column from the table it depends on:\r\n\r\n\r\n\r\nThe problem is that the view tracks dependencies based on ""needed columns"", which is a result of logical plan optimization. A column that is named by the query but not used by the view results becomes ""unneeded"" and is thus not tracked as dependency. When it is deleted from the source table, the view plan cannot be constructed any more.\r\n\r\ncc @cockroachdb/sql-planning @cockroachdb/sql-async \r\n",C-bug|A-schema-descriptors,knz,"It is possible to break a view by removing a column from the table it depends on:\r\n\r\n```sql\r\nCREATE TABLE kv(k INT, v INT);\r\nCREATE VIEW v1 AS SELECT k FROM (SELECT k,v FROM kv) AS t;\r\nALTER TABLE kv DROP COLUMN v;\r\nSELECT * FROM v1; -- fails with `pq: column 2 does not exist`\r\n```\r\n\r\nThe problem is that the view tracks dependencies based on ""needed columns"", which is a result of logical plan optimization. A column that is named by the query but not used by the view results becomes ""unneeded"" and is thus not tracked as dependency. When it is deleted from the source table, the view plan cannot be constructed any more.\r\n\r\ncc @cockroachdb/sql-planning @cockroachdb/sql-async \r\n","sql\r\nCREATE TABLE kv(k INT, v INT);\r\nCREATE VIEW v1 AS SELECT k FROM (SELECT k,v FROM kv) AS t;\r\nALTER TABLE kv DROP COLUMN v;\r\nSELECT * FROM v1; -- fails with `pq: column 2 does not exist`\r\n"
17086,"sql: implement constant-space aggregation when input ordered on group keyFound with:\r\n\r\n\r\nCurrently the aggregator buffers all possible values of `x` in memory because it doesn't ""see"" that the source is ordered by x and that can emit an output row for the previous value as soon as it sees a new value for the `x` column.\r\n\r\ncc @cockroachdb/sql-planning @cockroachdb/sql-execution ",C-performance,abhimadan,"Found with:\r\n```sql\r\ncreate table foo(x int, y int);\r\ncreate index on foo(x) storing(y);\r\nselect x, sum(y) from foo group by x;\r\n```\r\n\r\nCurrently the aggregator buffers all possible values of `x` in memory because it doesn't ""see"" that the source is ordered by x and that can emit an output row for the previous value as soon as it sees a new value for the `x` column.\r\n\r\ncc @cockroachdb/sql-planning @cockroachdb/sql-execution ","sql\r\ncreate table foo(x int, y int);\r\ncreate index on foo(x) storing(y);\r\nselect x, sum(y) from foo group by x;\r\n"
16971,"sql: support Hibernate's use of _pg_expandarrayAdding an index to a table in Hibernate issues a query which makes use of this. Note that supporting this also means that we would need to support record types. The query below should work, if this is implemented:\r\n\r\n\r\n\r\nThis function takes an array `a` and returns a set of records that looks like the following:\r\n```\r\n x   |  n\r\n----------\r\na[1] |  1\r\na[2] |  2\r\na[3] |  3\r\na[4] |  4\r\n...\r\n```\r\n\r\nThe work is to be decomposed as follows:\r\n\r\n- [x] support the set-generating function `information_schema._pg_expandarray` #24422\r\n- [x] support simple labeled tuple access\r\n  - [x] add tuple labels #25283\r\n  - [x] access different columns in SRF #24832 \r\n- [x] support in-flight composite types #24866.\r\n  - [x] propagate labels during typing, including subqueries #26621\r\n  - [x] labeled tuple serialization bug #26624.\r\n  - [x] labeled tuples in distsql #26627.\r\n- [x] verify that all the complex queries listed here and in the `srfs` logic test file work properly\r\n  - [x]  add pg_get_indexdef with 3 arguments #26629\r\n  - [x] fix the column pg_index.indclass #26504 ",C-enhancement|A-sql-semantics|A-sql-pgcompat|O-support,BramGruneir,"Adding an index to a table in Hibernate issues a query which makes use of this. Note that supporting this also means that we would need to support record types. The query below should work, if this is implemented:\r\n\r\n```sql\r\nSELECT (i.keys).n FROM (SELECT information_schema._pg_expandarray(indkey) AS keys FROM pg_index) AS i;\r\n```\r\n\r\nThis function takes an array `a` and returns a set of records that looks like the following:\r\n```\r\n x   |  n\r\n----------\r\na[1] |  1\r\na[2] |  2\r\na[3] |  3\r\na[4] |  4\r\n...\r\n```\r\n\r\nThe work is to be decomposed as follows:\r\n\r\n- [x] support the set-generating function `information_schema._pg_expandarray` #24422\r\n- [x] support simple labeled tuple access\r\n  - [x] add tuple labels #25283\r\n  - [x] access different columns in SRF #24832 \r\n- [x] support in-flight composite types #24866.\r\n  - [x] propagate labels during typing, including subqueries #26621\r\n  - [x] labeled tuple serialization bug #26624.\r\n  - [x] labeled tuples in distsql #26627.\r\n- [x] verify that all the complex queries listed here and in the `srfs` logic test file work properly\r\n  - [x]  add pg_get_indexdef with 3 arguments #26629\r\n  - [x] fix the column pg_index.indclass #26504 ",sql\r\nSELECT (i.keys).n FROM (SELECT information_schema._pg_expandarray(indkey) AS keys FROM pg_index) AS i;\r\n
16908,"sql: cluster_logical_timestamp() changes within transactionThe timestamp returned changes within a txn, before performing a KV operation. That's against what that function is supposed to provide.\r\n\r\nThis is because the code assigning timestamps is fooling itself:\r\nhttps://github.com/cockroachdb/cockroach/blob/bba65279d5ee5792a4d7f72e6901aeaec9fb1199/pkg/internal/client/txn.go#L670\r\nThe timestamp will be re-assigned over and over again while `txn.mu.Proto.IsInitialized()` returns `false`.\r\n\r\n\r\n\r\n\r\nMight be fixed once #13894 is fixed.\r\n\r\ncc @knz ",C-bug,andreimatei,"The timestamp returned changes within a txn, before performing a KV operation. That's against what that function is supposed to provide.\r\n\r\nThis is because the code assigning timestamps is fooling itself:\r\nhttps://github.com/cockroachdb/cockroach/blob/bba65279d5ee5792a4d7f72e6901aeaec9fb1199/pkg/internal/client/txn.go#L670\r\nThe timestamp will be re-assigned over and over again while `txn.mu.Proto.IsInitialized()` returns `false`.\r\n\r\n```sql\r\nroot@:26257/> begin;\r\nBEGIN\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371674802693537.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371676538090936.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SHOW DATABASES;\r\n+--------------------+\r\n|      Database      |\r\n+--------------------+\r\n| bank               |\r\n+--------------------+\r\n(8 rows)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371680139593657.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371680139593657.0000000000 |\r\n+--------------------------------+\r\n```\r\n\r\n\r\nMight be fixed once #13894 is fixed.\r\n\r\ncc @knz ",sql\r\nroot@:26257/> begin;\r\nBEGIN\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371674802693537.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371676538090936.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SHOW DATABASES;\r\n+--------------------+\r\n|      Database      |\r\n+--------------------+\r\n| bank               |\r\n+--------------------+\r\n(8 rows)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371680139593657.0000000000 |\r\n+--------------------------------+\r\n(1 row)\r\nroot@:26257/  OPEN> SELECT cluster_logical_timestamp();\r\n+--------------------------------+\r\n|  cluster_logical_timestamp()   |\r\n+--------------------------------+\r\n| 1499371680139593657.0000000000 |\r\n+--------------------------------+\r\n
16505,"ui: Replicas-per-node graph breaks when number of nodes increasesRepro steps used (using Kubernetes, but this is probably repro-able elsewhere or without load as well):\r\n\r\n\r\n\r\nThe replicas-per-node graph shows the correct decrease of replicas on the existing nodes, but shows zero replicas on the two new nodes:\r\n\r\n<img width=""939"" alt=""screen shot 2017-06-13 at 2 22 41 pm"" src=""https://user-images.githubusercontent.com/7085343/27098065-45197536-5044-11e7-91b0-8371fe67edd2.png"">\r\n\r\n<img width=""1271"" alt=""screen shot 2017-06-13 at 2 22 52 pm"" src=""https://user-images.githubusercontent.com/7085343/27098071-48b46f2a-5044-11e7-8f14-3cc239ae296a.png"">\r\n\r\nClicking into the per-node dashboards shows the correct data. Interestingly, reloading the page (even with ctrl+shift+r) doesn't fix the issue, and the problem is there no matter which node I connect to (even the two new nodes).\r\n\r\nRestarting the first cockroachdb process and reconnecting to the UI fixes it such that the stats display properly throughout the cluster's entire life.",C-bug|A-monitoring|A-webui-timeseries|X-stale|no-issue-activity,piyush-singh,"Repro steps used (using Kubernetes, but this is probably repro-able elsewhere or without load as well):\r\n\r\n```console\r\n$ minikube start\r\n$ kubectl create -f https://tiny.cc/cockroachdb-k8s-tutorial\r\n$ kubectl run loadgen --image=cockroachdb/loadgen-kv:0.1 -- /kv --max-rate=100 --tolerate-errors postgres://root@cockroachdb-public:26257/?sslmode=disable\r\n$ kubectl port-forward cockroachdb-0 8080 # then open the admin UI\r\n$ kubectl scale statefulset cockroachdb --replicas=5\r\n```\r\n\r\nThe replicas-per-node graph shows the correct decrease of replicas on the existing nodes, but shows zero replicas on the two new nodes:\r\n\r\n<img width=""939"" alt=""screen shot 2017-06-13 at 2 22 41 pm"" src=""https://user-images.githubusercontent.com/7085343/27098065-45197536-5044-11e7-91b0-8371fe67edd2.png"">\r\n\r\n<img width=""1271"" alt=""screen shot 2017-06-13 at 2 22 52 pm"" src=""https://user-images.githubusercontent.com/7085343/27098071-48b46f2a-5044-11e7-8f14-3cc239ae296a.png"">\r\n\r\nClicking into the per-node dashboards shows the correct data. Interestingly, reloading the page (even with ctrl+shift+r) doesn't fix the issue, and the problem is there no matter which node I connect to (even the two new nodes).\r\n\r\nRestarting the first cockroachdb process and reconnecting to the UI fixes it such that the stats display properly throughout the cluster's entire life.",console\r\n$ minikube start\r\n$ kubectl create -f https://tiny.cc/cockroachdb-k8s-tutorial\r\n$ kubectl run loadgen --image=cockroachdb/loadgen-kv:0.1 -- /kv --max-rate=100 --tolerate-errors postgres://root@cockroachdb-public:26257/?sslmode=disable\r\n$ kubectl port-forward cockroachdb-0 8080 # then open the admin UI\r\n$ kubectl scale statefulset cockroachdb --replicas=5\r\n
16471,"sql: dropping an interleave index causes backreference preventing DDL activities\r\n\r\n```\r\nx=> drop table t1;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\nx=> drop database x;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\nx=> drop table t1 cascade;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\n```\r\n\r\nbuild:      CCL v1.0.1 @ 2017/05/25 17:12:12 (go1.8.3)\r\n",C-bug|S-3-ux-surprise,m-schneider,"```sql\r\ncreate table t1 (id1 int primary key, id2 int, id3 int);\r\ncreate index c on t1 (id2)\r\n   storing (id1,id3)\r\n   interleave in parent t1 (id2);\r\ndrop index t1@c;\r\n```\r\n\r\n```\r\nx=> drop table t1;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\nx=> drop database x;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\nx=> drop table t1 cascade;\r\nERROR:  invalid interleave backreference table=t1 index=2: index-id ""2"" does not exist\r\n```\r\n\r\nbuild:      CCL v1.0.1 @ 2017/05/25 17:12:12 (go1.8.3)\r\n","sql\r\ncreate table t1 (id1 int primary key, id2 int, id3 int);\r\ncreate index c on t1 (id2)\r\n   storing (id1,id3)\r\n   interleave in parent t1 (id2);\r\ndrop index t1@c;\r\n"
16313,"sql: small LIMIT value causes results to be omitted1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. On most unix-based systems\r\n   running with defaults, this boils down to the output of\r\n\r\n     grep -F '[config]' cockroach-data/logs/cockroach.log\r\n\r\n   When log files are not available, supply the output of `cockroach version`\r\n   and all flags/environment variables passed to `cockroach start` instead.\r\n\r\n\r\n\r\n2. Please describe the issue you observed:\r\n\r\n- What did you do?\r\n\r\nHere's my table schema:\r\n\r\n\r\n\r\nI performed the following query against this table:\r\n\r\n\r\n\r\n- What did you expect to see?\r\n\r\nI expected to get a nonzero number of results (since there are rows for which `read` is false). This is confirmed by doing the query without the `limit` clause:\r\n\r\n\r\n\r\n- What did you see instead?\r\n\r\nI instead get no returned rows when the `limit` parameter is small:\r\n\r\n\r\nHowever, when the limit parameter is close to the number of rows in the table, I get results:\r\n\r\n \r\n\r\nI was able to reproduce my expected results against a previous (<1.0) version of cockroachdb. I can try and figure out which version that was if it would be helpful.",C-bug|A-sql-semantics,RaduBerinde,"1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. On most unix-based systems\r\n   running with defaults, this boils down to the output of\r\n\r\n     grep -F '[config]' cockroach-data/logs/cockroach.log\r\n\r\n   When log files are not available, supply the output of `cockroach version`\r\n   and all flags/environment variables passed to `cockroach start` instead.\r\n\r\n```shell\r\n$ grep -F '[config]' cockroach-data/logs/cockroach.log\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] file created at: 2017/06/05 02:04:31\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] running on machine: samurai\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] binary: CockroachDB CCL v1.0.1 (linux amd64, built 2017/05/25 15:17:49, go1.8.1)\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] arguments: [cockroach start --insecure]\r\n```\r\n\r\n2. Please describe the issue you observed:\r\n\r\n- What did you do?\r\n\r\nHere's my table schema:\r\n\r\n```sql\r\nroot@:26257/Goliath> SHOW CREATE TABLE article;\r\n+---------+------------------------------------------------------------------------------------------------------+\r\n|  Table  |                                             CreateTable                                              |\r\n+---------+------------------------------------------------------------------------------------------------------+\r\n| article | CREATE TABLE article (\u2424                                                                              |\r\n|         |     id INT NOT NULL DEFAULT unique_rowid(),\u2424                                                         |\r\n|         |     feed INT NOT NULL,\u2424                                                                              |\r\n|         |     folder INT NOT NULL,\u2424                                                                            |\r\n|         |     hash STRING NULL,\u2424                                                                               |\r\n|         |     title STRING NULL,\u2424                                                                              |\r\n|         |     summary STRING NULL,\u2424                                                                            |\r\n|         |     content STRING NULL,\u2424                                                                            |\r\n|         |     link STRING NULL,\u2424                                                                               |\r\n|         |     read BOOL NULL,\u2424                                                                                 |\r\n|         |     date TIMESTAMP WITH TIME ZONE NULL,\u2424                                                             |\r\n|         |     retrieved TIMESTAMP WITH TIME ZONE NULL,\u2424                                                        |\r\n|         |     CONSTRAINT ""primary"" PRIMARY KEY (folder ASC, feed ASC, id ASC),\u2424                                |\r\n|         |     UNIQUE INDEX article_id_key (id ASC),\u2424                                                           |\r\n|         |     UNIQUE INDEX article_hash_key (hash ASC),\u2424                                                       |\r\n|         |     FAMILY ""primary"" (id, feed, folder, hash, title, summary, content, link, read, date, retrieved)\u2424 |\r\n|         | ) INTERLEAVE IN PARENT feed (folder, feed)                                                           |\r\n+---------+------------------------------------------------------------------------------------------------------+\r\n(1 row)\r\n```\r\n\r\nI performed the following query against this table:\r\n\r\n```sql\r\nSELECT id FROM article WHERE NOT read AND id > 0 limit 50;\r\n```\r\n\r\n- What did you expect to see?\r\n\r\nI expected to get a nonzero number of results (since there are rows for which `read` is false). This is confirmed by doing the query without the `limit` clause:\r\n\r\n```sql\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0;\r\n+--------------------+\r\n|         id         |\r\n+--------------------+\r\n| 250885803841617921 |\r\n| 250889741747650561 |\r\n| 250895619105882113 |\r\n| 250917275225554945 |\r\n| 250923144155168769 |\r\n| 250925111932157953 |\r\n| 250925111980163073 |\r\n| 250934959014903809 |\r\n| 250936942286602241 |\r\n| 250940842501373953 |\r\n| 250944774940196865 |\r\n| 250950694693732353 |\r\n+--------------------+\r\n(12 rows)\r\n\r\n```\r\n\r\n- What did you see instead?\r\n\r\nI instead get no returned rows when the `limit` parameter is small:\r\n\r\n```sql\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0 limit 50;\r\n+----+\r\n| id |\r\n+----+\r\n+----+\r\n(0 rows)\r\n\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0 limit 500;\r\n+----+\r\n| id |\r\n+----+\r\n+----+\r\n(0 rows)\r\n```\r\nHowever, when the limit parameter is close to the number of rows in the table, I get results:\r\n\r\n```sql\r\nroot@:26257/Goliath> select count(*) from article;\r\n+----------+\r\n| count(*) |\r\n+----------+\r\n|      699 |\r\n+----------+\r\n(1 row)\r\n\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0 limit 690;\r\n+--------------------+\r\n|         id         |\r\n+--------------------+\r\n| 250885803841617921 |\r\n| 250889741747650561 |\r\n| 250895619105882113 |\r\n| 250917275225554945 |\r\n| 250923144155168769 |\r\n| 250925111932157953 |\r\n| 250925111980163073 |\r\n| 250934959014903809 |\r\n+--------------------+\r\n(8 rows)\r\n\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0 limit 698;\r\n+--------------------+\r\n|         id         |\r\n+--------------------+\r\n| 250885803841617921 |\r\n| 250889741747650561 |\r\n| 250895619105882113 |\r\n| 250917275225554945 |\r\n| 250923144155168769 |\r\n| 250925111932157953 |\r\n| 250925111980163073 |\r\n| 250934959014903809 |\r\n| 250936942286602241 |\r\n| 250940842501373953 |\r\n| 250944774940196865 |\r\n+--------------------+\r\n(11 rows)\r\n\r\nroot@:26257/Goliath> SELECT id FROM article WHERE NOT read AND id > 0 limit 699;\r\n+--------------------+\r\n|         id         |\r\n+--------------------+\r\n| 250885803841617921 |\r\n| 250889741747650561 |\r\n| 250895619105882113 |\r\n| 250917275225554945 |\r\n| 250923144155168769 |\r\n| 250925111932157953 |\r\n| 250925111980163073 |\r\n| 250934959014903809 |\r\n| 250936942286602241 |\r\n| 250940842501373953 |\r\n| 250944774940196865 |\r\n| 250950694693732353 |\r\n+--------------------+\r\n(12 rows)\r\n\r\n``` \r\n\r\nI was able to reproduce my expected results against a previous (<1.0) version of cockroachdb. I can try and figure out which version that was if it would be helpful.","shell\r\n$ grep -F '[config]' cockroach-data/logs/cockroach.log\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] file created at: 2017/06/05 02:04:31\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] running on machine: samurai\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] binary: CockroachDB CCL v1.0.1 (linux amd64, built 2017/05/25 15:17:49, go1.8.1)\r\nI170605 02:04:31.905759 1 util/log/clog.go:1011  [config] arguments: [cockroach start --insecure]\r\n"
16266,"storage: transitive prerequisites are not always transferred on command cancellationhttps://sentry.io/cockroach-labs/cockroachdb/issues/284616795/\n\n\n\n```\n*errors.errorString: replica_command.go:1304 string\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_command.go"", line 1304, in evalRangeLookup\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_command.go"", line 196, in evaluateCommand\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 4330, in evaluateBatch\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 2055, in executeReadOnlyBatch\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 1421, in Send\n...\n(10 additional frame(s) were not displayed)\n\nreplica_command.go:1304 string\n```",C-bug,nvanbenschoten,"https://sentry.io/cockroach-labs/cockroachdb/issues/284616795/\n\n```go\n\tif len(reply.Ranges) == 0 {\n\t\t// No matching results were returned from the scan. This should\n\t\t// never happen with the above logic.\n\t\tvar buf bytes.Buffer\n\t\tbuf.WriteString(""range lookup of meta key '"")\n\t\tbuf.Write(args.Key)\n\t\tbuf.WriteString(""' found only non-matching ranges:"")\n\t\tfor _, desc := range reply.PrefetchedRanges {\n\t\t\tbuf.WriteByte('\\n')\n\t\t\tbuf.WriteString(desc.String())\n\t\t}\n\t\tlog.Fatal(ctx, buf.String()) // <- boom\n\t}\n```\n\n```\n*errors.errorString: replica_command.go:1304 string\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_command.go"", line 1304, in evalRangeLookup\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica_command.go"", line 196, in evaluateCommand\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 4330, in evaluateBatch\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 2055, in executeReadOnlyBatch\n  File ""github.com/cockroachdb/cockroach/pkg/storage/replica.go"", line 1421, in Send\n...\n(10 additional frame(s) were not displayed)\n\nreplica_command.go:1304 string\n```","go\n\tif len(reply.Ranges) == 0 {\n\t\t// No matching results were returned from the scan. This should\n\t\t// never happen with the above logic.\n\t\tvar buf bytes.Buffer\n\t\tbuf.WriteString(""range lookup of meta key '"")\n\t\tbuf.Write(args.Key)\n\t\tbuf.WriteString(""' found only non-matching ranges:"")\n\t\tfor _, desc := range reply.PrefetchedRanges {\n\t\t\tbuf.WriteByte('\\n')\n\t\t\tbuf.WriteString(desc.String())\n\t\t}\n\t\tlog.Fatal(ctx, buf.String()) // <- boom\n\t}\n"
16173,"cli: unusable ui url printed when $(hostname) doesn't resolveIs this a question, feature request, or bug report?\r\n\r\nWhen running cockroach in single-node on macOS:\r\n\r\n\r\n\r\n```\r\nCockroachDB node starting at 2017-05-26 19:29:00.429726493 +0200 CEST\r\nbuild:      CCL v1.0.1 @ 2017/05/25 17:12:12 (go1.8.3)\r\nadmin:      http://polpc02585:8080\r\nsql:        postgresql://root@polpc02585:26257?sslmode=disable\r\n...\r\n```\r\n\r\nWhile accessing web UI I got following errors on requests to ```_status/*```: ```grpc: the connection is unavailable""```\r\n\r\nI have observed that this error is caused by my hostname not being resolvable to any IP:\r\n\r\n```\r\n~ hostname\r\npolpc02585\r\n~ ping polpc02585\r\nping: cannot resolve polpc02585: Unknown host\r\n```\r\n\r\nWhen I've added ```127.0.0.1 polpc02585``` to ```/etc/hosts``` everything works fine.\r\n\r\nSo my best guess is that cockroach should not use hostname name if it's not resolvable.",C-bug|S-3-ux-surprise,tamird,"Is this a question, feature request, or bug report?\r\n\r\nWhen running cockroach in single-node on macOS:\r\n\r\n```cockroach start --insecure```\r\n\r\n```\r\nCockroachDB node starting at 2017-05-26 19:29:00.429726493 +0200 CEST\r\nbuild:      CCL v1.0.1 @ 2017/05/25 17:12:12 (go1.8.3)\r\nadmin:      http://polpc02585:8080\r\nsql:        postgresql://root@polpc02585:26257?sslmode=disable\r\n...\r\n```\r\n\r\nWhile accessing web UI I got following errors on requests to ```_status/*```: ```grpc: the connection is unavailable""```\r\n\r\nI have observed that this error is caused by my hostname not being resolvable to any IP:\r\n\r\n```\r\n~ hostname\r\npolpc02585\r\n~ ping polpc02585\r\nping: cannot resolve polpc02585: Unknown host\r\n```\r\n\r\nWhen I've added ```127.0.0.1 polpc02585``` to ```/etc/hosts``` everything works fine.\r\n\r\nSo my best guess is that cockroach should not use hostname name if it's not resolvable.",cockroach start --insecure
16075,"storage: larger command size amplification for keysMost likely not a bug, posted here as suggested by @tschottdorf to just cross check why the following behavior is so.\r\n\r\nFor the following minimal test case (add to any of the `pkg/storage/client_*_test.go` files):\r\n\r\n\r\nAnd the following debug statement in `Replica.propose` after `proposal, pErr := r.requestToProposal(ctx, idKey, ba, endCmds, spans)`\r\n```\r\n\tlog.Infof(ctx, ""CMD SIZE: %d"", proposal.command.Size())\r\n```\r\n\r\nWe get the following:\r\n```\r\nI170522 19:09:19.572545 10 storage/replica.go:2677  [s1,r1/1:/M{in-ax}] CMD SIZE: 10361\r\nI170522 19:09:19.573194 10 storage/replica.go:2677  [s1,r1/1:/M{in-ax}] CMD SIZE: 30362\r\n```\r\n\r\nNote that for keys there's ~3x factor blow up in command size. This may not be an issue if we don't have large keys but it isn't immediately obvious why this is the case.\r\n",C-performance,tbg|nvanbenschoten,"Most likely not a bug, posted here as suggested by @tschottdorf to just cross check why the following behavior is so.\r\n\r\nFor the following minimal test case (add to any of the `pkg/storage/client_*_test.go` files):\r\n```go\r\nfunc TestCommandSizeAmplification(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\r\n\tconst rangeID = 1\r\n\tmtc := &multiTestContext{}\r\n\tmtc.Start(t, 1)\r\n\tdefer mtc.Stop()\r\n\r\n\tctx := context.Background()\r\n\trepl := mtc.getRaftLeader(rangeID)\r\n\tbig := bytes.Repeat([]byte(""a""), 10000)\r\n\tsmall := []byte(""a"")\r\n\r\n\tif _, pErr := client.SendWrapped(ctx, repl, putArgs(small, big)); pErr != nil {\r\n\t\tt.Fatal(pErr)\r\n\t}\r\n\r\n\tif _, pErr := client.SendWrapped(ctx, repl, putArgs(big, small)); pErr != nil {\r\n\t\tt.Fatal(pErr)\r\n\t}\r\n}\r\n```\r\n\r\nAnd the following debug statement in `Replica.propose` after `proposal, pErr := r.requestToProposal(ctx, idKey, ba, endCmds, spans)`\r\n```\r\n\tlog.Infof(ctx, ""CMD SIZE: %d"", proposal.command.Size())\r\n```\r\n\r\nWe get the following:\r\n```\r\nI170522 19:09:19.572545 10 storage/replica.go:2677  [s1,r1/1:/M{in-ax}] CMD SIZE: 10361\r\nI170522 19:09:19.573194 10 storage/replica.go:2677  [s1,r1/1:/M{in-ax}] CMD SIZE: 30362\r\n```\r\n\r\nNote that for keys there's ~3x factor blow up in command size. This may not be an issue if we don't have large keys but it isn't immediately obvious why this is the case.\r\n","go\r\nfunc TestCommandSizeAmplification(t *testing.T) {\r\n\tdefer leaktest.AfterTest(t)()\r\n\r\n\tconst rangeID = 1\r\n\tmtc := &multiTestContext{}\r\n\tmtc.Start(t, 1)\r\n\tdefer mtc.Stop()\r\n\r\n\tctx := context.Background()\r\n\trepl := mtc.getRaftLeader(rangeID)\r\n\tbig := bytes.Repeat([]byte(""a""), 10000)\r\n\tsmall := []byte(""a"")\r\n\r\n\tif _, pErr := client.SendWrapped(ctx, repl, putArgs(small, big)); pErr != nil {\r\n\t\tt.Fatal(pErr)\r\n\t}\r\n\r\n\tif _, pErr := client.SendWrapped(ctx, repl, putArgs(big, small)); pErr != nil {\r\n\t\tt.Fatal(pErr)\r\n\t}\r\n}\r\n"
16070,sql: adding and removing fk constraints can cause broken backward references\r\n\r\nGives:\r\n```\r\npq: broken fk backward reference from employee.primary to employee.employee_auto_index_emp_emp\r\n```\r\n\r\nFound via Hibernate's test suite.,C-bug,dt,"```sql\r\nCREATE TABLE employee (\r\n   id INT PRIMARY KEY,\r\n   manager INT\r\n);\r\n\r\nCREATE TABLE person (\r\n   id INT PRIMARY KEY\r\n);\r\n\r\nALTER TABLE employee \r\n   ADD CONSTRAINT emp_emp \r\n   FOREIGN KEY (manager) \r\n   REFERENCES employee;\r\n\r\nALTER TABLE employee \r\n   ADD CONSTRAINT emp_per \r\n   FOREIGN KEY (id) \r\n   REFERENCES person;\r\n\r\nALTER TABLE employee \r\n   DROP CONSTRAINT emp_emp;\r\n\r\nALTER TABLE employee \r\n   DROP CONSTRAINT emp_per;\r\n```\r\n\r\nGives:\r\n```\r\npq: broken fk backward reference from employee.primary to employee.employee_auto_index_emp_emp\r\n```\r\n\r\nFound via Hibernate's test suite.","sql\r\nCREATE TABLE employee (\r\n   id INT PRIMARY KEY,\r\n   manager INT\r\n);\r\n\r\nCREATE TABLE person (\r\n   id INT PRIMARY KEY\r\n);\r\n\r\nALTER TABLE employee \r\n   ADD CONSTRAINT emp_emp \r\n   FOREIGN KEY (manager) \r\n   REFERENCES employee;\r\n\r\nALTER TABLE employee \r\n   ADD CONSTRAINT emp_per \r\n   FOREIGN KEY (id) \r\n   REFERENCES person;\r\n\r\nALTER TABLE employee \r\n   DROP CONSTRAINT emp_emp;\r\n\r\nALTER TABLE employee \r\n   DROP CONSTRAINT emp_per;\r\n"
15769,"ui: expose GC queue statisticsSee https://github.com/cockroachdb/cockroach/issues/15756. The GC queue already collects the metrics below. I think we should expose some of them in the `GC Queue` graph, namely (for starters): `NumKeysAffected`, `PushTxn`, `ResolveTotal` more prominently, and, perhaps for drilling down, `sum(TransactionSpanGCAborted, TransactionSpanGCCommitted, TransactionSpanGCPending)` and `AbortSpanConsidered`.\r\n\r\n",C-enhancement|A-monitoring|X-stale|no-issue-activity,piyush-singh,"See https://github.com/cockroachdb/cockroach/issues/15756. The GC queue already collects the metrics below. I think we should expose some of them in the `GC Queue` graph, namely (for starters): `NumKeysAffected`, `PushTxn`, `ResolveTotal` more prominently, and, perhaps for drilling down, `sum(TransactionSpanGCAborted, TransactionSpanGCCommitted, TransactionSpanGCPending)` and `AbortSpanConsidered`.\r\n\r\n```go\r\n// GCInfo contains statistics and insights from a GC run.\r\ntype GCInfo struct {\r\n\t// Now is the timestamp used for age computations.\r\n\tNow hlc.Timestamp\r\n\t// Policy is the policy used for this garbage collection cycle.\r\n\tPolicy config.GCPolicy\r\n\t// Stats about the userspace key-values considered, namely the number of\r\n\t// keys with GC'able data, the number of ""old"" intents and the number of\r\n\t// associated distinct transactions.\r\n\tNumKeysAffected, IntentsConsidered, IntentTxns int\r\n\t// TransactionSpanTotal is the total number of entries in the transaction span.\r\n\tTransactionSpanTotal int\r\n\t// Summary of transactions which were found GCable (assuming that\r\n\t// potentially necessary intent resolutions did not fail).\r\n\tTransactionSpanGCAborted, TransactionSpanGCCommitted, TransactionSpanGCPending int\r\n\t// TxnSpanGCThreshold is the cutoff for transaction span GC. Transactions\r\n\t// with a smaller LastActive() were considered for GC.\r\n\tTxnSpanGCThreshold hlc.Timestamp\r\n\t// AbortSpanTotal is the total number of transactions present in the abort cache.\r\n\tAbortSpanTotal int\r\n\t// AbortSpanConsidered is the number of abort cache entries old enough to be\r\n\t// considered for removal. An ""entry"" corresponds to one transaction;\r\n\t// more than one key-value pair may be associated with it.\r\n\tAbortSpanConsidered int\r\n\t// AbortSpanGCNum is the number of abort cache entries fit for removal (due\r\n\t// to their transactions having terminated).\r\n\tAbortSpanGCNum int\r\n\t// PushTxn is the total number of pushes attempted in this cycle.\r\n\tPushTxn int\r\n\t// ResolveTotal is the total number of attempted intent resolutions in\r\n\t// this cycle.\r\n\tResolveTotal int\r\n\t// ResolveErrors is the number of successful intent resolutions.\r\n\tResolveSuccess int\r\n\t// Threshold is the computed expiration timestamp. Equal to `Now - Policy`.\r\n\tThreshold hlc.Timestamp\r\n}\r\n```","go\r\n// GCInfo contains statistics and insights from a GC run.\r\ntype GCInfo struct {\r\n\t// Now is the timestamp used for age computations.\r\n\tNow hlc.Timestamp\r\n\t// Policy is the policy used for this garbage collection cycle.\r\n\tPolicy config.GCPolicy\r\n\t// Stats about the userspace key-values considered, namely the number of\r\n\t// keys with GC'able data, the number of ""old"" intents and the number of\r\n\t// associated distinct transactions.\r\n\tNumKeysAffected, IntentsConsidered, IntentTxns int\r\n\t// TransactionSpanTotal is the total number of entries in the transaction span.\r\n\tTransactionSpanTotal int\r\n\t// Summary of transactions which were found GCable (assuming that\r\n\t// potentially necessary intent resolutions did not fail).\r\n\tTransactionSpanGCAborted, TransactionSpanGCCommitted, TransactionSpanGCPending int\r\n\t// TxnSpanGCThreshold is the cutoff for transaction span GC. Transactions\r\n\t// with a smaller LastActive() were considered for GC.\r\n\tTxnSpanGCThreshold hlc.Timestamp\r\n\t// AbortSpanTotal is the total number of transactions present in the abort cache.\r\n\tAbortSpanTotal int\r\n\t// AbortSpanConsidered is the number of abort cache entries old enough to be\r\n\t// considered for removal. An ""entry"" corresponds to one transaction;\r\n\t// more than one key-value pair may be associated with it.\r\n\tAbortSpanConsidered int\r\n\t// AbortSpanGCNum is the number of abort cache entries fit for removal (due\r\n\t// to their transactions having terminated).\r\n\tAbortSpanGCNum int\r\n\t// PushTxn is the total number of pushes attempted in this cycle.\r\n\tPushTxn int\r\n\t// ResolveTotal is the total number of attempted intent resolutions in\r\n\t// this cycle.\r\n\tResolveTotal int\r\n\t// ResolveErrors is the number of successful intent resolutions.\r\n\tResolveSuccess int\r\n\t// Threshold is the computed expiration timestamp. Equal to `Now - Policy`.\r\n\tThreshold hlc.Timestamp\r\n}\r\n"
15059,"cli: `cockroach start -s type=mem,size=1GiB` fails on WindowsI haven't investigated this at all.\r\n\r\n",B-os-windows,BramGruneir,"I haven't investigated this at all.\r\n\r\n```shell\r\n$ ./cockroach start -s type=mem,size=1GiB --logtostderr\r\nE170418 21:12:23.469769 1 cli/error.go:68  The filename, directory name, or volume label syntax is incorrect.\r\nError: The filename, directory name, or volume label syntax is incorrect.\r\nFailed running ""start""\r\n```","shell\r\n$ ./cockroach start -s type=mem,size=1GiB --logtostderr\r\nE170418 21:12:23.469769 1 cli/error.go:68  The filename, directory name, or volume label syntax is incorrect.\r\nError: The filename, directory name, or volume label syntax is incorrect.\r\nFailed running ""start""\r\n"
14970,"Check constraint kills server**BUG REPORT**\r\n\r\n- What did you do?\r\n\r\n- What did you expect to see?\r\n```\r\npq: failed to satisfy CHECK constraint (i > 10)\r\n```\r\n- What did you see instead?\r\n```\r\ndriver: bad connection\r\nconnection lost; opening new connection and resetting session parameters...\r\n```\r\nThe docker instance was killed and needed to be restarted manually.\r\nAfter logging in, trying to drop the table\r\n\r\nIt got stuck and took ~ 10 mins for the table to be finally dropped.\r\n\r\nHost: Windows 10 Pro\r\nTested on cockroachdb docker beta-20170413-dirty and alpha-18537-g00185f3\r\n\r\n\r\nPossibly related to #14513 \r\n\r\nOne liner for reproducibility\r\n```\r\n./cockroach sql -e ""create database test; set database = test; create table test (i int check (i > 10)); insert into test values ('11');""\r\n```",O-community|C-question,dt,"**BUG REPORT**\r\n\r\n- What did you do?\r\n```sql\r\ncreate database test;\r\nset database = test;\r\ncreate table test (i int check (i > 10));\r\ninsert into test values ('11'); -- deliberately input a string \r\n```\r\n- What did you expect to see?\r\n```\r\npq: failed to satisfy CHECK constraint (i > 10)\r\n```\r\n- What did you see instead?\r\n```\r\ndriver: bad connection\r\nconnection lost; opening new connection and resetting session parameters...\r\n```\r\nThe docker instance was killed and needed to be restarted manually.\r\nAfter logging in, trying to drop the table\r\n```sql\r\ndrop table test.test;\r\n--pq: table ""test"" is being dropped\r\n```\r\nIt got stuck and took ~ 10 mins for the table to be finally dropped.\r\n\r\nHost: Windows 10 Pro\r\nTested on cockroachdb docker beta-20170413-dirty and alpha-18537-g00185f3\r\n\r\n\r\nPossibly related to #14513 \r\n\r\nOne liner for reproducibility\r\n```\r\n./cockroach sql -e ""create database test; set database = test; create table test (i int check (i > 10)); insert into test values ('11');""\r\n```",sql\r\ncreate database test;\r\nset database = test;\r\ncreate table test (i int check (i > 10));\r\ninsert into test values ('11'); -- deliberately input a string \r\n
14554,"unsupported comparison operator: <oid> NOT IN <tuple{int}>As discussed with @jordanlewis in #12526, I believe the recent work on supporting the OID type created a small regression with the following error:\r\n\r\n\r\n\r\nLooks like there are some missing comparison operators between OID and int type now that OIDs are not int anymore.",C-enhancement|O-community,jordanlewis,"As discussed with @jordanlewis in #12526, I believe the recent work on supporting the OID type created a small regression with the following error:\r\n\r\n```sql/executor.go:722  [client=127.0.0.1:63505,user=root,n1] execRequest: error: unsupported comparison operator: <oid> NOT IN <tuple{int}>```\r\n\r\nLooks like there are some missing comparison operators between OID and int type now that OIDs are not int anymore.","sql/executor.go:722  [client=127.0.0.1:63505,user=root,n1] execRequest: error: unsupported comparison operator: <oid> NOT IN <tuple{int}>"
14238,"sql: node crash with explainThe node serving the request crashed if I try to explain a query on beta-20170309\r\n\r\n\r\nThe node will crash and in the log, I see\r\n```\r\npanic: runtime error: index out of range [recovered]\r\n        panic: runtime error: index out of range [recovered]\r\n        panic: runtime error: index out of range\r\n\r\ngoroutine 26996 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/util/stop.(*Stopper).Recover(0xc4203634a0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/stop/stopper.go:198 +0x6e\r\npanic(0x1ab98e0, 0x2878ec0)\r\n        /usr/local/go/src/runtime/panic.go:489 +0x2cf\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).serve.func1(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:340 +0x74\r\npanic(0x1ab98e0, 0x2878ec0)\r\n        /usr/local/go/src/runtime/panic.go:489 +0x2cf\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).sendResponse(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc421790aa0, 0x1, 0x1, 0x2dd4cd8, 0x0, 0x0, 0xc4348a3600, ...)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:961 +0x497\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).executeStatements(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc4348a364d, 0x48, 0xc42683b518, 0x2dd4cd8, 0x0, 0x0, 0x1acd900, ...)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:828 +0x202\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).handleExecute(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc42209c228, 0x9, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:805 +0x19d\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).serve(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc429460620, 0x5400, 0xc4202e0160, 0x7f614d43ada0, 0xc42942d300, 0x0, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:442 +0xa7b\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*Server).ServeConn(0xc4202e0000, 0x7f614d43ada0, 0xc42942d300, 0x28bed80, 0xc4261a2140, 0x0, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/server.go:415 +0x917\r\ngithub.com/cockroachdb/cockroach/pkg/server.(*Server).Start.func8.1(0x28bed80, 0xc4261a2140)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/server/server.go:544 +0x106\r\ngithub.com/cockroachdb/cockroach/pkg/util/netutil.(*Server).ServeWith.func1(0xc4203634a0, 0xc42000e040, 0x28bed80, 0xc4261a2140, 0xc421194240)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/netutil/net.go:136 +0x95\r\ncreated by github.com/cockroachdb/cockroach/pkg/util/netutil.(*Server).ServeWith\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/netutil/net.go:138 +0x239\r\n```",C-investigation,knz,"The node serving the request crashed if I try to explain a query on beta-20170309\r\n```sql\r\nCREATE TABLE Test (ID serial PRIMARY KEY, Date TimeStampTZ,INDEX date_idx (Date));\r\ninsert into Test (Date) values ('2017-03-17 15:33:00'::timestamptz);\r\n\r\nexplain select * from Test where Date >='2017-01-01 00:00:00'::TimeStamp;\r\n```\r\n\r\nThe node will crash and in the log, I see\r\n```\r\npanic: runtime error: index out of range [recovered]\r\n        panic: runtime error: index out of range [recovered]\r\n        panic: runtime error: index out of range\r\n\r\ngoroutine 26996 [running]:\r\ngithub.com/cockroachdb/cockroach/pkg/util/stop.(*Stopper).Recover(0xc4203634a0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/stop/stopper.go:198 +0x6e\r\npanic(0x1ab98e0, 0x2878ec0)\r\n        /usr/local/go/src/runtime/panic.go:489 +0x2cf\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).serve.func1(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:340 +0x74\r\npanic(0x1ab98e0, 0x2878ec0)\r\n        /usr/local/go/src/runtime/panic.go:489 +0x2cf\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).sendResponse(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc421790aa0, 0x1, 0x1, 0x2dd4cd8, 0x0, 0x0, 0xc4348a3600, ...)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:961 +0x497\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).executeStatements(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc4348a364d, 0x48, 0xc42683b518, 0x2dd4cd8, 0x0, 0x0, 0x1acd900, ...)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:828 +0x202\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).handleExecute(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc42209c228, 0x9, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:805 +0x19d\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*v3Conn).serve(0xc42209c200, 0x7f614d43acc0, 0xc4398842a0, 0xc429460620, 0x5400, 0xc4202e0160, 0x7f614d43ada0, 0xc42942d300, 0x0, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/v3.go:442 +0xa7b\r\ngithub.com/cockroachdb/cockroach/pkg/sql/pgwire.(*Server).ServeConn(0xc4202e0000, 0x7f614d43ada0, 0xc42942d300, 0x28bed80, 0xc4261a2140, 0x0, 0x0)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/sql/pgwire/server.go:415 +0x917\r\ngithub.com/cockroachdb/cockroach/pkg/server.(*Server).Start.func8.1(0x28bed80, 0xc4261a2140)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/server/server.go:544 +0x106\r\ngithub.com/cockroachdb/cockroach/pkg/util/netutil.(*Server).ServeWith.func1(0xc4203634a0, 0xc42000e040, 0x28bed80, 0xc4261a2140, 0xc421194240)\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/netutil/net.go:136 +0x95\r\ncreated by github.com/cockroachdb/cockroach/pkg/util/netutil.(*Server).ServeWith\r\n        /go/src/github.com/cockroachdb/cockroach/pkg/util/netutil/net.go:138 +0x239\r\n```","sql\r\nCREATE TABLE Test (ID serial PRIMARY KEY, Date TimeStampTZ,INDEX date_idx (Date));\r\ninsert into Test (Date) values ('2017-03-17 15:33:00'::timestamptz);\r\n\r\nexplain select * from Test where Date >='2017-01-01 00:00:00'::TimeStamp;\r\n"
13962,"sql: INSERT..ON CONFLICT updates columns not in UPDATE clauseRepro:\r\n\r\n1. Create a table and insert a row into it:\r\n\r\n\r\n2. Then do an upsert that updates all columns except `createdAt`:\r\n\r\n\r\n**SHA**: e66b654b73640d12c8c7e524fca03ef111431ce1\r\n**Actual**: both `createdAt` and `updatedAt` are updated\r\n**Expected**: only `updatedAt` is updated\r\n\r\nI've traced the issue to this line:\r\nhttps://github.com/cockroachdb/cockroach/blob/master/pkg/sql/tablewriter.go#L235\r\n\r\n`(*tableUpserter).init()` is terminating very early. When I remove that whole `if` block, the upsert works correctly. There are several things I don't understand here, so assigning this to @danhhz.\r\n\r\nAlso, this scenario needs a logic test.",A-sql-semantics,danhhz,"Repro:\r\n\r\n1. Create a table and insert a row into it:\r\n```sql\r\nCREATE DATABASE IF NOT EXISTS test;\r\n\r\nCREATE TABLE IF NOT EXISTS test.""users"" (""id""  SERIAL , ""name"" VARCHAR(255), ""createdAt"" TIMESTAMP WITH TIME ZONE NOT NULL, ""updatedAt"" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY (""id""));\r\n\r\nINSERT INTO test.""users"" (""id"",""name"",""createdAt"",""updatedAt"") VALUES (1,'original','2017-03-07 16:05:50.924 +00:00','2017-03-07 16:05:50.924 +00:00');\r\n\r\nSELECT * FROM test.users;\r\n+----+----------+-------------------------------+-------------------------------+\r\n| id |   name   |           createdAt           |           updatedAt           |\r\n+----+----------+-------------------------------+-------------------------------+\r\n|  1 | original | 2017-03-07 16:05:50.924+00:00 | 2017-03-07 16:05:50.924+00:00 |\r\n+----+----------+-------------------------------+-------------------------------+\r\n```\r\n\r\n2. Then do an upsert that updates all columns except `createdAt`:\r\n```sql\r\nINSERT INTO test.""users"" (""id"",""name"",""createdAt"",""updatedAt"") VALUES (1,'UPDATED','2017-03-07 16:05:51.946 +00:00','2017-03-07 16:05:51.946 +00:00')\r\nON CONFLICT (id) DO UPDATE SET ""id"" = excluded.""id"", ""name"" = excluded.""name"", ""updatedAt"" = excluded.""updatedAt"";\r\n\r\nSELECT * FROM test.users;\r\n+----+---------+-------------------------------+-------------------------------+\r\n| id |  name   |           createdAt           |           updatedAt           |\r\n+----+---------+-------------------------------+-------------------------------+\r\n|  1 | UPDATED | 2017-03-07 16:05:51.946+00:00 | 2017-03-07 16:05:51.946+00:00 |\r\n+----+---------+-------------------------------+-------------------------------+\r\n```\r\n\r\n**SHA**: e66b654b73640d12c8c7e524fca03ef111431ce1\r\n**Actual**: both `createdAt` and `updatedAt` are updated\r\n**Expected**: only `updatedAt` is updated\r\n\r\nI've traced the issue to this line:\r\nhttps://github.com/cockroachdb/cockroach/blob/master/pkg/sql/tablewriter.go#L235\r\n\r\n`(*tableUpserter).init()` is terminating very early. When I remove that whole `if` block, the upsert works correctly. There are several things I don't understand here, so assigning this to @danhhz.\r\n\r\nAlso, this scenario needs a logic test.","sql\r\nCREATE DATABASE IF NOT EXISTS test;\r\n\r\nCREATE TABLE IF NOT EXISTS test.""users"" (""id""  SERIAL , ""name"" VARCHAR(255), ""createdAt"" TIMESTAMP WITH TIME ZONE NOT NULL, ""updatedAt"" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY (""id""));\r\n\r\nINSERT INTO test.""users"" (""id"",""name"",""createdAt"",""updatedAt"") VALUES (1,'original','2017-03-07 16:05:50.924 +00:00','2017-03-07 16:05:50.924 +00:00');\r\n\r\nSELECT * FROM test.users;\r\n+----+----------+-------------------------------+-------------------------------+\r\n| id |   name   |           createdAt           |           updatedAt           |\r\n+----+----------+-------------------------------+-------------------------------+\r\n|  1 | original | 2017-03-07 16:05:50.924+00:00 | 2017-03-07 16:05:50.924+00:00 |\r\n+----+----------+-------------------------------+-------------------------------+\r\n"
13935,"perf: performance degradation in examples-go/ledger when running few-fewRun https://github.com/cockroachdb/examples-go/pull/66 and notice a performance degradation if the example is run with options where the same keys are accessed non-concurrently (i.e. back-to-back operations). There is no (or much less) degradation when a much larger set of keys is used.\r\n\r\n```\r\ngo run /Users/tschottdorf/go/src/github.com/cockroachdb/examples-go/ledger/main.go --concurrency 1 --generator few-few postgres://root@localhost:26257?sslmode=disable\r\n[...]\r\n2017/03/07 01:26:50 24 postings/sec // initially 300, but quickly more than 100, but ever decreasing\r\n2017/03/07 01:26:51 27 postings/sec\r\n2017/03/07 01:26:52 24 postings/sec\r\n```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5076964/23644391/23687156-02d5-11e7-9dfd-0334ce0d92e9.png)\r\n\r\nWith a fresh server (only difference is `s/few-few/many-many`):\r\n\r\n```\r\ngo run $GOPATH/src/github.com/cockroachdb/examples-go/ledger/main.go --concurrency 1 --generator many-many postgres://root@localhost:26257?sslmode=disable\r\n2017/03/07 01:28:48 353 postings/sec\r\n2017/03/07 01:28:49 329 postings/sec\r\n2017/03/07 01:28:50 298 postings/sec\r\n[...]\r\n2017/03/07 01:33:57 291 postings/sec\r\n2017/03/07 01:33:58 279 postings/sec\r\n2017/03/07 01:33:59 287 postings/sec\r\n2017/03/07 01:34:00 277 postings/sec\r\n2017/03/07 01:34:01 226 postings/sec\r\n```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5076964/23644608/21649f00-02d6-11e7-88e5-4baedc40bff1.png)\r\n\r\nI haven't looked closely, but one of the differences is that in the first case, perhaps we run into intents more if they cannot be resolved quickly enough. However, you'd think that when an actor runs back-to-back, the database should be able to resolve the intents fast enough to not pile up work like this - there's no contention in these examples.\r\n\r\nThe queries run here are\r\n\r\n\r\n\r\nPerhaps somehow related to #13875 (if we do run into our own intents, maybe we pile up more work than we ought to).\r\n\r\nAssigning @petermattis for triage.",C-performance,a-robinson,"Run https://github.com/cockroachdb/examples-go/pull/66 and notice a performance degradation if the example is run with options where the same keys are accessed non-concurrently (i.e. back-to-back operations). There is no (or much less) degradation when a much larger set of keys is used.\r\n\r\n```\r\ngo run /Users/tschottdorf/go/src/github.com/cockroachdb/examples-go/ledger/main.go --concurrency 1 --generator few-few postgres://root@localhost:26257?sslmode=disable\r\n[...]\r\n2017/03/07 01:26:50 24 postings/sec // initially 300, but quickly more than 100, but ever decreasing\r\n2017/03/07 01:26:51 27 postings/sec\r\n2017/03/07 01:26:52 24 postings/sec\r\n```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5076964/23644391/23687156-02d5-11e7-9dfd-0334ce0d92e9.png)\r\n\r\nWith a fresh server (only difference is `s/few-few/many-many`):\r\n\r\n```\r\ngo run $GOPATH/src/github.com/cockroachdb/examples-go/ledger/main.go --concurrency 1 --generator many-many postgres://root@localhost:26257?sslmode=disable\r\n2017/03/07 01:28:48 353 postings/sec\r\n2017/03/07 01:28:49 329 postings/sec\r\n2017/03/07 01:28:50 298 postings/sec\r\n[...]\r\n2017/03/07 01:33:57 291 postings/sec\r\n2017/03/07 01:33:58 279 postings/sec\r\n2017/03/07 01:33:59 287 postings/sec\r\n2017/03/07 01:34:00 277 postings/sec\r\n2017/03/07 01:34:01 226 postings/sec\r\n```\r\n\r\n![image](https://cloud.githubusercontent.com/assets/5076964/23644608/21649f00-02d6-11e7-88e5-4baedc40bff1.png)\r\n\r\nI haven't looked closely, but one of the differences is that in the first case, perhaps we run into intents more if they cannot be resolved quickly enough. However, you'd think that when an actor runs back-to-back, the database should be able to resolve the intents fast enough to not pile up work like this - there's no contention in these examples.\r\n\r\nThe queries run here are\r\n\r\n```sql\r\nSELECT causality_id, balance FROM accounts `+\r\n\t\t`WHERE account_id = $1 ORDER BY causality_id DESC LIMIT 1;\r\nINSERT INTO accounts (\r\n  posting_group_id,\r\n  amount,\r\n  account_id,\r\n  causality_id, -- strictly increasing in absolute time. Only used for running balance.\r\n  balance\r\n)\r\nVALUES (\r\n  $1,\t-- posting_group_id\r\n  $2, \t-- amount\r\n  $3, \t-- account_id (A)  (<10 with few-few, <MaxInt64 with many-many)\r\n  $4, \t-- causality_id\r\n  $5+CAST($2 AS BIGINT) -- (new) balance (Postgres needs the cast)\r\n), (\r\n  $1,   -- posting_group_id\r\n -$2,   -- amount\r\n  $6,   -- account_id (B) (<10 with few-few, <MaxInt64 with many-many)\r\n  $7,   -- causality_id\r\n  $8-$2 -- (new) balance\r\n)\r\n```\r\n\r\nPerhaps somehow related to #13875 (if we do run into our own intents, maybe we pile up more work than we ought to).\r\n\r\nAssigning @petermattis for triage.","sql\r\nSELECT causality_id, balance FROM accounts `+\r\n\t\t`WHERE account_id = $1 ORDER BY causality_id DESC LIMIT 1;\r\nINSERT INTO accounts (\r\n  posting_group_id,\r\n  amount,\r\n  account_id,\r\n  causality_id, -- strictly increasing in absolute time. Only used for running balance.\r\n  balance\r\n)\r\nVALUES (\r\n  $1,\t-- posting_group_id\r\n  $2, \t-- amount\r\n  $3, \t-- account_id (A)  (<10 with few-few, <MaxInt64 with many-many)\r\n  $4, \t-- causality_id\r\n  $5+CAST($2 AS BIGINT) -- (new) balance (Postgres needs the cast)\r\n), (\r\n  $1,   -- posting_group_id\r\n -$2,   -- amount\r\n  $6,   -- account_id (B) (<10 with few-few, <MaxInt64 with many-many)\r\n  $7,   -- causality_id\r\n  $8-$2 -- (new) balance\r\n)\r\n"
13924,"sql: allow directions specifications on PK/UNIQUE constraints.When a table defines a multi-column PK or UNIQUE constraint, this\r\ncauses an index to be created over these columns. For example in\r\n`CREATE TABLE foo(a INT, b INT, PRIMARY KEY(a,b))` there is an index\r\n`primary` defined over the two columns `(a,b)`. If queries\r\nsubsequently using the table use contra-ordering between the columns\r\ninvolved (e.g. `SELECT * FROM foo ORDER BY a DESC, b ASC`) this index\r\ncannot be used. Meanwhile, it is unnecessarily detrimental to\r\nperformance to mandate creating *another* index over exactly the same\r\ncolumns if *most* of the queries only use one particular (albeit\r\ncontrarian) ordering -- in that case, it is desirable instead to cause\r\nthe one and only index to be ordered properly from the get-go.\r\n\r\nThis patch addresses the issue by allowing the user to specify the index\r\nordering of the columns listed in a PRIMARY KEY or UNIQUE constraint.\r\nFor example:\r\n\r\n\r\n\r\n(Incidentally, for docs: this also allows the user to specify collation information, in the same way as for indices)\r\n\r\nFixes  #13882.\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/13924)\n<!-- Reviewable:end -->\n",docs-todo,dt,"When a table defines a multi-column PK or UNIQUE constraint, this\r\ncauses an index to be created over these columns. For example in\r\n`CREATE TABLE foo(a INT, b INT, PRIMARY KEY(a,b))` there is an index\r\n`primary` defined over the two columns `(a,b)`. If queries\r\nsubsequently using the table use contra-ordering between the columns\r\ninvolved (e.g. `SELECT * FROM foo ORDER BY a DESC, b ASC`) this index\r\ncannot be used. Meanwhile, it is unnecessarily detrimental to\r\nperformance to mandate creating *another* index over exactly the same\r\ncolumns if *most* of the queries only use one particular (albeit\r\ncontrarian) ordering -- in that case, it is desirable instead to cause\r\nthe one and only index to be ordered properly from the get-go.\r\n\r\nThis patch addresses the issue by allowing the user to specify the index\r\nordering of the columns listed in a PRIMARY KEY or UNIQUE constraint.\r\nFor example:\r\n\r\n```sql\r\n   CREATE TABLE foo(a INT, b INT, PRIMARY KEY(a DESC, b ASC))\r\n   CREATE TABLE foo(a INT, b INT, UNIQUE(a DESC, b ASC))\r\n```\r\n\r\n(Incidentally, for docs: this also allows the user to specify collation information, in the same way as for indices)\r\n\r\nFixes  #13882.\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/13924)\n<!-- Reviewable:end -->\n","sql\r\n   CREATE TABLE foo(a INT, b INT, PRIMARY KEY(a DESC, b ASC))\r\n   CREATE TABLE foo(a INT, b INT, UNIQUE(a DESC, b ASC))\r\n"
13921,"sql: CREATE TABLE AS is incorrect if the source table contains NULL valuesWhile investigating why the tests in #13187 are failing I found the following strange behavior:\r\n\r\n\r\n\r\nThe root cause of the issue (I think) is a combination of 1) that the ""create table as"" logic implicitly adds an incorrect ""not nullable"" constraint on the columns of the new table, and 2) the logic that adds the new rows silently drops the rows that fail the constraint instead of reporting an error.\r\n\r\nThe evidence that led me to propose cause (1) above is that with the patch in #13187 applied, another error occurs:\r\n\r\n\r\n",C-bug|A-sql-semantics,knz,"While investigating why the tests in #13187 are failing I found the following strange behavior:\r\n\r\n```sql\r\ncreate table bzn(x int); \r\ninsert into bzn values(null);\r\nselect * from bzn; -- this shows 1 row, with a null value\r\n\r\ncreate table bzo(z) as select * from bzn;\r\nselect * from bzo; -- this returns 0 rows\r\n```\r\n\r\nThe root cause of the issue (I think) is a combination of 1) that the ""create table as"" logic implicitly adds an incorrect ""not nullable"" constraint on the columns of the new table, and 2) the logic that adds the new rows silently drops the rows that fail the constraint instead of reporting an error.\r\n\r\nThe evidence that led me to propose cause (1) above is that with the patch in #13187 applied, another error occurs:\r\n\r\n```sql\r\ncreate table bzo(z) as select * from bzn; -- errors with ""null value in column z violates not-null constraint""\r\n```\r\n","sql\r\ncreate table bzn(x int); \r\ninsert into bzn values(null);\r\nselect * from bzn; -- this shows 1 row, with a null value\r\n\r\ncreate table bzo(z) as select * from bzn;\r\nselect * from bzo; -- this returns 0 rows\r\n"
13675,"sql: backfill.go crashes when verbosity=2The backfill code sets up a dummy planner that does not have a context. This causes crashes in `getTableLeaseByID`, `releaseLeases`.\r\n\r\n",C-bug,vivekmenezes,"The backfill code sets up a dummy planner that does not have a context. This causes crashes in `getTableLeaseByID`, `releaseLeases`.\r\n\r\n```go\r\n330 func (p *planner) getTableLeaseByID(tableID sqlbase.ID) (*sqlbase.TableDescriptor, error) {\r\n331   if log.V(2) {\r\n332     log.Infof(p.ctx(), ""planner acquiring lease on table ID %d"", tableID)\r\n333   }\r\n```","go\r\n330 func (p *planner) getTableLeaseByID(tableID sqlbase.ID) (*sqlbase.TableDescriptor, error) {\r\n331   if log.V(2) {\r\n332     log.Infof(p.ctx(), ""planner acquiring lease on table ID %d"", tableID)\r\n333   }\r\n"
13646,"sql,distsql: alter_table logic test failure with fake span resolverI am trying to make logic tests to run with DistSQL + fake span resolver in a 3 node testcluster.\r\n\r\nI am seeing a failure for https://github.com/cockroachdb/cockroach/blob/master/pkg/sql/testdata/alter_table#L477\r\n\r\n\r\n\r\nThe statement doesn't hit the expected error:\r\n```\r\n$ make test PKG=./pkg/sql TESTS=LogicDistSQL//alter_table TESTFLAGS='--max-errors 10'\r\ngo test -v  -tags '' -i ./pkg/sql\r\ngo test  -tags '' -run ""LogicDistSQL//alter_table"" -timeout 2m ./pkg/sql --max-errors 10\r\nI170217 10:12:34.204108 1 rand.go:76  Random seed: -7174852851567646062\r\nE170217 10:12:40.569642 8779 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.549548+00:00') violates unique constraint ""add_default_f_key""\r\nE170217 10:12:40.650652 8785 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.638531+00:00') violates unique constraint ""add_default_f_key""\r\nE170217 10:12:40.805665 8931 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.786231+00:00') violates unique constraint ""add_default_f_key""\r\ntest log files left over in: /tmp/TestLogic386323654\r\n--- FAIL: TestLogicDistSQL (6.92s)\r\n    --- FAIL: TestLogicDistSQL/testdata/alter_table (6.91s)\r\n    \tlogic_test.go:1485: \r\n    \t\ttestdata/alter_table:479: expected ""duplicate key value .* violates unique constraint \\\\\\""add_default_f_key\\\\\\"""", but found <nil>\r\n    \tlogic_test.go:1300: \r\n    \t\ttestdata/alter_table:479: error in statement, skipping to next file\r\nFAIL\r\nFAIL\tgithub.com/cockroachdb/cockroach/pkg/sql\t6.947s\r\nMakefile:136: recipe for target 'test' failed\r\n```\r\n\r\nI am guessing it is the new backfiller. I have pushed a temporary branch to the repo https://github.com/cockroachdb/cockroach/tree/radu-logic-test-cluster-for-vivek that can be used to repro.",C-bug,vivekmenezes,"I am trying to make logic tests to run with DistSQL + fake span resolver in a 3 node testcluster.\r\n\r\nI am seeing a failure for https://github.com/cockroachdb/cockroach/blob/master/pkg/sql/testdata/alter_table#L477\r\n\r\n```sql\r\n# Adding a unique column to an existing table with data with a default value\r\n# is illegal\r\nstatement error duplicate key value .* violates unique constraint \\""add_default_f_key\\""\r\nALTER TABLE add_default ADD f TIMESTAMP UNIQUE DEFAULT current_timestamp()\r\n```\r\n\r\nThe statement doesn't hit the expected error:\r\n```\r\n$ make test PKG=./pkg/sql TESTS=LogicDistSQL//alter_table TESTFLAGS='--max-errors 10'\r\ngo test -v  -tags '' -i ./pkg/sql\r\ngo test  -tags '' -run ""LogicDistSQL//alter_table"" -timeout 2m ./pkg/sql --max-errors 10\r\nI170217 10:12:34.204108 1 rand.go:76  Random seed: -7174852851567646062\r\nE170217 10:12:40.569642 8779 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.549548+00:00') violates unique constraint ""add_default_f_key""\r\nE170217 10:12:40.650652 8785 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.638531+00:00') violates unique constraint ""add_default_f_key""\r\nE170217 10:12:40.805665 8931 sql/distsqlrun/server.go:202  [n1] duplicate key value (f)=('2017-02-17 15:12:40.786231+00:00') violates unique constraint ""add_default_f_key""\r\ntest log files left over in: /tmp/TestLogic386323654\r\n--- FAIL: TestLogicDistSQL (6.92s)\r\n    --- FAIL: TestLogicDistSQL/testdata/alter_table (6.91s)\r\n    \tlogic_test.go:1485: \r\n    \t\ttestdata/alter_table:479: expected ""duplicate key value .* violates unique constraint \\\\\\""add_default_f_key\\\\\\"""", but found <nil>\r\n    \tlogic_test.go:1300: \r\n    \t\ttestdata/alter_table:479: error in statement, skipping to next file\r\nFAIL\r\nFAIL\tgithub.com/cockroachdb/cockroach/pkg/sql\t6.947s\r\nMakefile:136: recipe for target 'test' failed\r\n```\r\n\r\nI am guessing it is the new backfiller. I have pushed a temporary branch to the repo https://github.com/cockroachdb/cockroach/tree/radu-logic-test-cluster-for-vivek that can be used to repro.","sql\r\n# Adding a unique column to an existing table with data with a default value\r\n# is illegal\r\nstatement error duplicate key value .* violates unique constraint \\""add_default_f_key\\""\r\nALTER TABLE add_default ADD f TIMESTAMP UNIQUE DEFAULT current_timestamp()\r\n"
13399,"Table level unique constraint on columns which also have a foreign key constaint failsIs this a question, feature request, or bug report?\r\n> bug report\r\n\r\n**QUESTION**\r\n\r\nHave you checked our documentation at https://cockroachlabs.com/docs/? If you could not find an answer there, please consider asking your question in our community forum at https://forum.cockroachlabs.com/, as it would benefit other members of our community.\r\n\r\n>yes\r\n\r\nPrefer live chat? Message our engineers on our Gitter channel at https://gitter.im/cockroachdb/cockroach. \r\n\r\n**FEATURE REQUEST**\r\n\r\n1. Please describe the feature you are requesting.\r\n\r\n2. Indicate the importance of this issue to you (blocker, must-have, should-have, nice-to-have). Are you currently using any workarounds to address this issue?\r\n\r\n3. Provide any additional detail on your proposed use case for this feature.\r\n\r\n**BUG REPORT**\r\n\r\n1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. On most unix-based systems\r\n   running with defaults, this boils down to the output of\r\n\r\n     grep -F '[config]' cockroach-data/logs/cockroach.INFO\r\n\r\n   When log files are not available, supply the output of `cockroach version`\r\n   and all flags/environment variables passed to `cockroach start` instead.\r\n\r\n2. Please describe the issue you observed:\r\nColumn level unique constraint on columns having a Foreign Key constaint fails.\r\n- What did you do?\r\nCockrochDB gives this error at `./cockroach sql` client for `domain_modules`:\r\n```\r\npq: column ""module_id"" cannot be used by multiple foreign key constraints\r\n```\r\n\r\n- What did you expect to see?\r\n\r\n```\r\npq: column ""module_id"" cannot be used by multiple foreign key constraints\r\n```\r\n- What did you see instead?\r\nDocs says Column level unique constrain is supported.",O-community|C-question,dt,"Is this a question, feature request, or bug report?\r\n> bug report\r\n\r\n**QUESTION**\r\n\r\nHave you checked our documentation at https://cockroachlabs.com/docs/? If you could not find an answer there, please consider asking your question in our community forum at https://forum.cockroachlabs.com/, as it would benefit other members of our community.\r\n\r\n>yes\r\n\r\nPrefer live chat? Message our engineers on our Gitter channel at https://gitter.im/cockroachdb/cockroach. \r\n\r\n**FEATURE REQUEST**\r\n\r\n1. Please describe the feature you are requesting.\r\n\r\n2. Indicate the importance of this issue to you (blocker, must-have, should-have, nice-to-have). Are you currently using any workarounds to address this issue?\r\n\r\n3. Provide any additional detail on your proposed use case for this feature.\r\n\r\n**BUG REPORT**\r\n\r\n1. Please supply the header (i.e. the first few lines) of your most recent\r\n   log file **for each node in your cluster**. On most unix-based systems\r\n   running with defaults, this boils down to the output of\r\n\r\n     grep -F '[config]' cockroach-data/logs/cockroach.INFO\r\n\r\n   When log files are not available, supply the output of `cockroach version`\r\n   and all flags/environment variables passed to `cockroach start` instead.\r\n\r\n2. Please describe the issue you observed:\r\nColumn level unique constraint on columns having a Foreign Key constaint fails.\r\n- What did you do?\r\nCockrochDB gives this error at `./cockroach sql` client for `domain_modules`:\r\n```\r\npq: column ""module_id"" cannot be used by multiple foreign key constraints\r\n```\r\n```sql\r\n\r\nCREATE TABLE modules (\r\n  id          BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  name        VARCHAR(128) NOT NULL UNIQUE\r\n);\r\n\r\nCREATE TABLE domains (\r\n  id             BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  domain         VARCHAR(128) NOT NULL UNIQUE\r\n);\r\n\r\nCREATE TABLE domain_modules (\r\n  id         BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  domain_id  BIGINT       NOT NULL,\r\n  module_id  BIGINT       NOT NULL,\r\n  CONSTRAINT domain_modules_domain_id_fk FOREIGN KEY (domain_id) REFERENCES domains (id),\r\n  CONSTRAINT domain_modules_module_id_fk FOREIGN KEY (module_id) REFERENCES modules (id),\r\n  CONSTRAINT domain_modules_uq UNIQUE (domain_id, module_id)\r\n);\r\n```\r\n- What did you expect to see?\r\n\r\n```\r\npq: column ""module_id"" cannot be used by multiple foreign key constraints\r\n```\r\n- What did you see instead?\r\nDocs says Column level unique constrain is supported.","sql\r\n\r\nCREATE TABLE modules (\r\n  id          BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  name        VARCHAR(128) NOT NULL UNIQUE\r\n);\r\n\r\nCREATE TABLE domains (\r\n  id             BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  domain         VARCHAR(128) NOT NULL UNIQUE\r\n);\r\n\r\nCREATE TABLE domain_modules (\r\n  id         BIGSERIAL    NOT NULL PRIMARY KEY,\r\n  domain_id  BIGINT       NOT NULL,\r\n  module_id  BIGINT       NOT NULL,\r\n  CONSTRAINT domain_modules_domain_id_fk FOREIGN KEY (domain_id) REFERENCES domains (id),\r\n  CONSTRAINT domain_modules_module_id_fk FOREIGN KEY (module_id) REFERENCES modules (id),\r\n  CONSTRAINT domain_modules_uq UNIQUE (domain_id, module_id)\r\n);\r\n"
13371,"distsql: parallel streams may lose orderingWhen a query has an ORDER BY clause, we could have multiple streams that eventually get merged. We need to keep the ORDER BY columns around (throughout any additional stages may be added to these streams) so that we still have them when e eventually merge them (with an ordered synchronizer). For example:\r\n\r\n\r\nThe outer select will apply a projection and the column will go away, resulting in an incorrect plan:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/16544120/22556428/1da9f596-e935-11e6-9776-b26b06c5023f.png)\r\n\r\nThis is not as easy as tweaking projections; *any* stage that preserves some order needs to be adjusted. This ties into #10779; without that we may be pushing around a lot of unnecessary columns just because the planNodes advertise orderings. CC @andreimatei",C-bug,RaduBerinde,"When a query has an ORDER BY clause, we could have multiple streams that eventually get merged. We need to keep the ORDER BY columns around (throughout any additional stages may be added to these streams) so that we still have them when e eventually merge them (with an ordered synchronizer). For example:\r\n\r\n```sql\r\nSELECT y FROM (SELECT x, y FROM t ORDER BY x)\r\n```\r\nThe outer select will apply a projection and the column will go away, resulting in an incorrect plan:\r\n\r\n![image](https://cloud.githubusercontent.com/assets/16544120/22556428/1da9f596-e935-11e6-9776-b26b06c5023f.png)\r\n\r\nThis is not as easy as tweaking projections; *any* stage that preserves some order needs to be adjusted. This ties into #10779; without that we may be pushing around a lot of unnecessary columns just because the planNodes advertise orderings. CC @andreimatei","sql\r\nSELECT y FROM (SELECT x, y FROM t ORDER BY x)\r\n"
13171,"cli: add more choices for formatting table results.This patch replaces the boolean flag `--pretty` by a new string-valued\r\nflag `--format`.\r\n\r\nPossible values: `tsv`, `pretty`, `csv`, `html`, `records` (PostgreSQL-style), `sql`.\r\n\r\nAs previously, `--format=tsv` is the default for non-interactive\r\nsessions, and `--format=pretty` is the default for interactive\r\nsessions.\r\n\r\nFixes #12985.\r\n\r\nExamples:\r\n\r\n\r\n```\r\n1 row\r\na\tb\r\n3       4\r\n```\r\n\r\n\r\n```\r\n+---+---+\r\n| a | b |\r\n+---+---+\r\n| 3 | 4 |\r\n+---+---+\r\n(1 row)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\n-[ RECORD 1 ]\r\na | 3\r\nb | 4\r\n```\r\n\r\n\r\n\r\n\r\n<!-- Reviewable:start -->\r\n---\r\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/13171)\r\n<!-- Reviewable:end -->\r\n",docs-todo,mjibson,"This patch replaces the boolean flag `--pretty` by a new string-valued\r\nflag `--format`.\r\n\r\nPossible values: `tsv`, `pretty`, `csv`, `html`, `records` (PostgreSQL-style), `sql`.\r\n\r\nAs previously, `--format=tsv` is the default for non-interactive\r\nsessions, and `--format=pretty` is the default for interactive\r\nsessions.\r\n\r\nFixes #12985.\r\n\r\nExamples:\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=tsv\r\n```\r\n```\r\n1 row\r\na\tb\r\n3       4\r\n```\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=pretty\r\n```\r\n```\r\n+---+---+\r\n| a | b |\r\n+---+---+\r\n| 3 | 4 |\r\n+---+---+\r\n(1 row)\r\n```\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=csv\r\n```\r\n```csv\r\n1 row\r\na,b\r\n3,4\r\n```\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=html\r\n```\r\n```html\r\n<table>\r\n<thead><tr><th>a</th><th>b</th></tr></head>\r\n<tbody>\r\n<tr><td>3</td><td>4</td></tr>\r\n</tbody>\r\n</table>\r\n```\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=records\r\n```\r\n```\r\n-[ RECORD 1 ]\r\na | 3\r\nb | 4\r\n```\r\n\r\n```shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=sql\r\n```\r\n```sql\r\nCREATE TABLE results (\r\n  a STRING,\r\n  b STRING\r\n);\r\n\r\nINSERT INTO results VALUES ('3', '4');\r\n```\r\n\r\n<!-- Reviewable:start -->\r\n---\r\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/13171)\r\n<!-- Reviewable:end -->\r\n","shell\r\n./cockroach sql -e ""SELECT 3 AS a, 4 AS b"" --format=tsv\r\n"
13051,sql: panic in FLOAT -> DECIMAL conversion\r\n\r\n-> server panic\r\n\r\nIt's actually written black on white in the comment for `decimal.SetFromFloat`:\r\n```\r\n// The function will panic if the float is NaN or \xb1Inf.\r\n```\r\n\r\ncc @mjibson -- is this something we want to fix or are we just merging your new decimal code soon?,C-bug|A-sql-semantics,mjibson,```sql\r\nSELECT 'NaN'::FLOAT::DECIMAL\r\n```\r\n\r\n-> server panic\r\n\r\nIt's actually written black on white in the comment for `decimal.SetFromFloat`:\r\n```\r\n// The function will panic if the float is NaN or \xb1Inf.\r\n```\r\n\r\ncc @mjibson -- is this something we want to fix or are we just merging your new decimal code soon?,sql\r\nSELECT 'NaN'::FLOAT::DECIMAL\r\n
12637,"sql: enable the use of EXPLAIN as a data source.This patch introduces a new syntax which enables SQL queries on the\r\noutput of EXPLAIN. For example, the plan depth can be computed like\r\nthis:\r\n\r\n\r\n\r\nThis will be used to test/verify optimizations.\r\n\r\nIncidentally, the addition of this feature enables clients to emulate\r\nthe output of the `INDENT` option fully in SQL, removing the need for\r\na special case in the EXPLAIN code:\r\n\r\n\r\n\r\nThe INDENT code may thus be removed by a subsequent commit, pending\r\nthe addition of a suitable shortcut in the CLI shell.\r\n\r\n(Will be used to test the work around #10633)\r\n\r\n<!-- Reviewable:start -->\r\n---\r\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/12637)\r\n<!-- Reviewable:end -->\r\n",docs-todo,andreimatei,"This patch introduces a new syntax which enables SQL queries on the\r\noutput of EXPLAIN. For example, the plan depth can be computed like\r\nthis:\r\n\r\n```sql\r\nSELECT MAX(level) FROM [EXPLAIN SELECT * FROM kv ORDER BY v]\r\n```\r\n\r\nThis will be used to test/verify optimizations.\r\n\r\nIncidentally, the addition of this feature enables clients to emulate\r\nthe output of the `INDENT` option fully in SQL, removing the need for\r\na special case in the EXPLAIN code:\r\n\r\n```sql\r\nSELECT repeat(' ', level*3)\r\n       || CASE field\r\n          WHEN '' THEN '-> ' || type\r\n\t  ELSE         '   ' || field || ': ' || description\r\n\t  END\r\n       AS indent,\r\n       columns,\r\n       ordering\r\n  FROM [EXPLAIN(VERBOSE) SELECT * FROM kv ORDER BY v];\r\n+-------------------------------+---------+-----------+\r\n|            indent             | columns | ordering  |\r\n+-------------------------------+---------+-----------+\r\n| -> select                     | (k, v)  | +v        |\r\n| \u200c   -> sort                    | (k, v)  | +v        |\r\n| \u200c      order: +v               |         |           |\r\n| \u200c      -> render/filter        | (k, v)  | +k,unique |\r\n| \u200c         render 0: t.kv.k     |         |           |\r\n| \u200c         render 1: t.kv.v     |         |           |\r\n| \u200c         -> scan              | (k, v)  | +k,unique |\r\n| \u200c            table: kv@primary |         |           |\r\n| \u200c            spans: ALL        |         |           |\r\n+-------------------------------+---------+-----------+\r\n(9 rows)\r\n```\r\n\r\nThe INDENT code may thus be removed by a subsequent commit, pending\r\nthe addition of a suitable shortcut in the CLI shell.\r\n\r\n(Will be used to test the work around #10633)\r\n\r\n<!-- Reviewable:start -->\r\n---\r\nThis change is\u2002[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/12637)\r\n<!-- Reviewable:end -->\r\n",sql\r\nSELECT MAX(level) FROM [EXPLAIN SELECT * FROM kv ORDER BY v]\r\n
12525,"sql: rework the pre and post renderings for groupNode, remove identAggregator\r\n\r\n```\r\npanic: interface conversion: parser.Datum is nil, not *parser.DInt [recovered]\r\n        panic: SELECT 1 + count(*) FROM t.t;: interface conversion: parser.Datum is nil, not *parser.DInt [recovered]\r\n        panic: SELECT 1 + count(*) FROM t.t;: interface conversion: parser.Datum is nil, not *parser.DInt\r\n\r\ngoroutine 297 [running]:\r\npanic(0x17da0a0, 0xc4203f5090)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:500 +0x1a1\r\ngithub.com/cockroachdb/cockroach/pkg/util/stop.(*Stopper).Recover(0xc420499dd0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/util/stop/stopper.go:185 +0x6e\r\npanic(0x17da0a0, 0xc4203f5090)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:458 +0x243\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Executor).ExecuteStatements.func1(0xc420942000, 0x1d)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/executor.go:455 +0x149\r\npanic(0x180a580, 0xc420cadac0)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:458 +0x243\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser.glob..func133(0xc420aee1a8, 0x0, 0x0, 0x255ca20, 0xc4203f5068, 0x0, 0xc420211ec0, 0xc421620ea0, 0x2526cf0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/parser/eval.go:249 +0xdc\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser.(*BinaryExpr).Eval(0xc42090fc80, 0xc420aee1a8, 0xc42169bc30, 0xc4203f5040, 0x1, 0x1)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/parser/eval.go:1858 +0x1a8\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*groupNode).computeAggregates(0xc420a86c60, 0xc42090fc00, 0x0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/group.go:403 +0x287\r\n```\r\n",C-cleanup|A-sql-semantics,RaduBerinde,"```sql\r\nCREATE DATABASE t;\r\nCREATE TABLE t.t(x INT);\r\nINSERT INTO t.t(x) VALUES (42);\r\nSELECT 1 + count(*) FROM t.t; -- this works\r\nTRUNCATE t.t;\r\nSELECT 1 + count(*) FROM t.t; -- this panics!\r\n```\r\n\r\n```\r\npanic: interface conversion: parser.Datum is nil, not *parser.DInt [recovered]\r\n        panic: SELECT 1 + count(*) FROM t.t;: interface conversion: parser.Datum is nil, not *parser.DInt [recovered]\r\n        panic: SELECT 1 + count(*) FROM t.t;: interface conversion: parser.Datum is nil, not *parser.DInt\r\n\r\ngoroutine 297 [running]:\r\npanic(0x17da0a0, 0xc4203f5090)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:500 +0x1a1\r\ngithub.com/cockroachdb/cockroach/pkg/util/stop.(*Stopper).Recover(0xc420499dd0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/util/stop/stopper.go:185 +0x6e\r\npanic(0x17da0a0, 0xc4203f5090)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:458 +0x243\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*Executor).ExecuteStatements.func1(0xc420942000, 0x1d)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/executor.go:455 +0x149\r\npanic(0x180a580, 0xc420cadac0)\r\n        /mnt/kena/go1.7.3/src/runtime/panic.go:458 +0x243\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser.glob..func133(0xc420aee1a8, 0x0, 0x0, 0x255ca20, 0xc4203f5068, 0x0, 0xc420211ec0, 0xc421620ea0, 0x2526cf0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/parser/eval.go:249 +0xdc\r\ngithub.com/cockroachdb/cockroach/pkg/sql/parser.(*BinaryExpr).Eval(0xc42090fc80, 0xc420aee1a8, 0xc42169bc30, 0xc4203f5040, 0x1, 0x1)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/parser/eval.go:1858 +0x1a8\r\ngithub.com/cockroachdb/cockroach/pkg/sql.(*groupNode).computeAggregates(0xc420a86c60, 0xc42090fc00, 0x0)\r\n        /mnt/kena/go/src/github.com/cockroachdb/cockroach/pkg/sql/group.go:403 +0x287\r\n```\r\n",sql\r\nCREATE DATABASE t;\r\nCREATE TABLE t.t(x INT);\r\nINSERT INTO t.t(x) VALUES (42);\r\nSELECT 1 + count(*) FROM t.t; -- this works\r\nTRUNCATE t.t;\r\nSELECT 1 + count(*) FROM t.t; -- this panics!\r\n
12524,"sql: array_agg does not properly capture NULLsFound while investigating #5582 \r\n\r\n\r\n\r\nOn postgresql:\r\n```\r\n array_agg \r\n-----------\r\n {NULL}\r\n(1 row)\r\n```\r\n\r\non CockroachDB:\r\n```\r\n+----------------+\r\n| array_agg(b.x) |\r\n+----------------+\r\n| NULL           |\r\n+----------------+\r\n(1 row)\r\n```\r\n\r\nThe issue is that array_agg applied to a row containing NULL should accumulate the NULL in the array, instead of collapsing the entire array to NULL.\r\n\r\ncc @jordanlewis ",C-bug|A-sql-pgcompat,jordanlewis,"Found while investigating #5582 \r\n\r\n```sql\r\nselect array_agg(b.x) from (select 1 as x) a left join (select 2 as x) b using(x);\r\n```\r\n\r\nOn postgresql:\r\n```\r\n array_agg \r\n-----------\r\n {NULL}\r\n(1 row)\r\n```\r\n\r\non CockroachDB:\r\n```\r\n+----------------+\r\n| array_agg(b.x) |\r\n+----------------+\r\n| NULL           |\r\n+----------------+\r\n(1 row)\r\n```\r\n\r\nThe issue is that array_agg applied to a row containing NULL should accumulate the NULL in the array, instead of collapsing the entire array to NULL.\r\n\r\ncc @jordanlewis ",sql\r\nselect array_agg(b.x) from (select 1 as x) a left join (select 2 as x) b using(x);\r\n
11208,"sql,kv: exponential queries can cause ranges to grow faster than they can split and crash a nodeSo I was exploring alternatives to #11203 and ran the following experiment:\r\n\r\nFirst I create a single-node cluster with replication factor 1 (`echo 'num_replicas: 1' | ./cockroach set zone .default -f -`)\r\n\r\nThen in the SQL shell:\r\n\r\n\r\n\r\nA couple of observations, which perhaps should translate into issues:\r\n- if I run the inserts too fast behind each other, the node crashes with OOM (the ranges grow faster than they are split)\r\n- the raftMu lock is being held for very long (seconds at  a time)\r\n- the following error cropped up after ~200 rows:\r\n```\r\nW161123 18:37:17.126535 1888 kv/txn_coord_sender.go:771  [n1,split,s1,r22/1:/Table/51/1/196135{404\u2026-535\u2026}] heartbeat to ""storage/replica_command.go:2446 (*Replica).adminSplitWithDescriptor"" id=329f81be key=/Local/Range/""\\xbb\\x89\\xfd\\x02\\xb8\\xd0\\x1c\\x8e\\xa7\\x00\\x01""/RangeDescriptor rw=true pri=0.06858520 iso=SE\r\nRIALIZABLE stat=PENDING epo=0 ts=1479926225.099704909,0 orig=1479926225.099704909,0 max=1479926225.349704909,0 wto=false rop=false failed: failed to send RPC: sending to all 1 replicas failed; last error: range 22: replica {1 1 1} not lease holder; <nil> is\r\n```\r\n(I'm not sure that a `nil` lease holder is good news; remember this is a 1-node cluster)\r\n\r\ncc @petermattis @spencerkimball \r\n",docs-known-limitation,petermattis,"So I was exploring alternatives to #11203 and ran the following experiment:\r\n\r\nFirst I create a single-node cluster with replication factor 1 (`echo 'num_replicas: 1' | ./cockroach set zone .default -f -`)\r\n\r\nThen in the SQL shell:\r\n\r\n```sql\r\ncreate database t;\r\ncreate table t.b(x bytes);\r\n--- make a single 4MB row\r\ninsert into t.b(x) values(repeat(b'\\x01', 4*1024*1024));\r\n--- duplicate it a couple of times:\r\ninsert into t.b(x) select x from t.b; -- 1 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 2 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 4 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 8 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 16 more rows - 2 ranges total\r\n-- (from this point each new insert doubles the number of ranges)\r\n\r\n-- alternatively the following query creates ~80MB of data at a time, ie somewhat more than 1 range:\r\ninsert into t.b(x) select x from (select * from t.b limit 20); -- 20 more rows - +1 range\r\n```\r\n\r\nA couple of observations, which perhaps should translate into issues:\r\n- if I run the inserts too fast behind each other, the node crashes with OOM (the ranges grow faster than they are split)\r\n- the raftMu lock is being held for very long (seconds at  a time)\r\n- the following error cropped up after ~200 rows:\r\n```\r\nW161123 18:37:17.126535 1888 kv/txn_coord_sender.go:771  [n1,split,s1,r22/1:/Table/51/1/196135{404\u2026-535\u2026}] heartbeat to ""storage/replica_command.go:2446 (*Replica).adminSplitWithDescriptor"" id=329f81be key=/Local/Range/""\\xbb\\x89\\xfd\\x02\\xb8\\xd0\\x1c\\x8e\\xa7\\x00\\x01""/RangeDescriptor rw=true pri=0.06858520 iso=SE\r\nRIALIZABLE stat=PENDING epo=0 ts=1479926225.099704909,0 orig=1479926225.099704909,0 max=1479926225.349704909,0 wto=false rop=false failed: failed to send RPC: sending to all 1 replicas failed; last error: range 22: replica {1 1 1} not lease holder; <nil> is\r\n```\r\n(I'm not sure that a `nil` lease holder is good news; remember this is a 1-node cluster)\r\n\r\ncc @petermattis @spencerkimball \r\n","sql\r\ncreate database t;\r\ncreate table t.b(x bytes);\r\n--- make a single 4MB row\r\ninsert into t.b(x) values(repeat(b'\\x01', 4*1024*1024));\r\n--- duplicate it a couple of times:\r\ninsert into t.b(x) select x from t.b; -- 1 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 2 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 4 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 8 more rows - 1 range total\r\ninsert into t.b(x) select x from t.b; -- 16 more rows - 2 ranges total\r\n-- (from this point each new insert doubles the number of ranges)\r\n\r\n-- alternatively the following query creates ~80MB of data at a time, ie somewhat more than 1 range:\r\ninsert into t.b(x) select x from (select * from t.b limit 20); -- 20 more rows - +1 range\r\n"
11192,"sql: implement the logic to add a constraint to a joinFor #10633 we need a primitive that can add a new comparison to an existing join operation.\r\n(later we actually need to do this for any data source, but for #10633 joins are sufficient)\r\n\r\nThe primitive should be a new method `joinNode.addFilter` defined as follows:\r\n\r\n\r\nThe behavior should be defined as follows: \r\n- if `newFilter.Operator != Eq`, then return false\r\n- if the left and right operands are not IndexedVar, then return false\r\n- if the left and right operands are referring to the same side of the join, then return false\r\n- otherwise, given the left and right operands `x` and `y` transform the join predicate as follows:\r\n\r\n  - `crossPredicate` -> `ON EQUALS ((x), (y))`, return true\r\n  - `onPredicate{filter:E}` -> `onPredicate{filter:AndExpr{Left:E, Right:newFilter}}`, return true\r\n  - `ON EQUALS ((a, b, c...), (u, v, w...))` -> `ON EQUALS ((a, b, c, ..., x), (u, v, w..., y))`, then return true\r\n\r\nIn any case the method must not change the set of result columns (which columns are present in the result of the join)",help wanted|E-intermediate,knz,"For #10633 we need a primitive that can add a new comparison to an existing join operation.\r\n(later we actually need to do this for any data source, but for #10633 joins are sufficient)\r\n\r\nThe primitive should be a new method `joinNode.addFilter` defined as follows:\r\n```go\r\nfunc (n *joinNode) addFilter(newFilter parser.ComparisonExpr) bool\r\n```\r\n\r\nThe behavior should be defined as follows: \r\n- if `newFilter.Operator != Eq`, then return false\r\n- if the left and right operands are not IndexedVar, then return false\r\n- if the left and right operands are referring to the same side of the join, then return false\r\n- otherwise, given the left and right operands `x` and `y` transform the join predicate as follows:\r\n\r\n  - `crossPredicate` -> `ON EQUALS ((x), (y))`, return true\r\n  - `onPredicate{filter:E}` -> `onPredicate{filter:AndExpr{Left:E, Right:newFilter}}`, return true\r\n  - `ON EQUALS ((a, b, c...), (u, v, w...))` -> `ON EQUALS ((a, b, c, ..., x), (u, v, w..., y))`, then return true\r\n\r\nIn any case the method must not change the set of result columns (which columns are present in the result of the join)",go\r\nfunc (n *joinNode) addFilter(newFilter parser.ComparisonExpr) bool\r\n
10558,"sql: add support for WITH ORDINALITYThis patch introduces support for a new syntax ""WITH ORDINALITY"" that\r\ncan be tucked after data sources in FROM clauses and which adds an\r\nextra ""ordinality"" column that numbers the data rows. See some\r\nexamples and potential use cases below.\r\n\r\nThis feature was originally defined by PostgreSQL for set-generating\r\nfunctions; the CockroachDB version generalizes this form. See the pg\r\ndocumentation here:\r\nhttps://www.postgresql.org/docs/9.6/static/functions-srf.html\r\n\r\nIt produces results similar to using `row_number()` with a single SQL\r\nwindow, like this: `SELECT row_number() OVER () as ordinality, * FROM\r\n...` However WITH ORDINALITY achieves the same both with a shorter\r\nsyntax and, more importantly, *does not need to store all the source\r\nrows in memory* like the processing of window functions does (instead\r\nthe rows are numbered as they are found).\r\n\r\n**However, WITH ORDINALITY is not performance neutral either.**\r\n\r\n1) WITH ORDINALITY fully sequentializes the data source on which it is\r\n   applied and necessarily causes any parallelizations/distribution of\r\n   the query of said data source to be funneled sequentially through\r\n   one node.\r\n\r\n2) WITH ORDINALITY disables the algebraic-relational commutativity\r\n   between filtering and other operations: applying a filter before or\r\n   after WITH ORDINALITY produces different results. For example,\r\n   `SELECT * FROM foo WITH ORDINALITY WHERE x = y` does not produce\r\n   the same ordinality column as `SELECT * FROM (SELECT * FROM foo\r\n   WHERE x = y) WITH ORDINALITY`. This is not just a matter of\r\n   explaining how it works; this particular aspect fundamentally\r\n   limits which kinds of high-level optimizations can be performed on\r\n   a SQL query using WITH ORDINALITY.\r\n\r\n**In other words, introducing WITH ORDINALITY on a data source will\r\nprevent optimizations of the data source that produces the results\r\nbeing numbered.**\r\n\r\nAlso, it runs against the spirit of SQL by letting the user think\r\nabout rows as a list/stream instead of a set, and thus should be\r\ndiscouraged unless an operation cannot be achieved using regular\r\nSQL operators.\r\n\r\nExample use:\r\n\r\n\r\n\r\nSome example use cases for this feature:\r\n- taking one every N row of a result (`ordinality % N == 0`)\r\n- adding a row numbering for `CREATE TABLE .. AS`\r\n\r\n(Why? Felt inspired by #10520, realizing I've wanted that feature for quite a while myself.)\r\n\r\n<!-- Reviewable:start -->\r\n\r\n---\r\nThis change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/10558)\r\n<!-- Reviewable:end -->\r\n",docs-todo,nvanbenschoten,"This patch introduces support for a new syntax ""WITH ORDINALITY"" that\r\ncan be tucked after data sources in FROM clauses and which adds an\r\nextra ""ordinality"" column that numbers the data rows. See some\r\nexamples and potential use cases below.\r\n\r\nThis feature was originally defined by PostgreSQL for set-generating\r\nfunctions; the CockroachDB version generalizes this form. See the pg\r\ndocumentation here:\r\nhttps://www.postgresql.org/docs/9.6/static/functions-srf.html\r\n\r\nIt produces results similar to using `row_number()` with a single SQL\r\nwindow, like this: `SELECT row_number() OVER () as ordinality, * FROM\r\n...` However WITH ORDINALITY achieves the same both with a shorter\r\nsyntax and, more importantly, *does not need to store all the source\r\nrows in memory* like the processing of window functions does (instead\r\nthe rows are numbered as they are found).\r\n\r\n**However, WITH ORDINALITY is not performance neutral either.**\r\n\r\n1) WITH ORDINALITY fully sequentializes the data source on which it is\r\n   applied and necessarily causes any parallelizations/distribution of\r\n   the query of said data source to be funneled sequentially through\r\n   one node.\r\n\r\n2) WITH ORDINALITY disables the algebraic-relational commutativity\r\n   between filtering and other operations: applying a filter before or\r\n   after WITH ORDINALITY produces different results. For example,\r\n   `SELECT * FROM foo WITH ORDINALITY WHERE x = y` does not produce\r\n   the same ordinality column as `SELECT * FROM (SELECT * FROM foo\r\n   WHERE x = y) WITH ORDINALITY`. This is not just a matter of\r\n   explaining how it works; this particular aspect fundamentally\r\n   limits which kinds of high-level optimizations can be performed on\r\n   a SQL query using WITH ORDINALITY.\r\n\r\n**In other words, introducing WITH ORDINALITY on a data source will\r\nprevent optimizations of the data source that produces the results\r\nbeing numbered.**\r\n\r\nAlso, it runs against the spirit of SQL by letting the user think\r\nabout rows as a list/stream instead of a set, and thus should be\r\ndiscouraged unless an operation cannot be achieved using regular\r\nSQL operators.\r\n\r\nExample use:\r\n\r\n```sql\r\nSELECT * FROM mytable\r\n+---+---+------+\r\n| x | y |  z   |\r\n+---+---+------+\r\n| a | x | 5675 |\r\n| b | y |   45 |\r\n| c | z |  123 |\r\n+---+---+------+\r\n\r\nSELECT * FROM mytable WITH ORDINALITY\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| b | y |   45 |          2 |\r\n| c | z |  123 |          3 |\r\n+---+---+------+------------+\r\n\r\nSELECT * FROM mytable WITH ORDINALITY WHERE x != 'b'\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| c | z |  123 |          3 |\r\n+---+---+------+------------+\r\n\r\nSELECT * FROM (SELECT * FROM mytable WHERE x != b) WITH ORDINALITY\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| c | z |  123 |          2 |\r\n+---+---+------+------------+\r\n```\r\n\r\nSome example use cases for this feature:\r\n- taking one every N row of a result (`ordinality % N == 0`)\r\n- adding a row numbering for `CREATE TABLE .. AS`\r\n\r\n(Why? Felt inspired by #10520, realizing I've wanted that feature for quite a while myself.)\r\n\r\n<!-- Reviewable:start -->\r\n\r\n---\r\nThis change is [<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/cockroachdb/cockroach/10558)\r\n<!-- Reviewable:end -->\r\n",sql\r\nSELECT * FROM mytable\r\n+---+---+------+\r\n| x | y |  z   |\r\n+---+---+------+\r\n| a | x | 5675 |\r\n| b | y |   45 |\r\n| c | z |  123 |\r\n+---+---+------+\r\n\r\nSELECT * FROM mytable WITH ORDINALITY\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| b | y |   45 |          2 |\r\n| c | z |  123 |          3 |\r\n+---+---+------+------------+\r\n\r\nSELECT * FROM mytable WITH ORDINALITY WHERE x != 'b'\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| c | z |  123 |          3 |\r\n+---+---+------+------------+\r\n\r\nSELECT * FROM (SELECT * FROM mytable WHERE x != b) WITH ORDINALITY\r\n+---+---+------+------------+\r\n| x | y |  z   | ordinality |\r\n+---+---+------+------------+\r\n| a | x | 5675 |          1 |\r\n| c | z |  123 |          2 |\r\n+---+---+------+------------+\r\n
10493,"storage: Remove StoreTestingKnobs.TestingEvalFilterFrom `storagebase/base.go`, posted here for visibility. The below TODO should be addresses rather sooner than later (since folks will need to write new tests and shouldn't have to be confused by half-baked refactorings):\r\n\r\n\r\n\r\n@andreimatei assigning you since you know these things, but feel free to toss this to someone else.",C-cleanup|A-kv-client,andreimatei,"From `storagebase/base.go`, posted here for visibility. The below TODO should be addresses rather sooner than later (since folks will need to write new tests and shouldn't have to be confused by half-baked refactorings):\r\n\r\n```go\r\n// ReplicaCommandFilter may be used in tests through the StorageTestingMocker to\r\n// intercept the handling of commands and artificially generate errors. Return\r\n// nil to continue with regular processing or non-nil to terminate processing\r\n// with the returned error. Note that in a multi-replica test this filter will\r\n// be run once for each replica and must produce consistent results each time.\r\n//\r\n// TODO(tschottdorf): clean this up. Tests which use this all need to be\r\n// refactored to use explicitly a proposal-intercepting filter (not written\r\n// yet, but it's basically this one here when proposer-evaluated KV is on) or\r\n// a ReplicaApplyFilter (see below).\r\ntype ReplicaCommandFilter func(args FilterArgs) *roachpb.Error\r\n\r\n// A ReplicaApplyFilter can be used in testing to influence the error returned\r\n// from proposals after they apply.\r\ntype ReplicaApplyFilter func(args ApplyFilterArgs) *roachpb.Error\r\n\r\n// ReplicaResponseFilter is used in unittests to modify the outbound\r\n// response returned to a waiting client after a replica command has\r\n// been processed. This filter is invoked only by the command proposer.\r\ntype ReplicaResponseFilter func(roachpb.BatchRequest, *roachpb.BatchResponse) *roachpb.Error\r\n```\r\n\r\n@andreimatei assigning you since you know these things, but feel free to toss this to someone else.","go\r\n// ReplicaCommandFilter may be used in tests through the StorageTestingMocker to\r\n// intercept the handling of commands and artificially generate errors. Return\r\n// nil to continue with regular processing or non-nil to terminate processing\r\n// with the returned error. Note that in a multi-replica test this filter will\r\n// be run once for each replica and must produce consistent results each time.\r\n//\r\n// TODO(tschottdorf): clean this up. Tests which use this all need to be\r\n// refactored to use explicitly a proposal-intercepting filter (not written\r\n// yet, but it's basically this one here when proposer-evaluated KV is on) or\r\n// a ReplicaApplyFilter (see below).\r\ntype ReplicaCommandFilter func(args FilterArgs) *roachpb.Error\r\n\r\n// A ReplicaApplyFilter can be used in testing to influence the error returned\r\n// from proposals after they apply.\r\ntype ReplicaApplyFilter func(args ApplyFilterArgs) *roachpb.Error\r\n\r\n// ReplicaResponseFilter is used in unittests to modify the outbound\r\n// response returned to a waiting client after a replica command has\r\n// been processed. This filter is invoked only by the command proposer.\r\ntype ReplicaResponseFilter func(roachpb.BatchRequest, *roachpb.BatchResponse) *roachpb.Error\r\n"
10394,"distsql: outer joins, distinction between ON and WHERE conditionsThis is a follow up on #10346, there is the following unaddressed complication with outer joins. \r\n\r\nConsider this example:\r\n\r\n\r\n\r\nIn our case the streams will be ordered by K, but the rest of the expression `(U.A>V.C)` will have to be evaluated (like the filter). With inner joins, there is no difference between an `ON` condition and `WHERE` condition, but with outer joins there is. The example above results in:\r\n```\r\n k | a | b | k | c | d \r\n---+---+---+---+---+---\r\n 1 | 2 | 2 |   |   |  \r\n   |   |   | 1 | 3 | 3\r\n```\r\nWith the condition moved to `WHERE (SELECT * FROM U FULL OUTER JOIN V ON U.K=V.K WHERE U.A > V.C;)` we get no results.\r\n\r\nIn general this means we will need both an `ON` expression and a `WHERE` expression in the spec (but we should disallow setting both if the join is inner).",S-3-ux-surprise,knz,"This is a follow up on #10346, there is the following unaddressed complication with outer joins. \r\n\r\nConsider this example:\r\n\r\n```sql\r\nCREATE TABLE U (K INT, A INT, B INT);\r\nCREATE TABLE V (K INT, C INT, D INT);\r\n\r\nINSERT INTO U VALUES (1, 2, 2);\r\nINSERT INTO V VALUES (1, 3, 3);\r\n\r\nSELECT * FROM U FULL OUTER JOIN V ON U.K=V.K AND U.A>V.C;\r\n```\r\n\r\nIn our case the streams will be ordered by K, but the rest of the expression `(U.A>V.C)` will have to be evaluated (like the filter). With inner joins, there is no difference between an `ON` condition and `WHERE` condition, but with outer joins there is. The example above results in:\r\n```\r\n k | a | b | k | c | d \r\n---+---+---+---+---+---\r\n 1 | 2 | 2 |   |   |  \r\n   |   |   | 1 | 3 | 3\r\n```\r\nWith the condition moved to `WHERE (SELECT * FROM U FULL OUTER JOIN V ON U.K=V.K WHERE U.A > V.C;)` we get no results.\r\n\r\nIn general this means we will need both an `ON` expression and a `WHERE` expression in the spec (but we should disallow setting both if the join is inner).","sql\r\nCREATE TABLE U (K INT, A INT, B INT);\r\nCREATE TABLE V (K INT, C INT, D INT);\r\n\r\nINSERT INTO U VALUES (1, 2, 2);\r\nINSERT INTO V VALUES (1, 3, 3);\r\n\r\nSELECT * FROM U FULL OUTER JOIN V ON U.K=V.K AND U.A>V.C;\r\n"
