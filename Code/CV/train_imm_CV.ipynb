{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.466477Z",
     "start_time": "2024-04-06T07:03:20.823265Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "from datasets import *\n",
    "from model import *\n",
    "from losses import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3f81ba-3072-4f6a-91ad-1e39d6bf8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_text2tensor(_path, _ckpt):\n",
    "    # dataset label vectorize\n",
    "    dataset = pd.read_csv(_path)\n",
    "    print('dataset shape:{}'.format(dataset.shape))\n",
    "    dataset, D_ids2token, B_ids2token = label_vectorize(dataset)\n",
    "    n_classes = [len(D_ids2token), len(B_ids2token)]\n",
    "    print('n_classes: ', n_classes)\n",
    "\n",
    "    check_point = _ckpt\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "    # datset tensorize\n",
    "    dataset['x_C'] = dataset['Context'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['x_A'] = dataset['AST'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['y'] = dataset['Dev_vec'] + dataset['Btype_vec']\n",
    "    dataset['y'] = dataset['y'].map(tensor_func)\n",
    "\n",
    "    dataset = TextCodeDataset(dataset)\n",
    "    return dataset, n_classes, (D_ids2token, B_ids2token)\n",
    "    \n",
    "# todo: finish 'dataset_split_load' func\n",
    "def dataset_split_load(dataset, trn_ids, tst_ids):\n",
    "    # subsampler\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(trn_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(tst_ids)\n",
    "\n",
    "    # warp dataset into dataloader\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=10, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=10, sampler=test_subsampler)\n",
    "\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e077f9b-0e5f-4595-9b03-b7549b2e5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_forward(model, loss_fn, x, y, device, train_loss, l):\n",
    "    x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "    x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "    y = y.to(device)\n",
    "                \n",
    "    outputs = model(x_C, x_A)\n",
    "    loss = loss_fn(outputs, y.float())\n",
    "    train_loss += loss.item() / l\n",
    "\n",
    "    return outputs, loss, train_loss\n",
    "\n",
    "def one_backward(optimizer, loss, lr_scheduler):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "def update_metrics(y, outputs, n_classes, val_acc, val_f1, l):\n",
    "    metric = metrics(y, outputs, split_pos = n_classes)\n",
    "    val_acc[0] += metric['acc'][0]/l\n",
    "    val_acc[1] += metric['acc'][1]/l\n",
    "    val_f1[0] += metric['F1'][0]/l\n",
    "    val_f1[1] += metric['F1'][1]/l\n",
    "    return val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710ecaf-fa9b-4e52-b973-b544e363f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imm(_path, _logname, _loss_fn, _use_ast = True, _is_textcnn = False, _num_epochs = 50, _bsz = 8,\n",
    "              _lr = 3e-5, _ckpt = 'bert-base-uncased', _k_folds = 10, device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    logstr = _logname + '\\n' + '-'*60 + '\\n'\n",
    "    logname = '../res_log/' + _logname + '.txt'\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # tensorize dataset\n",
    "    dataset_tensor, n_classes, ids2token = dataset_text2tensor(_path, _ckpt)\n",
    "    logstr += 'dataset shape:{}\\n n_classes:{}\\n'.format(len(dataset_tensor), n_classes) + '-'*60 + '\\n'\n",
    "\n",
    "    # K-Flod\n",
    "    results = []\n",
    "    kfold = KFold(n_splits=_k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (trn_ids, tst_ids) in enumerate(kfold.split(dataset_tensor)):\n",
    "        print('-'*40 + '  FOLD{}  '.format(fold) + '_'*40)\n",
    "\n",
    "        # split train/test & wrap into dataloader According to (trn_ids, tst_ids)\n",
    "        train_dataloader, val_dataloader = dataset_split_load(dataset_tensor, trn_ids, tst_ids)\n",
    "        \n",
    "\n",
    "        # model\n",
    "        if _is_textcnn:\n",
    "            model = MetaModel(n_classes = n_classes, use_AST=_use_ast)\n",
    "        else:\n",
    "            model = PretrainModel(text_ckpt=_ckpt, code_ckpt=_ckpt, n_classes=n_classes, use_AST=_use_ast)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # loss, optimizer, lr_scheduler\n",
    "        loss_fn = _loss_fn.to(device)\n",
    "        # TODO: compare different optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=_lr)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=_num_epochs * len(train_dataloader),\n",
    "        )\n",
    "\n",
    "        # train process\n",
    "        for epoch in trange(_num_epochs):  \n",
    "            # train\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for x, y in train_dataloader:\n",
    "                # forward\n",
    "                outputs, loss, train_loss = one_forward(model, loss_fn, x, y, device, train_loss, len(train_dataloader))             \n",
    "                # backward\n",
    "                one_backward(optimizer, loss, lr_scheduler)\n",
    "            # logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\n",
    "            # print('{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss))\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            val_loss, val_acc, val_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "            for x, y in val_dataloader:\n",
    "                # forward\n",
    "                outputs, loss, val_loss = one_forward(model, loss_fn, x, y, device, val_loss, len(val_dataloader)) \n",
    "                # update metric\n",
    "                val_acc, val_f1 = update_metrics(y.to(device), outputs, n_classes, val_acc, val_f1, len(val_dataloader))\n",
    "            # logstr += '{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(epoch, val_loss, val_acc, val_f1)\n",
    "            # print('{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}'.format(epoch, val_loss, val_acc, val_f1))\n",
    "\n",
    "        # update n-Fold result\n",
    "        results.append([val_loss, val_acc[0], val_acc[1], val_f1[0], val_f1[1]])\n",
    "        tmpstr = '-'*30 + '{}TH FOLD RESULT'.format(fold) + '-'*30 + \\\n",
    "              '\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(results[-1][0], results[-1][1:3], results[-1][3:])\n",
    "        logstr += tmpstr\n",
    "        print(tmpstr)\n",
    "\n",
    "    ava_result = torch.mean(torch.tensor(results),dim=0)\n",
    "    tmp_str = '-'*30 + '10-FOLD AVA-RESULT' + '-'*30 + \\\n",
    "              '\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(ava_result[0], ava_result[1:3], ava_result[3:])\n",
    "    logstr += tmp_str\n",
    "    print(tmp_str)\n",
    "        \n",
    "    \n",
    "    with open(logname, 'w') as f:\n",
    "        f.write(logstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06eddbeb-50fe-4beb-bcfc-07638df32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 datasets\n",
    "pathlist = [\n",
    "    ('../Data/aspnet/aspnet_2.csv', 'aspnet'),\n",
    "    ('../Data/efcore/efcore_2.csv', 'efcore'),\n",
    "    ('../Data/elasticSearch/elasticSearch_2.csv', 'elasticSearch'),\n",
    "    # ('../Data/mixedRealityToolUnity/mixedRealityToolUnity_2.csv', 'mixedRealityToolUnity'),\n",
    "    # ('../Data/monoGame/monoGame_2.csv', 'monoGame'),\n",
    "    ('../Data/powershell/powerShell_2.csv', 'powerShell'),\n",
    "    ('../Data/realmJava/realmJava_2.csv', 'realmJava'),\n",
    "    ('../Data/roslyn/roslyn_2.csv', 'roslyn'),\n",
    "]\n",
    "losslist = [\n",
    "    (nn.BCEWithLogitsLoss(), 'BCE'),\n",
    "    # (CustomizedBCELoss(), 'CBCE'),\n",
    "    # (AsymmetricLossOptimized(), 'ASL'),\n",
    "]\n",
    "\n",
    "ckptlist = [\n",
    "    ('bert-base-uncased', 'Multi-triage'),  # just for tokenize\n",
    "    # ('bert-base-uncased', ' Bert'),\n",
    "    # ('roberta-base', 'Robert'),\n",
    "]\n",
    "\n",
    "astlist = [\n",
    "    'no_AST',\n",
    "    'use_AST',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9f5bcf-58f3-4100-b88f-f1f52b620be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1006, 7)\n",
      "n_classes:  [32, 88]\n",
      "----------------------------------------  FOLD0  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 50/50 [00:13<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------0TH FOLD RESULT------------------------------\n",
      " val_loss: 0.2643410915678198\n",
      " val_acc:[0.9869318387725138, 0.9790289185263894]\n",
      " val_f1: [0.790907836639731, 0.006060561984218199]\n",
      "\n",
      "----------------------------------------  FOLD1  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------1TH FOLD RESULT------------------------------\n",
      " val_loss: 0.2595035135746002\n",
      " val_acc:[0.9877841039137408, 0.9776859716935591]\n",
      " val_f1: [0.8090896374820052, 0.021211980891983243]\n",
      "\n",
      "----------------------------------------  FOLD2  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------2TH FOLD RESULT------------------------------\n",
      " val_loss: 0.26049606095660816\n",
      " val_acc:[0.9832386482845653, 0.9776859662749551]\n",
      " val_f1: [0.7225623402825693, 0.0]\n",
      "\n",
      "----------------------------------------  FOLD3  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------3TH FOLD RESULT------------------------------\n",
      " val_loss: 0.25390764732252463\n",
      " val_acc:[0.9781250140883706, 0.978719023140994]\n",
      " val_f1: [0.6545443757771479, 0.004545418405879658]\n",
      "\n",
      "----------------------------------------  FOLD4  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------4TH FOLD RESULT------------------------------\n",
      " val_loss: 0.26582675088535657\n",
      " val_acc:[0.9747159264304421, 0.9739669669758191]\n",
      " val_f1: [0.59760663416215, 0.008181751651088625]\n",
      "\n",
      "----------------------------------------  FOLD5  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------5TH FOLD RESULT------------------------------\n",
      " val_loss: 0.30496192384849896\n",
      " val_acc:[0.9880681850693445, 0.9776859879493716]\n",
      " val_f1: [0.8090896374820074, 0.0036363332452089667]\n",
      "\n",
      "----------------------------------------  FOLD6  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------6TH FOLD RESULT------------------------------\n",
      " val_loss: 0.2951791480183601\n",
      " val_acc:[0.9868750214576721, 0.9753409087657928]\n",
      " val_f1: [0.787418107125055, 0.013999907033236697]\n",
      "\n",
      "----------------------------------------  FOLD7  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------7TH FOLD RESULT------------------------------\n",
      " val_loss: 0.291102945804596\n",
      " val_acc:[0.9800000250339507, 0.9765909314155578]\n",
      " val_f1: [0.6799988515018538, 0.0]\n",
      "\n",
      "----------------------------------------  FOLD8  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------8TH FOLD RESULT------------------------------\n",
      " val_loss: 0.26519374400377277\n",
      " val_acc:[0.986250013113022, 0.9760227620601652]\n",
      " val_f1: [0.7874273236722782, 0.0]\n",
      "\n",
      "----------------------------------------  FOLD9  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------9TH FOLD RESULT------------------------------\n",
      " val_loss: 0.2668348804116249\n",
      " val_acc:[0.9809375107288362, 0.9777272820472717]\n",
      " val_f1: [0.7148375344097367, 0.0]\n",
      "\n",
      "------------------------------10-FOLD AVA-RESULT------------------------------\n",
      " val_loss: 0.27273476123809814\n",
      " val_acc:tensor([0.9833, 0.9770])\n",
      " val_f1: tensor([0.7353, 0.0058])\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage BCE use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1006, 7)\n",
      "n_classes:  [32, 88]\n",
      "----------------------------------------  FOLD0  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:24<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------0TH FOLD RESULT------------------------------\n",
      " val_loss: 0.26347632028839807\n",
      " val_acc:[0.9838068214329808, 0.9745867848396301]\n",
      " val_f1: [0.7454533345340496, 0.013636273372704052]\n",
      "\n",
      "----------------------------------------  FOLD1  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------1TH FOLD RESULT------------------------------\n",
      " val_loss: 0.26419164782220667\n",
      " val_acc:[0.9792613712224094, 0.9779958833347668]\n",
      " val_f1: [0.6430819391516377, 0.009090854966824395]\n",
      "\n",
      "----------------------------------------  FOLD2  ________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:06<00:19,  1.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m logname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([path[\u001b[38;5;241m1\u001b[39m], ckpt[\u001b[38;5;241m1\u001b[39m], loss[\u001b[38;5;241m1\u001b[39m], ast])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, logname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_imm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_logname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../res_log/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_loss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_use_ast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_ast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_textcnn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mis_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ckpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mtrain_imm\u001b[0;34m(_path, _logname, _loss_fn, _use_ast, _is_textcnn, _num_epochs, _bsz, _lr, _ckpt, _k_folds, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m     outputs, loss, train_loss \u001b[38;5;241m=\u001b[39m one_forward(model, loss_fn, x, y, device, train_loss, \u001b[38;5;28mlen\u001b[39m(train_dataloader))             \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mone_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# print('{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss))\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# val\u001b[39;00m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mone_backward\u001b[0;34m(optimizer, loss, lr_scheduler)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:361\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    360\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    364\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 648\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_ops.py:447\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OpOverload(op=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, overload=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overloadname\n\u001b[1;32m    445\u001b[0m     )\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in pathlist:\n",
    "    for ckpt in ckptlist:\n",
    "        for loss in losslist:\n",
    "            for ast in astlist:\n",
    "                is_t, u_ast = (ckpt[1] == 'Multi-triage'), (ast == 'use_AST') \n",
    "\n",
    "                logname = ' '.join([path[1], ckpt[1], loss[1], ast])\n",
    "                print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            \n",
    "                train_imm(_path = path[0], _logname = '../res_log/' + logname + '.txt', \n",
    "                      _loss_fn = loss[0], _use_ast = u_ast, _is_textcnn = is_t, _ckpt = ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f758c22-1da8-4bac-917e-a7309461873c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617388cb-09b9-46f4-8843-be99446f8fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
