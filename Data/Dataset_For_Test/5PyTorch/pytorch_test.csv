31515,"cumsum backward for zero-length zensors## \U0001f41b Bug\r\n\r\nCalling `cumsum` on a zero-dimension tensor raises an error in backward.\r\nThe error occurs for both `cumsum` and `cumprod` and on both CPU and GPU\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File ""narrow_test.py"", line 6, in <module>\r\n    xx.backward()\r\n  File ""xxx/torch/tensor.py"", line 166, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""xxx/torch/autograd/__init__.py"", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: start (0) + length (1) exceeds dimension size (0).\r\n```\r\n\r\n## Expected behavior\r\n\r\nI'm not sure if the gradient calculation makes any sense in this edge case, but at least the error message should be more clear.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\nGPU 2: TITAN RTX\r\nGPU 3: TITAN RTX\r\nGPU 4: TITAN RTX\r\nGPU 5: TITAN RTX\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py37_cu101    pytorch\r\n```\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen",module: bootcamp|module: autograd|triaged,mrshenli,"## \U0001f41b Bug\r\n\r\nCalling `cumsum` on a zero-dimension tensor raises an error in backward.\r\nThe error occurs for both `cumsum` and `cumprod` and on both CPU and GPU\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nx = torch.zeros(2, 0, requires_grad=True)\r\nxx = x.cumsum(dim=-1).sum()\r\nxx.backward()\r\n```\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File ""narrow_test.py"", line 6, in <module>\r\n    xx.backward()\r\n  File ""xxx/torch/tensor.py"", line 166, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""xxx/torch/autograd/__init__.py"", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: start (0) + length (1) exceeds dimension size (0).\r\n```\r\n\r\n## Expected behavior\r\n\r\nI'm not sure if the gradient calculation makes any sense in this edge case, but at least the error message should be more clear.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\nGPU 2: TITAN RTX\r\nGPU 3: TITAN RTX\r\nGPU 4: TITAN RTX\r\nGPU 5: TITAN RTX\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py37_cu101    pytorch\r\n```\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen","python\r\nimport torch\r\n\r\nx = torch.zeros(2, 0, requires_grad=True)\r\nxx = x.cumsum(dim=-1).sum()\r\nxx.backward()\r\n"
31497,"torch.no_grad() context manager seems to leak memoryI have a lazy loop that is used to evaluate the model. This version doesn't use any noticeable memory (memory_allocated < 1Gb):\r\n\r\n\r\nThe context manager version:\r\n\r\n\r\nleads to OOM even on a 32Gb gpu:\r\n```\r\n...\r\nmemory_allocated 7.163547136 memory_cached 19.10505472\r\nmemory_allocated 7.164415488 memory_cached 19.10505472\r\nmemory_allocated 8.457690624 memory_cached 17.953718272\r\nmemory_allocated 8.454446592 memory_cached 22.206742528\r\nmemory_allocated 8.457297408 memory_cached 22.206742528\r\nmemory_allocated 9.7478528 memory_cached 22.013804544\r\nmemory_allocated 11.03991552 memory_cached 24.29550592\r\n\r\nRuntimeError: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 31.72 GiB total capacity; 24.89 GiB already allocated; 6.12 MiB free; 30.70 GiB reserved in total by PyTorch)\r\n```\r\n\r\nA few things about this:\r\n1) context manager seems to be broken\r\n2) huge memory fragmentation: 11Gb of allocated memory, 24 Gb cached, 32Gb total, still OOM? it tried to allocate just 152Mb with 12Gb of free memory, fragmentation must be super-huge...\r\n3) memory_allocated of 11Gb is not reported in OOM message\r\n4) terminology discrepancy: torch.cuda.memory_cached seems to be equivalent to ""already allocated"" from OOM message. In presence of also existing torch.cuda.memory_allocated this is confusing. Probably the OOM message should also say cached. Otherwise, it's pretty easy to mix up allocated, cached, reserved.\r\n\r\nthis is on PyTorch nightly from a few days ago\r\n\r\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @ngimel",high priority|module: autograd|module: cuda|module: memory usage|triaged,peterbell10,"I have a lazy loop that is used to evaluate the model. This version doesn't use any noticeable memory (memory_allocated < 1Gb):\r\n```python\r\ndef apply_model(data_loader, model):\r\n    for dataset_name_, audio_path_, reference_, x, xlen, y, ylen in data_loader:\r\n        x, xlen, y, ylen = x.to(args.device), xlen.to(args.device), y.to(args.device), ylen.to(args.device)\r\n        with torch.no_grad():\r\n            log_probs, output_lengths, loss = map(model(x, xlen, y = y, ylen = ylen).get, ['log_probs', 'output_lengths', 'loss'])\r\n        yield log_probs\r\n```\r\n\r\nThe context manager version:\r\n```python\r\n@torch.no_grad()\r\ndef apply_model(data_loader, model):\r\n    print('memory_allocated', torch.cuda.memory_allocated() / 1e9, 'memory_cached', torch.cuda.memory_cached() / 1e9)\r\n    for dataset_name_, audio_path_, reference_, x, xlen, y, ylen in data_loader:\r\n        x, xlen, y, ylen = x.to(args.device), xlen.to(args.device), y.to(args.device), ylen.to(args.device)\r\n        log_probs, output_lengths, loss = map(model(x, xlen, y = y, ylen = ylen).get, ['log_probs', 'output_lengths', 'loss'])\r\n        yield log_probs\r\n```\r\n\r\nleads to OOM even on a 32Gb gpu:\r\n```\r\n...\r\nmemory_allocated 7.163547136 memory_cached 19.10505472\r\nmemory_allocated 7.164415488 memory_cached 19.10505472\r\nmemory_allocated 8.457690624 memory_cached 17.953718272\r\nmemory_allocated 8.454446592 memory_cached 22.206742528\r\nmemory_allocated 8.457297408 memory_cached 22.206742528\r\nmemory_allocated 9.7478528 memory_cached 22.013804544\r\nmemory_allocated 11.03991552 memory_cached 24.29550592\r\n\r\nRuntimeError: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 31.72 GiB total capacity; 24.89 GiB already allocated; 6.12 MiB free; 30.70 GiB reserved in total by PyTorch)\r\n```\r\n\r\nA few things about this:\r\n1) context manager seems to be broken\r\n2) huge memory fragmentation: 11Gb of allocated memory, 24 Gb cached, 32Gb total, still OOM? it tried to allocate just 152Mb with 12Gb of free memory, fragmentation must be super-huge...\r\n3) memory_allocated of 11Gb is not reported in OOM message\r\n4) terminology discrepancy: torch.cuda.memory_cached seems to be equivalent to ""already allocated"" from OOM message. In presence of also existing torch.cuda.memory_allocated this is confusing. Probably the OOM message should also say cached. Otherwise, it's pretty easy to mix up allocated, cached, reserved.\r\n\r\nthis is on PyTorch nightly from a few days ago\r\n\r\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @ngimel","python\r\ndef apply_model(data_loader, model):\r\n    for dataset_name_, audio_path_, reference_, x, xlen, y, ylen in data_loader:\r\n        x, xlen, y, ylen = x.to(args.device), xlen.to(args.device), y.to(args.device), ylen.to(args.device)\r\n        with torch.no_grad():\r\n            log_probs, output_lengths, loss = map(model(x, xlen, y = y, ylen = ylen).get, ['log_probs', 'output_lengths', 'loss'])\r\n        yield log_probs\r\n"
31413,"Allreduce for sparse tensors is not working```\r\n~/Workspace$ pip show torch\r\nName: torch\r\nVersion: 1.3.1\r\n```\r\n\r\nCode for allreduce sparse tensors\r\n\r\n\r\n\r\nResults\r\n\r\n```\r\n~/Workspace$ python main.py\r\nMessage from 0 Before\t tensor([0.1167, 0.0000, 0.8951, 0.4808, 0.8937])\r\nMessage from 1 Before\t tensor([0.2334, 0.0000, 1.7901, 0.9616, 1.7874])\r\nMessage from 1 After\t tensor([0.2334, 0.0000, 1.7901, 0.9616, 1.7874])\r\nMessage from 0 After\t tensor([0.1167, 0.0000, 0.8951, 0.4808, 0.8937])\r\n```\r\n\r\nSince the usage (sparse allreduce) is not documented yet https://github.com/pytorch/pytorch/issues/1303, I assume the usage is the same as dense tensors. However, it does not yield the expected result.\r\n\r\nThe full code snippet is attached below\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",high priority|oncall: distributed|triaged,zhaojuanmao,"```\r\n~/Workspace$ pip show torch\r\nName: torch\r\nVersion: 1.3.1\r\n```\r\n\r\nCode for allreduce sparse tensors\r\n\r\n```python\r\ndef run_sparse(rank, size):\r\n    a = torch.randn(5) * (rank + 1)\r\n    mask = a > 0  # sparse mask\r\n    i = mask.nonzero()  # value indexes\r\n    v = a[mask]  # sparse values\r\n    t = torch.sparse.FloatTensor(i.t(), v, a.size())\r\n\r\n    pprint(""Before\\t"", t.to_dense())\r\n    dist.all_reduce(t)\r\n    pprint(""After\\t"", t.to_dense())\r\n```\r\n\r\nResults\r\n\r\n```\r\n~/Workspace$ python main.py\r\nMessage from 0 Before\t tensor([0.1167, 0.0000, 0.8951, 0.4808, 0.8937])\r\nMessage from 1 Before\t tensor([0.2334, 0.0000, 1.7901, 0.9616, 1.7874])\r\nMessage from 1 After\t tensor([0.2334, 0.0000, 1.7901, 0.9616, 1.7874])\r\nMessage from 0 After\t tensor([0.1167, 0.0000, 0.8951, 0.4808, 0.8937])\r\n```\r\n\r\nSince the usage (sparse allreduce) is not documented yet https://github.com/pytorch/pytorch/issues/1303, I assume the usage is the same as dense tensors. However, it does not yield the expected result.\r\n\r\nThe full code snippet is attached below\r\n\r\n```python\r\nimport os, time\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom torch.multiprocessing import Process\r\n\r\ndef pprint(*msg):\r\n    print(""Message from %s"" % rank, *msg)\r\n\r\ndef run(rank, size):\r\n    t = torch.ones(5) * (rank + 1)\r\n    pprint(""Before\\t"", t)\r\n    dist.all_reduce(t)\r\n    pprint(""After\\t"", t)\r\n\r\ndef run_sparse(rank, size):\r\n    a = torch.randn(5) * (rank + 1)\r\n    mask = a > 0  # sparse mask\r\n    i = mask.nonzero()  # value indexes\r\n    v = a[mask]  # sparse values\r\n    t = torch.sparse.FloatTensor(i.t(), v, a.size())\r\n\r\n    pprint(""Before\\t"", t.to_dense())\r\n    dist.all_reduce(t)\r\n    pprint(""After\\t"", t.to_dense())\r\n\r\n\r\ndef init_process(rank, size, fn, backend='gloo'):\r\n    """""" Initialize the distributed environment. """"""\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_process, args=(rank, size, run_sparse))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\n\ncc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528","python\r\ndef run_sparse(rank, size):\r\n    a = torch.randn(5) * (rank + 1)\r\n    mask = a > 0  # sparse mask\r\n    i = mask.nonzero()  # value indexes\r\n    v = a[mask]  # sparse values\r\n    t = torch.sparse.FloatTensor(i.t(), v, a.size())\r\n\r\n    pprint(""Before\\t"", t.to_dense())\r\n    dist.all_reduce(t)\r\n    pprint(""After\\t"", t.to_dense())\r\n"
31412,"torch.stft causes segmentation fault with data parellel.## \U0001f41b Bug\r\n\r\ntorch.stft causes segmentation fault with data parellel.\r\n\r\n## To Reproduce\r\n\r\n\r\nThis exits with segmentation fault after `single gpu passed`\r\n\r\n## Expected behavior\r\n\r\nWork with data_parallel.\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.1-py3.7_cuda10.0.130_cudnn7.6.3_0 (1.0.1 can work)\r\n - OS (e.g., Linux): CentOS7\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: cuda 10.0\r\n - GPU models and configuration: Tesla K80 \r\n - Any other relevant information:\r\n\r\n\r\ncc @ezyang @gchanan @zou3519",high priority|module: crash|module: nn|triaged|module: data parallel,peterbell10,"## \U0001f41b Bug\r\n\r\ntorch.stft causes segmentation fault with data parellel.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nclass Stft(torch.nn.Module):\r\n    def forward(self, x):\r\n        y = torch.stft(x, n_fft=512)\r\n        return y\r\na = Stft()\r\nx = torch.randn(2, 2000)\r\ntorch.nn.parallel.data_parallel(a, (x,), device_ids=[0])\r\nprint(""single gpu passed"")\r\ntorch.nn.parallel.data_parallel(a, (x,), device_ids=[0, 1])\r\nprint(""two gpu passed"")\r\n```\r\n\r\nThis exits with segmentation fault after `single gpu passed`\r\n\r\n## Expected behavior\r\n\r\nWork with data_parallel.\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.1-py3.7_cuda10.0.130_cudnn7.6.3_0 (1.0.1 can work)\r\n - OS (e.g., Linux): CentOS7\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: cuda 10.0\r\n - GPU models and configuration: Tesla K80 \r\n - Any other relevant information:\r\n\r\n\r\ncc @ezyang @gchanan @zou3519","python\r\nimport torch\r\nclass Stft(torch.nn.Module):\r\n    def forward(self, x):\r\n        y = torch.stft(x, n_fft=512)\r\n        return y\r\na = Stft()\r\nx = torch.randn(2, 2000)\r\ntorch.nn.parallel.data_parallel(a, (x,), device_ids=[0])\r\nprint(""single gpu passed"")\r\ntorch.nn.parallel.data_parallel(a, (x,), device_ids=[0, 1])\r\nprint(""two gpu passed"")\r\n"
31283,"Compile Error for .cu files when using libtorch (caused by nvcc flags)## \U0001f41b Bug\r\n\r\nAfter the cmake is done, when compiling .cu files in my project, the following error will occur:\r\n\r\nThe `flags.make` shows the following result:\r\n\r\nThe `-O3` flag was added using `CMAKE_CUDA_FLAGS` in the `CMakeLists.txt`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Write a .cu file in the project\r\n2. cmake then make\r\n3. The error occurs\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThose -Wxxx flags should not be added as cuda flags.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Compile from source\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source):\r\n\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.2/7.5\r\n - GPU models and configuration: NVIDIA GTX Titan\r\n - Any other relevant information:\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee @ngimel",high priority|module: build|module: cpp|module: cuda|triaged,malfet,"## \U0001f41b Bug\r\n\r\nAfter the cmake is done, when compiling .cu files in my project, the following error will occur:\r\n```bash\r\nnvcc fatal   : Unknown option 'Wall'\r\n```\r\nThe `flags.make` shows the following result:\r\n```bash\r\n$cat CMakeFiles/train.dir/flags.make\r\n# CMAKE generated file: DO NOT EDIT!\r\n# Generated by ""Unix Makefiles"" Generator, CMake Version 3.15\r\n\r\n# compile CUDA with /usr/local/cuda-9.2/bin/nvcc\r\n# compile CXX with /usr/bin/c++\r\nCUDA_FLAGS =  -O3   -D_GLIBCXX_USE_CXX11_ABI=1 -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp -std=c++14\r\n\r\nCUDA_DEFINES = -DAT_PARALLEL_OPENMP=1\r\n......\r\n```\r\nThe `-O3` flag was added using `CMAKE_CUDA_FLAGS` in the `CMakeLists.txt`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Write a .cu file in the project\r\n2. cmake then make\r\n3. The error occurs\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThose -Wxxx flags should not be added as cuda flags.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Compile from source\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source):\r\n```bash\r\nBUILD_TORCH=ON \\\r\nCMAKE_PREFIX_PATH=""/usr/bin/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/;/opt/intel/compilers_and_libraries_2018.1.163/linux/mkl/include/"" \\\r\nCUDA_BIN_PATH=/usr/local/cuda/bin \\\r\nCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda/ \\\r\nCUDNN_LIB_DIR=/usr/local/cuda/lib64 \\\r\nUSE_CUDA=1 \\\r\nUSE_NNPACK=1 \\\r\nMAX_JOBS=8 \\\r\npython3 setup.py build\r\n```\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.2/7.5\r\n - GPU models and configuration: NVIDIA GTX Titan\r\n - Any other relevant information:\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @malfet @yf225 @glaringlee @ngimel",bash\r\nnvcc fatal   : Unknown option 'Wall'\r\n
31271,"Performance regression in CPU copy kernel since 1.3.1## \U0001f41b Bug\r\n\r\nThe copy kernel is ~3-4x slower compared to 1.3.1 when copying from bytes to floats. I think that same type copy performance has also regressed, but I'm not entirely sure.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n1.3.1:\r\n```\r\n56.1 ms \xb1 2.07 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n54.6 ms \xb1 1.33 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n55.2 ms \xb1 1.46 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n1.4.0 nightly (1.4.0dev201):\r\n```\r\n186 ms \xb1 204 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n187 ms \xb1 1.53 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n187 ms \xb1 1.57 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel @mruberry",high priority|triage review|module: performance|triaged,zasdfgbnm,"## \U0001f41b Bug\r\n\r\nThe copy kernel is ~3-4x slower compared to 1.3.1 when copying from bytes to floats. I think that same type copy performance has also regressed, but I'm not entirely sure.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\ntorch.set_num_threads(1)\r\nx = torch.empty(1024*1024*100, dtype=torch.uint8)\r\ny = torch.empty(1024*1024*100, dtype=torch.float)\r\n\r\n%timeit y.copy_(x)\r\n%timeit y.copy_(x)\r\n%timeit y.copy_(x)\r\n```\r\n\r\n1.3.1:\r\n```\r\n56.1 ms \xb1 2.07 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n54.6 ms \xb1 1.33 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n55.2 ms \xb1 1.46 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n1.4.0 nightly (1.4.0dev201):\r\n```\r\n186 ms \xb1 204 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n187 ms \xb1 1.53 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n187 ms \xb1 1.57 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel @mruberry","python\r\nimport torch\r\ntorch.set_num_threads(1)\r\nx = torch.empty(1024*1024*100, dtype=torch.uint8)\r\ny = torch.empty(1024*1024*100, dtype=torch.float)\r\n\r\n%timeit y.copy_(x)\r\n%timeit y.copy_(x)\r\n%timeit y.copy_(x)\r\n"
31268,"JIT trace() -> save() -> load() -> copy() for modules with submodules is broken## \U0001f41b Bug\r\n\r\nModules with registered submodules (tested with resnet and mobilenet v2), cannot be copied after saved and loaded. **Note: copying without saving and loading does not produce an error**. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nCreates stacktrace:\r\n\r\n\r\n## Expected behavior\r\n\r\nWe would expect there to not be an issue with copying a loaded JIT module with a submodule.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.3.1\r\n - OS: MacOS / Linux\r\n - How you installed PyTorch: Pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.0.130 (Linux)\r\n - GPU models and configuration: 4 X GeForce RTX 2080 Ti (Linux)\r\n - Any other relevant information: Reproducible using libtorch `torch::jit::script::Module::clone` in the currently nightly build.\r\n\r\ncc @suo",oncall: jit|triaged,jerryzh168,"## \U0001f41b Bug\r\n\r\nModules with registered submodules (tested with resnet and mobilenet v2), cannot be copied after saved and loaded. **Note: copying without saving and loading does not produce an error**. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nfrom torchvision import models\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super(MyModule, self).__init__()\r\n        self.submodule = models.resnet50()\r\n    def forward(self, x):\r\n        return self.submodule(x)\r\n\r\nmy_module = MyModule()\r\nmy_module.eval()\r\nx = torch.rand(1, 3, 224, 224)\r\ntraced_cell = torch.jit.trace(my_module, x)\r\ntraced_cell.save('my_module.pth')\r\nloaded_model = torch.jit.load('my_module.pth')\r\nloaded_model.copy()\r\n```\r\n\r\nCreates stacktrace:\r\n```bash\r\nUsers/****/venv/bin/python /Users/****/temporal-shift-module/test.py\r\nTraceback (most recent call last):\r\n  File ""/Users/****/temporal-shift-module/test.py"", line 20, in <module>\r\n    loaded_model.copy()\r\n  File ""/Users/****/venv/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1644, in copy\r\n    return ScriptModule(_cpp_module=self._c._clone())\r\nRuntimeError: class '__torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d' already defined. (register_type at ../torch/csrc/jit/script/compilation_unit.h:166)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x114e249e7 in libc10.dylib)\r\nframe #1: torch::jit::script::CompilationUnit::register_type(std::__1::shared_ptr<c10::NamedType>) + 524 (0x11ecaeefc in libtorch.dylib)\r\nframe #2: torch::jit::script::create_module_object(c10::QualifiedName, std::__1::shared_ptr<torch::jit::script::CompilationUnit>, bool) + 1178 (0x11ef3819a in libtorch.dylib)\r\nframe #3: torch::jit::script::Module::Module(c10::QualifiedName, std::__1::shared_ptr<torch::jit::script::CompilationUnit>, bool) + 247 (0x11ef38567 in libtorch.dylib)\r\nframe #4: torch::jit::script::Module::clone_impl(std::__1::unordered_map<std::__1::shared_ptr<c10::Type>, std::__1::shared_ptr<c10::Type>, std::__1::hash<std::__1::shared_ptr<c10::Type> >, std::__1::equal_to<std::__1::shared_ptr<c10::Type> >, std::__1::allocator<std::__1::pair<std::__1::shared_ptr<c10::Type> const, std::__1::shared_ptr<c10::Type> > > >&) const + 328 (0x11ef3e128 in libtorch.dylib)\r\nframe #5: torch::jit::script::Module::clone_impl(std::__1::unordered_map<std::__1::shared_ptr<c10::Type>, std::__1::shared_ptr<c10::Type>, std::__1::hash<std::__1::shared_ptr<c10::Type> >, std::__1::equal_to<std::__1::shared_ptr<c10::Type> >, std::__1::allocator<std::__1::pair<std::__1::shared_ptr<c10::Type> const, std::__1::shared_ptr<c10::Type> > > >&) const + 1291 (0x11ef3e4eb in libtorch.dylib)\r\nframe #6: torch::jit::script::Module::clone_impl(std::__1::unordered_map<std::__1::shared_ptr<c10::Type>, std::__1::shared_ptr<c10::Type>, std::__1::hash<std::__1::shared_ptr<c10::Type> >, std::__1::equal_to<std::__1::shared_ptr<c10::Type> >, std::__1::allocator<std::__1::pair<std::__1::shared_ptr<c10::Type> const, std::__1::shared_ptr<c10::Type> > > >&) const + 1291 (0x11ef3e4eb in libtorch.dylib)\r\nframe #7: torch::jit::script::Module::clone() const + 43 (0x11ef3df1b in libtorch.dylib)\r\nframe #8: _ZZN8pybind1112cpp_function10initializeIZNS0_C1IN5torch3jit6script6ModuleES6_JEJNS_4nameENS_9is_methodENS_7siblingEEEEMT0_KFT_DpT1_EDpRKT2_EUlPKS6_E_S6_JSL_EJS7_S8_S9_EEEvOSB_PFSA_SD_ESJ_ENKUlRNS_6detail13function_callEE_clESS_ + 116 (0x11b77cab4 in libtorch_python.dylib)\r\nframe #9: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x11b2b3cfc in libtorch_python.dylib)\r\nframe #10: _PyMethodDef_RawFastCallKeywords + 545 (0x10b3dbf28 in Python)\r\nframe #11: _PyCFunction_FastCallKeywords + 44 (0x10b3db493 in Python)\r\nframe #12: call_function + 746 (0x10b4711d6 in Python)\r\nframe #13: _PyEval_EvalFrameDefault + 7016 (0x10b469da9 in Python)\r\nframe #14: function_code_fastcall + 112 (0x10b3db86e in Python)\r\nframe #15: call_function + 753 (0x10b4711dd in Python)\r\nframe #16: _PyEval_EvalFrameDefault + 7016 (0x10b469da9 in Python)\r\nframe #17: _PyEval_EvalCodeWithName + 1835 (0x10b471a6f in Python)\r\nframe #18: PyEval_EvalCode + 42 (0x10b4681b9 in Python)\r\nframe #19: run_mod + 54 (0x10b496f61 in Python)\r\nframe #20: PyRun_FileExFlags + 164 (0x10b495f7c in Python)\r\nframe #21: PyRun_SimpleFileExFlags + 266 (0x10b495636 in Python)\r\nframe #22: pymain_main + 4896 (0x10b4ae106 in Python)\r\nframe #23: _Py_UnixMain + 75 (0x10b4ae993 in Python)\r\nframe #24: start + 1 (0x7fff54d36015 in libdyld.dylib)\r\n```\r\n\r\n## Expected behavior\r\n\r\nWe would expect there to not be an issue with copying a loaded JIT module with a submodule.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.3.1\r\n - OS: MacOS / Linux\r\n - How you installed PyTorch: Pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.0.130 (Linux)\r\n - GPU models and configuration: 4 X GeForce RTX 2080 Ti (Linux)\r\n - Any other relevant information: Reproducible using libtorch `torch::jit::script::Module::clone` in the currently nightly build.\r\n\r\ncc @suo","python\r\nimport torch\r\nfrom torchvision import models\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super(MyModule, self).__init__()\r\n        self.submodule = models.resnet50()\r\n    def forward(self, x):\r\n        return self.submodule(x)\r\n\r\nmy_module = MyModule()\r\nmy_module.eval()\r\nx = torch.rand(1, 3, 224, 224)\r\ntraced_cell = torch.jit.trace(my_module, x)\r\ntraced_cell.save('my_module.pth')\r\nloaded_model = torch.jit.load('my_module.pth')\r\nloaded_model.copy()\r\n"
31192,"Quantized LSTM: ""RuntimeError: expected scalar type Float but found QInt32""## \U0001f41b Bug\r\n\r\nWhen using the quantized LSTM, an error pops up saying `RuntimeError: expected scalar type Float but found QInt32`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nThe behavior should be equivalent to the non-quantized LSTM. Replacing the above code with `nn` instead of `nnqd` works.\r\n\r\n## Environment\r\n\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,jamesr66a,"## \U0001f41b Bug\r\n\r\nWhen using the quantized LSTM, an error pops up saying `RuntimeError: expected scalar type Float but found QInt32`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn.quantized.dynamic as nnqd\r\n\r\nseq_len = 128\r\nbatch = 16\r\ninput_size = 3\r\nhidden_size = 7\r\nnum_layers = 2\r\nbias = True\r\nbidirectional = False\r\n\r\nx = torch.rand(seq_len, batch, input_size)\r\nh = torch.rand(num_layers * (bidirectional + 1), batch, hidden_size)\r\nc = torch.rand(num_layers * (bidirectional + 1), batch, hidden_size)\r\n\r\ndtype = torch.qint8\r\n\r\ncell_dq = nnqd.LSTM(input_size=input_size,\r\n                    hidden_size=hidden_size,\r\n                    num_layers=num_layers,\r\n                    bias=bias,\r\n                    batch_first=False,\r\n                    dropout=0.0,\r\n                    bidirectional=bidirectional,\r\n                    dtype=dtype)\r\n\r\ny, (h, c) = cell_dq(x, (h, c))\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe behavior should be equivalent to the non-quantized LSTM. Replacing the above code with `nn` instead of `nnqd` works.\r\n\r\n## Environment\r\n\r\n```console\r\npython ./torch/utils/collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+bf60aba\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.1\r\nGCC version: Could not collect\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.4.0a0+bf60aba\r\n[pip3] torchsummary==1.5.1\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nimport torch\r\nimport torch.nn.quantized.dynamic as nnqd\r\n\r\nseq_len = 128\r\nbatch = 16\r\ninput_size = 3\r\nhidden_size = 7\r\nnum_layers = 2\r\nbias = True\r\nbidirectional = False\r\n\r\nx = torch.rand(seq_len, batch, input_size)\r\nh = torch.rand(num_layers * (bidirectional + 1), batch, hidden_size)\r\nc = torch.rand(num_layers * (bidirectional + 1), batch, hidden_size)\r\n\r\ndtype = torch.qint8\r\n\r\ncell_dq = nnqd.LSTM(input_size=input_size,\r\n                    hidden_size=hidden_size,\r\n                    num_layers=num_layers,\r\n                    bias=bias,\r\n                    batch_first=False,\r\n                    dropout=0.0,\r\n                    bidirectional=bidirectional,\r\n                    dtype=dtype)\r\n\r\ny, (h, c) = cell_dq(x, (h, c))\r\n"
31108,"test_int_pow_cuda failed on Windows## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun the following code:\r\n\r\n\r\nError message:\r\n```\r\n  File ""<string>"", line 1, in <module>\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\tensor.py"", line 162, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 311, in _str\r\n    tensor_str = _tensor_str(self, indent)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 209, in _tensor_str\r\n    formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 83, in __init__\r\n    value_str = '{}'.format(value)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\tensor.py"", line 412, in __format__\r\n    return self.item().__format__(format_spec)\r\nRuntimeError: CUDA error: unspecified launch failure\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error,\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows Server 2019 Datacenter\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 426.00\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nI'm pretty sure that TDR is not a problem because the GPU is already in TCC mode.\r\nBTW, it is not a problem in 1.3.1 too.\n\ncc @ngimel @peterjc123",module: windows|module: cuda|module: tests|triaged,peterjc123,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun the following code:\r\n```python\r\nimport torch\r\n\r\n# The following ones will fail.\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int64).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int32).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int16).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int8).pow(4); print(a)\r\n\r\n# The following ones will be okay.\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int64).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int32).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int16).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int8).pow(2); print(a)\r\n```\r\n\r\nError message:\r\n```\r\n  File ""<string>"", line 1, in <module>\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\tensor.py"", line 162, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 311, in _str\r\n    tensor_str = _tensor_str(self, indent)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 209, in _tensor_str\r\n    formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\_tensor_str.py"", line 83, in __init__\r\n    value_str = '{}'.format(value)\r\n  File ""C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\tensor.py"", line 412, in __format__\r\n    return self.item().__format__(format_spec)\r\nRuntimeError: CUDA error: unspecified launch failure\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nNo error,\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows Server 2019 Datacenter\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 426.00\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\nI'm pretty sure that TDR is not a problem because the GPU is already in TCC mode.\r\nBTW, it is not a problem in 1.3.1 too.\n\ncc @ngimel @peterjc123","python\r\nimport torch\r\n\r\n# The following ones will fail.\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int64).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int32).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int16).pow(4); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int8).pow(4); print(a)\r\n\r\n# The following ones will be okay.\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int64).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int32).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int16).pow(2); print(a)\r\na = torch.tensor([-1,  1], device='cuda:0', dtype=torch.int8).pow(2); print(a)\r\n"
30986,"Wrong result for CPU implementation (m,).addmv((m, 0), (0,)) when BLAS is not used## \U0001f41b Bug\r\n\r\n\r\n\r\ngives\r\n\r\n```\r\ntensor([3., 3., 3., 3., 3.])\r\ntensor([9, 9, 9, 9, 9], dtype=torch.int32)\r\n```\r\n\r\nTo be fixed in https://github.com/pytorch/pytorch/pull/30898",triaged,zasdfgbnm,"## \U0001f41b Bug\r\n\r\n```python\r\nimport torch\r\n\r\ndef run(dtype):\r\n    a = torch.ones((5,), device='cpu', dtype=dtype)\r\n    b = torch.empty((5, 0), device='cpu', dtype=dtype)\r\n    c = torch.empty((0,), device='cpu', dtype=dtype)\r\n    print(a.addmv(b, c, alpha=1, beta=3))\r\n\r\nrun(torch.float)\r\nrun(torch.int)\r\n```\r\n\r\ngives\r\n\r\n```\r\ntensor([3., 3., 3., 3., 3.])\r\ntensor([9, 9, 9, 9, 9], dtype=torch.int32)\r\n```\r\n\r\nTo be fixed in https://github.com/pytorch/pytorch/pull/30898","python\r\nimport torch\r\n\r\ndef run(dtype):\r\n    a = torch.ones((5,), device='cpu', dtype=dtype)\r\n    b = torch.empty((5, 0), device='cpu', dtype=dtype)\r\n    c = torch.empty((0,), device='cpu', dtype=dtype)\r\n    print(a.addmv(b, c, alpha=1, beta=3))\r\n\r\nrun(torch.float)\r\nrun(torch.int)\r\n"
30865,"stft does not consistently check window device## \U0001f41b Bug\r\n\r\nConsider a situation when applying `torch.stft` to audio `x`, which is in CUDA memory. We also have a given tensor `window`, which is not in CUDA memory yet.\r\nWhen `n_fft` is equal to `win_length`, it causes `RuntimeError: expected device cuda:0 but got device cpu`.\r\n\r\n## To Reproduce\r\n\r\nSee my Google Colab notebook:\r\nhttps://colab.research.google.com/drive/15ZOc5SnFwXsb3-vgIbzdd2roeY6kOV2H\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nNo error is expected here.\r\n\n\ncc @mruberry @peterbell10",module: error checking|triaged|module: fft,peterbell10,"## \U0001f41b Bug\r\n\r\nConsider a situation when applying `torch.stft` to audio `x`, which is in CUDA memory. We also have a given tensor `window`, which is not in CUDA memory yet.\r\nWhen `n_fft` is equal to `win_length`, it causes `RuntimeError: expected device cuda:0 but got device cpu`.\r\n\r\n## To Reproduce\r\n\r\nSee my Google Colab notebook:\r\nhttps://colab.research.google.com/drive/15ZOc5SnFwXsb3-vgIbzdd2roeY6kOV2H\r\n\r\n```python\r\nimport torch\r\nimport librosa\r\n\r\nprint(torch.__version__) # 1.3.1\r\nprint(librosa.__version__) # 0.6.3\r\n\r\nx, sr = librosa.load(librosa.util.example_audio_file(), offset=15.0, duration=5.0)\r\nprint(x.shape) # (110250,)\r\nx = torch.from_numpy(x).cuda()\r\nwindow = torch.hann_window(window_length=400)\r\n\r\ntorch.stft(x, n_fft=512, hop_length=160, win_length=400, window=window).shape # torch.Size([257, 690, 2])\r\n\r\ntorch.stft(x, n_fft=400, hop_length=160, win_length=400, window=window).shape # RuntimeError: expected device cuda:0 but got device cpu\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo error is expected here.\r\n\n\ncc @mruberry @peterbell10","python\r\nimport torch\r\nimport librosa\r\n\r\nprint(torch.__version__) # 1.3.1\r\nprint(librosa.__version__) # 0.6.3\r\n\r\nx, sr = librosa.load(librosa.util.example_audio_file(), offset=15.0, duration=5.0)\r\nprint(x.shape) # (110250,)\r\nx = torch.from_numpy(x).cuda()\r\nwindow = torch.hann_window(window_length=400)\r\n\r\ntorch.stft(x, n_fft=512, hop_length=160, win_length=400, window=window).shape # torch.Size([257, 690, 2])\r\n\r\ntorch.stft(x, n_fft=400, hop_length=160, win_length=400, window=window).shape # RuntimeError: expected device cuda:0 but got device cpu\r\n"
30813,"[quant] QuantizedCUDA## \U0001f680 Feature\r\n1. Introduction of the **QuantizedCUDA** backend(similar to the **QuantizedCPU** backend). \r\n2. Implementation of the PER_TENSOR_AFFINE QScheme for QuantizedCUDA tensors. (others can be implemented later on)\r\n\r\nEnd users should be able to quantize(qint8, quint8, qint32) and dequantize CUDA tensors with float[16|32|64] dtype.\r\n\r\n## Motivation\r\n*General necessity:*\r\nThe last generation of the GPU produced by Nvidia (Turing) includes support of the integer Tensor Cores (TC). There are several ways that we can make use of them besides cheap and fast inference:\r\n1. In perspective, we could replace Fake-Quantization during training and work with a fully quantized forward pass to reduce possible training-inference biases and, perhaps, speed up things. \r\n2. This could stimulate researchers to investigate the limits of the cheap and dirty, fully quantized training (backward + forward).\r\n\r\nAnyway, the first step to make use of the Tensor Cores is to introduce quantized CUDA tensors.\r\n\r\n*My user-case:*\r\nUp to date, I implemented TVM modules for common layers in the nn (conv, pooling) with use of the tensor cores, both forward and backward pass. Unfortunately, it is not simple to use them with PyTorch framework:\r\n1. Autograd makes a lot of assumptions(enforcing float dtype, etc) about gradients, so I had to remove several of them. \r\n2. I have to store quantization parameters for integer tensors somewhere.\r\n\r\nThe first problem should be considered elsewhere (out of the PR scope).\r\nAny hacks that came to my mind to solve the second problem didn't work (see alternatives), and it seems that I need a new backend. With a simple functionality to pass tensors around and quantize/dequantize them. We could add more support later.\r\n\r\n## Pitch\r\nHere is the code:\r\n\r\nShould work without errors.\r\n\r\n## Alternatives\r\nIf end-users wish to simulate QuantizedGPU backend, they should store tensor's quantization parameters somewhere. And there is no reliable way to do so, especially given autograd backward strategy. Tensor's hash is not reliable because different views on the same memory have different hashes. One can use data_ptr to simulate hash, yet it leads to errors when PyTorch uses cached memory allocation. I didn't come up with any other simple hack, looks like adding QuantizedGPU backend is more fruitful.\r\n\r\n## Additional context\r\nI have started to dig into [QuantizedCPU PRs](https://github.com/pytorch/pytorch/pull/18546); it looks like I will need to go the same way. However, I would highly appreciate any comments and suggestions. Ideally just a list of files to update to get things to work :).\r\n\r\nEdits: more clarity.\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,alnfedorov,"## \U0001f680 Feature\r\n1. Introduction of the **QuantizedCUDA** backend(similar to the **QuantizedCPU** backend). \r\n2. Implementation of the PER_TENSOR_AFFINE QScheme for QuantizedCUDA tensors. (others can be implemented later on)\r\n\r\nEnd users should be able to quantize(qint8, quint8, qint32) and dequantize CUDA tensors with float[16|32|64] dtype.\r\n\r\n## Motivation\r\n*General necessity:*\r\nThe last generation of the GPU produced by Nvidia (Turing) includes support of the integer Tensor Cores (TC). There are several ways that we can make use of them besides cheap and fast inference:\r\n1. In perspective, we could replace Fake-Quantization during training and work with a fully quantized forward pass to reduce possible training-inference biases and, perhaps, speed up things. \r\n2. This could stimulate researchers to investigate the limits of the cheap and dirty, fully quantized training (backward + forward).\r\n\r\nAnyway, the first step to make use of the Tensor Cores is to introduce quantized CUDA tensors.\r\n\r\n*My user-case:*\r\nUp to date, I implemented TVM modules for common layers in the nn (conv, pooling) with use of the tensor cores, both forward and backward pass. Unfortunately, it is not simple to use them with PyTorch framework:\r\n1. Autograd makes a lot of assumptions(enforcing float dtype, etc) about gradients, so I had to remove several of them. \r\n2. I have to store quantization parameters for integer tensors somewhere.\r\n\r\nThe first problem should be considered elsewhere (out of the PR scope).\r\nAny hacks that came to my mind to solve the second problem didn't work (see alternatives), and it seems that I need a new backend. With a simple functionality to pass tensors around and quantize/dequantize them. We could add more support later.\r\n\r\n## Pitch\r\nHere is the code:\r\n```python\r\nimport torch\r\n\r\nt = torch.rand(10)\r\nprint(t)\r\n# tensor([0.6088, 0.3496, 0.3973, 0.0884, 0.5340, 0.9819, 0.5057, 0.2072, 0.6677,\r\n#         0.6197], device='cuda:0')\r\nt = torch.quantize_per_tensor(t, 0.01, 0, torch.qint8)\r\nprint(t)\r\n# tensor([0.6100, 0.3500, 0.4000, 0.0900, 0.5300, 0.9800, 0.5100, 0.2100, 0.6700,\r\n#         0.6200], size=(10,), dtype=torch.qint8, device='cuda:0',\r\n#        quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)\r\nt = t.dequantize()\r\nprint(t)\r\n# tensor([0.6100, 0.3500, 0.4000, 0.0900, 0.5300, 0.9800, 0.5100, 0.2100, 0.6700,\r\n#         0.6200], device='cuda:0')\r\n```\r\nShould work without errors.\r\n\r\n## Alternatives\r\nIf end-users wish to simulate QuantizedGPU backend, they should store tensor's quantization parameters somewhere. And there is no reliable way to do so, especially given autograd backward strategy. Tensor's hash is not reliable because different views on the same memory have different hashes. One can use data_ptr to simulate hash, yet it leads to errors when PyTorch uses cached memory allocation. I didn't come up with any other simple hack, looks like adding QuantizedGPU backend is more fruitful.\r\n\r\n## Additional context\r\nI have started to dig into [QuantizedCPU PRs](https://github.com/pytorch/pytorch/pull/18546); it looks like I will need to go the same way. However, I would highly appreciate any comments and suggestions. Ideally just a list of files to update to get things to work :).\r\n\r\nEdits: more clarity.\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nimport torch\r\n\r\nt = torch.rand(10)\r\nprint(t)\r\n# tensor([0.6088, 0.3496, 0.3973, 0.0884, 0.5340, 0.9819, 0.5057, 0.2072, 0.6677,\r\n#         0.6197], device='cuda:0')\r\nt = torch.quantize_per_tensor(t, 0.01, 0, torch.qint8)\r\nprint(t)\r\n# tensor([0.6100, 0.3500, 0.4000, 0.0900, 0.5300, 0.9800, 0.5100, 0.2100, 0.6700,\r\n#         0.6200], size=(10,), dtype=torch.qint8, device='cuda:0',\r\n#        quantization_scheme=torch.per_tensor_affine, scale=0.01, zero_point=0)\r\nt = t.dequantize()\r\nprint(t)\r\n# tensor([0.6100, 0.3500, 0.4000, 0.0900, 0.5300, 0.9800, 0.5100, 0.2100, 0.6700,\r\n#         0.6200], device='cuda:0')\r\n"
30708,"tensor.max() does not behave equally in cuda and cpu in some cases## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn some cases - as in the example below, tensor.max() fails to log the indices in the cuda result.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI'd've expected torch.max to return same order of max indices in  cuda and cpu.\r\n\r\n## Environment\r\n\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: TITAN RTX\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchaudio==0.2\r\n[pip] torchfile==0.1.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.2.0                    py36_1    pytorch\r\n[conda] torchfile                 0.1.0                      py_0    conda-forge\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n\r\n<!-- Add any other context about the problem here. -->\r\nThis is a special case, clearly, I encountered debugging some results. I would expect the indices to return the same preference: if one calls .max() on  a tensor, the index reutrned by cpu is the highest index of the highest value, in cuda, it is the lowest index of the highest value.\r\n\r\n## Workaround found: argmax returns the same order of indices\r\n\r\n\n\ncc @ezyang @gchanan @zou3519",high priority|module: docs|triaged,nikitaved,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nIn some cases - as in the example below, tensor.max() fails to log the indices in the cuda result.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nrnd = torch.tensor([[ 0.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 7.0000,  7.0000,  7.0000,  7.0000,  7.0000,  7.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000]])\r\n\r\nprint(rnd.max(dim=1))\r\nprint((rnd.cuda()).max(dim=1))\r\n"""""" my results: \r\n# CPU\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000]),\r\nindices=tensor([0, 0, 6, 5]))\r\n#CUDA\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000], device='cuda:0'),\r\nindices=tensor([0, 0, 0, 0], device='cuda:0'))\r\n""""""\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nI'd've expected torch.max to return same order of max indices in  cuda and cpu.\r\n\r\n## Environment\r\n\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: TITAN RTX\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchaudio==0.2\r\n[pip] torchfile==0.1.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.2.0                    py36_1    pytorch\r\n[conda] torchfile                 0.1.0                      py_0    conda-forge\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n\r\n<!-- Add any other context about the problem here. -->\r\nThis is a special case, clearly, I encountered debugging some results. I would expect the indices to return the same preference: if one calls .max() on  a tensor, the index reutrned by cpu is the highest index of the highest value, in cuda, it is the lowest index of the highest value.\r\n\r\n## Workaround found: argmax returns the same order of indices\r\n```python\r\nprint(""argmax"")\r\nprint(torch.argmax(rnd, dim=1))\r\n\r\nprint(""argmax cuda"")\r\nprint(torch.argmax(rnd.cuda(), dim=1))\r\n\r\n"""""" argmax results\r\nargmax\r\ntensor([0, 0, 6, 5])\r\nargmax cuda\r\ntensor([0, 0, 6, 5], device='cuda:0')\r\n""""""\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519","python\r\nimport torch\r\nrnd = torch.tensor([[ 0.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000,  8.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000],\r\n        [ 7.0000,  7.0000,  7.0000,  7.0000,  7.0000,  7.0000, -1.0000, -1.0000,\r\n         -1.0000, -1.0000, -1.0000]])\r\n\r\nprint(rnd.max(dim=1))\r\nprint((rnd.cuda()).max(dim=1))\r\n"""""" my results: \r\n# CPU\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000]),\r\nindices=tensor([0, 0, 6, 5]))\r\n#CUDA\r\ntorch.return_types.max(\r\nvalues=tensor([0.0000, 0.5000, 8.0000, 7.0000], device='cuda:0'),\r\nindices=tensor([0, 0, 0, 0], device='cuda:0'))\r\n""""""\r\n"
30572,"Running fp32 softmax on a fp16 tensor for dimensions other than the last fails during the backward pass## \U0001f41b Bug\r\n\r\nWhen training models in FP16, it is typical to run softmax operations (such as within attention) in FP32. It seems that when you run softmax on a dimension other than the last dimension of the tensor, the backward pass fails.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nOutput is\r\n```\r\nTraceback (most recent call last):\r\n  File ""softmax_bug.py"", line 9, in <module>\r\n    c.backward(retain_graph=True)\r\n  File ""---/torch/tensor.py"", line 166, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""---/torch/autograd/__init__.py"", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: expected scalar type Float but found Half\r\n```\r\nNote that `b.backward` does not fail.\r\n\r\n## Expected behavior\r\n\r\nNo exception\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\nGPU 2: TITAN RTX\r\nGPU 3: TITAN RTX\r\nGPU 4: TITAN RTX\r\nGPU 5: TITAN RTX\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py37_cu101    pytorch\r\n```\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen @ngimel",high priority|module: autograd|module: cuda|triaged,zasdfgbnm,"## \U0001f41b Bug\r\n\r\nWhen training models in FP16, it is typical to run softmax operations (such as within attention) in FP32. It seems that when you run softmax on a dimension other than the last dimension of the tensor, the backward pass fails.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\na = torch.rand(2, 2, 2, 2, requires_grad=True).half().cuda()\r\nb = F.softmax(a, dim=-1, dtype=torch.float32)[0, 0, 0, 0]\r\nc = F.softmax(a, dim=1, dtype=torch.float32)[0, 0, 0, 0]\r\n\r\nb.backward(retain_graph=True)\r\nc.backward(retain_graph=True)\r\n```\r\n\r\nOutput is\r\n```\r\nTraceback (most recent call last):\r\n  File ""softmax_bug.py"", line 9, in <module>\r\n    c.backward(retain_graph=True)\r\n  File ""---/torch/tensor.py"", line 166, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""---/torch/autograd/__init__.py"", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: expected scalar type Float but found Half\r\n```\r\nNote that `b.backward` does not fail.\r\n\r\n## Expected behavior\r\n\r\nNo exception\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\nGPU 2: TITAN RTX\r\nGPU 3: TITAN RTX\r\nGPU 4: TITAN RTX\r\nGPU 5: TITAN RTX\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py37_cu101    pytorch\r\n```\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen @ngimel","python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\na = torch.rand(2, 2, 2, 2, requires_grad=True).half().cuda()\r\nb = F.softmax(a, dim=-1, dtype=torch.float32)[0, 0, 0, 0]\r\nc = F.softmax(a, dim=1, dtype=torch.float32)[0, 0, 0, 0]\r\n\r\nb.backward(retain_graph=True)\r\nc.backward(retain_graph=True)\r\n"
30563,[ONNX] Support affine_grid_generator\r\n\r\nResult:\r\n`RuntimeError: Exporting the operator affine_grid_generator to ONNX opset version 9 is not supported.`,module: onnx|triaged|enhancement|onnx-triaged,justinchuby,"```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  def forward(self, theta, size):\r\n    return torch.nn.functional.affine_grid(theta, size, align_corners=None)\r\n\r\n\r\nmodel = Model()\r\ntheta = torch.ones((1, 2, 3))\r\nsize = torch.Size((1,3,24,24))\r\ntorch.onnx.export(model, (theta, size), 'test.onnx', verbose=True)\r\n```\r\n\r\nResult:\r\n`RuntimeError: Exporting the operator affine_grid_generator to ONNX opset version 9 is not supported.`","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  def forward(self, theta, size):\r\n    return torch.nn.functional.affine_grid(theta, size, align_corners=None)\r\n\r\n\r\nmodel = Model()\r\ntheta = torch.ones((1, 2, 3))\r\nsize = torch.Size((1,3,24,24))\r\ntorch.onnx.export(model, (theta, size), 'test.onnx', verbose=True)\r\n"
30562,"`torch.cat` probably modified after being freed## \U0001f41b Use After Free Bug in `torch.cat`\r\n\r\nWhen using `torch.cat` with zero sized arrays and an out parameter, `torch.cat` produces a memory error:\r\n\r\n```\r\nPython(2479,0x11b476dc0) malloc: Incorrect checksum for freed object 0x7f9834954060: probably modified after being freed.\r\nCorrupt value: 0x3f61f243bf3752ad\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run this code:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nWhile this may be an incorrect use of `cat` (I'm not really sure if it is incorrect), it should still perform necessary dynamic checks so as not to expose a memory error.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.1\r\nGCC version: Could not collect\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[pip3] torchviz==0.0.1\r\n[conda] Could not collect\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168",high priority|module: crash|module: error checking|triaged,peterbell10,"## \U0001f41b Use After Free Bug in `torch.cat`\r\n\r\nWhen using `torch.cat` with zero sized arrays and an out parameter, `torch.cat` produces a memory error:\r\n\r\n```\r\nPython(2479,0x11b476dc0) malloc: Incorrect checksum for freed object 0x7f9834954060: probably modified after being freed.\r\nCorrupt value: 0x3f61f243bf3752ad\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run this code:\r\n\r\n```python\r\nimport torch\r\nz = torch.zeros((0))\r\nr = torch.randn(4, 6)\r\ntorch.cat([z, r], dim=0, out=z)\r\n```\r\n\r\n## Expected behavior\r\n\r\nWhile this may be an incorrect use of `cat` (I'm not really sure if it is incorrect), it should still perform necessary dynamic checks so as not to expose a memory error.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.1\r\nGCC version: Could not collect\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[pip3] torchviz==0.0.1\r\n[conda] Could not collect\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168","python\r\nimport torch\r\nz = torch.zeros((0))\r\nr = torch.randn(4, 6)\r\ntorch.cat([z, r], dim=0, out=z)\r\n"
30508,"libtorch: Moving Module to other device fails when not using bias## \U0001f41b Bug\r\n\r\nIf a Linear or Conv module is constructed without bias, a subsequent move to another device fails\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  tensor does not have a device (device at /opt/pytorch/c10/core/TensorImpl.h:463)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7fffe51c9b1a in /usr/local/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1fd83a6 (0x7fffe73ba3a6 in /usr/local/torch/lib/libtorch.so)\r\nframe #2: at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) + 0xb36 (0x7fffe8c6b1f6 in /usr/local/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x3c22636 (0x7fffe9004636 in /usr/local/torch/lib/libtorch.so)\r\nframe #4: <unknown function> + 0x5882857 (0x7fffeac64857 in /usr/local/torch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x3c89f2f (0x7fffe906bf2f in /usr/local/torch/lib/libtorch.so)\r\nframe #6: <unknown function> + 0x25e1277 (0x7fffe79c3277 in /usr/local/torch/lib/libtorch.so)\r\nframe #7: void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) + 0x196 (0x7fffeb3a8266 in /usr/local/torch/lib/libtorch.so)\r\nframe #8: torch::nn::Module::to(c10::Device, bool) + 0x18 (0x7fffeb3a2fe8 in /usr/local/torch/lib/libtorch.so)\r\nframe #9: main + 0xba (0x4052e8 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\nframe #10: __libc_start_main + 0xf0 (0x7fffe4855830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #11: _start + 0x29 (0x405049 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\n\r\n\r\nThread 1 ""dev_stuff"" received signal SIGABRT, Aborted.\r\n0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace\r\n#0  0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fffe486c02a in __GI_abort () at abort.c:89\r\n#2  0x00007fffe4ea484d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe4ea26b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe4ea2701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe4ea2919 in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe73ba3fc in at::Tensor::options() const () from /usr/local/torch/lib/libtorch.so\r\n#7  0x00007fffe8c6b1f6 in at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#8  0x00007fffe9004636 in at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#9  0x00007fffeac64857 in torch::autograd::VariableType::(anonymous namespace)::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#10 0x00007fffe906bf2f in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat> > >, at::Tensor (at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#11 0x00007fffe79c3277 in at::Tensor::to(c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) const () from /usr/local/torch/lib/libtorch.so\r\n#12 0x00007fffeb3a8266 in void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) () from /usr/local/torch/lib/libtorch.so\r\n#13 0x00007fffeb3a2fe8 in torch::nn::Module::to(c10::Device, bool) [clone .localalias.489] () from /usr/local/torch/lib/libtorch.so\r\n#14 0x00000000004052e8 in main (argc=1, argv=0x7fffffffccc8) at /home/tobi/coar_ws/src/dgcnn/src/dev_stuff.cpp:5\r\n```\r\n</p>\r\n</details>\r\n\r\n## Expected behavior\r\n\r\nGetting no error.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+829499e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro RTX 6000\r\nGPU 1: Quadro RTX 6000\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.4.0a0+94016b1\r\n[conda] Could not collect\r\n\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0a0+829499e\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): compiled from source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6\r\n - CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n - GPU models and configuration: 2x Quadro RTX 6000\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225",high priority|module: crash|module: cpp,yf225,"## \U0001f41b Bug\r\n\r\nIf a Linear or Conv module is constructed without bias, a subsequent move to another device fails\r\n\r\n## To Reproduce\r\n\r\n```cpp\r\n#include <torch/torch.h>\r\n\r\nint main(int argc, char** argv) {\r\n  torch::nn::Linear test(torch::nn::LinearOptions(10,20).bias(false));\r\n  test->to(torch::kCUDA);\r\n  return 0;\r\n}\r\n```\r\n\r\n\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  tensor does not have a device (device at /opt/pytorch/c10/core/TensorImpl.h:463)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7fffe51c9b1a in /usr/local/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1fd83a6 (0x7fffe73ba3a6 in /usr/local/torch/lib/libtorch.so)\r\nframe #2: at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) + 0xb36 (0x7fffe8c6b1f6 in /usr/local/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x3c22636 (0x7fffe9004636 in /usr/local/torch/lib/libtorch.so)\r\nframe #4: <unknown function> + 0x5882857 (0x7fffeac64857 in /usr/local/torch/lib/libtorch.so)\r\nframe #5: <unknown function> + 0x3c89f2f (0x7fffe906bf2f in /usr/local/torch/lib/libtorch.so)\r\nframe #6: <unknown function> + 0x25e1277 (0x7fffe79c3277 in /usr/local/torch/lib/libtorch.so)\r\nframe #7: void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) + 0x196 (0x7fffeb3a8266 in /usr/local/torch/lib/libtorch.so)\r\nframe #8: torch::nn::Module::to(c10::Device, bool) + 0x18 (0x7fffeb3a2fe8 in /usr/local/torch/lib/libtorch.so)\r\nframe #9: main + 0xba (0x4052e8 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\nframe #10: __libc_start_main + 0xf0 (0x7fffe4855830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #11: _start + 0x29 (0x405049 in /home/tobi/coar_ws/devel/lib/dgcnn/dev_stuff)\r\n\r\n\r\nThread 1 ""dev_stuff"" received signal SIGABRT, Aborted.\r\n0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace\r\n#0  0x00007fffe486a428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fffe486c02a in __GI_abort () at abort.c:89\r\n#2  0x00007fffe4ea484d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe4ea26b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe4ea2701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe4ea2919 in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe73ba3fc in at::Tensor::options() const () from /usr/local/torch/lib/libtorch.so\r\n#7  0x00007fffe8c6b1f6 in at::native::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#8  0x00007fffe9004636 in at::TypeDefault::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#9  0x00007fffeac64857 in torch::autograd::VariableType::(anonymous namespace)::to(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#10 0x00007fffe906bf2f in c10::detail::wrap_kernel_functor_unboxed_<c10::detail::WrapRuntimeKernelFunctor_<at::Tensor (*)(at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat> > >, at::Tensor (at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>)>::call(c10::OperatorKernel*, at::Tensor const&, c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) () from /usr/local/torch/lib/libtorch.so\r\n#11 0x00007fffe79c3277 in at::Tensor::to(c10::TensorOptions const&, bool, bool, c10::optional<c10::MemoryFormat>) const () from /usr/local/torch/lib/libtorch.so\r\n#12 0x00007fffeb3a8266 in void torch::nn::Module::to_impl<c10::Device&, bool&>(c10::Device&, bool&) () from /usr/local/torch/lib/libtorch.so\r\n#13 0x00007fffeb3a2fe8 in torch::nn::Module::to(c10::Device, bool) [clone .localalias.489] () from /usr/local/torch/lib/libtorch.so\r\n#14 0x00000000004052e8 in main (argc=1, argv=0x7fffffffccc8) at /home/tobi/coar_ws/src/dgcnn/src/dev_stuff.cpp:5\r\n```\r\n</p>\r\n</details>\r\n\r\n## Expected behavior\r\n\r\nGetting no error.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+829499e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro RTX 6000\r\nGPU 1: Quadro RTX 6000\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.4.0a0+94016b1\r\n[conda] Could not collect\r\n\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0a0+829499e\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): compiled from source\r\n - Build command you used (if compiling from source): ```CFLAGS=' -D_GLICXX_USE_CXX11_ABI ' USE_OPENCV=1 USE_CUDA=1 MAX_JOBS=7 BUILD_TEST=0 python3 setup.py install ```\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.5\r\n - GPU models and configuration: 2x Quadro RTX 6000\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @yf225","cpp\r\n#include <torch/torch.h>\r\n\r\nint main(int argc, char** argv) {\r\n  torch::nn::Linear test(torch::nn::LinearOptions(10,20).bias(false));\r\n  test->to(torch::kCUDA);\r\n  return 0;\r\n}\r\n"
30494,"RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED## \U0001f41b Bug\r\n\r\nReproduce:\r\n\r\n\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 14, in <module>\r\n    f()\r\nRuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/interpreter.cpp:1025, please report a bug to PyTorch. \r\nThe above operation failed in interpreter, with the following stack trace:\r\n```\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo",high priority|triage review|oncall: jit,Krovatkin,"## \U0001f41b Bug\r\n\r\nReproduce:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f():\r\n    l = []\r\n    for n in [2, 1]:\r\n        l.append(torch.zeros(n))\r\n\r\n    return l[0]\r\n\r\nf()\r\nf()\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 14, in <module>\r\n    f()\r\nRuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/interpreter.cpp:1025, please report a bug to PyTorch. \r\nThe above operation failed in interpreter, with the following stack trace:\r\n```\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f():\r\n    l = []\r\n    for n in [2, 1]:\r\n        l.append(torch.zeros(n))\r\n\r\n    return l[0]\r\n\r\nf()\r\nf()\r\n"
30303,"reshape with non-contiguous input has wrong gradient on CUDA (breaking einsum)## \U0001f41b Bug\r\n\r\n`einsum`, which takes **incontiguous** tensor as input, can not back-propagate correct gradient\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nthis code will output as below, but as you notice `einsum.jacobian` of `incontiguous input tensor` is different from all the others.\r\n\r\n```\r\nincontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.5583, 0.5421, 0.5583, 0.5421], device='cuda:0')\r\ncontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nincontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\ncontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Homebrew gcc 5.5.0_4) 5.5.0\r\nCMake version: version 3.15.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.87.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[conda] pytorch-crf               0.7.0                    pypi_0    pypi\r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] pytorch-transformers      1.0.0                    pypi_0    pypi\r\n[conda] torch                     1.3.1                    pypi_0    pypi\r\n[conda] torch-scatter             1.4.0                    pypi_0    pypi\r\n[conda] torch-struct              0.3                      pypi_0    pypi\r\n[conda] torchcrf                  1.0.4                    pypi_0    pypi\r\n[conda] torchsnooper              0.3                      pypi_0    pypi\r\n[conda] torchtext                 0.4.0                    pypi_0    pypi\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen",high priority|triage review|module: autograd|triaged,t-vi,"## \U0001f41b Bug\r\n\r\n`einsum`, which takes **incontiguous** tensor as input, can not back-propagate correct gradient\r\n\r\n## To Reproduce\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch import Tensor\r\n\r\n\r\ndef check_jacobian(x: Tensor, weight: Tensor):\r\n    print(f'{""contiguous"" if x.is_contiguous() else ""incontiguous""} input tensor')\r\n\r\n    y1 = x @ weight\r\n    y2 = torch.einsum('...x,xy->...y', [x, weight])\r\n    assert torch.allclose(y1, y2)\r\n\r\n    x.grad = None\r\n    y1.backward(torch.eye(2, device=x.device))\r\n    print(f'matmul.jacobian => {x.grad.view(-1)}')\r\n\r\n    x.grad = None\r\n    y2.backward(torch.eye(2, device=x.device))\r\n    print(f'einsum.jacobian => {x.grad.view(-1)}')\r\n\r\n\r\nif __name__ == '__main__':\r\n    argument_parser = ArgumentParser()\r\n    argument_parser.add_argument('--device', type=int, default=-1)\r\n    args = argument_parser.parse_args()\r\n\r\n    torch.cuda.set_device(args.device)\r\n    torch.manual_seed(42)\r\n    device = torch.device(f'cuda:{args.device}')\r\n\r\n    _x = torch.rand(1, 2, device=device)\r\n    x1 = _x.expand(2, -1).requires_grad_()\r\n    x2 = _x.expand(2, -1).contiguous().requires_grad_()\r\n    w = torch.rand(2, 2, device=device)\r\n\r\n    check_jacobian(x1, w)\r\n    check_jacobian(x2, w)\r\n```\r\n\r\nthis code will output as below, but as you notice `einsum.jacobian` of `incontiguous input tensor` is different from all the others.\r\n\r\n```\r\nincontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.5583, 0.5421, 0.5583, 0.5421], device='cuda:0')\r\ncontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nincontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\ncontiguous input tensor\r\nmatmul.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\neinsum.jacobian => tensor([0.9877, 0.5621, 0.1289, 0.5221], device='cuda:0')\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Homebrew gcc 5.5.0_4) 5.5.0\r\nCMake version: version 3.15.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.87.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[conda] pytorch-crf               0.7.0                    pypi_0    pypi\r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] pytorch-transformers      1.0.0                    pypi_0    pypi\r\n[conda] torch                     1.3.1                    pypi_0    pypi\r\n[conda] torch-scatter             1.4.0                    pypi_0    pypi\r\n[conda] torch-struct              0.3                      pypi_0    pypi\r\n[conda] torchcrf                  1.0.4                    pypi_0    pypi\r\n[conda] torchsnooper              0.3                      pypi_0    pypi\r\n[conda] torchtext                 0.4.0                    pypi_0    pypi\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen","python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nfrom torch import Tensor\r\n\r\n\r\ndef check_jacobian(x: Tensor, weight: Tensor):\r\n    print(f'{""contiguous"" if x.is_contiguous() else ""incontiguous""} input tensor')\r\n\r\n    y1 = x @ weight\r\n    y2 = torch.einsum('...x,xy->...y', [x, weight])\r\n    assert torch.allclose(y1, y2)\r\n\r\n    x.grad = None\r\n    y1.backward(torch.eye(2, device=x.device))\r\n    print(f'matmul.jacobian => {x.grad.view(-1)}')\r\n\r\n    x.grad = None\r\n    y2.backward(torch.eye(2, device=x.device))\r\n    print(f'einsum.jacobian => {x.grad.view(-1)}')\r\n\r\n\r\nif __name__ == '__main__':\r\n    argument_parser = ArgumentParser()\r\n    argument_parser.add_argument('--device', type=int, default=-1)\r\n    args = argument_parser.parse_args()\r\n\r\n    torch.cuda.set_device(args.device)\r\n    torch.manual_seed(42)\r\n    device = torch.device(f'cuda:{args.device}')\r\n\r\n    _x = torch.rand(1, 2, device=device)\r\n    x1 = _x.expand(2, -1).requires_grad_()\r\n    x2 = _x.expand(2, -1).contiguous().requires_grad_()\r\n    w = torch.rand(2, 2, device=device)\r\n\r\n    check_jacobian(x1, w)\r\n    check_jacobian(x2, w)\r\n"
30237,"[discussion] conj() method copy/nocopy semantics for real tensorsIt seems to copy:\r\n\r\n\r\nHowever, if the caller wants to save memory for real tensors and does not use inplace ops, they need to do ifs (and enumerate all complex types) like in https://github.com/pytorch/pytorch/pull/29488/files#diff-62a3ecc69398a4b080599d5222c535b1R70\r\n\r\nAlso this copy semantics seems not documented: https://pytorch.org/docs/master/torch.html?highlight=conj#torch.conj\n\ncc @VitalyFedyunin @ngimel @mruberry",module: performance|module: docs|module: cpu|triaged,VitalyFedyunin,"It seems to copy:\r\n```python\r\nimport torch\r\na = torch.rand(3, 3)\r\nprint(a.data_ptr(), a.conj().data_ptr()) # 94290110120192 94290110120320\r\n```\r\n\r\nHowever, if the caller wants to save memory for real tensors and does not use inplace ops, they need to do ifs (and enumerate all complex types) like in https://github.com/pytorch/pytorch/pull/29488/files#diff-62a3ecc69398a4b080599d5222c535b1R70\r\n\r\nAlso this copy semantics seems not documented: https://pytorch.org/docs/master/torch.html?highlight=conj#torch.conj\n\ncc @VitalyFedyunin @ngimel @mruberry","python\r\nimport torch\r\na = torch.rand(3, 3)\r\nprint(a.data_ptr(), a.conj().data_ptr()) # 94290110120192 94290110120320\r\n"
30211,"A problem of nn.ConvTranspose2d in PyTorchI met a strange `cuDNN error: CUDNN_STATUS_INTERNAL_ERROR` when I run the `nn.ConvTranspose2d` function in gpu-cluster.\r\nCodes like:(in the ipdb debug mode)\r\n\r\nI'm not sure whether it is because `nn.ConvTranspose2d` cannot handle the strange spatial size (128, 80).  \r\nAfter I change the spatial size to (64,64), no error.\r\nI think there is no problem with pytorch environment, with `pytorch=1.0.1` and `cuda=9.0`.  \r\n\r\nSolutions for `cuDNN error: CUDNN_STATUS_INTERNAL_ERROR` also didn't work, e.g., `rm -rf ~/.nv`  \r\n\r\n**It seems the problem is most likely from inappropriate input spatial size. Is there any constrain about `nn.ConvTranspose2d`?**  \r\n\r\nBesides, when I try to run the aforementioned codes in the command line after run `python`, also no error reported. When I use the same (128, 80) spatial size but `kernel_size=(3,3)`, no problem. \r\n\r\nIs there any suggestion? Thanks",module: cudnn|module: convolution|triaged,cpuhrsch,"I met a strange `cuDNN error: CUDNN_STATUS_INTERNAL_ERROR` when I run the `nn.ConvTranspose2d` function in gpu-cluster.\r\nCodes like:(in the ipdb debug mode)\r\n```python\r\nmy_func = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))).cuda()\r\nmy_input = torch.randn(8, 128, 128, 80).cuda()\r\nprint(my_func(my_input).shape)\r\n```\r\nI'm not sure whether it is because `nn.ConvTranspose2d` cannot handle the strange spatial size (128, 80).  \r\nAfter I change the spatial size to (64,64), no error.\r\nI think there is no problem with pytorch environment, with `pytorch=1.0.1` and `cuda=9.0`.  \r\n\r\nSolutions for `cuDNN error: CUDNN_STATUS_INTERNAL_ERROR` also didn't work, e.g., `rm -rf ~/.nv`  \r\n\r\n**It seems the problem is most likely from inappropriate input spatial size. Is there any constrain about `nn.ConvTranspose2d`?**  \r\n\r\nBesides, when I try to run the aforementioned codes in the command line after run `python`, also no error reported. When I use the same (128, 80) spatial size but `kernel_size=(3,3)`, no problem. \r\n\r\nIs there any suggestion? Thanks","python\r\nmy_func = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))).cuda()\r\nmy_input = torch.randn(8, 128, 128, 80).cuda()\r\nprint(my_func(my_input).shape)\r\n"
30204,"JIT does not supporting `retain_graph` in autograd## \U0001f41b Bug\r\n\r\n\r\nFails with\r\n\r\n```\r\nRuntimeError: \r\n\r\naten::grad(Tensor[] outputs, Tensor[] inputs, Tensor?[]? grad_outputs=None, bool? keep_graph=None, bool create_graph=False, bool allow_unused=False) -> (Tensor[]):\r\nKeyword argument retain_graph unknown.\r\n:\r\nat test.py:5:11\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.autograd.grad([x.sum()], [x], retain_graph=True)\r\n           ~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nFixed in https://github.com/pytorch/pytorch/pull/30203\n\ncc @suo",oncall: jit,zasdfgbnm,"## \U0001f41b Bug\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.autograd.grad([x.sum()], [x], retain_graph=True)\r\n```\r\nFails with\r\n\r\n```\r\nRuntimeError: \r\n\r\naten::grad(Tensor[] outputs, Tensor[] inputs, Tensor?[]? grad_outputs=None, bool? keep_graph=None, bool create_graph=False, bool allow_unused=False) -> (Tensor[]):\r\nKeyword argument retain_graph unknown.\r\n:\r\nat test.py:5:11\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.autograd.grad([x.sum()], [x], retain_graph=True)\r\n           ~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nFixed in https://github.com/pytorch/pytorch/pull/30203\n\ncc @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(x):\r\n    return torch.autograd.grad([x.sum()], [x], retain_graph=True)\r\n"
30194,"Support broadcasting in einsum## \U0001f680 Feature\r\n`torch.einsum` errors when trying to broadcast a size-1 dimension. Both Numpy and Tensorflow support einsum broadcasting:\r\n\r\nNumpy:\r\n\r\n\r\nTensorflow:\r\n\r\n\r\nPytorch:\r\n\r\ngets\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-31a4a0f0f2d6> in <module>\r\n      5 \r\n      6 Y2 = Y.reshape(1, 2)\r\n----> 7 Z2 = torch.einsum(""ab,ab->ab"", X, Y2)\r\n\r\n~/anaconda3/envs/gpu/lib/python3.7/site-packages/torch/functional.py in einsum(equation, *operands)\r\n    199         # the old interface of passing the operands as one list argument\r\n    200         operands = operands[0]\r\n--> 201     return torch._C._VariableFunctions.einsum(equation, operands)\r\n    202 \r\n    203 \r\n\r\nRuntimeError: size of dimension does not match previous size, operand 1, dim 0\r\n```\r\n\n\ncc @vincentqb @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff",triaged|module: linear algebra|function request,cpuhrsch|heitorschueroff,"## \U0001f680 Feature\r\n`torch.einsum` errors when trying to broadcast a size-1 dimension. Both Numpy and Tensorflow support einsum broadcasting:\r\n\r\nNumpy:\r\n```python\r\nimport numpy as np\r\nX = np.random.randn(4, 2)\r\nY = np.random.randn(2)\r\nZ = np.einsum(""ab,b->ab"", X, Y)\r\n\r\nY2 = Y.reshape(1, 2)\r\nZ2 = np.einsum(""ab,ab->ab"", X, Y2)\r\nassert np.all(Z == Z2)\r\n```\r\n\r\nTensorflow:\r\n```python\r\nimport tensorflow as tf\r\nX = tf.random.normal((4, 2))\r\nY = tf.random.normal((2,))\r\nZ = tf.einsum(""ab,b->ab"", X, Y)\r\n\r\nY2 = tf.reshape(Y, (1, 2))\r\nZ2 = tf.einsum(""ab,ab->ab"", X, Y2)\r\nassert tf.reduce_all(Z == Z2)\r\n```\r\n\r\nPytorch:\r\n```python\r\nimport torch\r\nX = torch.randn(4, 2)\r\nY = torch.randn(2)\r\nZ = torch.einsum(""ab,b->ab"", X, Y)\r\n\r\nY2 = Y.reshape(1, 2)\r\nZ2 = torch.einsum(""ab,ab->ab"", X, Y2)\r\n```\r\ngets\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-31a4a0f0f2d6> in <module>\r\n      5 \r\n      6 Y2 = Y.reshape(1, 2)\r\n----> 7 Z2 = torch.einsum(""ab,ab->ab"", X, Y2)\r\n\r\n~/anaconda3/envs/gpu/lib/python3.7/site-packages/torch/functional.py in einsum(equation, *operands)\r\n    199         # the old interface of passing the operands as one list argument\r\n    200         operands = operands[0]\r\n--> 201     return torch._C._VariableFunctions.einsum(equation, operands)\r\n    202 \r\n    203 \r\n\r\nRuntimeError: size of dimension does not match previous size, operand 1, dim 0\r\n```\r\n\n\ncc @vincentqb @vishwakftw @jianyuh @nikitaved @pearu @mruberry @heitorschueroff","python\r\nimport numpy as np\r\nX = np.random.randn(4, 2)\r\nY = np.random.randn(2)\r\nZ = np.einsum(""ab,b->ab"", X, Y)\r\n\r\nY2 = Y.reshape(1, 2)\r\nZ2 = np.einsum(""ab,ab->ab"", X, Y2)\r\nassert np.all(Z == Z2)\r\n"
30019,"[jit] Values of constants can't be accessedConstant values are inlined in the graph without names, so they can't be accessed after compilation. This is a problem since a lot of `nn.Module`s are scripted using constants instead of attributes, so users have no way of accessing these values once they have scripted a module. \r\n\r\n\r\n\r\n\r\nAn easy fix would be to delete all the `__constants__` arrays in `torch/nn` so everything would be made into an attribute.\r\n\r\ncc @suo",oncall: jit,jerryzh168,"Constant values are inlined in the graph without names, so they can't be accessed after compilation. This is a problem since a lot of `nn.Module`s are scripted using constants instead of attributes, so users have no way of accessing these values once they have scripted a module. \r\n\r\n\r\n```python\r\nclass M(nn.Module):\r\n    __constants__ = ['my_const']\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.my_const = 2\r\n\r\n    def forward(self, x):\r\n        return x + self.my_const\r\n\r\n\r\nsm = torch.jit.script(M())\r\n\r\n# AttributeError: 'RecursiveScriptModule' object has no attribute 'my_const'\r\nprint(sm.my_const)\r\n```\r\n\r\nAn easy fix would be to delete all the `__constants__` arrays in `torch/nn` so everything would be made into an attribute.\r\n\r\ncc @suo","python\r\nclass M(nn.Module):\r\n    __constants__ = ['my_const']\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.my_const = 2\r\n\r\n    def forward(self, x):\r\n        return x + self.my_const\r\n\r\n\r\nsm = torch.jit.script(M())\r\n\r\n# AttributeError: 'RecursiveScriptModule' object has no attribute 'my_const'\r\nprint(sm.my_const)\r\n"
30015,"JIT fails for multihead attention## \U0001f41b Bug\r\n\r\nI can run the model before torchscript while the torchscripted model fails in running time for the multihead attention @driazati\r\n\r\n## To Reproduce\r\n\r\nThe following code sample:\r\n\r\n\r\n## Expected behavior\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-66-6d725915459d> in <module>\r\n     53 data = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\n     54 lengths = torch.tensor([74])\r\n---> 55 output = ts_model_multihead.forward(data=data, lengths=lengths)\r\n     56 print(output)\r\n\r\nRuntimeError: \r\n\r\n\r\nnew_type INTERNAL ASSERT FAILED at caffe2/torch/csrc/jit/passes/shape_analysis.cpp:280, please report a bug to PyTorch. \r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3312:12\r\n                                                  dtype=attn_mask.dtype,\r\n                                                  device=attn_mask.device)], dim=1)\r\n            if key_padding_mask is not None:\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n                key_padding_mask = torch.cat(\r\n                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\r\n\r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3303:4\r\n    q = q * scaling\r\n\r\n    if bias_k is not None and bias_v is not None:\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n        if static_k is None and static_v is None:\r\n            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\r\n\r\nThe above operation failed shape propagation in this context:\r\nat <ipython-input-60-415a6b53c56b>:127:8\r\n          L is the target sequence length, S is the source sequence length.\r\n        """"""\r\n        if not self._qkv_same_embed_dim:\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n            return nn.functional.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n```\r\n\r\n## Environment\r\n\r\nbento classyvision env\r\n\r\n\r\ncc @suo",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\nI can run the model before torchscript while the torchscripted model fails in running time for the multihead attention @driazati\r\n\r\n## To Reproduce\r\n\r\nThe following code sample:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ndef _lengths2mask(lengths: torch.Tensor, seq_len: int, lt=torch.tensor(True)) -> torch.Tensor:\r\n    """"""\r\n    Input lengths is a tensor of shape (batch_size) with value in [0, seq_len],\r\n    Return a tensor of shape (batch_size x seq_len) with binary value\r\n    """"""\r\n    if lt:\r\n        return torch.lt(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n    else:\r\n        return torch.ge(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n\r\nclass AttentionMultihead(nn.Module):\r\n    def __init__(self, dim_in, method, use_variable_lengths=False, num_heads=1):\r\n        super(AttentionMultihead, self).__init__()\r\n\r\n        # choose from supported attention methods\r\n        assert method in (""multihead"")\r\n        self.method = method\r\n        self.use_variable_length = use_variable_lengths\r\n        \r\n        self.attention = nn.MultiheadAttention(\r\n            embed_dim=dim_in, num_heads=num_heads\r\n        )\r\n\r\n        # record output dim\r\n        self.dim_out = dim_in\r\n\r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor):\r\n        assert data.dim() == 3, ""Require input shape (batch_size x seq_len x embed_dim)""\r\n        \r\n        if self.use_variable_length is True:\r\n            mask = _lengths2mask(lengths.clamp(min=1), data.size(1), torch.tensor(False))\r\n        else:\r\n            mask = None\r\n\r\n        data = data.transpose(0, 1)\r\n        attn_output, attn_weights = self.attention(\r\n            data, data, data, key_padding_mask=mask, need_weights=True, attn_mask=None\r\n        )\r\n        # transpose output data to (batch_size, seq_length, embed_dim)\r\n        attn_output = attn_output.transpose(0, 1)\r\n        \r\n        return attn_output, lengths, attn_weights\r\n\r\n    \r\nclass DeployAttentionMultihead(torch.nn.Module):\r\n    def __init__(self, dim_in=100, length=torch.tensor([300])[0]):\r\n        super(DeployAttentionMultihead, self).__init__()\r\n        self.length = length\r\n        self.use_variable_length = True\r\n        self.attention = AttentionMultihead(dim_in=dim_in, method=""multihead"", use_variable_lengths=True, num_heads=5)\r\n        \r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\r\n        attn_output, lengths, attn_weights = self.attention(data, lengths)\r\n        return attn_output\r\n    \r\nmodel = DeployAttentionMultihead()\r\n\r\nts_model_multihead = torch.jit.script(model)\r\nprint(ts_model_multihead.code)\r\n\r\n# run ts model of multihead\r\n\r\nvue_clip_emb = [0.0840825,5.65534,2.23576,2.21367,1.5883,0.493749,0,1.49034,1.54749,1.15795,0.118591,2.54162,2.70971,2.10865,0.115242,0.0115912,5.39253,2.83803,1.96323,0.935772,1.33328,1.16219,0.15678,1.33033,0,2.80253,0.258661,1.19186,0.429935,1.14404,4.58615,0.11558,2.31151,3.45441,0.117317,2.62973,2.57802,3.12865,0.632287,0.280647,1.08333,4.14262,3.5503,0.67525,0.889924,0.128302,3.943,0.825888,3.11752,2.46894,4.05448,3.0315,5.9825,2.54857,0.390271,0.105513,1.76064,3.53628,0.496927,0.850136,3.52684,1.92267,1.50376,3.43571,2.36993,0.0011008,0.101535,0.324544,0.875401,1.6104,0.497361,1.72176,4.01326,1.31662,0.262011,0.837993,0.622429,2.66528,1.13911,1.21707,1.56516,6.33717,1.5675,0.136495,0.355789,1.48718,0.108055,3.47766,1.0657,0.00356666,1.77421,3.86285,1.42049,0.819946,0.340235,0.109702,0.867129,2.71539,0.206769,2.02203,0.0292637,5.49686,1.74569,2.19925,1.51534,0.46074,0.0161071,1.24292,1.44666,0.562175,0.104977,2.05992,2.41354,2.51859,0.165397,0.011803,4.64355,4.19187,2.1993,1.15172,1.72702,1.62523,0.343265,1.45464,0,2.05193,0.619562,0.983955,0.432762,1.09673,4.58744,0.209633,2.02799,3.54795,0.156958,2.19384,2.25872,2.84524,0.120264,0.432246,0.800701,3.83577,3.56383,0.501572,0.458101,0.0500416,3.77406,0.618967,2.51559,2.57312,3.63771,4.04265,5.3371,2.427,0.555011,0.393777,1.45245,4.48355,0.642099,1.48505,3.90371,2.12527,1.10275,3.13394,2.88107,0,0.0938793,0.432923,1.40834,1.47035,0.199206,0.899871,4.19796,1.37373,0.108242,0.541182,0.60768,2.56032,1.05987,1.05486,2.31946,6.57515,1.71218,0.191091,0.523715,0.562181,0.163343,3.41128,1.90795,0.103194,2.18321,4.01785,1.40121,0.712813,0.40542,0.0130574,0.482167,3.61689,0.541964,1.54621,0.00721372,3.53875,1.99706,2.16941,2.30858,0.139537,0.0292894,0.59817,1.67502,1.06869,0.15484,1.32557,2.84414,1.81534,0.0359029,0.155307,4.85468,3.37413,1.90302,1.30877,0.734729,1.76238,0.150899,0.547858,0.0230488,2.15477,0.665912,2.1411,0.486219,0.981247,4.02336,0.340909,2.64124,1.94704,0.127592,2.22661,3.51697,2.28332,0.23371,0.0208469,0.71869,3.26233,3.37803,0.440662,0.510871,0.00753081,5.16184,0.492385,2.8764,2.53306,2.73119,3.92737,4.78175,3.25056,0.738502,0.171556,1.14462,4.28251,0.983006,1.13122,3.53485,1.87881,0.758899,3.47704,3.11202,0.00754782,0.223614,0.554179,2.12652,1.15958,0.225096,1.4953,3.90192,1.51579,0.0205165,1.09822,0.339162,3.14576,2.58463,0.766622,2.68842,5.81371,1.30858,0.0567763,0.362448,0.427235,0.103554,1.55612,1.40707,0.327143,2.06921,3.1817,1.11314,0.293359,0.491569,0.0237787,0.335975,1.49355,0.080589,0.798883,0.0042814,4.00296,1.46194,1.97508,2.93223,0.369621,0.114972,1.43744,0.953024,1.51951,0.0664168,1.36838,1.12008,2.37583,0.0656217,0.0550487,5.11532,3.41184,2.20438,1.50039,1.4365,1.97503,0.0544394,1.24929,0.00753079,1.44055,0.312355,1.65135,0.778141,0.717496,3.53737,0.872259,1.93065,2.75865,0.0802059,2.52429,2.25681,2.45514,0.0332121,0.272185,1.78893,4.1729,3.72535,0.296955,0.977887,0.0917793,5.16411,0.227299,2.63355,2.93526,3.32066,3.49133,3.54922,3.74034,0.651864,0.379803,1.49085,4.07919,0.729488,0.412124,3.82536,0.844314,0.661833,2.39382,2.20183,0.0319286,0.20516,0.6255,1.46891,1.14456,0.44578,1.54798,4.63112,1.32129,0,0.868121,0.800939,4.08268,1.32296,0.520707,3.65064,6.09133,0.723082,0.158022,0.261037,0.397029,0.0556116,2.45283,1.20808,0.051552,1.35081,3.32664,0.930835,0.830775,0.637906,0.00174865,0.0430316,1.45825,0.147698,1.924,0.00532853,3.77304,1.63318,1.94748,2.129,0.0999379,0.0187992,0.764959,1.12453,0.710663,0.221404,1.04146,2.84609,2.1028,0.0512187,0.0633501,4.60868,4.72056,1.81119,1.71785,1.46575,1.1067,0.22531,1.22568,0.0114571,1.80884,0.322822,1.30182,0.343267,0.780105,4.02635,0.468035,2.11109,2.30884,0.225344,1.73228,3.32626,2.14921,0.0652682,0.179686,0.794495,4.04693,3.94471,0.479574,0.372132,0.0392945,5.12383,0.643861,3.05057,2.2246,3.45714,4.82734,4.22932,3.33674,0.602254,0.160737,1.20358,5.0604,0.894768,1.28898,4.45525,1.44087,0.816369,1.96161,3.89633,0.0192901,0.130743,0.526546,0.911684,0.790611,0.290481,0.535833,3.96858,0.660851,0.183099,1.0221,0.527058,2.76179,2.45929,0.418104,1.77804,6.29833,1.67065,0.412998,0.493972,0.239688,0.143934,2.45849,2.14727,0.0778733,2.5178,3.35156,1.2997,0.506005,0.775292,0.0188194,0.447015,2.64902,0.5688,1.19909,0.0417836,3.4061,2.37674,1.87274,1.59598,0.440209,0.0390666,0.459501,1.47975,2.99328,0.293728,3.44896,2.98324,0.377933,0.136621,0.0858318,3.94416,2.30151,1.82907,2.10093,0.403424,1.47967,0.348227,1.58975,0.0860919,1.47215,0.111787,3.21831,2.28628,1.27021,2.6389,0.099854,2.80626,1.31564,0.256794,2.7791,3.04503,3.35439,0.637234,0.873176,1.78056,1.96819,3.70617,0.573749,0.86824,0.0227969,4.80532,1.19482,4.30919,3.18403,2.90808,3.55253,5.32479,2.20803,0.292511,0.917928,0.917466,3.6687,1.65847,1.24795,2.66524,0.583895,1.36363,3.3847,3.8937,0.104704,0.242911,1.92325,1.26514,1.42388,0.702984,2.26055,4.22127,0.507518,0.104398,1.00863,1.40142,2.62522,3.89506,0.304073,1.87341,4.88558,0.57671,0.101682,0.376967,0.151231,0.302362,0.964678,0.314604,0.357943,2.24616,3.35078,0.934013,0.87141,0.291341,0.00906326,0.116968,0.5981,0.0642974,0.59632,0.070716,3.52079,2.46395,2.44951,2.9654,0.663779,0.298566,0.532495,0.605621,1.02062,1.23134,2.30953,2.70189,0.311687,0.250861,0.157656,2.54278,1.19941,1.6287,1.38798,0.378133,1.65116,0.462127,2.0553,0.183922,1.911,0.26869,2.17268,1.65513,1.10857,1.4878,0.285458,3.24843,1.04712,1.4921,2.49815,3.25619,2.89996,0.615719,1.52029,1.29669,0.594365,3.33981,0.7246,0.386914,0.18093,3.73587,1.15124,4.20692,2.63089,4.41739,4.75388,4.79367,1.61047,0.177007,1.5846,2.15091,2.80859,2.03466,1.22181,1.88013,0.966551,1.62572,2.95286,3.22257,0.171248,0.0488555,1.50401,0.564802,1.11984,1.97094,1.09832,4.79174,1.00213,0.0561535,1.06735,2.31487,3.13783,3.00876,0.280711,1.51538,4.03358,0.577055,0.256786,0.520065,0.197432,1.37471,1.67807,0.111243,1.28862,3.27064,3.1356,0.810865,0.226676,0.090544,0.923369,0.273451,0.463472,0.740315,0.466818,0.0792522,3.41677,1.65213,1.87478,2.9709,0.649614,0.287107,0.742373,2.44696,1.98299,0.362439,3.38517,3.0017,1.15386,0.123824,0.71546,3.2347,2.01491,1.11258,2.1084,1.17668,1.4841,0.585114,2.00054,0.0496953,1.25047,0.316964,2.6968,1.75643,0.580133,1.54092,0.0531415,2.72435,1.84557,0.622192,2.4177,3.2615,2.47776,0.355572,1.18969,2.80585,1.48045,2.73792,1.27207,1.31757,0.00647418,5.56059,0.929273,2.86031,2.55737,3.45009,1.33754,4.2221,2.44413,0.265876,1.38801,0.652557,2.74723,2.27915,1.15483,2.48536,0.862942,1.00924,2.87263,2.60592,0.341337,0.789075,2.61353,1.51038,1.42327,0.692666,1.54161,4.74268,2.04178,0.52153,1.57625,1.82753,2.29875,2.95661,0.794998,2.05961,4.72058,1.10073,0.154321,0.499115,0.0518899,0.51693,0.409915,0.405275,0.53946,2.23644,2.72192,0.94019,1.03312,0.339981,0.0105235,0.0286632,0.641563,0.176992,1.62913,0.173662,2.71067,1.12456,0.980614,1.26853,0.502585,0.0405315,0.0835886,0.801681,2.17712,0.600408,2.7253,3.26336,1.98236,0.19047,0.160637,4.27236,4.15545,0.692021,2.13214,1.37671,1.35437,1.51932,0.632685,0,2.57451,0.0496026,3.45484,0.800954,1.08736,2.65716,0.288336,3.76113,0.877107,0.343407,1.93358,3.87112,3.22765,0.771835,0.544815,2.16071,3.16342,1.80419,0.929744,2.37628,0.0166744,5.67011,0.584775,4.03636,2.40968,1.96826,2.96728,4.39782,2.26301,1.22459,0.56253,0.861167,3.46662,0.947368,1.0488,3.56452,1.12366,1.08122,3.01743,2.8018,0.545803,0.861587,0.610711,1.44031,0.526162,0.620491,1.9175,3.08667,0.663501,0.35154,1.79684,0.938363,2.92849,2.60115,0.880397,2.2393,5.30998,0.792989,0.532533,0.38656,0.475731,0.0857692,0.487966,1.6554,0.17406,3.03141,2.42888,2.00751,0.879513,0.731286,0.102841,0.637661,0.50266,0,1.66406,0.0403251,2.60657,1.94436,0.987926,2.60567,0.490878,0.00202899,0.331029,2.03788,2.20193,1.82084,3.72021,3.44797,0.278736,0.0147163,0.169737,3.7098,2.93405,1.24039,2.8275,0.505545,1.41782,0.345053,1.34328,0.0294194,2.0961,0.0848584,3.29406,1.32525,2.41475,3.38123,0.0721113,2.92625,0.847645,0.420569,1.33231,3.7629,2.30329,0.388465,1.12179,2.35953,1.66306,2.70488,0.503543,0.822229,0,4.34317,0.605498,5.77145,1.83385,1.98237,3.45013,5.29985,2.02454,0.0475114,0.395792,2.11374,3.54273,0.528591,1.56819,3.8756,0.558113,0.941327,2.41993,3.57062,0.187795,0.31701,1.55603,0.604092,0.648071,0.516138,1.93505,3.75648,0.439664,0.0350492,1.06238,2.00205,3.3981,4.43062,0.284624,1.01425,4.71788,0.761708,0.0611976,1.14644,0.125763,0.42916,0.839016,0.331764,0.328137,3.00843,3.59455,1.00545,0.717833,0.128019,0.00645771,0.328582,0.54553,0.188525,0.895525,0.0408872,3.24783,2.71351,2.49815,2.20143,0.676868,0.0287524,0.661264,1.41991,1.15123,0.467025,1.38601,2.40322,0.770031,0.00989118,0.386181,3.90587,4.04182,1.06631,1.77359,1.07598,1.38183,0.577455,1.14628,0.0020014,2.77807,0,2.90588,0.480905,1.61046,3.46837,0.355569,0.968912,1.83375,0.248112,2.19061,3.11325,2.65638,0.0367746,0.0603686,1.67572,2.31816,3.60933,1.29633,0.11909,0.0600984,5.61985,0.720068,2.48464,2.86905,2.13853,1.98833,3.15317,2.22118,0.639354,1.25785,0.281851,2.72698,0.921203,2.52241,1.82059,0.669081,0.100118,1.23264,3.06482,0.0135905,0.641079,0.403843,0.999941,1.04065,2.18396,1.00285,1.85417,0.877147,0.144436,2.53481,0.977721,1.64681,3.45587,0.818207,2.53121,4.49658,0.887782,0.234846,1.14279,0.659315,0.109713,1.36542,0.899438,0.175104,2.23193,2.60381,1.32821,0.915714,0.445052,0.00288133,0.163356,0.982402,0.0125229,0.666454,0.0555231,2.31373,1.38932,2.6543,1.35873,0.143295,0,0.226475,1.68297,0.660947,0.487611,1.4304,4.92722,1.75121,0,0.143175,3.02567,3.35532,0.272801,1.61775,0.645526,0.531874,0.614343,1.03124,0.0053275,3.25492,0.00264338,2.77116,0.430088,0.649697,2.29188,0.722446,2.50275,0.544916,0.204628,1.21149,3.88391,2.39757,0.000352105,0.212974,0.0384491,4.97835,2.7165,1.15199,0.413043,0.0146025,4.46414,0.0112177,2.61585,0.861757,2.28923,3.35354,5.43353,1.99584,1.30369,0.10671,0.500256,5.71459,0.963861,3.3774,3.52604,0.217327,0.0892254,3.19477,4.90951,0.10896,0.394621,0.600351,0.810254,0.513302,0.580245,1.14567,1.09709,0.362707,0.198018,1.85646,0.661291,3.34353,3.2775,0.385556,0.675552,6.33126,1.56004,0.641665,0.847008,1.38089,0.0555137,1.03894,1.94796,0.00552486,3.4941,3.26301,1.9274,0.253463,1.10076,0.0425154,0.904492,2.41034,0.0463368,1.52589,0.0862676,2.75297,1.88521,2.06296,1.15305,0.212366,0,0.394245,1.78298,0.888924,0.672979,1.35533,4.45294,1.48784,0.0605392,0.176757,4.75529,3.85554,0.844591,2.16973,0.272653,1.32838,0.688873,1.4805,0.00044684,2.74095,0.0685647,2.77912,1.24349,1.33388,3.78668,0.0939944,2.98083,1.01641,0.336904,2.41045,3.56117,2.63229,0.0388405,0.126486,0.872781,4.13501,2.94543,1.10456,0.801741,0,4.4144,0.153719,4.29423,1.82199,2.28633,3.094,5.33176,2.21881,0.693673,0.156806,1.09274,4.26824,0.379416,2.21975,3.53672,1.08108,0.89244,3.1856,4.76257,0.0451743,0.0949684,0.228956,0.373762,0.289384,1.2531,1.81112,2.30036,0.625152,0.00985754,1.77277,0.283007,3.30557,3.30533,0.619986,1.35398,6.38698,1.73076,0.126091,0.871414,0.356507,0.174712,1.66457,1.17135,0.0491385,2.67154,4.05349,2.17985,0.523685,0.576262,0.157504,0.308839,1.56192,0.0512652,0.95207,0.0805058,2.48621,1.37535,2.45468,1.87533,0.468265,0.122033,0.321863,1.33016,0.549338,0.494591,0.874955,2.63495,0.740472,0.0438563,0.141534,3.29394,3.68166,1.18674,1.08292,0.244218,1.11172,0.828344,1.42215,0.00554828,1.77962,0.189551,2.12145,0.546716,1.04505,3.95568,0.3655,2.37688,1.71594,0.224391,2.74793,3.33626,2.97977,0.122376,0.0632366,0.972671,2.11059,2.25442,0.85038,0.124168,0.0187264,4.50107,0.534927,3.1567,2.6244,2.49565,3.01174,4.25579,1.62496,0.592326,1.05417,0.562298,3.77764,1.02897,1.90008,2.77085,1.05659,0.747173,2.30213,3.18592,0.0147369,0.023573,0.207013,1.03249,0.925702,0.932221,1.22497,2.04939,0.744914,0.328359,1.31802,0.387632,1.70093,2.47217,1.20024,1.79619,5.47078,1.69118,0.165774,0.794053,0.341791,0.129126,1.44495,1.19076,0.0475776,2.66236,3.06191,2.4638,0.453825,0.488893,0.0543923,0.0634928,1.52558,0.22285,0.553228,0.221582,2.48304,1.022,1.56536,2.29826,0.209171,0.138548,0.309978,1.81818,1.4621,1.09224,2.08636,2.86538,0.603495,0.366823,0.329206,3.08912,2.58224,0.700127,2.0108,0.0986592,2.0601,0.895608,1.33536,0.101057,1.51155,0.290049,2.36485,1.46373,1.06817,3.15438,0.0736844,2.95347,1.35243,0.867988,2.56145,3.61033,1.72861,0.562523,0.60729,1.96837,1.20481,4.01519,1.00243,1.17049,0.0626663,4.00606,0.959066,4.73986,3.9961,2.07039,1.85597,4.51874,2.15147,0.0866293,0.160415,0.776263,3.42877,1.5827,1.208,3.27742,1.12566,1.38087,2.50217,2.11579,0.168926,0.47073,0.789089,1.18347,0.692235,0.859697,2.53243,2.11629,0.874951,0,1.84621,1.05134,2.79519,4.17778,2.43341,1.46079,4.95973,0.990473,0.00451488,0.498543,0.136919,0.747515,1.02114,0.164363,0.20047,2.40017,2.5119,2.07164,1.25656,0.222824,0.0637718,0.0677823,0.407466,0.0305444,1.12243,0.191263,2.95274,0.788903,2.04562,0.821009,0.390528,0.106878,0.290616,0.74644,1.30808,0.7783,2.74388,3.67038,3.43589,0.0119235,0.327381,4.70921,3.99718,0.309318,1.34281,0.799294,0.795629,2.50191,0.502497,0.0563869,3.90016,0.16275,2.25545,0.906753,0.884097,3.51065,0.653698,3.58473,1.43594,0.368113,3.07226,3.27478,2.61307,0.0926574,0.20586,1.01696,3.96841,1.73393,1.60189,1.61205,0.332741,5.6585,0.151749,3.11497,1.19252,2.06011,2.88685,5.18958,1.93425,1.74641,0.143624,0.980521,3.32435,0.240595,2.76726,3.43182,0.829861,0.513481,3.03082,3.14267,0.450467,0.384188,0.203941,1.57999,0.0863299,1.05067,1.62805,2.49849,0.624522,0.0210923,3.38103,0.0787131,3.06627,3.05254,0.724761,2.51799,5.99396,1.44315,0.48246,0.460933,1.27126,0.178611,0.842834,2.41634,0.0746336,2.28044,1.60962,1.77188,0.850597,1.19109,0.296519,0.604338,1.35025,0,0.855787,0.24927,2.99181,2.2486,2.37542,1.3126,0.350791,0.024723,0.201999,1.45714,1.48941,1.53887,2.55214,5.06456,0.994865,0.0734257,0.083538,3.51118,2.41169,0.631832,2.0792,0.115151,2.3912,0.772408,2.02382,0,3.3883,0,2.83265,1.07762,1.06044,3.32062,0.152621,3.1467,0.999532,0.691353,2.18066,4.07433,3.76381,0.564451,0.288695,1.46631,1.80245,3.96539,0.75673,1.25008,0.209399,4.06395,0.238434,5.02981,2.71844,2.61667,2.93738,5.24963,1.91154,0.472515,0.478448,1.34589,4.0873,1.01529,1.9998,4.08209,1.06896,1.46918,4.13979,3.46283,0.222857,0.165366,0.435843,1.03564,0.934988,0.827622,2.30924,2.56759,1.11041,0.339095,2.17876,0.246572,3.56566,2.52839,1.91746,1.52709,7.18276,1.46865,0.0681596,0.578342,0.243158,0.224508,1.52765,0.580382,0.030582,2.47407,4.58363,2.31677,0.465353,0.39472,0.587289,0.213973,0.600313,0.120027,0.89201,0.185326,3.94036,1.52176,2.62566,1.78063,0.208037,0,0.558397,1.04916,0.900976,0.483306,1.48558,3.60931,1.59509,0.0638275,0.0647773,3.8862,3.17611,0.8943,0.841184,0.323702,1.04602,0.258091,1.67474,0,1.82776,0.0908958,1.57279,0.929668,1.49998,4.81248,0.228259,2.91749,2.00505,0.0470934,2.84799,3.22272,3.68171,0.0594875,0.0733745,0.54319,3.34397,2.55439,0.507932,0.75085,0.108581,4.24962,0.145146,3.65904,2.06542,2.62613,3.56953,5.05274,1.96623,0.779482,0.634575,0.948498,4.4603,0.50401,1.81869,3.3978,1.04111,0.447499,2.99711,3.06425,0.0197619,0.0390933,0.276516,0.752591,0.459888,0.672086,2.00618,2.43392,0.363539,0.0871918,1.12805,0.738331,2.49416,1.82035,1.00751,2.02311,6.0054,1.90343,0.143459,0.798544,0.837159,0.103327,1.35308,1.02193,0.0155476,2.08821,4.37539,1.69243,0.628235,0.612107,0.0970055,0.407801,1.79539,0.123628,0.747089,0.163579,2.35004,0.852999,0.910148,1.80315,0.59716,0,0.851198,0.778011,2.19988,0.402101,0.933004,2.09489,3.05333,0.0252962,0.0977166,2.9703,1.6115,1.0476,0.400364,0.66365,0.440661,0.829402,1.43691,0.796235,1.38266,0.161254,0.934144,0.411609,1.29833,2.81847,0.583717,1.70186,2.00263,0.344199,3.56151,1.28074,1.83579,0.0387559,0.0243303,1.23755,4.48576,1.36396,2.05456,0.473,0,3.95411,0.00469383,1.32001,0.745603,1.49426,1.90903,2.37102,0.920934,2.45122,0.736415,0.664711,3.55602,2.10916,0.70585,2.62348,1.12938,0.956224,1.17342,2.17879,1.02419,0.309579,0.363046,1.38165,0.74992,0.428322,1.58538,1.77151,0.166842,1.10562,1.36608,0.289836,1.93779,1.48751,0.142338,1.41565,4.64051,1.12845,1.44624,0.618646,1.53882,0.55646,0.11152,1.63253,0.13781,3.06238,1.53316,2.18355,0.44066,1.92378,0.46227,0.37104,1.70604,0.23205,1.86809,0.102674,3.19703,2.5658,1.93422,2.27114,0.657241,0.0382914,0.290383,1.35741,1.49755,0.741304,2.09531,2.14157,1.03596,0.0978403,0.203721,4.29982,4.3951,1.17579,2.35823,0.722298,1.5228,1.03373,1.08911,0.0532115,1.63872,0.00680868,1.80044,0.4208,2.52569,3.74694,0.2112,1.57353,2.23563,0.457424,3.61216,4.27956,1.83665,0.107581,0.134045,1.37063,2.4452,2.30221,1.81058,1.01463,0.0911855,5.20949,0.789669,4.47411,2.57445,1.41898,1.25783,3.56169,1.19806,0.501017,0.323389,0.956781,1.86047,0.423411,2.34943,2.48705,0.592364,0.878147,1.79065,2.40255,0.415444,0.630449,1.11744,0.658741,0.73657,1.44287,1.21421,2.56012,1.30955,0.0461261,2.81246,0.494368,2.01135,3.04064,1.15881,1.88349,3.73473,0.919362,0.107721,1.49636,0.617467,0.305044,0.784687,1.16902,0.0493862,2.54482,2.76247,1.1756,1.09797,1.07768,0.0691202,0.0770139,0.696618,0.0298994,0.735747,0.0129393,1.88335,0.905986,1.03496,2.101,0.597856,0,0.19022,1.55955,1.9505,1.45855,2.44006,3.13068,0.374842,0.109704,0.0564849,4.41647,3.26477,0.885778,2.43465,0.350469,1.09961,0.447687,1.88259,0.0782037,1.40383,0.0368739,2.70133,1.89626,1.92466,3.58989,0.122527,3.11725,1.05007,0.51717,2.40146,3.33289,1.93742,0.252541,1.02291,2.0183,1.3077,2.65496,1.15508,1.23102,0.000266209,4.37571,1.61635,5.65003,1.84072,1.86486,3.17868,5.39525,1.75413,0.261276,0.306518,0.943247,3.54729,0.883223,1.44945,3.65783,0.709348,1.23866,3.18681,3.92327,0.106278,0.093275,1.28177,0.542254,0.68957,0.528313,1.24936,2.35226,0.303834,0.0594064,1.31486,1.1578,2.43625,4.4125,0.904337,1.00147,4.81197,1.31189,0.0296455,1.11495,0.724461,0.520235,1.25262,0.250807,0.0281229,2.57911,2.92775,2.22562,0.75527,0.242219,0.118131,0.363648,0.497668,0.0814463,1.30973,0.0366483,3.00148,1.25068,1.63096,1.52441,0.621856,0.00641068,0.230657,1.73529,1.45144,0.646578,2.25007,3.25211,0.563205,0.0751726,0.0163225,3.54842,3.84219,1.02355,1.92965,0.817386,1.16032,0.510203,1.44918,0.094062,2.12093,0.18292,2.18219,0.247555,1.35479,3.45437,0.0852659,2.62705,0.895785,0.287165,2.27176,3.76671,2.21876,0.34165,1.02308,1.78778,1.66692,2.17194,0.78184,0.405094,0.0208693,4.54682,0.971125,3.88979,2.00045,2.23424,2.78853,5.28522,2.13774,0.237709,0.150786,0.456535,3.60444,0.698361,1.24292,3.25507,1.02311,0.797662,2.94663,3.29284,0.155182,0.0697566,0.439491,0.730536,0.809481,0.477256,1.35965,2.22005,0.560803,0.0364263,1.49128,0.700289,2.63805,3.72576,0.408458,0.932113,4.24421,1.17882,0.134419,0.657352,0.368504,0.184188,1.30552,0.460816,0.028444,2.41779,2.85773,2.01615,0.72293,0.287015,0.0462368,0.160295,0.856651,0.098313,0.130786,0.046435,2.05407,1.54281,2.34828,2.53102,0.442987,0.0561916,0.268094,1.58142,0.8853,1.28698,2.50667,3.04347,0.423114,0.0185859,0.262836,3.83576,3.57948,0.914873,2.57641,0.271825,1.59572,0.423181,1.84506,0.0146318,1.08426,0.0235161,3.13197,0.697155,1.7896,3.38426,0.0901369,2.18545,1.48666,0.392883,2.32617,3.22082,2.32532,0.0902337,0.145137,1.08237,1.66407,2.63171,1.45994,0.131882,0.0723601,5.2952,0.436551,4.01131,2.07321,1.4268,2.99351,4.44962,1.59591,0.584692,0.999917,0.741395,3.61155,1.21488,2.08004,3.2875,1.06792,0.701226,2.57786,3.9834,0.0356573,0.0402862,0.258593,0.57421,1.35641,0.611022,1.26718,1.16932,1.30904,0.0148628,1.97011,0.861008,1.83741,3.52019,0.66894,1.38401,4.86006,0.991134,0.0453428,0.595723,0.376409,0.243376,1.10378,0.772661,0.104803,2.20183,2.70189,1.8724,0.874469,0.276714,0.0854222,0.109846,0.469759,0.0341609,0.822008,0.0663278,2.82431,1.98922,2.58916,2.33014,0.36916,0.000405256,0.440166,1.42747,2.06229,0.839913,2.56837,3.54221,0.806237,0.0238713,0.0986123,4.16513,3.2629,1.11632,2.24042,0.744196,1.76337,0.456146,1.41652,0.0272024,2.58901,0.167701,2.60603,1.09688,1.16621,3.30133,0.272464,2.55796,1.90589,0.180866,2.41701,3.38609,3.11584,0.191306,0.749476,1.46686,1.90682,2.87457,0.694,0.343411,0.0147392,4.91275,0.464997,3.85381,1.65685,1.67556,3.0329,4.73798,2.09576,0.312937,0.517346,1.04992,3.59476,1.26293,1.085,3.32468,1.59134,1.01859,3.35532,3.53533,0.084303,0.0628613,0.648484,1.09515,1.50538,0.17028,1.40541,2.64826,1.06457,0.100404,2.33836,0.694368,2.53434,3.55765,0.68602,1.77681,5.59932,1.25979,0.0556799,0.505265,0.470065,0.221458,1.14363,0.721429,0.0939817,2.04274,3.13907,1.29704,0.464239,0.875146,0.118063,0.167729,0.691054,0.0293266,0.611965,0.0768707,2.41301,1.51695,1.93647,2.18609,0.344321,0.0389794,0.454889,1.37358,1.48123,0.780079,2.11202,3.10542,0.999507,0.0394063,0.204987,4.05465,3.69151,0.976356,1.60781,0.698079,1.13738,0.537282,1.42179,0.00870474,1.36949,0.023588,2.30823,1.14589,1.14334,3.29495,0.192186,2.55375,1.98119,0.272244,2.98279,3.39615,2.60981,0.0805932,0.603192,1.68799,1.85609,2.74516,0.505839,0.325397,0.0708398,5.28361,0.430745,3.53773,2.20675,1.66348,2.58096,4.62437,2.74623,0.222936,0.392539,1.03132,3.74876,1.04139,1.11657,3.41723,0.882675,0.70088,2.92961,3.14528,0.0404496,0.197252,0.654779,1.25583,0.600679,0.254489,1.76305,2.48888,0.957629,0.0304134,1.98809,0.68718,2.4288,3.55978,0.906977,2.39657,4.87962,1.07763,0.0113303,0.629885,0.284732,0.307247,0.693691,0.863577,0.0309877,1.49964,2.85926,1.40856,0.633111,0.401133,0.0182732,0.0368716,0.753268,0.00673428,0.534558,0.00958632,2.73017,2.39266,2.17597,2.33269,0.836123,0.0222678,0.178695,1.06635,1.69794,1.20227,2.49672,4.13729,0.191926,0.0308835,0.0363431,4.61121,3.23564,1.00583,2.72539,0.298151,2.01082,0.769828,1.96608,0.0858546,2.23029,0.0122647,2.23588,0.633349,1.20338,3.37658,0.0692665,2.29815,1.05922,0.480622,2.08249,4.0234,2.28867,0.415034,0.983109,1.92329,1.10093,1.73856,1.04592,0.487978,0.0242384,4.47649,0.681652,5.14066,1.34459,2.59875,3.29062,4.8603,1.86614,0.227459,0.307617,1.15525,2.771,0.254383,2.10122,3.77412,0.778152,1.06289,3.55692,4.12524,0.0866851,0.039943,0.282242,0.490172,1.49075,0.340438,1.89202,3.13522,0.571204,0.0840356,1.08281,0.748803,2.67685,3.2724,0.322296,1.01903,5.3433,1.32265,0.079348,0.697845,0.370772,0.0963784,1.51029,0.731301,0.0519211,3.12523,3.5217,1.55013,0.490105,0.309106,0.152689,0.126233,0.900529,0.0508995,0.470179,0.0570541,2.93815,1.03723,1.31152,1.88639,0.53211,0.0120982,0.19104,1.59915,0.789835,1.25294,2.21089,3.76776,0.442896,0.0824799,0.0142063,4.1578,3.41276,0.872557,2.83429,0.41081,1.12326,0.520284,2.29224,0.034131,2.31664,0.0527101,1.93802,0.23192,1.07679,3.58917,0.103749,2.88465,0.943151,0.274026,1.80946,4.21325,1.81319,0.25273,0.727642,1.31808,1.68266,2.21464,0.861174,0.387389,0.0198225,3.8402,0.379294,4.60951,1.26635,1.76706,3.3753,4.86082,1.96672,0.269316,0.196363,1.33864,3.62479,0.627859,2.09711,3.85491,0.609686,0.881531,2.71234,3.77832,0.0278493,0.0952197,0.350846,0.639852,0.512732,0.316809,1.29115,2.19078,0.408391,0.0650802,1.27545,0.848432,2.89082,3.46501,0.429315,0.532971,4.65582,1.2727,0.0419573,0.771441,0.57798,0.189393,1.16264,0.666373,0.011883,2.61725,2.62028,2.10185,0.367404,0.307311,0.375864,0.115727,1.16286,0.145557,0.639334,0.0189169,2.85532,1.43457,1.51701,1.08727,0.655423,0,0.39513,1.46804,1.47166,0.837551,2.10679,3.85416,0.534801,0.0470078,0.124029,3.15903,3.37361,0.818388,2.83651,0.451094,0.913231,0.778017,1.86247,0.0487507,2.85288,0.0583686,2.2793,0.732785,1.39704,2.60797,0.122074,2.77161,1.1196,0.590158,2.21509,3.73356,2.73063,0.499842,0.927976,1.42161,2.06318,2.38936,0.891494,0.883829,0.209371,4.37902,0.608832,3.81845,2.51543,2.54147,3.3838,4.15177,1.59683,0.192216,0.537215,0.455513,3.93832,1.0829,1.92547,2.39969,1.00714,0.802947,2.15678,3.95803,0.20991,0.124253,0.479957,1.0012,0.444251,0.969963,1.81434,2.18901,0.321689,0.0278072,1.64864,0.651427,2.12086,4.46436,0.998611,1.1613,4.08807,1.39377,0.127706,0.700906,0.552711,0.515313,1.00938,1.48769,0.0935584,3.28137,2.79706,1.85369,0.296472,0.487268,0.0317496,0.19594,1.23039,0.0388062,0.67017,0.0934113,2.86187,1.86251,1.88909,1.87347,0.596602,0.00377345,0.192182,1.35662,1.60304,1.6355,2.39665,4.20327,0.978054,0.0436493,0.0764731,3.90868,3.21256,0.552857,2.17574,0.720971,1.57867,1.00832,1.66714,0.0796119,1.50929,0.0632518,3.0211,0.78774,1.44251,3.49814,0.0800848,2.00075,1.36371,0.743488,2.33431,2.60721,2.96996,0.278091,0.484778,1.58918,2.23393,1.81321,0.76124,1.05518,0.180481,5.67809,1.19608,4.3703,2.42004,1.21932,3.44784,4.91669,1.31593,0.246808,0.215084,0.619059,3.59653,0.725094,1.33354,2.42733,1.78544,1.59301,2.10466,3.41312,0.251837,0.0452664,0.893701,0.983479,0.592334,0.906095,1.89089,2.11333,0.918553,0,1.45872,0.963036,1.41516,4.55745,0.836861,2.22682,4.22401,0.894384,0.0436534,1.00384,0.145187,0.258983,1.36973,1.11032,0.156456,2.41609,2.70471,2.1322,0.526215,0.252542,0.0258043,0.349993,0.35663,0.104863,0.225369,0.24749,3.35109,0.963801,1.10813,0.800809,0.467626,0.107116,0.0552091,2.47139,2.62246,1.78614,3.4856,5.1379,2.94626,0.0129506,0.137714,4.38971,4.69919,0.188566,2.24813,0.520747,0.79837,1.95592,0.550285,0.00599656,3.57328,0.080133,2.81194,0.483219,1.56046,3.52163,0.658864,3.26069,1.05765,1.0118,2.94407,4.80952,1.89636,0.259329,0.0739332,1.2724,3.2216,1.86089,1.15171,1.37013,0.828185,6.12156,0.314778,3.56917,0.780318,1.08123,4.30628,4.93581,2.55729,1.14319,0.0388253,0.890784,3.77139,0.236146,3.92972,3.29398,1.21922,0.728362,2.22584,3.33834,0.835542,0.443234,0.266086,1.38535,0.148491,1.23911,1.60022,1.91213,0.431745,0.0798277,3.92146,0.100696,2.10039,5.02391,0.322355,2.43659,4.82507,0.601774,0.46191,0.594377,1.27198,0.325763,0.846408,2.22323,0.0822376,3.12431,1.4243,2.1481,1.00837,0.604639,0.100744,0.806879,0.668582,0,0.35246,0.0494527,2.93895,0.883624,1.23582,1.33622,0.579971,0.00303413,0.12075,1.26554,0.868126,0.683931,2.13174,3.34147,0.507017,0.034596,0.407799,3.14334,3.45889,0.514382,2.67469,0.30144,0.566191,0.578803,1.58267,0.0177247,1.72527,0.00918701,2.09903,0.249594,1.94982,3.16641,0.119972,2.91085,1.27837,0.211139,2.1288,3.62004,1.90441,0.0502565,0.314473,1.26084,2.53492,2.32336,1.24451,0.584151,0.09295,3.83199,0.583287,4.35442,2.26039,2.16336,3.10987,4.73173,1.53667,0.242611,0.268393,0.520367,3.39473,0.656522,2.33191,2.41795,0.737193,0.88631,1.90878,3.36651,0.160806,0.30254,0.598468,1.03497,0.637035,1.40763,1.29467,1.22339,0.127561,0.0349597,1.86225,0.668291,2.26833,4.25181,0.515916,0.231848,3.73252,1.58088,0.390363,1.09529,0.602876,0.307385,1.07234,1.07695,0.00450348,3.96513,2.2456,2.13224,0.583686,0.3779,0.0323832,1.12609,0.944197,0.0601499,0.516288,0.0505858,3.97665,1.66554,1.54787,2.25417,0.843666,0.109033,0.411015,0.34539,0.718933,0.611431,2.28845,2.05871,0.923705,0.216646,0.0154038,3.44349,2.49335,0.979974,1.59057,0.453513,1.40971,0.303805,1.07996,0.0104552,2.15765,0.575598,1.24831,0.146673,0.647196,3.51472,0.223764,2.36423,2.38342,0.979161,2.10142,3.73741,2.76451,0.864586,0.278941,1.18661,2.21592,2.22132,1.20689,1.51193,0.020477,3.30165,0.550168,2.83984,2.15062,4.65036,4.78363,3.84807,1.75357,0.539525,1.41835,1.5899,3.9677,0.71811,1.18531,4.55276,0.925562,1.40166,2.73467,3.21768,0.090259,0.000825712,0.866169,2.0545,0.279414,0.582492,1.44524,3.59893,0.470387,0.00659904,0.221317,1.14851,2.84489,2.55217,0.285621,1.01121,5.44992,1.04718,0.384958,0.209347,0.269595,0.214903,1.98371,0.982846,0.446999,3.39738,2.89421,1.69827,0.18654,0.0606241,0.572584,0.845817,0.83125,0.142304,0.511877,0.101747,3.0588,1.25144,1.19462,1.74543,0.190779,0.00891978,0.442403,0.66589,0.917971,0.111052,1.06662,2.67585,1.32598,0.0668443,0.0650987,3.3711,3.52265,1.85074,1.12053,0.144353,0.746337,0.826837,2.03384,0.0279713,1.60881,0.160879,1.02235,1.06167,1.20357,4.12033,0.136176,2.46433,2.25091,0.329999,3.34333,2.89288,2.42422,0.0518891,0.154113,0.650573,3.72507,2.40731,0.693673,0.657181,0.0400677,4.00169,0.610979,3.20096,2.30301,3.29098,3.8978,4.15155,2.06398,0.739552,0.580522,0.728652,4.44951,1.04893,2.03725,3.53141,0.726454,0.643724,1.8834,3.04606,0.0857924,0.340497,0.160281,1.16203,0.133217,0.954035,1.35878,1.93417,0.0825962,0.0565659,0.919205,0.37644,2.29047,3.81102,0.682563,1.71354,6.10332,1.62584,0.381732,0.857583,0.478908,0.310714,1.69634,1.53788,0.093128,2.65451,3.03056,2.00864,0.622983,0.242817,0.0446907,0.31917,2.12676,0.584698,0.508028,0.312854,3.57535,1.01957,1.76515,2.32122,0.309599,0.0135759,0.426861,0.866511,1.5532,0.567531,1.69692,3.17837,1.28109,0.138402,0.0340985,4.34716,3.71548,1.77937,2.15931,0.173799,0.726289,0.202157,2.69981,0.0611559,1.09466,0.21049,1.1993,1.28511,1.93591,5.0003,0.160311,3.53014,1.60281,0.552014,2.56209,3.43234,3.58124,1.18037,0.927042,2.41542,2.06958,1.71808,1.0145,0.981867,0,4.76112,1.02332,4.90183,2.32996,3.75912,4.43126,3.90499,1.79408,0.407988,0.975094,1.80331,3.76273,0.680769,1.02908,4.06174,1.33209,1.37562,3.4585,3.2734,0.0544218,0.048771,0.182318,0.83412,0.339921,1.93744,1.76138,3.12507,0.333666,0.603001,0.910121,1.11202,1.64939,2.83296,0.878977,1.53331,5.16968,2.19303,0.26282,0.56399,0.559927,0.339195,1.69228,0.502092,0.0285549,2.2546,4.01991,1.49154,0.449443,0.216892,0.285517,0.116664,1.16992,0.968088,0.554891,0.372601,4.17086,0.611138,1.43107,0.835574,0.497291,0.0541932,0.0318258,1.75179,3.45455,2.30135,4.44617,5.07075,4.09356,0.168165,0.118873,3.71805,5.38558,0.359948,1.17537,0.401529,1.74139,3.03928,0.641873,0.0452245,3.51162,0.225924,1.38501,0.975901,1.95064,3.55211,0.409476,3.11961,2.09502,0.283407,3.57552,3.27555,2.12664,0.34929,0.474149,2.16918,3.38105,0.957036,1.66978,2.51291,1.40494,5.48474,0.223458,5.07802,0.565896,1.52222,3.37747,5.36741,2.40689,1.75742,0.177935,1.56984,2.99916,0.108959,3.20956,2.76626,1.30163,0.704933,4.24807,1.97075,2.02187,0.711818,0.606899,0.713849,0.580595,1.98833,1.63021,3.92626,1.18241,0.0354319,4.58759,0.332903,2.27938,5.06359,0.173173,3.92383,4.6605,1.06388,0.556589,0.171921,1.86755,0.878197,1.36928,0.658514,0.0190993,3.09929,2.0158,2.27263,1.96783,0.792823,0.213694,1.07014,0.38616,0.0420136,0.121149,0.419845,2.73868,1.23557,1.6829,1.17198,0.422408,0,0.212392,2.15687,1.06676,0.841352,3.33135,5.16731,2.13337,0.220289,0.325544,2.79493,2.97439,0.37262,1.64285,0.181783,1.72529,1.00698,1.78506,0.0236844,2.2001,0.10197,1.8321,1.11312,1.18017,2.91984,0.108672,3.7586,0.671367,0.93762,3.10085,3.06066,1.9627,0.179142,0.317111,1.18516,2.57359,2.41962,1.47094,1.01224,0.000268379,4.35446,0.484724,4.49661,1.5371,1.99019,2.96908,6.3332,0.812003,1.23299,0.243958,0.250655,3.52692,1.45084,2.45594,2.48154,0.441547,0.970346,4.3034,3.72199,0.833116,0.0537236,0.949839,1.43612,0.538444,0.225747,1.98101,2.63878,0.72098,0.148604,2.18236,0.28725,2.9508,3.88237,1.00137,0.888707,6.01463,2.11074,0.386278,0.708657,0.847301,0.235339,1.14889,0.734926,0.0110711,4.25795,3.09755,2.88656,0.214374,0.615552,0.0784606,0.794541,1.4357,0.00623513,0.514883,0.03413,3.33517,0.812137,1.2248,1.74864,0.315777,0.013393,0.210911,1.90697,1.91836,0.767752,1.65758,3.16942,1.14282,0.0459913,0.220999,4.17401,4.52362,0.804498,2.51992,0.268469,0.642483,0.851914,1.99326,0,2.50734,0.0381192,2.03101,0.542559,2.1662,4.10502,0.0503313,2.32466,1.89728,0.167142,3.0341,3.65051,1.81492,0.173916,0.377965,0.68109,2.95184,1.96136,1.56334,0.696108,0.00445108,4.07859,0.114465,4.67323,1.05034,1.61055,3.35983,4.21886,1.91939,0.436332,0.254994,0.68775,2.84131,0.521681,2.10881,3.26495,1.07813,0.664651,1.33711,3.38636,0.0436841,0.419039,0.227035,0.94118,0.196337,1.36326,1.60736,1.10145,0.0341857,0,2.58226,0.202405,2.20619,4.68477,0.452925,0.957097,4.00319,1.80225,0.122734,1.26034,0.979403,0.635432,0.928647,1.13046,0.0393081,2.84849,2.48059,2.28962,1.31035,0.412608,0.0703817,0.296971,0.897599,0.0375367,0.294765,0.160068,2.55941,1.3605,1.34182,3.14562,0.427241,0.0111013,0.150922,1.19132,1.69642,1.51754,3.49422,2.81836,0.123653,0.0371704,0.242947,3.68749,3.71244,1.78137,3.2637,0.947374,1.67937,0.796561,2.51693,0.134468,1.68809,0.116397,1.39079,1.24835,1.9665,3.59129,0.0179134,2.62591,0.670484,0.865052,3.07211,3.2161,1.61743,0.771014,0.855325,1.22421,0.375836,1.87587,1.42771,0.939178,0.00956868,4.43019,1.83368,6.28362,2.02937,2.98059,3.77094,4.369,0.936155,0.152694,1.12217,1.54499,2.27825,0.381153,1.99007,2.66481,0.41185,1.30618,2.36695,3.4729,0.175739,0.0572974,0.920726,0.575249,1.12185,0.886291,1.77656,3.09869,0.271949,0.42442,0.827352,1.61262,2.53005,4.59954,0.966383,0.271037,3.76151,1.69681,0.196308,0.92964,0.292832,1.10383,0.979889,0.0910277,0.456767,4.02827,3.03481,0.945839,0.690891,0.159811,0.335634,0.407737,0.668719,0.211333,0.512295,0.0159793,3.84225,0.861139,1.66746,2.88913,1.05076,0.017604,0.28309,1.60989,1.0752,0.338429,1.99783,1.83507,0.453097,0.152038,0.072195,3.54355,4.65087,1.40336,1.6848,0.314609,1.67955,0.824161,2.18883,0.240514,1.77434,0.279308,1.42163,1.27125,1.40641,4.79524,0.0708822,2.12524,1.796,0.135306,1.86181,3.34326,2.62422,0.273251,0.466707,1.28652,1.4584,1.91578,1.2838,0.758845,0.0115877,4.82758,1.02023,4.63398,2.57347,3.50164,3.95303,3.70157,1.64042,0.440699,0.761368,0.495789,1.64867,0.70766,1.93985,2.99937,0.903021,0.935193,1.87404,3.39784,0.0792058,0.0470492,0.257649,0.60916,0.915243,1.18901,1.08066,1.89791,0.142627,0.186183,1.73326,1.11931,2.12438,3.13803,0.993114,1.108,4.62309,2.45593,0.0847078,1.31495,0.32682,0.559858,1.30904,0.349134,0.0207973,2.69449,2.65408,1.76183,0.526198,0.360282,0.0162377,0.452695,1.67969,0.786978,0.738353,0.0760599,2.98748,0.974157,1.96799,1.90436,0.93187,0.0125455,0.144638,1.44335,0.501522,0.782217,1.49275,3.55478,0.51408,0.0272756,0.00305018,3.47825,4.65626,0.926729,1.82176,0.461906,0.700576,1.0538,1.82246,0.0129438,3.10561,0.129999,2.23367,0.193773,1.35113,4.37293,0.408169,2.48598,1.12841,0.26154,2.01435,3.85373,3.28516,0.033117,0.569275,1.23854,1.83767,1.66214,1.18243,0.607491,0.127385,4.84697,0.817407,3.33022,2.42144,2.63031,3.68531,4.49616,1.40106,0.68475,0.911053,0.511081,3.96689,0.497705,2.66518,3.59399,0.693997,0.303797,2.3589,3.17979,0.120173,0.0840141,0.0773957,1.22968,0.873476,0.766364,0.745777,1.11064,0.114887,0.175923,2.15218,0.573491,1.95948,3.04424,0.916047,1.46429,4.72096,1.51816,0.327772,1.1236,0.827795,0.180154,0.970699,0.992361,0.0485862,3.01936,2.58218,1.8066,0.349257,0.142384,0.0494521,0.425016,1.76525,0.178625,0.188875,0.0699539,4.07327,0.691051,1.58693,2.37945,0.216532,0,1.81234,1.15422,0.581613,0.271597,0.473457,2.50652,2.63144,0.181997,0.0424526,2.76629,2.03107,0.867031,0.152904,0.0139542,0.236293,0.0618427,2.29404,0.0216631,1.18051,0.0467634,0.860408,0.57037,0.724652,4.19656,0.0281417,2.31023,1.71416,0.147774,3.36904,2.19917,2.46426,0.0893236,0.077964,1.17619,3.83219,2.2351,0.284899,0.717847,0.00192396,4.0828,0.0221865,2.47903,1.85622,2.79813,3.13349,3.78653,2.18643,1.42026,0.848296,1.66738,4.36045,2.32377,0.300671,2.82945,0.368193,0.19111,1.17597,2.81472,0.231004,0.0665675,0.404295,1.22482,0.0443175,0.790282,1.99073,2.7893,0.349134,0.273168,0.396511,0.449771,2.04238,1.17213,0.0075335,1.75189,4.58506,1.10772,0.211991,0.926428,0.222057,0.110032,0.107369,0.978691,0.0444489,1.09031,3.1166,1.84598,0.23379,0.0729769,0.087661,0.16739,2.09003,0.564941,0.553636,0.98773,3.2317,2.84104,1.344,1.61252,0.924333,0.390287,0.523503,0.745252,1.81386,0.3759,0.961302,0.74361,1.92746,0.0367375,0.254657,4.22352,3.2171,0.928905,0.551574,1.87886,0.820247,1.85763,1.40116,0.460056,1.42202,0.0111928,0.189119,0.220919,1.58414,4.81054,1.16823,0.408528,2.59343,0.0531654,2.90222,2.50481,0.482151,0.0355503,1.4243,0.97523,5.23268,1.0987,1.421,1.03101,0.0672638,3.65836,0.717889,2.82239,2.06995,2.53594,1.60664,1.07846,2.54499,0.898502,0.0471799,0.700152,1.64357,0.0399906,0.482931,2.05672,0.797772,1.13379,0.274555,0.707143,0.234836,1.07659,0.178635,0.615385,0.297467,0.860431,0.110594,2.59657,0.600154,0.479032,0.296133,0.993862,2.26152,1.45347,0.364953,0.894827,2.17797,2.07657,0.611407,0.822876,1.5875,0.144002,1.58942,1.88468,0.114159,1.42093,0.818327,1.9349,0.802866,0.243312,0.301177,0.429983,1.43563,0.656302,0.0938108,0.430553,4.81325,0.0807951,1.46824,2.15321,1.30366,0.0564281,0.156204,2.13588,1.46877,0.472374,0.611697,1.42164,1.90515,0.231403,0.129641,2.6591,2.61479,0.637518,0.904208,2.13806,0.372701,1.13394,1.90304,0.58043,1.33959,0.553732,1.06007,0.985018,0.543808,3.25897,0.206441,1.39186,0.804308,0.384461,0.844792,2.90573,0.300399,0.213936,0.998085,1.63261,3.31909,1.73901,1.18092,0.349687,0.109669,3.74661,1.23421,2.04492,2.26541,2.26596,1.53822,2.52296,2.75313,0.220269,0.019697,0.46049,1.82769,0.388736,1.27916,3.24384,0.741025,0.487426,0.531523,2.76641,0.0748161,0.547243,0.837052,1.31265,1.02772,1.34277,1.34924,1.00636,0.103381,0.0574457,0.980765,0.820282,0.887506,3.20173,0.046969,0.528082,2.77855,1.84989,1.00946,1.86239,0.608829,0.516535,2.2498,0.342212,0.0634859,1.29077,1.86591,2.0693,0.417948,0.261626,0.13938,0.409343,2.22424,1.9599,0.663104,0.11887,6.45383,0.38252,4.04982,1.77296,0.789522,0.227368,0.340924,3.34547,2.93424,1.29059,1.17446,1.97037,1.24899,0.574112,0.254942,2.74412,2.74275,0.509385,0.330373,1.52191,0.451901,2.00786,2.16086,1.21977,1.45518,0.822138,1.62224,2.1564,0.630753,2.69039,0.0497901,1.52492,1.13065,0.222982,1.43297,1.86954,0.621498,0.639015,1.63963,2.85328,2.43762,1.38037,0.973973,1.20674,0.148073,4.70571,3.20529,2.91054,0.694522,3.6054,2.29621,3.79459,0.683008,0.239141,0.2356,0.30097,1.11056,1.80002,1.15592,2.84758,0.432422,0.857427,1.14139,1.32843,0.153965,0.989956,0.686348,0.316861,0.115731,1.02845,3.41758,2.12211,1.2383,0.485448,2.75098,1.07798,1.46718,3.1013,0.249525,1.36975,3.21075,1.94395,0.38638,1.62883,0.523149,2.01228,1.84046,0.501834,0.593366,1.46181,3.40543,1.10476,1.44816,1.1251,0.0295201,0.133602,3.52492,2.39285,0.594473,0.67307,7.33662,0.312076,2.17701,1.45173,0.573155,0.34617,0.487049,3.08279,2.93026,0.649512,1.89446,1.756,0.609289,1.22079,0.398642,2.5188,1.94528,0.986859,0.457446,0.847528,0.412541,0.976149,1.2117,0.705849,1.9876,0.722498,1.24018,2.55111,0.489519,2.72867,0.0291009,2.35459,0.235688,0.316869,1.69132,1.35021,1.12745,0.435903,1.41336,2.33648,2.03596,1.68084,0.584443,1.15148,0.0612399,4.65578,1.55388,4.58667,0.963235,4.25159,2.54535,3.56805,0.892973,0.33514,0.181146,1.26509,1.56171,1.0552,1.02058,2.56878,0.821836,0.81548,2.27766,2.08943,0.648175,0.884804,0.96172,1.02797,0.711759,1.21395,2.56594,3.44538,0.760028,0.0488547,1.48262,0.799258,1.08438,3.27935,0.0382538,1.45882,3.31775,2.27778,0.715653,1.53316,0.778512,1.63186,1.1993,0.220446,0.778951,2.07157,3.78488,0.823545,0.834017,0.654665,0.0800226,0.337342,1.8533,2.51725,0.702393,0.102364,7.71211,0.286745,2.43409,2.67686,0.619262,0.335597,0.0531147,3.60267,2.38416,0.416732,2.09647,1.71686,0.757883,0.930876,0.0560811,1.97866,3.51794,0.56192,0.535995,0.996723,0.851703,0.930008,2.42682,0.615996,1.27864,1.03836,1.36461,1.88232,0.572575,1.65992,0.166636,1.97775,0.721565,0.134376,1.03473,1.5387,0.73856,0.193663,1.30164,2.81109,2.61607,2.13191,1.14167,1.21546,0,4.85786,1.71485,3.11446,0.752463,4.40693,1.40171,4.11217,1.04395,0.399484,0.80766,0.345108,1.13839,0.892735,1.10168,3.1794,0.556764,0.76763,1.86924,1.66639,0.354613,0.812082,1.13795,0.466964,0.466748,2.14874,3.45943,3.79036,0.872065,0.592503,2.39201,2.14517,1.32333,3.32057,0.065606,1.8166,3.61239,1.98733,1.4363,1.76843,0.745389,1.04846,1.55632,0.433596,0.657589,2.06524,3.66612,0.766329,1.20404,0.482788,0.039347,0.568658,3.09411,3.26962,1.34813,0.10506,5.60434,0.0889655,2.88713,2.08657,1.16381,0,0.320336,3.15232,2.11339,0.581482,0.599109,1.64684,0.136905,0.509498,0.0594996,3.93309,3.1175,0.723839,1.43198,1.64233,1.06164,1.62169,1.82454,1.04357,1.74279,0.442203,2.06128,2.01575,0.252877,3.43121,0.0215486,1.07699,0.155145,0.312614,0.946017,1.93533,0.400249,0.157762,2.43273,2.4283,2.45243,2.85849,1.08282,0.64019,0.0445874,4.83998,2.06011,3.49755,0.916848,3.19959,1.67709,2.80862,2.02959,0.462559,0.40393,0.567347,1.60233,0.82948,1.03516,4.08745,0.758877,0.558853,0.81449,2.77149,0.020525,0.424165,0.548064,1.4836,1.51395,1.17532,2.14801,2.16371,0.536734,0.20805,2.26247,0.393927,1.15546,3.80855,0.062539,0.560503,3.57414,1.74579,0.403144,1.8321,0.521171,0.281189,2.58205,0.165068,0.21593,1.33553,2.64503,1.22495,0.95273,0.758475,0.0459459,0.0536204,2.62278,3.09329,0.743081,0.355141,3.07225,0.99825,0.949002,1.12931,1.44038,0.223063,0.0110416,2.39146,1.64444,0.935833,1.60383,0.529734,1.10079,0.362805,0.828244,2.59053,4.22452,0.686707,0.93297,3.42002,0.0990215,0.910626,0.453031,0.468966,2.71115,0.0979038,0.878076,0.580586,1.66404,2.94335,1.24273,1.59951,1.76682,0.486193,2.11952,2.20781,0.287483,0.00329437,3.36734,0.745545,4.66653,0.703305,3.2894,3.1307,0.324362,2.55482,0.777873,1.86771,1.24968,1.76596,0.422246,2.00153,3.43406,0.229743,0.130489,1.09743,2.68829,0.149078,0.925941,3.05519,2.28496,0.148349,1.08481,0.311612,0.871743,1.85163,0.623889,2.28469,1.59312,2.77352,0.738557,1.06193,0.625584,0.341742,2.5183,2.62624,2.89115,3.69003,0.861826,0.267442,1.87515,2.30547,0.962704,0.719618,1.73405,0.0303122,1.83939,2.17335,0.257229,1.65153,1.09087,0.755912,1.62221,1.2047,0.0661021,0.732646,2.63772,0.807986,0.478311,1.44167,3.3315,1.92262,1.10396,2.1373,2.94314,0.605313,0.589089,3.41747,1.81504,1.43408,0.91407,1.51313,1.78559,0.543597,0.0665963,0.848016,4.45425,0.375745,2.11154,2.0208,0.331684,0.872671,0.837342,1.35013,3.21238,0.285412,0.122489,0.0346207,1.26419,2.99989,0.387741,0.275004,0.779306,1.11022,0.949502,1.27925,0.00285642,0.139194,3.4054,0.815279,1.51069,0.314028,2.58728,0.931934,0.0288647,0.85859,1.04216,2.46264,0.431734,0.340156,0.22502,2.04931,2.61658,0.785478,0.0418276,2.72806,0.686128,0.114332,0.593864,0.881908,1.11965,0.0711848,0.935166,0.3465,1.27528,0.173925,0.981481,3.06613,1.73727,2.21345,0.102727,0.694715,0.258337,0.0755219,1.85269,3.1383,2.03729,2.63149,0.560199,0.010192,0.823945,1.67051,2.69149,0.669242,1.35486,1.68324,0.539925,1.1993,0.0525289,0.577206,0.48674,1.16002,0.269629,0.6042,0.215148,0.47513,1.01106,0.912632,0.722549,0.141901,6.56804,0.332258,1.45423,1.84598,1.82717,0.607185,0.573186,2.45562,1.12138,0.721186,1.19955,2.16764,2.1298,0.463881,0.178236,1.82724,3.83526,0.128018,0.762716,0.459628,0.192741,1.12438,1.60296,0.963624,0.888673,0.645426,0.701608,0.815011,0.682288,1.60415,0.127339,1.19198,0.86594,0.288694,1.62697,1.99353,0.0730802,0.0582204,1.01377,1.65823,1.42085,3.36974,1.82254,0.886563,0.110572,3.70404,1.13102,2.06875,2.42856,1.66822,0.726381,3.11729,1.03275,0.200753,0.104538,0.586497,0.485197,0.89011,1.04055,3.76498,0.289018,0.173404,0.913941,0.603713,0.771254,1.16629,1.70345,0.877945,0.199141,1.21614,2.2545,1.42328,0.443216,0.0665258,2.377,1.73775,2.94043,4.8777,0.528893,0.166291,2.38448,2.42238,1.55654,0.459104,0.535974,1.50026,1.24673,0.8198,0.0119287,1.99609,3.31431,2.55657,0.570893,1.3003,0.0539923,0.52299,1.41024,1.28207,0.630163,0.105739,5.2302,0.227859,1.18988,1.75374,2.91697,0.386402,0.277785,2.83866,2.86109,0.331147,1.23632,2.39778,1.80306,0.259275,0.0913708,2.5227,3.94372,0.183673,1.56848,2.52969,0.532227,1.35426,1.27724,0.66081,1.88493,0.0528896,0.384378,0.398594,1.99346,2.60009,0.319524,0.562004,0.493854,0.701511,1.39409,3.22114,0.00298254,0.000888258,2.58557,1.696,3.47152,1.68618,1.24888,1.71971,0.0914841,3.61332,1.02518,2.04624,0.821979,1.69475,0.152026,2.5869,3.31264,0.179382,0.226419,0.27463,0.567847,0.686924,0.527543,2.87715,0.480118,0.376092,0.896157,0.540858,0.364544,1.60616,1.842,2.10057,0.225728,1.059,1.49925,1.6298,0.26304,0.268784,3.2952,1.58951,2.43822,4.05653,0.189399,0.416057,1.68345,1.14149,0.173652,1.17457,1.47797,0.139466,1.87393,1.34337,0.0421757,1.9166,2.95541,1.46175,1.41858,0.866783,0,0.0498921,2.34725,2.31223,1.62391,0.375117,4.67734,1.96945,1.1104,0.41031,2.23528,0.0241686,0.108533,3.11117,1.68433,0.227382,1.86509,2.15314,0.75385,0.14333,0.0874208,2.50929,3.98529,0.110356,1.13896,1.43417,0.313859,1.31874,0.309791,1.05315,1.39793,0.283745,0.876908,0.435468,1.45572,4.32967,0.356555,1.59968,0.916453,0.208693,0.985789,2.3605,0.577597,0.0338435,2.86926,1.3007,4.79799,0.899941,0.919981,1.93226,0.287895,4.67877,1.19555,2.69553,1.09768,1.7825,1.0552,3.7481,1.68595,1.15395,0.121583,0.967309,1.42674,0.24203,1.80173,2.21328,0.455632,0.0445802,2.03796,0.194335,0.623538,0.673932,3.03886,1.03398,0.341118,0.814646,1.06452,2.10121,0.458083,0.307486,2.18301,2.82394,2.79605,4.71776,0.0169554,0.514094,1.82997,0.227555,1.4776,1.54838,1.73153,0.134113,0.53203,1.91166,1.06839,2.55417,2.87113,0.928262,0.717707,1.47587,0.0368642,0.395106,2.57313,2.36082,0.21963,0.323391,5.03808,0.082217,1.79875,1.31236,3.24022,0.0172295,0.314153,2.91945,3.53399,0.424727,2.75348,2.03163,2.58717,0.19609,0.163325,1.67908,4.54266,0.128016,1.19594,4.72996,0.151521,2.57415,0.585961,0.137761,3.29643,0.257186,0.867165,0.450505,1.81,2.83961,0.772029,0.758587,1.41232,0.355646,1.14016,2.61898,0.0082014,0,2.93451,1.90219,3.77763,1.32164,1.4126,2.36852,0.146113,2.66697,1.39908,2.6182,0.160965,1.32218,0.233872,4.31484,3.86704,0.212293,0.0289287,0.211306,0.948793,0.0361328,2.27068,2.3723,0.391514,0.132712,1.97043,0.661183,0.168071,1.03141,1.16468,1.77397,0.436936,1.85067,1.08954,1.75647,0.943836,0.312196,3.22857,2.07949,2.32178,4.79149,0.109242,1.39368,1.99084,0.881735,0.252822,1.66833,2.07736,0.235665,1.47536,2.16852,0.126876,1.65064,1.7142,1.02378,1.67983,0.853408,0.000578408,0.429056,1.95147,2.63166,0.514809,0.292234,6.86733,0.708982,1.07439,2.01243,1.70541,0.0322334,0.125035,2.63003,3.71278,0.252346,1.74815,1.72679,2.06691,0.478085,0.00483701,3.22095,4.07958,0.0830733,0.856439,1.46784,0.161831,1.89061,0.873866,0.556582,2.05569,0.800068,0.749068,1.25719,1.60501,1.70926,0.275427,1.74127,1.1998,0.342481,1.02666,2.29009,0.0140432,0.00846901,1.14974,2.9381,2.94817,1.66574,0.967391,2.07723,0.00166173,4.34336,1.6463,4.26598,0.295507,2.75264,0.581208,2.98656,1.74961,0.267475,0.00369199,0.515228,0.689112,0.215311,2.00409,1.85591,0.100661,0.215568,1.28994,0.443553,0.628079,1.4706,1.66099,0.469097,0.0821688,2.06333,1.43894,3.49428,1.17021,0.0637752,2.23233,1.96668,1.91407,4.57395,0.201747,1.19394,1.94423,1.59399,0.4392,1.26067,1.52161,0.432527,0.68314,0.536641,0.0698201,2.35767,2.47381,1.03315,0.914498,0.835524,0.00987089,1.05986,2.03397,2.89699,0.325294,0.00119871,6.21996,0.00835778,2.5288,2.36303,0.318735,0.496379,0.934812,3.41399,3.82237,0.600501,1.72741,1.0386,1.41984,0.81225,0.117677,3.28885,4.08034,1.41401,1.32952,2.14254,0.730038,0.948184,2.88913,1.35151,1.5385,1.24999,1.30351,2.03615,0.644665,1.51591,0.0244568,0.981655,0.301641,0.643772,1.83247,1.88254,0.108825,0.131312,1.1616,1.51908,2.31524,2.62664,1.71108,1.12073,0.00699734,4.42007,0.975613,2.27487,0.766412,2.54045,0.633935,3.36548,2.42673,1.16776,0.927902,0.710297,1.08234,2.17219,1.42854,3.42324,0.489319,0.301125,0.260754,1.54756,0.0837248,1.38027,0.686394,0.481088,0.567501,1.99992,3.4116,2.68451,1.27092,0.627177,2.89755,1.2995,0.600107,3.28343,0.0155647,0.63508,3.19827,3.25178,0.778291,1.28901,1.18572,1.21674,1.48707,0.20439,0.127491,2.70604,3.09741,1.14486,1.68527,0.38197,0.0627786,0.381863,2.5112,3.71608,1.92274,0.573895,3.72506,2.36487,0.619444,3.19856,3.11657,0.189383,0.180706,3.70595,0.91928,1.20836,1.04514,2.23861,0.245261,0.174829,0.00154734,1.3084,5.75153,1.18626,2.96441,1.93846,0.390841,0.375997,1.0069,0.364289,2.65606,0.102186,0.147101,0.00865398,2.66011,5.33546,1.41967,0.488059,0.293369,1.84573,2.50777,3.20392,0,0.0109018,1.09469,1.82154,2.8126,2.0021,1.60885,0.630014,0,2.09139,1.30937,3.7491,0.664126,0.123373,0.11209,1.19551,1.35644,0.606804,0.187993,1.10878,0.810724,0.0113032,1.7821,1.91432,0.149821,0.293472,0.222547,0.215981,0.556262,0.414154,2.70669,3.63663,2.35668,2.95834,0.267524,1.63889,0.187753,0.12711,3.3947,1.63793,1.35076,2.79242,0.86852,0,0.567694,0.799586,1.22088,2.3433,1.85411,0.88546,0.820162,0.647504,0.123085,0.758855,0.639487,0.812279,0.49055,0.598673,0.366934,0.603286,1.46246,0.445609,0.898841,0.586868,8.73982,1.02971,1.20077,1.94829,1.83092,0.0220182,0.261601,3.15512,3.30329,0.551034,2.94538,1.72373,1.97993,0.779542,0.0822677,2.9852,3.75193,0.144997,0.836988,1.69875,0.504745,2.41732,1.30603,0.608154,1.05779,1.73576,0.36456,1.38593,2.02989,2.08777,0.164489,1.48814,1.37225,0.809428,0.972248,2.35817,0.251876,0.0687024,0.744842,3.6117,3.38709,1.54124,2.41988,2.61364,0.361874,6.00888,1.67688,4.14727,0.158881,2.55793,0.111752,2.84868,1.40215,0.489402,0.271234,0.513271,0.497959,0.11324,3.00346,1.56173,0.205529,0.497307,2.00518,0.614836,0.908684,1.41282,3.29453,0.969603,0.547435,2.81178,2.94911,4.6194,1.27235,0.716387,3.19781,1.2433,0.992348,3.85399,0.407022,1.57019,2.38822,0.998639,0.221525,2.03134,1.33124,0.617507,1.30361,0.708646,0.759568,2.45856,3.54255,0.812994,0.204393,0.436802,0.0270976,1.53677,1.46702,3.65913,0.466971,0.251133,6.95959,0.0333794,1.40907,2.13019,1.31999,0.292266,0.181486,3.14676,2.28961,0.490431,1.35744,1.39209,0.084761,0.816483,0.0671694,2.58854,3.1498,0.61749,1.58415,1.30812,0.149177,1.0595,2.15829,0.968012,2.30158,1.45327,1.81182,2.29565,0.689341,2.11324,0.112973,1.12802,0.219814,0.877145,0.537364,2.30229,0.153106,0.00261487,0.789265,2.10999,1.36101,1.68151,1.41636,1.05213,0.0044397,4.59925,0.991876,3.37134,0.220099,3.13481,1.31909,2.52578,1.33094,0.627478,1.09398,0.353905,0.799904,1.02119,2.29109,3.5297,0.463236,0.436492,1.10343,2.8353,0.123138,0.790711,1.18252,0.823587,1.04403,1.73025,2.1707,3.33867,1.07313,1.18701,3.20581,1.10758,0.958429,3.31983,0.0549896,1.10189,2.74074,2.45557,1.44002,2.93435,0.970824,0.895067,0.71077,0.124661,0.765065,1.71309,3.49349,0.65441,0.672074,0.354003,0.0919241,0.506844,1.93992,3.61502,0.446271,0.3745,4.5924,1.23532,0.241716,1.43527,1.75488,0.539591,1.15005,2.69045,1.74025,0.110312,1.61856,1.47826,0.826395,0.502828,0.422565,3.59079,3.1942,1.72556,0.845413,0.360287,0.274238,0.797396,1.93063,1.06382,1.0984,1.65286,0.27872,0.481458,2.84992,2.61509,0.0257836,0.255934,1.07027,0.218782,3.23025,3.41362,0.0452738,0.0335229,0.955382,1.47124,4.39593,1.03073,2.08899,0.991484,0,2.82603,0.44292,2.68303,1.31588,0.534263,0.0979332,1.1765,1.88004,1.33126,0.51734,1.27743,0.82657,0.43469,1.03023,1.61289,0.637126,0.246193,0.66784,0.261125,0.202319,2.98351,1.77656,1.7038,0.869062,2.82035,0.667508,1.84661,0.584493,0.0369572,2.16519,1.93956,0.818924,2.80943,0.160816,0.135175,0.51689,0.825145,1.074,3.13664,1.27932,0.117167,0.531846,0.91422,0.00876264,0.795256,2.56697,1.44093,1.00984,0.455143,0.0774129,0.100801,1.35025,1.91794,0.257685,0.174504,6.99856,0.132885,2.00133,1.88682,1.80335,0.785178,1.18023,3.76684,3.1939,0.565615,2.31022,2.75415,1.73442,0.41269,0.291212,3.30776,3.32774,0.126696,0.606345,1.4245,0.384673,1.15666,1.35154,0.979975,0.766484,2.00923,2.16857,2.27555,0.398939,1.1732,0.138618,1.23992,0.447373,0.616605,1.00978,3.01147,0.0924396,0.0674492,1.66365,4.12297,1.79219,1.82543,1.29482,2.48605,0.0176754,5.19097,0.986877,2.57266,1.57903,3.02494,0.390604,4.12094,1.05613,0.470683,1.23008,0.247726,1.68997,1.28396,1.19812,3.29264,0.0823708,0.131314,1.13449,1.04483,0.462526,1.5643,2.42756,0.356194,0.295758,2.51568,4.19365,3.52803,1.47537,0.420603,3.22312,2.51929,3.17259,3.80346,1.02889,0.487563,3.3617,1.2777,0.312199,1.22358,0.259978,0.480599,1.02987,0.188222,0.151817,1.71614,3.593,2.10357,0.849569,0.942012,0.042104,0.509642,1.40164,3.83614,0.184156,0.221195,5.11221,1.0615,1.43651,1.90652,1.32315,0.0532484,0.439429,1.16011,1.90052,0.532259,1.15372,1.07451,0.656496,0.354654,0.384508,4.31573,2.85546,0.325853,0.566785,2.11056,1.70249,2.32448,0.330881,0.140268,1.50035,0.0365847,0.707865,1.58583,0.280018,3.56375,0.158765,2.68258,0.408648,1.22888,1.0269,1.02123,0.407549,0.233926,1.51785,3.37066,2.94239,1.27334,1.87556,3.5836,0.133146,5.41036,1.91804,3.84347,0.619772,2.98138,0.331498,2.28954,1.83127,0.710477,0.604542,1.88388,1.43014,0.224193,0.667574,3.3583,0.680597,0.304905,1.52853,0.296768,0.588059,1.01646,3.68636,2.01778,0.569689,0.464187,2.63521,5.09287,0.647101,0.151575,1.0164,0.403723,0.650813,1.84017,0.0552277,2.31462,2.00682,1.79648,0.236092,1.40605,2.18151,0.082038,0.504618,0.31107,0.398998,1.36783,3.16728,1.43395,0.365569,1.51801,0.0374257,0.849416,0.351533,2.94804,0.974565,0.861189,5.25728,1.02628,0.873751,2.13586,2.61117,0.703581,0.0308287,2.81073,1.29858,1.12471,0.25401,1.83754,1.22727,1.3554,0.0133957,0.857975,3.37658,0.100097,0.995698,0.856568,0.0534877,1.25937,0.299307,0.51188,1.93843,0.844398,0.459178,0.082004,1.25152,1.87234,0.852074,1.10209,0.0674482,1.17181,0.702495,1.02526,0.0325532,0.026385,2.39219,0.415488,1.07483,0.60647,1.98251,0.304612,0.022602,2.08917,1.07849,1.73861,0.589407,0.532113,0.741026,2.9143,1.63904,0.408422,0.097312,0.769933,0.521111,0.45975,0.5108,0.961989,0.21067,0.00781904,0.431027,1.33048,1.45045,0.258897,0.41484,2.11862,1.19518,1.78641,0.192141,0.124088,0.00156289,0.125198,1.91346,2.21974,1.95588,3.44095,0.166916,0.00183741,1.91274,1.71632,2.48322,0.294634,0.865751,2.8646,0.642169,1.80255,0.098533,0.583954,0.872587,1.32031,0.517327,0.975735,0.0171696,0.60458,0.940533,1.24504,0.663863,0.111222,3.36156,0.589114,0.958669,1.90041,2.92993,0.261462,0.312009,2.1037,2.22552,0.867818,2.59185,1.03615,1.82114,0.324595,0.457084,2.42399,4.60721,0.708082,1.73009,4.23178,0.44775,2.33453,0.453969,0.344531,2.91467,0.121897,0.114587,0.0283162,2.26917,3.26856,0.847829,0.408003,1.85722,0.238222,2.36447,2.69791,0.0400725,0.0149142,2.89335,0.929881,4.48752,0.794999,2.79169,2.13796,0.270315,2.84102,0.44938,1.6643,0.504644,0.379103,0.457211,2.70984,4.60011,0.656926,0.192714,0.40539,1.16158,0.0783707,2.54728,3.43145,1.0045,0.1937,1.56081,1.80243,0.372579,1.26293,1.02895,2.1259,2.32369,2.18291,0.625517,1.5817,0.675317,0.897016,3.24683,1.99096,2.37221,3.13066,0.00317319,0.286411,2.29131,1.21139,1.66276,0.589232,1.81656,0.144804,2.01761,2.55133,0.0883723,0.728125,0.771948,0.999863,1.90504,1.11528,0.352635,1.09363,2.05708,1.93355,0.253309,0.116556,4.68634,0.223883,1.67129,1.93171,1.97933,0.00383753,0.280216,1.40586,3.09517,0.213545,1.66106,2.13006,2.98515,0.271883,0.0522383,3.09997,3.51699,0.141877,0.651085,2.91886,0.343161,2.02151,1.13365,1.30164,2.875,1.17437,1.1995,0.855122,1.28774,2.12559,0.458248,2.22088,1.93161,0.952154,2.20498,3.26444,0.250488,0.00407381,1.85257,2.29864,2.46343,0.561894,1.62252,2.49955,0.309508,2.61178,1.33307,3.72915,0.643366,3.51015,0.562023,3.57493,3.10522,0.328935,0.119799,0.897191,1.89314,0.119459,2.02567,2.35492,1.34613,0.383373,2.32709,1.71177,0.134538,0.375472,0.889519,1.85584,1.17967,1.46707,1.70745,2.37053,0.720302,0.472549,2.28538,2.25924,3.50832,5.28705,0.84084,0.875018,3.52472,1.67576,0.387816,0.449753,2.36395,0.0993472,1.84713,1.42916,0.0308633,2.50824,1.53989,1.30368,0.730894,0.759442,0.111097,1.70707,1.80198,2.37097,0.812136,0.120098,4.86855,1.24059,2.86626,2.44454,0.765356,0.204651,1.52859,1.3326,0.527706,0.962032,0.919145,2.30383,1.64707,0.193598,0.293331,2.66928,2.69068,0.708493,0.227696,1.3636,1.16759,0.13128,2.24242,0.647674,1.27019,0.198226,0.660604,0.416808,1.01276,3.12247,0.387053,1.73044,2.10827,0.6178,0.565228,1.56219,0.447435,0.4185,0.961366,2.13676,2.59692,2.04515,0.937313,0.196836,0.00168212,2.53829,0.734472,2.94557,0.865219,3.53086,4.39869,3.41571,3.60505,0.38702,0.440112,2.58771,2.84441,0.811425,2.02781,3.80813,0.565964,0.126715,0.61499,2.82767,0.0637746,0.335573,0.393531,0.976523,0.75281,0.915117,1.70388,1.42754,0.468946,0.0694357,1.1903,2.43293,2.69881,2.03315,0.0570462,1.02948,6.27247,3.65036,0.71304,1.27246,0.271345,0.530685,1.8314,0.463416,0.141136,2.70229,2.4328,1.15523,0.550393,0.0891806,0.0283122,0.325709,2.4861,2.15164,0.898232,0.210259,3.1975,0.997738,0.191175,1.55085,1.71338,0.0577143,2.03634,0.873271,1.88994,0.713991,0.665725,0.109941,1.71841,0.186993,0.268377,3.74585,2.38165,0.328973,0.538873,2.25382,1.36053,4.35583,0.320201,0.641422,1.44763,0.416863,1.08291,0.267269,0.679639,1.50867,0.542548,0.490702,1.41823,0.432038,1.9593,1.10792,0.105017,0.0269861,4.09409,0.96845,6.05413,0.186122,3.53427,1.90091,0.421887,1.95093,0.348475,1.01732,2.2948,1.17767,0.642385,1.31366,2.82119,1.42115,0.539733,2.13342,2.80341,1.1287,0.10518,1.32477,1.47408,0.525935,0.423997,1.4702,0.819589,0.996782,0.507418,0.792482,0.117183,1.16909,0.60062,0.798,1.19358,0.211437,2.14669,0.834304,2.43454,2.85052,0.211237,0.51729,2.33352,0.254928,0.25645,0.38028,1.65891,0.0968816,1.5055,1.407,0.0182651,1.11253,0.616646,1.46619,1.64317,0.659965,0.00739392,0.163936,1.33386,1.29004,2.11317,1.03964,3.65167,1.30471,2.29612,1.38089,2.01531,0.0406192,0.331822,1.60388,1.93104,0.812058,1.03308,1.75459,0.86162,0.152441,0.2124,1.42098,2.72777,0.214831,0.677203,2.25576,0.731401,2.00064,1.31908,1.27204,1.79025,0.149946,0.0791397,0.266467,1.14631,3.07066,1.06992,0.606958,0.239376,0.956309,2.76435,1.11536,0.000791357,0.0550667,2.86583,1.17624,6.24452,1.18306,1.69543,1.13793,0.320872,3.57882,1.89732,0.935178,0.783058,1.5818,1.3382,2.74709,1.92389,2.03014,0.229265,1.39986,1.77446,0.403916,1.45285,2.01894,0.317412,0.105496,1.13236,0.569124,0.535925,1.0535,1.46998,1.68174,0.253648,0.816185,1.05546,1.41445,1.19722,0.0818693,2.06625,0.997922,0.737934,2.35428,0.0058167,0.145901,1.48014,1.18936,1.02932,1.85306,1.51759,0.428515,1.52003,0.803402,0,1.38276,1.61682,1.30886,1.18338,0.586477,0.0902724,0.831467,3.56661,1.31524,0.876652,0.0631791,4.7749,1.07338,1.69869,1.58828,0.25187,0,0.947262,1.35063,1.7544,0.335397,0.602412,1.81719,0.83498,0.0757302,0.0206605,1.9908,2.08454,0.59629,0.108872,0.0560465,0.0712629,0.71132,1.64435,0.262862,1.37295,0.0695136,0.742151,0.55555,1.31163,4.53364,0.144293,1.48394,0.706466,0.220706,0.92508,2.26944,0.495825,0.0551027,0.185759,1.27448,4.2655,2.43653,0.900594,0.135071,0.0394816,4.60703,0.308671,2.70279,1.00552,2.895,2.59958,2.97339,3.17499,1.61685,0.646072,0.582307,3.176,1.11632,0.462667,2.5227,0.720804,0.0654785,1.10443,3.22618,0.166501,0.0845012,0.269973,0.851177,0.141087,1.16259,0.60383,1.48998,0.0792487,0.130158,0.53897,0.779274,1.28965,1.09309,0.00114253,0.760716,3.85795,0.544777,0.670001,1.39091,1.41372,0.537886,1.09387,1.16429,0.00110852,1.36474,1.96186,1.17676,1.34232,0.392489,0.0028248,0.820288,3.33081,1.9707,0.141534,1.32752,4.32412,1.36519,0.7822,1.24212,1.29921,0.00449213,2.96646,0.890678,0.947418,0.524281,1.01502,1.19485,2.00927,0.140004,0.0336756,3.82305,2.6648,0.808076,0.175849,0.379751,1.27281,1.64596,0.85119,0.304467,1.05965,0.88702,0.673684,0.412961,1.16726,5.23773,0.181942,1.30786,2.51306,0.0140293,1.52968,1.47284,1.31953,0.115669,1.00374,2.17982,7.58773,1.60782,2.21347,1.31729,0.489387,2.94165,0.132097,1.60515,0.845589,1.8837,2.59501,1.74942,3.93922,2.10887,0.885373,1.38633,3.0048,0.850073,0.156651,2.41793,1.78144,0.102717,1.45141,2.25859,0.553741,0.352278,1.39596,0.605364,0.722566,2.26311,0.989234,3.1654,1.32028,0.131032,0.430185,0.63604,1.89814,0.840887,0.0145833,1.12403,3.63153,0.461552,0.338114,1.41944,1.2121,0.0547536,1.15151,2.13569,0.183726,1.35507,2.37558,0.880525,2.10045,0.313574,0.158706,0.819517,2.97787,1.5779,0.617671,0.468382,4.2573,2.68184,0.405576,1.66272,2.19618,0.454411,0.112507,2.0079,0.605678,0.0785262,0.808339,0.763635,0.514089,0.251851,0.199175,2.23691,3.86687,1.66287,1.84195,1.37314,0.902038,0.777063,1.49601,0.37944,1.16528,0.189918,0.0748914,0.0220248,2.78947,5.58867,1.3001,0.0293565,1.03021,0.822084,2.649,3.1179,0.0243542,0.0170031,2.31794,2.32811,4.56341,1.56396,2.14732,0.507619,0.0942021,1.91325,0.484128,3.22408,2.23196,0.548829,0.251454,0.504564,1.64646,1.11299,0.448078,0.00928187,1.99006,0.222099,0.00566369,2.7882,0.721351,0.716473,1.48018,0.348745,0.0468219,1.15817,1.77556,1.69238,1.53411,2.84495,0.437648,1.22891,0.149386,0.50927,1.36986,0.939152,4.01539,1.81847,0.116702,0.0480713,0.592695,0.635974,0.522637,1.95381,1.00631,0.00983327,2.25001,0.568138,0,0.0477794,0.946258,1.89782,2.10002,0.680816,0.118034,0.289923,0.534844,1.14276,0.17787,0.0671842,3.74189,0.598143,1.46102,1.59229,0.573945,0.0248033,0.18219,2.10823,0.628571,0.277635,0.55642,1.37147,0.189093,0.304289,0.021872,0.766755,3.72452,0.849935,0.742728,0.336274,0.727936,0.875328,1.5026,0.148357,1.73686,0.0130925,1.14383,0.142981,0.537176,3.29533,0.0895149,2.22682,0.590893,0.123592,0.956116,1.90591,0.335271,0.144899,0.965926,0.810877,3.06673,2.55084,0.898227,0.188399,0.218642,4.21891,0.949401,2.19536,2.56585,2.28743,1.50976,4.43636,2.14383,1.21491,0.238991,0.119743,3.13956,0.7063,0.698713,1.79337,0.713203,0.239497,1.12453,2.11674,0.215012,0.148944,0.371673,0.688459,0.584677,0.668979,1.1467,1.24969,0.365664,0.00402623,0.913581,1.31169,0.679478,3.68138,0.128535,0.452051,2.87009,0.942447,0.473775,0.990575,0.495187,0.543496,1.7609,0.644544,0.352128,2.7088,1.69498,2.96797,0.55632,0.00166314,0.037931,0.867557,2.67301,1.17689,0.590685,0.0582472,4.19662,0.179836,1.69836,1.31358,1.49582,0.406852,1.29723,1.39667,0.477145,0.447816,1.19232,2.0995,1.4763,0.0372801,0.558062,2.76918,3.73471,0.308949,1.57176,0.553801,0.365138,1.15407,2.44019,0.297839,0.90976,0.615724,0.813148,0.463448,0.654772,2.71199,0.237753,0.909471,1.26044,0.74914,1.39616,2.38322,0.167203,0.0126121,0.225377,1.5502,2.26877,2.24333,0.973906,0.709005,0.0451798,3.05034,0.189501,2.54107,1.02928,2.93998,1.85342,3.96938,2.19005,0.321954,0.162484,0.813151,2.76814,1.26534,2.03445,3.33698,0.651145,0.120352,1.04319,2.61028,0.264721,0.305187,0.897256,1.02103,0.58373,1.0037,1.71082,1.52767,0.668524,0.0208841,1.75831,1.83232,3.35193,4.0909,0.294792,0.394428,3.74717,3.01426,0.839781,1.40738,0.364597,0.526923,1.33506,0.342368,0.0127373,2.4178,2.62937,2.66246,0.298979,0.372313,0.149647,0.286726,2.90082,1.18625,0.520868,0.100791,3.63484,0.177705,1.7658,2.12389,0.915643,0.0530187,0.868138,1.60942,0.807279,0.734285,1.02882,2.66665,1.09878,0.207759,0.0152958,1.39496,2.98445,0.345051,1.99615,1.6004,0.905284,0.662348,2.13196,0.395938,2.09453,0.133119,2.15142,0.367175,0.262221,2.86103,0.118603,1.45757,0.690362,0.78626,2.20591,2.73903,0.103179,0.00699327,0.469433,1.13614,1.86629,2.94935,0.47088,0.0390566,0.00386581,4.85439,0.915615,1.80378,1.58185,2.70266,1.99958,3.65516,2.39768,0.321273,0.287234,0.443654,3.43264,0.878042,1.2525,4.4268,0.624393,0.499974,0.693653,3.4787,0.0673157,0.22458,1.16722,2.52149,0.631295,0.267162,1.50803,0.86045,0.36575,0.0676028,1.27223,0.49721,1.87069,4.34851,0.0029013,0.405826,3.7857,3.4135,0.674171,1.74218,0.42268,0.708406,1.79269,0.464328,0.139114,2.36464,1.41014,2.57096,0.355134,0.604991,0.391504,0.319167,2.55726,0.739781,1.02642,0.205901,3.9757,1.18295,1.24104,1.63107,1.51492,0.0216242,0.0775006,2.32664,2.68503,0.487947,1.85133,1.7975,1.55172,0.089643,0.0868945,2.00716,3.74157,0.401673,1.4517,2.23622,0.503565,0.864392,0.640068,0.360182,2.73337,0.0533497,0.796302,0.0789252,0.493384,3.57012,0.750832,0.837013,1.6331,0.361186,0.937966,3.20455,0.354754,0,1.34457,2.48876,3.36057,1.98883,0.920533,0.856483,0.0131143,3.56402,0.398756,3.64401,0.859795,2.60172,0.820675,3.51612,3.01152,0.256439,0.0793038,0.217291,2.4773,0.0733517,1.59288,2.78761,0.257391,0.0951554,0.812235,1.26376,0.108545,0.287873,1.1668,1.39608,1.13534,1.01982,1.83664,2.25875,0.785398,0.145803,0.978086,1.04681,3.51331,4.46406,0.338991,0.440636,2.46854,0.93062,0.387732,0.645399,2.80796,0.110147,2.04517,2.49583,0.257262,0.930653,0.876773,1.16229,1.00568,0.579815,0.02377,0.9081,1.58965,1.33536,0.390522,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\ndata = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\nlengths = torch.tensor([74])\r\noutput = ts_model_multihead.forward(data=data, lengths=lengths)\r\nprint(output)\r\n```\r\n## Expected behavior\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-66-6d725915459d> in <module>\r\n     53 data = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\n     54 lengths = torch.tensor([74])\r\n---> 55 output = ts_model_multihead.forward(data=data, lengths=lengths)\r\n     56 print(output)\r\n\r\nRuntimeError: \r\n\r\n\r\nnew_type INTERNAL ASSERT FAILED at caffe2/torch/csrc/jit/passes/shape_analysis.cpp:280, please report a bug to PyTorch. \r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3312:12\r\n                                                  dtype=attn_mask.dtype,\r\n                                                  device=attn_mask.device)], dim=1)\r\n            if key_padding_mask is not None:\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n                key_padding_mask = torch.cat(\r\n                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),\r\n\r\nThe above operation failed shape propagation in this context:\r\nat /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3303:4\r\n    q = q * scaling\r\n\r\n    if bias_k is not None and bias_v is not None:\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n        if static_k is None and static_v is None:\r\n            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\r\n\r\nThe above operation failed shape propagation in this context:\r\nat <ipython-input-60-415a6b53c56b>:127:8\r\n          L is the target sequence length, S is the source sequence length.\r\n        """"""\r\n        if not self._qkv_same_embed_dim:\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n            return nn.functional.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n```\r\n\r\n## Environment\r\n\r\nbento classyvision env\r\n\r\n\r\ncc @suo","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ndef _lengths2mask(lengths: torch.Tensor, seq_len: int, lt=torch.tensor(True)) -> torch.Tensor:\r\n    """"""\r\n    Input lengths is a tensor of shape (batch_size) with value in [0, seq_len],\r\n    Return a tensor of shape (batch_size x seq_len) with binary value\r\n    """"""\r\n    if lt:\r\n        return torch.lt(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n    else:\r\n        return torch.ge(\r\n            torch.arange(seq_len, device=lengths.device)[None, :],\r\n            lengths[:, None].long(),\r\n        )\r\n\r\nclass AttentionMultihead(nn.Module):\r\n    def __init__(self, dim_in, method, use_variable_lengths=False, num_heads=1):\r\n        super(AttentionMultihead, self).__init__()\r\n\r\n        # choose from supported attention methods\r\n        assert method in (""multihead"")\r\n        self.method = method\r\n        self.use_variable_length = use_variable_lengths\r\n        \r\n        self.attention = nn.MultiheadAttention(\r\n            embed_dim=dim_in, num_heads=num_heads\r\n        )\r\n\r\n        # record output dim\r\n        self.dim_out = dim_in\r\n\r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor):\r\n        assert data.dim() == 3, ""Require input shape (batch_size x seq_len x embed_dim)""\r\n        \r\n        if self.use_variable_length is True:\r\n            mask = _lengths2mask(lengths.clamp(min=1), data.size(1), torch.tensor(False))\r\n        else:\r\n            mask = None\r\n\r\n        data = data.transpose(0, 1)\r\n        attn_output, attn_weights = self.attention(\r\n            data, data, data, key_padding_mask=mask, need_weights=True, attn_mask=None\r\n        )\r\n        # transpose output data to (batch_size, seq_length, embed_dim)\r\n        attn_output = attn_output.transpose(0, 1)\r\n        \r\n        return attn_output, lengths, attn_weights\r\n\r\n    \r\nclass DeployAttentionMultihead(torch.nn.Module):\r\n    def __init__(self, dim_in=100, length=torch.tensor([300])[0]):\r\n        super(DeployAttentionMultihead, self).__init__()\r\n        self.length = length\r\n        self.use_variable_length = True\r\n        self.attention = AttentionMultihead(dim_in=dim_in, method=""multihead"", use_variable_lengths=True, num_heads=5)\r\n        \r\n    def forward(self, data: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\r\n        attn_output, lengths, attn_weights = self.attention(data, lengths)\r\n        return attn_output\r\n    \r\nmodel = DeployAttentionMultihead()\r\n\r\nts_model_multihead = torch.jit.script(model)\r\nprint(ts_model_multihead.code)\r\n\r\n# run ts model of multihead\r\n\r\nvue_clip_emb = [0.0840825,5.65534,2.23576,2.21367,1.5883,0.493749,0,1.49034,1.54749,1.15795,0.118591,2.54162,2.70971,2.10865,0.115242,0.0115912,5.39253,2.83803,1.96323,0.935772,1.33328,1.16219,0.15678,1.33033,0,2.80253,0.258661,1.19186,0.429935,1.14404,4.58615,0.11558,2.31151,3.45441,0.117317,2.62973,2.57802,3.12865,0.632287,0.280647,1.08333,4.14262,3.5503,0.67525,0.889924,0.128302,3.943,0.825888,3.11752,2.46894,4.05448,3.0315,5.9825,2.54857,0.390271,0.105513,1.76064,3.53628,0.496927,0.850136,3.52684,1.92267,1.50376,3.43571,2.36993,0.0011008,0.101535,0.324544,0.875401,1.6104,0.497361,1.72176,4.01326,1.31662,0.262011,0.837993,0.622429,2.66528,1.13911,1.21707,1.56516,6.33717,1.5675,0.136495,0.355789,1.48718,0.108055,3.47766,1.0657,0.00356666,1.77421,3.86285,1.42049,0.819946,0.340235,0.109702,0.867129,2.71539,0.206769,2.02203,0.0292637,5.49686,1.74569,2.19925,1.51534,0.46074,0.0161071,1.24292,1.44666,0.562175,0.104977,2.05992,2.41354,2.51859,0.165397,0.011803,4.64355,4.19187,2.1993,1.15172,1.72702,1.62523,0.343265,1.45464,0,2.05193,0.619562,0.983955,0.432762,1.09673,4.58744,0.209633,2.02799,3.54795,0.156958,2.19384,2.25872,2.84524,0.120264,0.432246,0.800701,3.83577,3.56383,0.501572,0.458101,0.0500416,3.77406,0.618967,2.51559,2.57312,3.63771,4.04265,5.3371,2.427,0.555011,0.393777,1.45245,4.48355,0.642099,1.48505,3.90371,2.12527,1.10275,3.13394,2.88107,0,0.0938793,0.432923,1.40834,1.47035,0.199206,0.899871,4.19796,1.37373,0.108242,0.541182,0.60768,2.56032,1.05987,1.05486,2.31946,6.57515,1.71218,0.191091,0.523715,0.562181,0.163343,3.41128,1.90795,0.103194,2.18321,4.01785,1.40121,0.712813,0.40542,0.0130574,0.482167,3.61689,0.541964,1.54621,0.00721372,3.53875,1.99706,2.16941,2.30858,0.139537,0.0292894,0.59817,1.67502,1.06869,0.15484,1.32557,2.84414,1.81534,0.0359029,0.155307,4.85468,3.37413,1.90302,1.30877,0.734729,1.76238,0.150899,0.547858,0.0230488,2.15477,0.665912,2.1411,0.486219,0.981247,4.02336,0.340909,2.64124,1.94704,0.127592,2.22661,3.51697,2.28332,0.23371,0.0208469,0.71869,3.26233,3.37803,0.440662,0.510871,0.00753081,5.16184,0.492385,2.8764,2.53306,2.73119,3.92737,4.78175,3.25056,0.738502,0.171556,1.14462,4.28251,0.983006,1.13122,3.53485,1.87881,0.758899,3.47704,3.11202,0.00754782,0.223614,0.554179,2.12652,1.15958,0.225096,1.4953,3.90192,1.51579,0.0205165,1.09822,0.339162,3.14576,2.58463,0.766622,2.68842,5.81371,1.30858,0.0567763,0.362448,0.427235,0.103554,1.55612,1.40707,0.327143,2.06921,3.1817,1.11314,0.293359,0.491569,0.0237787,0.335975,1.49355,0.080589,0.798883,0.0042814,4.00296,1.46194,1.97508,2.93223,0.369621,0.114972,1.43744,0.953024,1.51951,0.0664168,1.36838,1.12008,2.37583,0.0656217,0.0550487,5.11532,3.41184,2.20438,1.50039,1.4365,1.97503,0.0544394,1.24929,0.00753079,1.44055,0.312355,1.65135,0.778141,0.717496,3.53737,0.872259,1.93065,2.75865,0.0802059,2.52429,2.25681,2.45514,0.0332121,0.272185,1.78893,4.1729,3.72535,0.296955,0.977887,0.0917793,5.16411,0.227299,2.63355,2.93526,3.32066,3.49133,3.54922,3.74034,0.651864,0.379803,1.49085,4.07919,0.729488,0.412124,3.82536,0.844314,0.661833,2.39382,2.20183,0.0319286,0.20516,0.6255,1.46891,1.14456,0.44578,1.54798,4.63112,1.32129,0,0.868121,0.800939,4.08268,1.32296,0.520707,3.65064,6.09133,0.723082,0.158022,0.261037,0.397029,0.0556116,2.45283,1.20808,0.051552,1.35081,3.32664,0.930835,0.830775,0.637906,0.00174865,0.0430316,1.45825,0.147698,1.924,0.00532853,3.77304,1.63318,1.94748,2.129,0.0999379,0.0187992,0.764959,1.12453,0.710663,0.221404,1.04146,2.84609,2.1028,0.0512187,0.0633501,4.60868,4.72056,1.81119,1.71785,1.46575,1.1067,0.22531,1.22568,0.0114571,1.80884,0.322822,1.30182,0.343267,0.780105,4.02635,0.468035,2.11109,2.30884,0.225344,1.73228,3.32626,2.14921,0.0652682,0.179686,0.794495,4.04693,3.94471,0.479574,0.372132,0.0392945,5.12383,0.643861,3.05057,2.2246,3.45714,4.82734,4.22932,3.33674,0.602254,0.160737,1.20358,5.0604,0.894768,1.28898,4.45525,1.44087,0.816369,1.96161,3.89633,0.0192901,0.130743,0.526546,0.911684,0.790611,0.290481,0.535833,3.96858,0.660851,0.183099,1.0221,0.527058,2.76179,2.45929,0.418104,1.77804,6.29833,1.67065,0.412998,0.493972,0.239688,0.143934,2.45849,2.14727,0.0778733,2.5178,3.35156,1.2997,0.506005,0.775292,0.0188194,0.447015,2.64902,0.5688,1.19909,0.0417836,3.4061,2.37674,1.87274,1.59598,0.440209,0.0390666,0.459501,1.47975,2.99328,0.293728,3.44896,2.98324,0.377933,0.136621,0.0858318,3.94416,2.30151,1.82907,2.10093,0.403424,1.47967,0.348227,1.58975,0.0860919,1.47215,0.111787,3.21831,2.28628,1.27021,2.6389,0.099854,2.80626,1.31564,0.256794,2.7791,3.04503,3.35439,0.637234,0.873176,1.78056,1.96819,3.70617,0.573749,0.86824,0.0227969,4.80532,1.19482,4.30919,3.18403,2.90808,3.55253,5.32479,2.20803,0.292511,0.917928,0.917466,3.6687,1.65847,1.24795,2.66524,0.583895,1.36363,3.3847,3.8937,0.104704,0.242911,1.92325,1.26514,1.42388,0.702984,2.26055,4.22127,0.507518,0.104398,1.00863,1.40142,2.62522,3.89506,0.304073,1.87341,4.88558,0.57671,0.101682,0.376967,0.151231,0.302362,0.964678,0.314604,0.357943,2.24616,3.35078,0.934013,0.87141,0.291341,0.00906326,0.116968,0.5981,0.0642974,0.59632,0.070716,3.52079,2.46395,2.44951,2.9654,0.663779,0.298566,0.532495,0.605621,1.02062,1.23134,2.30953,2.70189,0.311687,0.250861,0.157656,2.54278,1.19941,1.6287,1.38798,0.378133,1.65116,0.462127,2.0553,0.183922,1.911,0.26869,2.17268,1.65513,1.10857,1.4878,0.285458,3.24843,1.04712,1.4921,2.49815,3.25619,2.89996,0.615719,1.52029,1.29669,0.594365,3.33981,0.7246,0.386914,0.18093,3.73587,1.15124,4.20692,2.63089,4.41739,4.75388,4.79367,1.61047,0.177007,1.5846,2.15091,2.80859,2.03466,1.22181,1.88013,0.966551,1.62572,2.95286,3.22257,0.171248,0.0488555,1.50401,0.564802,1.11984,1.97094,1.09832,4.79174,1.00213,0.0561535,1.06735,2.31487,3.13783,3.00876,0.280711,1.51538,4.03358,0.577055,0.256786,0.520065,0.197432,1.37471,1.67807,0.111243,1.28862,3.27064,3.1356,0.810865,0.226676,0.090544,0.923369,0.273451,0.463472,0.740315,0.466818,0.0792522,3.41677,1.65213,1.87478,2.9709,0.649614,0.287107,0.742373,2.44696,1.98299,0.362439,3.38517,3.0017,1.15386,0.123824,0.71546,3.2347,2.01491,1.11258,2.1084,1.17668,1.4841,0.585114,2.00054,0.0496953,1.25047,0.316964,2.6968,1.75643,0.580133,1.54092,0.0531415,2.72435,1.84557,0.622192,2.4177,3.2615,2.47776,0.355572,1.18969,2.80585,1.48045,2.73792,1.27207,1.31757,0.00647418,5.56059,0.929273,2.86031,2.55737,3.45009,1.33754,4.2221,2.44413,0.265876,1.38801,0.652557,2.74723,2.27915,1.15483,2.48536,0.862942,1.00924,2.87263,2.60592,0.341337,0.789075,2.61353,1.51038,1.42327,0.692666,1.54161,4.74268,2.04178,0.52153,1.57625,1.82753,2.29875,2.95661,0.794998,2.05961,4.72058,1.10073,0.154321,0.499115,0.0518899,0.51693,0.409915,0.405275,0.53946,2.23644,2.72192,0.94019,1.03312,0.339981,0.0105235,0.0286632,0.641563,0.176992,1.62913,0.173662,2.71067,1.12456,0.980614,1.26853,0.502585,0.0405315,0.0835886,0.801681,2.17712,0.600408,2.7253,3.26336,1.98236,0.19047,0.160637,4.27236,4.15545,0.692021,2.13214,1.37671,1.35437,1.51932,0.632685,0,2.57451,0.0496026,3.45484,0.800954,1.08736,2.65716,0.288336,3.76113,0.877107,0.343407,1.93358,3.87112,3.22765,0.771835,0.544815,2.16071,3.16342,1.80419,0.929744,2.37628,0.0166744,5.67011,0.584775,4.03636,2.40968,1.96826,2.96728,4.39782,2.26301,1.22459,0.56253,0.861167,3.46662,0.947368,1.0488,3.56452,1.12366,1.08122,3.01743,2.8018,0.545803,0.861587,0.610711,1.44031,0.526162,0.620491,1.9175,3.08667,0.663501,0.35154,1.79684,0.938363,2.92849,2.60115,0.880397,2.2393,5.30998,0.792989,0.532533,0.38656,0.475731,0.0857692,0.487966,1.6554,0.17406,3.03141,2.42888,2.00751,0.879513,0.731286,0.102841,0.637661,0.50266,0,1.66406,0.0403251,2.60657,1.94436,0.987926,2.60567,0.490878,0.00202899,0.331029,2.03788,2.20193,1.82084,3.72021,3.44797,0.278736,0.0147163,0.169737,3.7098,2.93405,1.24039,2.8275,0.505545,1.41782,0.345053,1.34328,0.0294194,2.0961,0.0848584,3.29406,1.32525,2.41475,3.38123,0.0721113,2.92625,0.847645,0.420569,1.33231,3.7629,2.30329,0.388465,1.12179,2.35953,1.66306,2.70488,0.503543,0.822229,0,4.34317,0.605498,5.77145,1.83385,1.98237,3.45013,5.29985,2.02454,0.0475114,0.395792,2.11374,3.54273,0.528591,1.56819,3.8756,0.558113,0.941327,2.41993,3.57062,0.187795,0.31701,1.55603,0.604092,0.648071,0.516138,1.93505,3.75648,0.439664,0.0350492,1.06238,2.00205,3.3981,4.43062,0.284624,1.01425,4.71788,0.761708,0.0611976,1.14644,0.125763,0.42916,0.839016,0.331764,0.328137,3.00843,3.59455,1.00545,0.717833,0.128019,0.00645771,0.328582,0.54553,0.188525,0.895525,0.0408872,3.24783,2.71351,2.49815,2.20143,0.676868,0.0287524,0.661264,1.41991,1.15123,0.467025,1.38601,2.40322,0.770031,0.00989118,0.386181,3.90587,4.04182,1.06631,1.77359,1.07598,1.38183,0.577455,1.14628,0.0020014,2.77807,0,2.90588,0.480905,1.61046,3.46837,0.355569,0.968912,1.83375,0.248112,2.19061,3.11325,2.65638,0.0367746,0.0603686,1.67572,2.31816,3.60933,1.29633,0.11909,0.0600984,5.61985,0.720068,2.48464,2.86905,2.13853,1.98833,3.15317,2.22118,0.639354,1.25785,0.281851,2.72698,0.921203,2.52241,1.82059,0.669081,0.100118,1.23264,3.06482,0.0135905,0.641079,0.403843,0.999941,1.04065,2.18396,1.00285,1.85417,0.877147,0.144436,2.53481,0.977721,1.64681,3.45587,0.818207,2.53121,4.49658,0.887782,0.234846,1.14279,0.659315,0.109713,1.36542,0.899438,0.175104,2.23193,2.60381,1.32821,0.915714,0.445052,0.00288133,0.163356,0.982402,0.0125229,0.666454,0.0555231,2.31373,1.38932,2.6543,1.35873,0.143295,0,0.226475,1.68297,0.660947,0.487611,1.4304,4.92722,1.75121,0,0.143175,3.02567,3.35532,0.272801,1.61775,0.645526,0.531874,0.614343,1.03124,0.0053275,3.25492,0.00264338,2.77116,0.430088,0.649697,2.29188,0.722446,2.50275,0.544916,0.204628,1.21149,3.88391,2.39757,0.000352105,0.212974,0.0384491,4.97835,2.7165,1.15199,0.413043,0.0146025,4.46414,0.0112177,2.61585,0.861757,2.28923,3.35354,5.43353,1.99584,1.30369,0.10671,0.500256,5.71459,0.963861,3.3774,3.52604,0.217327,0.0892254,3.19477,4.90951,0.10896,0.394621,0.600351,0.810254,0.513302,0.580245,1.14567,1.09709,0.362707,0.198018,1.85646,0.661291,3.34353,3.2775,0.385556,0.675552,6.33126,1.56004,0.641665,0.847008,1.38089,0.0555137,1.03894,1.94796,0.00552486,3.4941,3.26301,1.9274,0.253463,1.10076,0.0425154,0.904492,2.41034,0.0463368,1.52589,0.0862676,2.75297,1.88521,2.06296,1.15305,0.212366,0,0.394245,1.78298,0.888924,0.672979,1.35533,4.45294,1.48784,0.0605392,0.176757,4.75529,3.85554,0.844591,2.16973,0.272653,1.32838,0.688873,1.4805,0.00044684,2.74095,0.0685647,2.77912,1.24349,1.33388,3.78668,0.0939944,2.98083,1.01641,0.336904,2.41045,3.56117,2.63229,0.0388405,0.126486,0.872781,4.13501,2.94543,1.10456,0.801741,0,4.4144,0.153719,4.29423,1.82199,2.28633,3.094,5.33176,2.21881,0.693673,0.156806,1.09274,4.26824,0.379416,2.21975,3.53672,1.08108,0.89244,3.1856,4.76257,0.0451743,0.0949684,0.228956,0.373762,0.289384,1.2531,1.81112,2.30036,0.625152,0.00985754,1.77277,0.283007,3.30557,3.30533,0.619986,1.35398,6.38698,1.73076,0.126091,0.871414,0.356507,0.174712,1.66457,1.17135,0.0491385,2.67154,4.05349,2.17985,0.523685,0.576262,0.157504,0.308839,1.56192,0.0512652,0.95207,0.0805058,2.48621,1.37535,2.45468,1.87533,0.468265,0.122033,0.321863,1.33016,0.549338,0.494591,0.874955,2.63495,0.740472,0.0438563,0.141534,3.29394,3.68166,1.18674,1.08292,0.244218,1.11172,0.828344,1.42215,0.00554828,1.77962,0.189551,2.12145,0.546716,1.04505,3.95568,0.3655,2.37688,1.71594,0.224391,2.74793,3.33626,2.97977,0.122376,0.0632366,0.972671,2.11059,2.25442,0.85038,0.124168,0.0187264,4.50107,0.534927,3.1567,2.6244,2.49565,3.01174,4.25579,1.62496,0.592326,1.05417,0.562298,3.77764,1.02897,1.90008,2.77085,1.05659,0.747173,2.30213,3.18592,0.0147369,0.023573,0.207013,1.03249,0.925702,0.932221,1.22497,2.04939,0.744914,0.328359,1.31802,0.387632,1.70093,2.47217,1.20024,1.79619,5.47078,1.69118,0.165774,0.794053,0.341791,0.129126,1.44495,1.19076,0.0475776,2.66236,3.06191,2.4638,0.453825,0.488893,0.0543923,0.0634928,1.52558,0.22285,0.553228,0.221582,2.48304,1.022,1.56536,2.29826,0.209171,0.138548,0.309978,1.81818,1.4621,1.09224,2.08636,2.86538,0.603495,0.366823,0.329206,3.08912,2.58224,0.700127,2.0108,0.0986592,2.0601,0.895608,1.33536,0.101057,1.51155,0.290049,2.36485,1.46373,1.06817,3.15438,0.0736844,2.95347,1.35243,0.867988,2.56145,3.61033,1.72861,0.562523,0.60729,1.96837,1.20481,4.01519,1.00243,1.17049,0.0626663,4.00606,0.959066,4.73986,3.9961,2.07039,1.85597,4.51874,2.15147,0.0866293,0.160415,0.776263,3.42877,1.5827,1.208,3.27742,1.12566,1.38087,2.50217,2.11579,0.168926,0.47073,0.789089,1.18347,0.692235,0.859697,2.53243,2.11629,0.874951,0,1.84621,1.05134,2.79519,4.17778,2.43341,1.46079,4.95973,0.990473,0.00451488,0.498543,0.136919,0.747515,1.02114,0.164363,0.20047,2.40017,2.5119,2.07164,1.25656,0.222824,0.0637718,0.0677823,0.407466,0.0305444,1.12243,0.191263,2.95274,0.788903,2.04562,0.821009,0.390528,0.106878,0.290616,0.74644,1.30808,0.7783,2.74388,3.67038,3.43589,0.0119235,0.327381,4.70921,3.99718,0.309318,1.34281,0.799294,0.795629,2.50191,0.502497,0.0563869,3.90016,0.16275,2.25545,0.906753,0.884097,3.51065,0.653698,3.58473,1.43594,0.368113,3.07226,3.27478,2.61307,0.0926574,0.20586,1.01696,3.96841,1.73393,1.60189,1.61205,0.332741,5.6585,0.151749,3.11497,1.19252,2.06011,2.88685,5.18958,1.93425,1.74641,0.143624,0.980521,3.32435,0.240595,2.76726,3.43182,0.829861,0.513481,3.03082,3.14267,0.450467,0.384188,0.203941,1.57999,0.0863299,1.05067,1.62805,2.49849,0.624522,0.0210923,3.38103,0.0787131,3.06627,3.05254,0.724761,2.51799,5.99396,1.44315,0.48246,0.460933,1.27126,0.178611,0.842834,2.41634,0.0746336,2.28044,1.60962,1.77188,0.850597,1.19109,0.296519,0.604338,1.35025,0,0.855787,0.24927,2.99181,2.2486,2.37542,1.3126,0.350791,0.024723,0.201999,1.45714,1.48941,1.53887,2.55214,5.06456,0.994865,0.0734257,0.083538,3.51118,2.41169,0.631832,2.0792,0.115151,2.3912,0.772408,2.02382,0,3.3883,0,2.83265,1.07762,1.06044,3.32062,0.152621,3.1467,0.999532,0.691353,2.18066,4.07433,3.76381,0.564451,0.288695,1.46631,1.80245,3.96539,0.75673,1.25008,0.209399,4.06395,0.238434,5.02981,2.71844,2.61667,2.93738,5.24963,1.91154,0.472515,0.478448,1.34589,4.0873,1.01529,1.9998,4.08209,1.06896,1.46918,4.13979,3.46283,0.222857,0.165366,0.435843,1.03564,0.934988,0.827622,2.30924,2.56759,1.11041,0.339095,2.17876,0.246572,3.56566,2.52839,1.91746,1.52709,7.18276,1.46865,0.0681596,0.578342,0.243158,0.224508,1.52765,0.580382,0.030582,2.47407,4.58363,2.31677,0.465353,0.39472,0.587289,0.213973,0.600313,0.120027,0.89201,0.185326,3.94036,1.52176,2.62566,1.78063,0.208037,0,0.558397,1.04916,0.900976,0.483306,1.48558,3.60931,1.59509,0.0638275,0.0647773,3.8862,3.17611,0.8943,0.841184,0.323702,1.04602,0.258091,1.67474,0,1.82776,0.0908958,1.57279,0.929668,1.49998,4.81248,0.228259,2.91749,2.00505,0.0470934,2.84799,3.22272,3.68171,0.0594875,0.0733745,0.54319,3.34397,2.55439,0.507932,0.75085,0.108581,4.24962,0.145146,3.65904,2.06542,2.62613,3.56953,5.05274,1.96623,0.779482,0.634575,0.948498,4.4603,0.50401,1.81869,3.3978,1.04111,0.447499,2.99711,3.06425,0.0197619,0.0390933,0.276516,0.752591,0.459888,0.672086,2.00618,2.43392,0.363539,0.0871918,1.12805,0.738331,2.49416,1.82035,1.00751,2.02311,6.0054,1.90343,0.143459,0.798544,0.837159,0.103327,1.35308,1.02193,0.0155476,2.08821,4.37539,1.69243,0.628235,0.612107,0.0970055,0.407801,1.79539,0.123628,0.747089,0.163579,2.35004,0.852999,0.910148,1.80315,0.59716,0,0.851198,0.778011,2.19988,0.402101,0.933004,2.09489,3.05333,0.0252962,0.0977166,2.9703,1.6115,1.0476,0.400364,0.66365,0.440661,0.829402,1.43691,0.796235,1.38266,0.161254,0.934144,0.411609,1.29833,2.81847,0.583717,1.70186,2.00263,0.344199,3.56151,1.28074,1.83579,0.0387559,0.0243303,1.23755,4.48576,1.36396,2.05456,0.473,0,3.95411,0.00469383,1.32001,0.745603,1.49426,1.90903,2.37102,0.920934,2.45122,0.736415,0.664711,3.55602,2.10916,0.70585,2.62348,1.12938,0.956224,1.17342,2.17879,1.02419,0.309579,0.363046,1.38165,0.74992,0.428322,1.58538,1.77151,0.166842,1.10562,1.36608,0.289836,1.93779,1.48751,0.142338,1.41565,4.64051,1.12845,1.44624,0.618646,1.53882,0.55646,0.11152,1.63253,0.13781,3.06238,1.53316,2.18355,0.44066,1.92378,0.46227,0.37104,1.70604,0.23205,1.86809,0.102674,3.19703,2.5658,1.93422,2.27114,0.657241,0.0382914,0.290383,1.35741,1.49755,0.741304,2.09531,2.14157,1.03596,0.0978403,0.203721,4.29982,4.3951,1.17579,2.35823,0.722298,1.5228,1.03373,1.08911,0.0532115,1.63872,0.00680868,1.80044,0.4208,2.52569,3.74694,0.2112,1.57353,2.23563,0.457424,3.61216,4.27956,1.83665,0.107581,0.134045,1.37063,2.4452,2.30221,1.81058,1.01463,0.0911855,5.20949,0.789669,4.47411,2.57445,1.41898,1.25783,3.56169,1.19806,0.501017,0.323389,0.956781,1.86047,0.423411,2.34943,2.48705,0.592364,0.878147,1.79065,2.40255,0.415444,0.630449,1.11744,0.658741,0.73657,1.44287,1.21421,2.56012,1.30955,0.0461261,2.81246,0.494368,2.01135,3.04064,1.15881,1.88349,3.73473,0.919362,0.107721,1.49636,0.617467,0.305044,0.784687,1.16902,0.0493862,2.54482,2.76247,1.1756,1.09797,1.07768,0.0691202,0.0770139,0.696618,0.0298994,0.735747,0.0129393,1.88335,0.905986,1.03496,2.101,0.597856,0,0.19022,1.55955,1.9505,1.45855,2.44006,3.13068,0.374842,0.109704,0.0564849,4.41647,3.26477,0.885778,2.43465,0.350469,1.09961,0.447687,1.88259,0.0782037,1.40383,0.0368739,2.70133,1.89626,1.92466,3.58989,0.122527,3.11725,1.05007,0.51717,2.40146,3.33289,1.93742,0.252541,1.02291,2.0183,1.3077,2.65496,1.15508,1.23102,0.000266209,4.37571,1.61635,5.65003,1.84072,1.86486,3.17868,5.39525,1.75413,0.261276,0.306518,0.943247,3.54729,0.883223,1.44945,3.65783,0.709348,1.23866,3.18681,3.92327,0.106278,0.093275,1.28177,0.542254,0.68957,0.528313,1.24936,2.35226,0.303834,0.0594064,1.31486,1.1578,2.43625,4.4125,0.904337,1.00147,4.81197,1.31189,0.0296455,1.11495,0.724461,0.520235,1.25262,0.250807,0.0281229,2.57911,2.92775,2.22562,0.75527,0.242219,0.118131,0.363648,0.497668,0.0814463,1.30973,0.0366483,3.00148,1.25068,1.63096,1.52441,0.621856,0.00641068,0.230657,1.73529,1.45144,0.646578,2.25007,3.25211,0.563205,0.0751726,0.0163225,3.54842,3.84219,1.02355,1.92965,0.817386,1.16032,0.510203,1.44918,0.094062,2.12093,0.18292,2.18219,0.247555,1.35479,3.45437,0.0852659,2.62705,0.895785,0.287165,2.27176,3.76671,2.21876,0.34165,1.02308,1.78778,1.66692,2.17194,0.78184,0.405094,0.0208693,4.54682,0.971125,3.88979,2.00045,2.23424,2.78853,5.28522,2.13774,0.237709,0.150786,0.456535,3.60444,0.698361,1.24292,3.25507,1.02311,0.797662,2.94663,3.29284,0.155182,0.0697566,0.439491,0.730536,0.809481,0.477256,1.35965,2.22005,0.560803,0.0364263,1.49128,0.700289,2.63805,3.72576,0.408458,0.932113,4.24421,1.17882,0.134419,0.657352,0.368504,0.184188,1.30552,0.460816,0.028444,2.41779,2.85773,2.01615,0.72293,0.287015,0.0462368,0.160295,0.856651,0.098313,0.130786,0.046435,2.05407,1.54281,2.34828,2.53102,0.442987,0.0561916,0.268094,1.58142,0.8853,1.28698,2.50667,3.04347,0.423114,0.0185859,0.262836,3.83576,3.57948,0.914873,2.57641,0.271825,1.59572,0.423181,1.84506,0.0146318,1.08426,0.0235161,3.13197,0.697155,1.7896,3.38426,0.0901369,2.18545,1.48666,0.392883,2.32617,3.22082,2.32532,0.0902337,0.145137,1.08237,1.66407,2.63171,1.45994,0.131882,0.0723601,5.2952,0.436551,4.01131,2.07321,1.4268,2.99351,4.44962,1.59591,0.584692,0.999917,0.741395,3.61155,1.21488,2.08004,3.2875,1.06792,0.701226,2.57786,3.9834,0.0356573,0.0402862,0.258593,0.57421,1.35641,0.611022,1.26718,1.16932,1.30904,0.0148628,1.97011,0.861008,1.83741,3.52019,0.66894,1.38401,4.86006,0.991134,0.0453428,0.595723,0.376409,0.243376,1.10378,0.772661,0.104803,2.20183,2.70189,1.8724,0.874469,0.276714,0.0854222,0.109846,0.469759,0.0341609,0.822008,0.0663278,2.82431,1.98922,2.58916,2.33014,0.36916,0.000405256,0.440166,1.42747,2.06229,0.839913,2.56837,3.54221,0.806237,0.0238713,0.0986123,4.16513,3.2629,1.11632,2.24042,0.744196,1.76337,0.456146,1.41652,0.0272024,2.58901,0.167701,2.60603,1.09688,1.16621,3.30133,0.272464,2.55796,1.90589,0.180866,2.41701,3.38609,3.11584,0.191306,0.749476,1.46686,1.90682,2.87457,0.694,0.343411,0.0147392,4.91275,0.464997,3.85381,1.65685,1.67556,3.0329,4.73798,2.09576,0.312937,0.517346,1.04992,3.59476,1.26293,1.085,3.32468,1.59134,1.01859,3.35532,3.53533,0.084303,0.0628613,0.648484,1.09515,1.50538,0.17028,1.40541,2.64826,1.06457,0.100404,2.33836,0.694368,2.53434,3.55765,0.68602,1.77681,5.59932,1.25979,0.0556799,0.505265,0.470065,0.221458,1.14363,0.721429,0.0939817,2.04274,3.13907,1.29704,0.464239,0.875146,0.118063,0.167729,0.691054,0.0293266,0.611965,0.0768707,2.41301,1.51695,1.93647,2.18609,0.344321,0.0389794,0.454889,1.37358,1.48123,0.780079,2.11202,3.10542,0.999507,0.0394063,0.204987,4.05465,3.69151,0.976356,1.60781,0.698079,1.13738,0.537282,1.42179,0.00870474,1.36949,0.023588,2.30823,1.14589,1.14334,3.29495,0.192186,2.55375,1.98119,0.272244,2.98279,3.39615,2.60981,0.0805932,0.603192,1.68799,1.85609,2.74516,0.505839,0.325397,0.0708398,5.28361,0.430745,3.53773,2.20675,1.66348,2.58096,4.62437,2.74623,0.222936,0.392539,1.03132,3.74876,1.04139,1.11657,3.41723,0.882675,0.70088,2.92961,3.14528,0.0404496,0.197252,0.654779,1.25583,0.600679,0.254489,1.76305,2.48888,0.957629,0.0304134,1.98809,0.68718,2.4288,3.55978,0.906977,2.39657,4.87962,1.07763,0.0113303,0.629885,0.284732,0.307247,0.693691,0.863577,0.0309877,1.49964,2.85926,1.40856,0.633111,0.401133,0.0182732,0.0368716,0.753268,0.00673428,0.534558,0.00958632,2.73017,2.39266,2.17597,2.33269,0.836123,0.0222678,0.178695,1.06635,1.69794,1.20227,2.49672,4.13729,0.191926,0.0308835,0.0363431,4.61121,3.23564,1.00583,2.72539,0.298151,2.01082,0.769828,1.96608,0.0858546,2.23029,0.0122647,2.23588,0.633349,1.20338,3.37658,0.0692665,2.29815,1.05922,0.480622,2.08249,4.0234,2.28867,0.415034,0.983109,1.92329,1.10093,1.73856,1.04592,0.487978,0.0242384,4.47649,0.681652,5.14066,1.34459,2.59875,3.29062,4.8603,1.86614,0.227459,0.307617,1.15525,2.771,0.254383,2.10122,3.77412,0.778152,1.06289,3.55692,4.12524,0.0866851,0.039943,0.282242,0.490172,1.49075,0.340438,1.89202,3.13522,0.571204,0.0840356,1.08281,0.748803,2.67685,3.2724,0.322296,1.01903,5.3433,1.32265,0.079348,0.697845,0.370772,0.0963784,1.51029,0.731301,0.0519211,3.12523,3.5217,1.55013,0.490105,0.309106,0.152689,0.126233,0.900529,0.0508995,0.470179,0.0570541,2.93815,1.03723,1.31152,1.88639,0.53211,0.0120982,0.19104,1.59915,0.789835,1.25294,2.21089,3.76776,0.442896,0.0824799,0.0142063,4.1578,3.41276,0.872557,2.83429,0.41081,1.12326,0.520284,2.29224,0.034131,2.31664,0.0527101,1.93802,0.23192,1.07679,3.58917,0.103749,2.88465,0.943151,0.274026,1.80946,4.21325,1.81319,0.25273,0.727642,1.31808,1.68266,2.21464,0.861174,0.387389,0.0198225,3.8402,0.379294,4.60951,1.26635,1.76706,3.3753,4.86082,1.96672,0.269316,0.196363,1.33864,3.62479,0.627859,2.09711,3.85491,0.609686,0.881531,2.71234,3.77832,0.0278493,0.0952197,0.350846,0.639852,0.512732,0.316809,1.29115,2.19078,0.408391,0.0650802,1.27545,0.848432,2.89082,3.46501,0.429315,0.532971,4.65582,1.2727,0.0419573,0.771441,0.57798,0.189393,1.16264,0.666373,0.011883,2.61725,2.62028,2.10185,0.367404,0.307311,0.375864,0.115727,1.16286,0.145557,0.639334,0.0189169,2.85532,1.43457,1.51701,1.08727,0.655423,0,0.39513,1.46804,1.47166,0.837551,2.10679,3.85416,0.534801,0.0470078,0.124029,3.15903,3.37361,0.818388,2.83651,0.451094,0.913231,0.778017,1.86247,0.0487507,2.85288,0.0583686,2.2793,0.732785,1.39704,2.60797,0.122074,2.77161,1.1196,0.590158,2.21509,3.73356,2.73063,0.499842,0.927976,1.42161,2.06318,2.38936,0.891494,0.883829,0.209371,4.37902,0.608832,3.81845,2.51543,2.54147,3.3838,4.15177,1.59683,0.192216,0.537215,0.455513,3.93832,1.0829,1.92547,2.39969,1.00714,0.802947,2.15678,3.95803,0.20991,0.124253,0.479957,1.0012,0.444251,0.969963,1.81434,2.18901,0.321689,0.0278072,1.64864,0.651427,2.12086,4.46436,0.998611,1.1613,4.08807,1.39377,0.127706,0.700906,0.552711,0.515313,1.00938,1.48769,0.0935584,3.28137,2.79706,1.85369,0.296472,0.487268,0.0317496,0.19594,1.23039,0.0388062,0.67017,0.0934113,2.86187,1.86251,1.88909,1.87347,0.596602,0.00377345,0.192182,1.35662,1.60304,1.6355,2.39665,4.20327,0.978054,0.0436493,0.0764731,3.90868,3.21256,0.552857,2.17574,0.720971,1.57867,1.00832,1.66714,0.0796119,1.50929,0.0632518,3.0211,0.78774,1.44251,3.49814,0.0800848,2.00075,1.36371,0.743488,2.33431,2.60721,2.96996,0.278091,0.484778,1.58918,2.23393,1.81321,0.76124,1.05518,0.180481,5.67809,1.19608,4.3703,2.42004,1.21932,3.44784,4.91669,1.31593,0.246808,0.215084,0.619059,3.59653,0.725094,1.33354,2.42733,1.78544,1.59301,2.10466,3.41312,0.251837,0.0452664,0.893701,0.983479,0.592334,0.906095,1.89089,2.11333,0.918553,0,1.45872,0.963036,1.41516,4.55745,0.836861,2.22682,4.22401,0.894384,0.0436534,1.00384,0.145187,0.258983,1.36973,1.11032,0.156456,2.41609,2.70471,2.1322,0.526215,0.252542,0.0258043,0.349993,0.35663,0.104863,0.225369,0.24749,3.35109,0.963801,1.10813,0.800809,0.467626,0.107116,0.0552091,2.47139,2.62246,1.78614,3.4856,5.1379,2.94626,0.0129506,0.137714,4.38971,4.69919,0.188566,2.24813,0.520747,0.79837,1.95592,0.550285,0.00599656,3.57328,0.080133,2.81194,0.483219,1.56046,3.52163,0.658864,3.26069,1.05765,1.0118,2.94407,4.80952,1.89636,0.259329,0.0739332,1.2724,3.2216,1.86089,1.15171,1.37013,0.828185,6.12156,0.314778,3.56917,0.780318,1.08123,4.30628,4.93581,2.55729,1.14319,0.0388253,0.890784,3.77139,0.236146,3.92972,3.29398,1.21922,0.728362,2.22584,3.33834,0.835542,0.443234,0.266086,1.38535,0.148491,1.23911,1.60022,1.91213,0.431745,0.0798277,3.92146,0.100696,2.10039,5.02391,0.322355,2.43659,4.82507,0.601774,0.46191,0.594377,1.27198,0.325763,0.846408,2.22323,0.0822376,3.12431,1.4243,2.1481,1.00837,0.604639,0.100744,0.806879,0.668582,0,0.35246,0.0494527,2.93895,0.883624,1.23582,1.33622,0.579971,0.00303413,0.12075,1.26554,0.868126,0.683931,2.13174,3.34147,0.507017,0.034596,0.407799,3.14334,3.45889,0.514382,2.67469,0.30144,0.566191,0.578803,1.58267,0.0177247,1.72527,0.00918701,2.09903,0.249594,1.94982,3.16641,0.119972,2.91085,1.27837,0.211139,2.1288,3.62004,1.90441,0.0502565,0.314473,1.26084,2.53492,2.32336,1.24451,0.584151,0.09295,3.83199,0.583287,4.35442,2.26039,2.16336,3.10987,4.73173,1.53667,0.242611,0.268393,0.520367,3.39473,0.656522,2.33191,2.41795,0.737193,0.88631,1.90878,3.36651,0.160806,0.30254,0.598468,1.03497,0.637035,1.40763,1.29467,1.22339,0.127561,0.0349597,1.86225,0.668291,2.26833,4.25181,0.515916,0.231848,3.73252,1.58088,0.390363,1.09529,0.602876,0.307385,1.07234,1.07695,0.00450348,3.96513,2.2456,2.13224,0.583686,0.3779,0.0323832,1.12609,0.944197,0.0601499,0.516288,0.0505858,3.97665,1.66554,1.54787,2.25417,0.843666,0.109033,0.411015,0.34539,0.718933,0.611431,2.28845,2.05871,0.923705,0.216646,0.0154038,3.44349,2.49335,0.979974,1.59057,0.453513,1.40971,0.303805,1.07996,0.0104552,2.15765,0.575598,1.24831,0.146673,0.647196,3.51472,0.223764,2.36423,2.38342,0.979161,2.10142,3.73741,2.76451,0.864586,0.278941,1.18661,2.21592,2.22132,1.20689,1.51193,0.020477,3.30165,0.550168,2.83984,2.15062,4.65036,4.78363,3.84807,1.75357,0.539525,1.41835,1.5899,3.9677,0.71811,1.18531,4.55276,0.925562,1.40166,2.73467,3.21768,0.090259,0.000825712,0.866169,2.0545,0.279414,0.582492,1.44524,3.59893,0.470387,0.00659904,0.221317,1.14851,2.84489,2.55217,0.285621,1.01121,5.44992,1.04718,0.384958,0.209347,0.269595,0.214903,1.98371,0.982846,0.446999,3.39738,2.89421,1.69827,0.18654,0.0606241,0.572584,0.845817,0.83125,0.142304,0.511877,0.101747,3.0588,1.25144,1.19462,1.74543,0.190779,0.00891978,0.442403,0.66589,0.917971,0.111052,1.06662,2.67585,1.32598,0.0668443,0.0650987,3.3711,3.52265,1.85074,1.12053,0.144353,0.746337,0.826837,2.03384,0.0279713,1.60881,0.160879,1.02235,1.06167,1.20357,4.12033,0.136176,2.46433,2.25091,0.329999,3.34333,2.89288,2.42422,0.0518891,0.154113,0.650573,3.72507,2.40731,0.693673,0.657181,0.0400677,4.00169,0.610979,3.20096,2.30301,3.29098,3.8978,4.15155,2.06398,0.739552,0.580522,0.728652,4.44951,1.04893,2.03725,3.53141,0.726454,0.643724,1.8834,3.04606,0.0857924,0.340497,0.160281,1.16203,0.133217,0.954035,1.35878,1.93417,0.0825962,0.0565659,0.919205,0.37644,2.29047,3.81102,0.682563,1.71354,6.10332,1.62584,0.381732,0.857583,0.478908,0.310714,1.69634,1.53788,0.093128,2.65451,3.03056,2.00864,0.622983,0.242817,0.0446907,0.31917,2.12676,0.584698,0.508028,0.312854,3.57535,1.01957,1.76515,2.32122,0.309599,0.0135759,0.426861,0.866511,1.5532,0.567531,1.69692,3.17837,1.28109,0.138402,0.0340985,4.34716,3.71548,1.77937,2.15931,0.173799,0.726289,0.202157,2.69981,0.0611559,1.09466,0.21049,1.1993,1.28511,1.93591,5.0003,0.160311,3.53014,1.60281,0.552014,2.56209,3.43234,3.58124,1.18037,0.927042,2.41542,2.06958,1.71808,1.0145,0.981867,0,4.76112,1.02332,4.90183,2.32996,3.75912,4.43126,3.90499,1.79408,0.407988,0.975094,1.80331,3.76273,0.680769,1.02908,4.06174,1.33209,1.37562,3.4585,3.2734,0.0544218,0.048771,0.182318,0.83412,0.339921,1.93744,1.76138,3.12507,0.333666,0.603001,0.910121,1.11202,1.64939,2.83296,0.878977,1.53331,5.16968,2.19303,0.26282,0.56399,0.559927,0.339195,1.69228,0.502092,0.0285549,2.2546,4.01991,1.49154,0.449443,0.216892,0.285517,0.116664,1.16992,0.968088,0.554891,0.372601,4.17086,0.611138,1.43107,0.835574,0.497291,0.0541932,0.0318258,1.75179,3.45455,2.30135,4.44617,5.07075,4.09356,0.168165,0.118873,3.71805,5.38558,0.359948,1.17537,0.401529,1.74139,3.03928,0.641873,0.0452245,3.51162,0.225924,1.38501,0.975901,1.95064,3.55211,0.409476,3.11961,2.09502,0.283407,3.57552,3.27555,2.12664,0.34929,0.474149,2.16918,3.38105,0.957036,1.66978,2.51291,1.40494,5.48474,0.223458,5.07802,0.565896,1.52222,3.37747,5.36741,2.40689,1.75742,0.177935,1.56984,2.99916,0.108959,3.20956,2.76626,1.30163,0.704933,4.24807,1.97075,2.02187,0.711818,0.606899,0.713849,0.580595,1.98833,1.63021,3.92626,1.18241,0.0354319,4.58759,0.332903,2.27938,5.06359,0.173173,3.92383,4.6605,1.06388,0.556589,0.171921,1.86755,0.878197,1.36928,0.658514,0.0190993,3.09929,2.0158,2.27263,1.96783,0.792823,0.213694,1.07014,0.38616,0.0420136,0.121149,0.419845,2.73868,1.23557,1.6829,1.17198,0.422408,0,0.212392,2.15687,1.06676,0.841352,3.33135,5.16731,2.13337,0.220289,0.325544,2.79493,2.97439,0.37262,1.64285,0.181783,1.72529,1.00698,1.78506,0.0236844,2.2001,0.10197,1.8321,1.11312,1.18017,2.91984,0.108672,3.7586,0.671367,0.93762,3.10085,3.06066,1.9627,0.179142,0.317111,1.18516,2.57359,2.41962,1.47094,1.01224,0.000268379,4.35446,0.484724,4.49661,1.5371,1.99019,2.96908,6.3332,0.812003,1.23299,0.243958,0.250655,3.52692,1.45084,2.45594,2.48154,0.441547,0.970346,4.3034,3.72199,0.833116,0.0537236,0.949839,1.43612,0.538444,0.225747,1.98101,2.63878,0.72098,0.148604,2.18236,0.28725,2.9508,3.88237,1.00137,0.888707,6.01463,2.11074,0.386278,0.708657,0.847301,0.235339,1.14889,0.734926,0.0110711,4.25795,3.09755,2.88656,0.214374,0.615552,0.0784606,0.794541,1.4357,0.00623513,0.514883,0.03413,3.33517,0.812137,1.2248,1.74864,0.315777,0.013393,0.210911,1.90697,1.91836,0.767752,1.65758,3.16942,1.14282,0.0459913,0.220999,4.17401,4.52362,0.804498,2.51992,0.268469,0.642483,0.851914,1.99326,0,2.50734,0.0381192,2.03101,0.542559,2.1662,4.10502,0.0503313,2.32466,1.89728,0.167142,3.0341,3.65051,1.81492,0.173916,0.377965,0.68109,2.95184,1.96136,1.56334,0.696108,0.00445108,4.07859,0.114465,4.67323,1.05034,1.61055,3.35983,4.21886,1.91939,0.436332,0.254994,0.68775,2.84131,0.521681,2.10881,3.26495,1.07813,0.664651,1.33711,3.38636,0.0436841,0.419039,0.227035,0.94118,0.196337,1.36326,1.60736,1.10145,0.0341857,0,2.58226,0.202405,2.20619,4.68477,0.452925,0.957097,4.00319,1.80225,0.122734,1.26034,0.979403,0.635432,0.928647,1.13046,0.0393081,2.84849,2.48059,2.28962,1.31035,0.412608,0.0703817,0.296971,0.897599,0.0375367,0.294765,0.160068,2.55941,1.3605,1.34182,3.14562,0.427241,0.0111013,0.150922,1.19132,1.69642,1.51754,3.49422,2.81836,0.123653,0.0371704,0.242947,3.68749,3.71244,1.78137,3.2637,0.947374,1.67937,0.796561,2.51693,0.134468,1.68809,0.116397,1.39079,1.24835,1.9665,3.59129,0.0179134,2.62591,0.670484,0.865052,3.07211,3.2161,1.61743,0.771014,0.855325,1.22421,0.375836,1.87587,1.42771,0.939178,0.00956868,4.43019,1.83368,6.28362,2.02937,2.98059,3.77094,4.369,0.936155,0.152694,1.12217,1.54499,2.27825,0.381153,1.99007,2.66481,0.41185,1.30618,2.36695,3.4729,0.175739,0.0572974,0.920726,0.575249,1.12185,0.886291,1.77656,3.09869,0.271949,0.42442,0.827352,1.61262,2.53005,4.59954,0.966383,0.271037,3.76151,1.69681,0.196308,0.92964,0.292832,1.10383,0.979889,0.0910277,0.456767,4.02827,3.03481,0.945839,0.690891,0.159811,0.335634,0.407737,0.668719,0.211333,0.512295,0.0159793,3.84225,0.861139,1.66746,2.88913,1.05076,0.017604,0.28309,1.60989,1.0752,0.338429,1.99783,1.83507,0.453097,0.152038,0.072195,3.54355,4.65087,1.40336,1.6848,0.314609,1.67955,0.824161,2.18883,0.240514,1.77434,0.279308,1.42163,1.27125,1.40641,4.79524,0.0708822,2.12524,1.796,0.135306,1.86181,3.34326,2.62422,0.273251,0.466707,1.28652,1.4584,1.91578,1.2838,0.758845,0.0115877,4.82758,1.02023,4.63398,2.57347,3.50164,3.95303,3.70157,1.64042,0.440699,0.761368,0.495789,1.64867,0.70766,1.93985,2.99937,0.903021,0.935193,1.87404,3.39784,0.0792058,0.0470492,0.257649,0.60916,0.915243,1.18901,1.08066,1.89791,0.142627,0.186183,1.73326,1.11931,2.12438,3.13803,0.993114,1.108,4.62309,2.45593,0.0847078,1.31495,0.32682,0.559858,1.30904,0.349134,0.0207973,2.69449,2.65408,1.76183,0.526198,0.360282,0.0162377,0.452695,1.67969,0.786978,0.738353,0.0760599,2.98748,0.974157,1.96799,1.90436,0.93187,0.0125455,0.144638,1.44335,0.501522,0.782217,1.49275,3.55478,0.51408,0.0272756,0.00305018,3.47825,4.65626,0.926729,1.82176,0.461906,0.700576,1.0538,1.82246,0.0129438,3.10561,0.129999,2.23367,0.193773,1.35113,4.37293,0.408169,2.48598,1.12841,0.26154,2.01435,3.85373,3.28516,0.033117,0.569275,1.23854,1.83767,1.66214,1.18243,0.607491,0.127385,4.84697,0.817407,3.33022,2.42144,2.63031,3.68531,4.49616,1.40106,0.68475,0.911053,0.511081,3.96689,0.497705,2.66518,3.59399,0.693997,0.303797,2.3589,3.17979,0.120173,0.0840141,0.0773957,1.22968,0.873476,0.766364,0.745777,1.11064,0.114887,0.175923,2.15218,0.573491,1.95948,3.04424,0.916047,1.46429,4.72096,1.51816,0.327772,1.1236,0.827795,0.180154,0.970699,0.992361,0.0485862,3.01936,2.58218,1.8066,0.349257,0.142384,0.0494521,0.425016,1.76525,0.178625,0.188875,0.0699539,4.07327,0.691051,1.58693,2.37945,0.216532,0,1.81234,1.15422,0.581613,0.271597,0.473457,2.50652,2.63144,0.181997,0.0424526,2.76629,2.03107,0.867031,0.152904,0.0139542,0.236293,0.0618427,2.29404,0.0216631,1.18051,0.0467634,0.860408,0.57037,0.724652,4.19656,0.0281417,2.31023,1.71416,0.147774,3.36904,2.19917,2.46426,0.0893236,0.077964,1.17619,3.83219,2.2351,0.284899,0.717847,0.00192396,4.0828,0.0221865,2.47903,1.85622,2.79813,3.13349,3.78653,2.18643,1.42026,0.848296,1.66738,4.36045,2.32377,0.300671,2.82945,0.368193,0.19111,1.17597,2.81472,0.231004,0.0665675,0.404295,1.22482,0.0443175,0.790282,1.99073,2.7893,0.349134,0.273168,0.396511,0.449771,2.04238,1.17213,0.0075335,1.75189,4.58506,1.10772,0.211991,0.926428,0.222057,0.110032,0.107369,0.978691,0.0444489,1.09031,3.1166,1.84598,0.23379,0.0729769,0.087661,0.16739,2.09003,0.564941,0.553636,0.98773,3.2317,2.84104,1.344,1.61252,0.924333,0.390287,0.523503,0.745252,1.81386,0.3759,0.961302,0.74361,1.92746,0.0367375,0.254657,4.22352,3.2171,0.928905,0.551574,1.87886,0.820247,1.85763,1.40116,0.460056,1.42202,0.0111928,0.189119,0.220919,1.58414,4.81054,1.16823,0.408528,2.59343,0.0531654,2.90222,2.50481,0.482151,0.0355503,1.4243,0.97523,5.23268,1.0987,1.421,1.03101,0.0672638,3.65836,0.717889,2.82239,2.06995,2.53594,1.60664,1.07846,2.54499,0.898502,0.0471799,0.700152,1.64357,0.0399906,0.482931,2.05672,0.797772,1.13379,0.274555,0.707143,0.234836,1.07659,0.178635,0.615385,0.297467,0.860431,0.110594,2.59657,0.600154,0.479032,0.296133,0.993862,2.26152,1.45347,0.364953,0.894827,2.17797,2.07657,0.611407,0.822876,1.5875,0.144002,1.58942,1.88468,0.114159,1.42093,0.818327,1.9349,0.802866,0.243312,0.301177,0.429983,1.43563,0.656302,0.0938108,0.430553,4.81325,0.0807951,1.46824,2.15321,1.30366,0.0564281,0.156204,2.13588,1.46877,0.472374,0.611697,1.42164,1.90515,0.231403,0.129641,2.6591,2.61479,0.637518,0.904208,2.13806,0.372701,1.13394,1.90304,0.58043,1.33959,0.553732,1.06007,0.985018,0.543808,3.25897,0.206441,1.39186,0.804308,0.384461,0.844792,2.90573,0.300399,0.213936,0.998085,1.63261,3.31909,1.73901,1.18092,0.349687,0.109669,3.74661,1.23421,2.04492,2.26541,2.26596,1.53822,2.52296,2.75313,0.220269,0.019697,0.46049,1.82769,0.388736,1.27916,3.24384,0.741025,0.487426,0.531523,2.76641,0.0748161,0.547243,0.837052,1.31265,1.02772,1.34277,1.34924,1.00636,0.103381,0.0574457,0.980765,0.820282,0.887506,3.20173,0.046969,0.528082,2.77855,1.84989,1.00946,1.86239,0.608829,0.516535,2.2498,0.342212,0.0634859,1.29077,1.86591,2.0693,0.417948,0.261626,0.13938,0.409343,2.22424,1.9599,0.663104,0.11887,6.45383,0.38252,4.04982,1.77296,0.789522,0.227368,0.340924,3.34547,2.93424,1.29059,1.17446,1.97037,1.24899,0.574112,0.254942,2.74412,2.74275,0.509385,0.330373,1.52191,0.451901,2.00786,2.16086,1.21977,1.45518,0.822138,1.62224,2.1564,0.630753,2.69039,0.0497901,1.52492,1.13065,0.222982,1.43297,1.86954,0.621498,0.639015,1.63963,2.85328,2.43762,1.38037,0.973973,1.20674,0.148073,4.70571,3.20529,2.91054,0.694522,3.6054,2.29621,3.79459,0.683008,0.239141,0.2356,0.30097,1.11056,1.80002,1.15592,2.84758,0.432422,0.857427,1.14139,1.32843,0.153965,0.989956,0.686348,0.316861,0.115731,1.02845,3.41758,2.12211,1.2383,0.485448,2.75098,1.07798,1.46718,3.1013,0.249525,1.36975,3.21075,1.94395,0.38638,1.62883,0.523149,2.01228,1.84046,0.501834,0.593366,1.46181,3.40543,1.10476,1.44816,1.1251,0.0295201,0.133602,3.52492,2.39285,0.594473,0.67307,7.33662,0.312076,2.17701,1.45173,0.573155,0.34617,0.487049,3.08279,2.93026,0.649512,1.89446,1.756,0.609289,1.22079,0.398642,2.5188,1.94528,0.986859,0.457446,0.847528,0.412541,0.976149,1.2117,0.705849,1.9876,0.722498,1.24018,2.55111,0.489519,2.72867,0.0291009,2.35459,0.235688,0.316869,1.69132,1.35021,1.12745,0.435903,1.41336,2.33648,2.03596,1.68084,0.584443,1.15148,0.0612399,4.65578,1.55388,4.58667,0.963235,4.25159,2.54535,3.56805,0.892973,0.33514,0.181146,1.26509,1.56171,1.0552,1.02058,2.56878,0.821836,0.81548,2.27766,2.08943,0.648175,0.884804,0.96172,1.02797,0.711759,1.21395,2.56594,3.44538,0.760028,0.0488547,1.48262,0.799258,1.08438,3.27935,0.0382538,1.45882,3.31775,2.27778,0.715653,1.53316,0.778512,1.63186,1.1993,0.220446,0.778951,2.07157,3.78488,0.823545,0.834017,0.654665,0.0800226,0.337342,1.8533,2.51725,0.702393,0.102364,7.71211,0.286745,2.43409,2.67686,0.619262,0.335597,0.0531147,3.60267,2.38416,0.416732,2.09647,1.71686,0.757883,0.930876,0.0560811,1.97866,3.51794,0.56192,0.535995,0.996723,0.851703,0.930008,2.42682,0.615996,1.27864,1.03836,1.36461,1.88232,0.572575,1.65992,0.166636,1.97775,0.721565,0.134376,1.03473,1.5387,0.73856,0.193663,1.30164,2.81109,2.61607,2.13191,1.14167,1.21546,0,4.85786,1.71485,3.11446,0.752463,4.40693,1.40171,4.11217,1.04395,0.399484,0.80766,0.345108,1.13839,0.892735,1.10168,3.1794,0.556764,0.76763,1.86924,1.66639,0.354613,0.812082,1.13795,0.466964,0.466748,2.14874,3.45943,3.79036,0.872065,0.592503,2.39201,2.14517,1.32333,3.32057,0.065606,1.8166,3.61239,1.98733,1.4363,1.76843,0.745389,1.04846,1.55632,0.433596,0.657589,2.06524,3.66612,0.766329,1.20404,0.482788,0.039347,0.568658,3.09411,3.26962,1.34813,0.10506,5.60434,0.0889655,2.88713,2.08657,1.16381,0,0.320336,3.15232,2.11339,0.581482,0.599109,1.64684,0.136905,0.509498,0.0594996,3.93309,3.1175,0.723839,1.43198,1.64233,1.06164,1.62169,1.82454,1.04357,1.74279,0.442203,2.06128,2.01575,0.252877,3.43121,0.0215486,1.07699,0.155145,0.312614,0.946017,1.93533,0.400249,0.157762,2.43273,2.4283,2.45243,2.85849,1.08282,0.64019,0.0445874,4.83998,2.06011,3.49755,0.916848,3.19959,1.67709,2.80862,2.02959,0.462559,0.40393,0.567347,1.60233,0.82948,1.03516,4.08745,0.758877,0.558853,0.81449,2.77149,0.020525,0.424165,0.548064,1.4836,1.51395,1.17532,2.14801,2.16371,0.536734,0.20805,2.26247,0.393927,1.15546,3.80855,0.062539,0.560503,3.57414,1.74579,0.403144,1.8321,0.521171,0.281189,2.58205,0.165068,0.21593,1.33553,2.64503,1.22495,0.95273,0.758475,0.0459459,0.0536204,2.62278,3.09329,0.743081,0.355141,3.07225,0.99825,0.949002,1.12931,1.44038,0.223063,0.0110416,2.39146,1.64444,0.935833,1.60383,0.529734,1.10079,0.362805,0.828244,2.59053,4.22452,0.686707,0.93297,3.42002,0.0990215,0.910626,0.453031,0.468966,2.71115,0.0979038,0.878076,0.580586,1.66404,2.94335,1.24273,1.59951,1.76682,0.486193,2.11952,2.20781,0.287483,0.00329437,3.36734,0.745545,4.66653,0.703305,3.2894,3.1307,0.324362,2.55482,0.777873,1.86771,1.24968,1.76596,0.422246,2.00153,3.43406,0.229743,0.130489,1.09743,2.68829,0.149078,0.925941,3.05519,2.28496,0.148349,1.08481,0.311612,0.871743,1.85163,0.623889,2.28469,1.59312,2.77352,0.738557,1.06193,0.625584,0.341742,2.5183,2.62624,2.89115,3.69003,0.861826,0.267442,1.87515,2.30547,0.962704,0.719618,1.73405,0.0303122,1.83939,2.17335,0.257229,1.65153,1.09087,0.755912,1.62221,1.2047,0.0661021,0.732646,2.63772,0.807986,0.478311,1.44167,3.3315,1.92262,1.10396,2.1373,2.94314,0.605313,0.589089,3.41747,1.81504,1.43408,0.91407,1.51313,1.78559,0.543597,0.0665963,0.848016,4.45425,0.375745,2.11154,2.0208,0.331684,0.872671,0.837342,1.35013,3.21238,0.285412,0.122489,0.0346207,1.26419,2.99989,0.387741,0.275004,0.779306,1.11022,0.949502,1.27925,0.00285642,0.139194,3.4054,0.815279,1.51069,0.314028,2.58728,0.931934,0.0288647,0.85859,1.04216,2.46264,0.431734,0.340156,0.22502,2.04931,2.61658,0.785478,0.0418276,2.72806,0.686128,0.114332,0.593864,0.881908,1.11965,0.0711848,0.935166,0.3465,1.27528,0.173925,0.981481,3.06613,1.73727,2.21345,0.102727,0.694715,0.258337,0.0755219,1.85269,3.1383,2.03729,2.63149,0.560199,0.010192,0.823945,1.67051,2.69149,0.669242,1.35486,1.68324,0.539925,1.1993,0.0525289,0.577206,0.48674,1.16002,0.269629,0.6042,0.215148,0.47513,1.01106,0.912632,0.722549,0.141901,6.56804,0.332258,1.45423,1.84598,1.82717,0.607185,0.573186,2.45562,1.12138,0.721186,1.19955,2.16764,2.1298,0.463881,0.178236,1.82724,3.83526,0.128018,0.762716,0.459628,0.192741,1.12438,1.60296,0.963624,0.888673,0.645426,0.701608,0.815011,0.682288,1.60415,0.127339,1.19198,0.86594,0.288694,1.62697,1.99353,0.0730802,0.0582204,1.01377,1.65823,1.42085,3.36974,1.82254,0.886563,0.110572,3.70404,1.13102,2.06875,2.42856,1.66822,0.726381,3.11729,1.03275,0.200753,0.104538,0.586497,0.485197,0.89011,1.04055,3.76498,0.289018,0.173404,0.913941,0.603713,0.771254,1.16629,1.70345,0.877945,0.199141,1.21614,2.2545,1.42328,0.443216,0.0665258,2.377,1.73775,2.94043,4.8777,0.528893,0.166291,2.38448,2.42238,1.55654,0.459104,0.535974,1.50026,1.24673,0.8198,0.0119287,1.99609,3.31431,2.55657,0.570893,1.3003,0.0539923,0.52299,1.41024,1.28207,0.630163,0.105739,5.2302,0.227859,1.18988,1.75374,2.91697,0.386402,0.277785,2.83866,2.86109,0.331147,1.23632,2.39778,1.80306,0.259275,0.0913708,2.5227,3.94372,0.183673,1.56848,2.52969,0.532227,1.35426,1.27724,0.66081,1.88493,0.0528896,0.384378,0.398594,1.99346,2.60009,0.319524,0.562004,0.493854,0.701511,1.39409,3.22114,0.00298254,0.000888258,2.58557,1.696,3.47152,1.68618,1.24888,1.71971,0.0914841,3.61332,1.02518,2.04624,0.821979,1.69475,0.152026,2.5869,3.31264,0.179382,0.226419,0.27463,0.567847,0.686924,0.527543,2.87715,0.480118,0.376092,0.896157,0.540858,0.364544,1.60616,1.842,2.10057,0.225728,1.059,1.49925,1.6298,0.26304,0.268784,3.2952,1.58951,2.43822,4.05653,0.189399,0.416057,1.68345,1.14149,0.173652,1.17457,1.47797,0.139466,1.87393,1.34337,0.0421757,1.9166,2.95541,1.46175,1.41858,0.866783,0,0.0498921,2.34725,2.31223,1.62391,0.375117,4.67734,1.96945,1.1104,0.41031,2.23528,0.0241686,0.108533,3.11117,1.68433,0.227382,1.86509,2.15314,0.75385,0.14333,0.0874208,2.50929,3.98529,0.110356,1.13896,1.43417,0.313859,1.31874,0.309791,1.05315,1.39793,0.283745,0.876908,0.435468,1.45572,4.32967,0.356555,1.59968,0.916453,0.208693,0.985789,2.3605,0.577597,0.0338435,2.86926,1.3007,4.79799,0.899941,0.919981,1.93226,0.287895,4.67877,1.19555,2.69553,1.09768,1.7825,1.0552,3.7481,1.68595,1.15395,0.121583,0.967309,1.42674,0.24203,1.80173,2.21328,0.455632,0.0445802,2.03796,0.194335,0.623538,0.673932,3.03886,1.03398,0.341118,0.814646,1.06452,2.10121,0.458083,0.307486,2.18301,2.82394,2.79605,4.71776,0.0169554,0.514094,1.82997,0.227555,1.4776,1.54838,1.73153,0.134113,0.53203,1.91166,1.06839,2.55417,2.87113,0.928262,0.717707,1.47587,0.0368642,0.395106,2.57313,2.36082,0.21963,0.323391,5.03808,0.082217,1.79875,1.31236,3.24022,0.0172295,0.314153,2.91945,3.53399,0.424727,2.75348,2.03163,2.58717,0.19609,0.163325,1.67908,4.54266,0.128016,1.19594,4.72996,0.151521,2.57415,0.585961,0.137761,3.29643,0.257186,0.867165,0.450505,1.81,2.83961,0.772029,0.758587,1.41232,0.355646,1.14016,2.61898,0.0082014,0,2.93451,1.90219,3.77763,1.32164,1.4126,2.36852,0.146113,2.66697,1.39908,2.6182,0.160965,1.32218,0.233872,4.31484,3.86704,0.212293,0.0289287,0.211306,0.948793,0.0361328,2.27068,2.3723,0.391514,0.132712,1.97043,0.661183,0.168071,1.03141,1.16468,1.77397,0.436936,1.85067,1.08954,1.75647,0.943836,0.312196,3.22857,2.07949,2.32178,4.79149,0.109242,1.39368,1.99084,0.881735,0.252822,1.66833,2.07736,0.235665,1.47536,2.16852,0.126876,1.65064,1.7142,1.02378,1.67983,0.853408,0.000578408,0.429056,1.95147,2.63166,0.514809,0.292234,6.86733,0.708982,1.07439,2.01243,1.70541,0.0322334,0.125035,2.63003,3.71278,0.252346,1.74815,1.72679,2.06691,0.478085,0.00483701,3.22095,4.07958,0.0830733,0.856439,1.46784,0.161831,1.89061,0.873866,0.556582,2.05569,0.800068,0.749068,1.25719,1.60501,1.70926,0.275427,1.74127,1.1998,0.342481,1.02666,2.29009,0.0140432,0.00846901,1.14974,2.9381,2.94817,1.66574,0.967391,2.07723,0.00166173,4.34336,1.6463,4.26598,0.295507,2.75264,0.581208,2.98656,1.74961,0.267475,0.00369199,0.515228,0.689112,0.215311,2.00409,1.85591,0.100661,0.215568,1.28994,0.443553,0.628079,1.4706,1.66099,0.469097,0.0821688,2.06333,1.43894,3.49428,1.17021,0.0637752,2.23233,1.96668,1.91407,4.57395,0.201747,1.19394,1.94423,1.59399,0.4392,1.26067,1.52161,0.432527,0.68314,0.536641,0.0698201,2.35767,2.47381,1.03315,0.914498,0.835524,0.00987089,1.05986,2.03397,2.89699,0.325294,0.00119871,6.21996,0.00835778,2.5288,2.36303,0.318735,0.496379,0.934812,3.41399,3.82237,0.600501,1.72741,1.0386,1.41984,0.81225,0.117677,3.28885,4.08034,1.41401,1.32952,2.14254,0.730038,0.948184,2.88913,1.35151,1.5385,1.24999,1.30351,2.03615,0.644665,1.51591,0.0244568,0.981655,0.301641,0.643772,1.83247,1.88254,0.108825,0.131312,1.1616,1.51908,2.31524,2.62664,1.71108,1.12073,0.00699734,4.42007,0.975613,2.27487,0.766412,2.54045,0.633935,3.36548,2.42673,1.16776,0.927902,0.710297,1.08234,2.17219,1.42854,3.42324,0.489319,0.301125,0.260754,1.54756,0.0837248,1.38027,0.686394,0.481088,0.567501,1.99992,3.4116,2.68451,1.27092,0.627177,2.89755,1.2995,0.600107,3.28343,0.0155647,0.63508,3.19827,3.25178,0.778291,1.28901,1.18572,1.21674,1.48707,0.20439,0.127491,2.70604,3.09741,1.14486,1.68527,0.38197,0.0627786,0.381863,2.5112,3.71608,1.92274,0.573895,3.72506,2.36487,0.619444,3.19856,3.11657,0.189383,0.180706,3.70595,0.91928,1.20836,1.04514,2.23861,0.245261,0.174829,0.00154734,1.3084,5.75153,1.18626,2.96441,1.93846,0.390841,0.375997,1.0069,0.364289,2.65606,0.102186,0.147101,0.00865398,2.66011,5.33546,1.41967,0.488059,0.293369,1.84573,2.50777,3.20392,0,0.0109018,1.09469,1.82154,2.8126,2.0021,1.60885,0.630014,0,2.09139,1.30937,3.7491,0.664126,0.123373,0.11209,1.19551,1.35644,0.606804,0.187993,1.10878,0.810724,0.0113032,1.7821,1.91432,0.149821,0.293472,0.222547,0.215981,0.556262,0.414154,2.70669,3.63663,2.35668,2.95834,0.267524,1.63889,0.187753,0.12711,3.3947,1.63793,1.35076,2.79242,0.86852,0,0.567694,0.799586,1.22088,2.3433,1.85411,0.88546,0.820162,0.647504,0.123085,0.758855,0.639487,0.812279,0.49055,0.598673,0.366934,0.603286,1.46246,0.445609,0.898841,0.586868,8.73982,1.02971,1.20077,1.94829,1.83092,0.0220182,0.261601,3.15512,3.30329,0.551034,2.94538,1.72373,1.97993,0.779542,0.0822677,2.9852,3.75193,0.144997,0.836988,1.69875,0.504745,2.41732,1.30603,0.608154,1.05779,1.73576,0.36456,1.38593,2.02989,2.08777,0.164489,1.48814,1.37225,0.809428,0.972248,2.35817,0.251876,0.0687024,0.744842,3.6117,3.38709,1.54124,2.41988,2.61364,0.361874,6.00888,1.67688,4.14727,0.158881,2.55793,0.111752,2.84868,1.40215,0.489402,0.271234,0.513271,0.497959,0.11324,3.00346,1.56173,0.205529,0.497307,2.00518,0.614836,0.908684,1.41282,3.29453,0.969603,0.547435,2.81178,2.94911,4.6194,1.27235,0.716387,3.19781,1.2433,0.992348,3.85399,0.407022,1.57019,2.38822,0.998639,0.221525,2.03134,1.33124,0.617507,1.30361,0.708646,0.759568,2.45856,3.54255,0.812994,0.204393,0.436802,0.0270976,1.53677,1.46702,3.65913,0.466971,0.251133,6.95959,0.0333794,1.40907,2.13019,1.31999,0.292266,0.181486,3.14676,2.28961,0.490431,1.35744,1.39209,0.084761,0.816483,0.0671694,2.58854,3.1498,0.61749,1.58415,1.30812,0.149177,1.0595,2.15829,0.968012,2.30158,1.45327,1.81182,2.29565,0.689341,2.11324,0.112973,1.12802,0.219814,0.877145,0.537364,2.30229,0.153106,0.00261487,0.789265,2.10999,1.36101,1.68151,1.41636,1.05213,0.0044397,4.59925,0.991876,3.37134,0.220099,3.13481,1.31909,2.52578,1.33094,0.627478,1.09398,0.353905,0.799904,1.02119,2.29109,3.5297,0.463236,0.436492,1.10343,2.8353,0.123138,0.790711,1.18252,0.823587,1.04403,1.73025,2.1707,3.33867,1.07313,1.18701,3.20581,1.10758,0.958429,3.31983,0.0549896,1.10189,2.74074,2.45557,1.44002,2.93435,0.970824,0.895067,0.71077,0.124661,0.765065,1.71309,3.49349,0.65441,0.672074,0.354003,0.0919241,0.506844,1.93992,3.61502,0.446271,0.3745,4.5924,1.23532,0.241716,1.43527,1.75488,0.539591,1.15005,2.69045,1.74025,0.110312,1.61856,1.47826,0.826395,0.502828,0.422565,3.59079,3.1942,1.72556,0.845413,0.360287,0.274238,0.797396,1.93063,1.06382,1.0984,1.65286,0.27872,0.481458,2.84992,2.61509,0.0257836,0.255934,1.07027,0.218782,3.23025,3.41362,0.0452738,0.0335229,0.955382,1.47124,4.39593,1.03073,2.08899,0.991484,0,2.82603,0.44292,2.68303,1.31588,0.534263,0.0979332,1.1765,1.88004,1.33126,0.51734,1.27743,0.82657,0.43469,1.03023,1.61289,0.637126,0.246193,0.66784,0.261125,0.202319,2.98351,1.77656,1.7038,0.869062,2.82035,0.667508,1.84661,0.584493,0.0369572,2.16519,1.93956,0.818924,2.80943,0.160816,0.135175,0.51689,0.825145,1.074,3.13664,1.27932,0.117167,0.531846,0.91422,0.00876264,0.795256,2.56697,1.44093,1.00984,0.455143,0.0774129,0.100801,1.35025,1.91794,0.257685,0.174504,6.99856,0.132885,2.00133,1.88682,1.80335,0.785178,1.18023,3.76684,3.1939,0.565615,2.31022,2.75415,1.73442,0.41269,0.291212,3.30776,3.32774,0.126696,0.606345,1.4245,0.384673,1.15666,1.35154,0.979975,0.766484,2.00923,2.16857,2.27555,0.398939,1.1732,0.138618,1.23992,0.447373,0.616605,1.00978,3.01147,0.0924396,0.0674492,1.66365,4.12297,1.79219,1.82543,1.29482,2.48605,0.0176754,5.19097,0.986877,2.57266,1.57903,3.02494,0.390604,4.12094,1.05613,0.470683,1.23008,0.247726,1.68997,1.28396,1.19812,3.29264,0.0823708,0.131314,1.13449,1.04483,0.462526,1.5643,2.42756,0.356194,0.295758,2.51568,4.19365,3.52803,1.47537,0.420603,3.22312,2.51929,3.17259,3.80346,1.02889,0.487563,3.3617,1.2777,0.312199,1.22358,0.259978,0.480599,1.02987,0.188222,0.151817,1.71614,3.593,2.10357,0.849569,0.942012,0.042104,0.509642,1.40164,3.83614,0.184156,0.221195,5.11221,1.0615,1.43651,1.90652,1.32315,0.0532484,0.439429,1.16011,1.90052,0.532259,1.15372,1.07451,0.656496,0.354654,0.384508,4.31573,2.85546,0.325853,0.566785,2.11056,1.70249,2.32448,0.330881,0.140268,1.50035,0.0365847,0.707865,1.58583,0.280018,3.56375,0.158765,2.68258,0.408648,1.22888,1.0269,1.02123,0.407549,0.233926,1.51785,3.37066,2.94239,1.27334,1.87556,3.5836,0.133146,5.41036,1.91804,3.84347,0.619772,2.98138,0.331498,2.28954,1.83127,0.710477,0.604542,1.88388,1.43014,0.224193,0.667574,3.3583,0.680597,0.304905,1.52853,0.296768,0.588059,1.01646,3.68636,2.01778,0.569689,0.464187,2.63521,5.09287,0.647101,0.151575,1.0164,0.403723,0.650813,1.84017,0.0552277,2.31462,2.00682,1.79648,0.236092,1.40605,2.18151,0.082038,0.504618,0.31107,0.398998,1.36783,3.16728,1.43395,0.365569,1.51801,0.0374257,0.849416,0.351533,2.94804,0.974565,0.861189,5.25728,1.02628,0.873751,2.13586,2.61117,0.703581,0.0308287,2.81073,1.29858,1.12471,0.25401,1.83754,1.22727,1.3554,0.0133957,0.857975,3.37658,0.100097,0.995698,0.856568,0.0534877,1.25937,0.299307,0.51188,1.93843,0.844398,0.459178,0.082004,1.25152,1.87234,0.852074,1.10209,0.0674482,1.17181,0.702495,1.02526,0.0325532,0.026385,2.39219,0.415488,1.07483,0.60647,1.98251,0.304612,0.022602,2.08917,1.07849,1.73861,0.589407,0.532113,0.741026,2.9143,1.63904,0.408422,0.097312,0.769933,0.521111,0.45975,0.5108,0.961989,0.21067,0.00781904,0.431027,1.33048,1.45045,0.258897,0.41484,2.11862,1.19518,1.78641,0.192141,0.124088,0.00156289,0.125198,1.91346,2.21974,1.95588,3.44095,0.166916,0.00183741,1.91274,1.71632,2.48322,0.294634,0.865751,2.8646,0.642169,1.80255,0.098533,0.583954,0.872587,1.32031,0.517327,0.975735,0.0171696,0.60458,0.940533,1.24504,0.663863,0.111222,3.36156,0.589114,0.958669,1.90041,2.92993,0.261462,0.312009,2.1037,2.22552,0.867818,2.59185,1.03615,1.82114,0.324595,0.457084,2.42399,4.60721,0.708082,1.73009,4.23178,0.44775,2.33453,0.453969,0.344531,2.91467,0.121897,0.114587,0.0283162,2.26917,3.26856,0.847829,0.408003,1.85722,0.238222,2.36447,2.69791,0.0400725,0.0149142,2.89335,0.929881,4.48752,0.794999,2.79169,2.13796,0.270315,2.84102,0.44938,1.6643,0.504644,0.379103,0.457211,2.70984,4.60011,0.656926,0.192714,0.40539,1.16158,0.0783707,2.54728,3.43145,1.0045,0.1937,1.56081,1.80243,0.372579,1.26293,1.02895,2.1259,2.32369,2.18291,0.625517,1.5817,0.675317,0.897016,3.24683,1.99096,2.37221,3.13066,0.00317319,0.286411,2.29131,1.21139,1.66276,0.589232,1.81656,0.144804,2.01761,2.55133,0.0883723,0.728125,0.771948,0.999863,1.90504,1.11528,0.352635,1.09363,2.05708,1.93355,0.253309,0.116556,4.68634,0.223883,1.67129,1.93171,1.97933,0.00383753,0.280216,1.40586,3.09517,0.213545,1.66106,2.13006,2.98515,0.271883,0.0522383,3.09997,3.51699,0.141877,0.651085,2.91886,0.343161,2.02151,1.13365,1.30164,2.875,1.17437,1.1995,0.855122,1.28774,2.12559,0.458248,2.22088,1.93161,0.952154,2.20498,3.26444,0.250488,0.00407381,1.85257,2.29864,2.46343,0.561894,1.62252,2.49955,0.309508,2.61178,1.33307,3.72915,0.643366,3.51015,0.562023,3.57493,3.10522,0.328935,0.119799,0.897191,1.89314,0.119459,2.02567,2.35492,1.34613,0.383373,2.32709,1.71177,0.134538,0.375472,0.889519,1.85584,1.17967,1.46707,1.70745,2.37053,0.720302,0.472549,2.28538,2.25924,3.50832,5.28705,0.84084,0.875018,3.52472,1.67576,0.387816,0.449753,2.36395,0.0993472,1.84713,1.42916,0.0308633,2.50824,1.53989,1.30368,0.730894,0.759442,0.111097,1.70707,1.80198,2.37097,0.812136,0.120098,4.86855,1.24059,2.86626,2.44454,0.765356,0.204651,1.52859,1.3326,0.527706,0.962032,0.919145,2.30383,1.64707,0.193598,0.293331,2.66928,2.69068,0.708493,0.227696,1.3636,1.16759,0.13128,2.24242,0.647674,1.27019,0.198226,0.660604,0.416808,1.01276,3.12247,0.387053,1.73044,2.10827,0.6178,0.565228,1.56219,0.447435,0.4185,0.961366,2.13676,2.59692,2.04515,0.937313,0.196836,0.00168212,2.53829,0.734472,2.94557,0.865219,3.53086,4.39869,3.41571,3.60505,0.38702,0.440112,2.58771,2.84441,0.811425,2.02781,3.80813,0.565964,0.126715,0.61499,2.82767,0.0637746,0.335573,0.393531,0.976523,0.75281,0.915117,1.70388,1.42754,0.468946,0.0694357,1.1903,2.43293,2.69881,2.03315,0.0570462,1.02948,6.27247,3.65036,0.71304,1.27246,0.271345,0.530685,1.8314,0.463416,0.141136,2.70229,2.4328,1.15523,0.550393,0.0891806,0.0283122,0.325709,2.4861,2.15164,0.898232,0.210259,3.1975,0.997738,0.191175,1.55085,1.71338,0.0577143,2.03634,0.873271,1.88994,0.713991,0.665725,0.109941,1.71841,0.186993,0.268377,3.74585,2.38165,0.328973,0.538873,2.25382,1.36053,4.35583,0.320201,0.641422,1.44763,0.416863,1.08291,0.267269,0.679639,1.50867,0.542548,0.490702,1.41823,0.432038,1.9593,1.10792,0.105017,0.0269861,4.09409,0.96845,6.05413,0.186122,3.53427,1.90091,0.421887,1.95093,0.348475,1.01732,2.2948,1.17767,0.642385,1.31366,2.82119,1.42115,0.539733,2.13342,2.80341,1.1287,0.10518,1.32477,1.47408,0.525935,0.423997,1.4702,0.819589,0.996782,0.507418,0.792482,0.117183,1.16909,0.60062,0.798,1.19358,0.211437,2.14669,0.834304,2.43454,2.85052,0.211237,0.51729,2.33352,0.254928,0.25645,0.38028,1.65891,0.0968816,1.5055,1.407,0.0182651,1.11253,0.616646,1.46619,1.64317,0.659965,0.00739392,0.163936,1.33386,1.29004,2.11317,1.03964,3.65167,1.30471,2.29612,1.38089,2.01531,0.0406192,0.331822,1.60388,1.93104,0.812058,1.03308,1.75459,0.86162,0.152441,0.2124,1.42098,2.72777,0.214831,0.677203,2.25576,0.731401,2.00064,1.31908,1.27204,1.79025,0.149946,0.0791397,0.266467,1.14631,3.07066,1.06992,0.606958,0.239376,0.956309,2.76435,1.11536,0.000791357,0.0550667,2.86583,1.17624,6.24452,1.18306,1.69543,1.13793,0.320872,3.57882,1.89732,0.935178,0.783058,1.5818,1.3382,2.74709,1.92389,2.03014,0.229265,1.39986,1.77446,0.403916,1.45285,2.01894,0.317412,0.105496,1.13236,0.569124,0.535925,1.0535,1.46998,1.68174,0.253648,0.816185,1.05546,1.41445,1.19722,0.0818693,2.06625,0.997922,0.737934,2.35428,0.0058167,0.145901,1.48014,1.18936,1.02932,1.85306,1.51759,0.428515,1.52003,0.803402,0,1.38276,1.61682,1.30886,1.18338,0.586477,0.0902724,0.831467,3.56661,1.31524,0.876652,0.0631791,4.7749,1.07338,1.69869,1.58828,0.25187,0,0.947262,1.35063,1.7544,0.335397,0.602412,1.81719,0.83498,0.0757302,0.0206605,1.9908,2.08454,0.59629,0.108872,0.0560465,0.0712629,0.71132,1.64435,0.262862,1.37295,0.0695136,0.742151,0.55555,1.31163,4.53364,0.144293,1.48394,0.706466,0.220706,0.92508,2.26944,0.495825,0.0551027,0.185759,1.27448,4.2655,2.43653,0.900594,0.135071,0.0394816,4.60703,0.308671,2.70279,1.00552,2.895,2.59958,2.97339,3.17499,1.61685,0.646072,0.582307,3.176,1.11632,0.462667,2.5227,0.720804,0.0654785,1.10443,3.22618,0.166501,0.0845012,0.269973,0.851177,0.141087,1.16259,0.60383,1.48998,0.0792487,0.130158,0.53897,0.779274,1.28965,1.09309,0.00114253,0.760716,3.85795,0.544777,0.670001,1.39091,1.41372,0.537886,1.09387,1.16429,0.00110852,1.36474,1.96186,1.17676,1.34232,0.392489,0.0028248,0.820288,3.33081,1.9707,0.141534,1.32752,4.32412,1.36519,0.7822,1.24212,1.29921,0.00449213,2.96646,0.890678,0.947418,0.524281,1.01502,1.19485,2.00927,0.140004,0.0336756,3.82305,2.6648,0.808076,0.175849,0.379751,1.27281,1.64596,0.85119,0.304467,1.05965,0.88702,0.673684,0.412961,1.16726,5.23773,0.181942,1.30786,2.51306,0.0140293,1.52968,1.47284,1.31953,0.115669,1.00374,2.17982,7.58773,1.60782,2.21347,1.31729,0.489387,2.94165,0.132097,1.60515,0.845589,1.8837,2.59501,1.74942,3.93922,2.10887,0.885373,1.38633,3.0048,0.850073,0.156651,2.41793,1.78144,0.102717,1.45141,2.25859,0.553741,0.352278,1.39596,0.605364,0.722566,2.26311,0.989234,3.1654,1.32028,0.131032,0.430185,0.63604,1.89814,0.840887,0.0145833,1.12403,3.63153,0.461552,0.338114,1.41944,1.2121,0.0547536,1.15151,2.13569,0.183726,1.35507,2.37558,0.880525,2.10045,0.313574,0.158706,0.819517,2.97787,1.5779,0.617671,0.468382,4.2573,2.68184,0.405576,1.66272,2.19618,0.454411,0.112507,2.0079,0.605678,0.0785262,0.808339,0.763635,0.514089,0.251851,0.199175,2.23691,3.86687,1.66287,1.84195,1.37314,0.902038,0.777063,1.49601,0.37944,1.16528,0.189918,0.0748914,0.0220248,2.78947,5.58867,1.3001,0.0293565,1.03021,0.822084,2.649,3.1179,0.0243542,0.0170031,2.31794,2.32811,4.56341,1.56396,2.14732,0.507619,0.0942021,1.91325,0.484128,3.22408,2.23196,0.548829,0.251454,0.504564,1.64646,1.11299,0.448078,0.00928187,1.99006,0.222099,0.00566369,2.7882,0.721351,0.716473,1.48018,0.348745,0.0468219,1.15817,1.77556,1.69238,1.53411,2.84495,0.437648,1.22891,0.149386,0.50927,1.36986,0.939152,4.01539,1.81847,0.116702,0.0480713,0.592695,0.635974,0.522637,1.95381,1.00631,0.00983327,2.25001,0.568138,0,0.0477794,0.946258,1.89782,2.10002,0.680816,0.118034,0.289923,0.534844,1.14276,0.17787,0.0671842,3.74189,0.598143,1.46102,1.59229,0.573945,0.0248033,0.18219,2.10823,0.628571,0.277635,0.55642,1.37147,0.189093,0.304289,0.021872,0.766755,3.72452,0.849935,0.742728,0.336274,0.727936,0.875328,1.5026,0.148357,1.73686,0.0130925,1.14383,0.142981,0.537176,3.29533,0.0895149,2.22682,0.590893,0.123592,0.956116,1.90591,0.335271,0.144899,0.965926,0.810877,3.06673,2.55084,0.898227,0.188399,0.218642,4.21891,0.949401,2.19536,2.56585,2.28743,1.50976,4.43636,2.14383,1.21491,0.238991,0.119743,3.13956,0.7063,0.698713,1.79337,0.713203,0.239497,1.12453,2.11674,0.215012,0.148944,0.371673,0.688459,0.584677,0.668979,1.1467,1.24969,0.365664,0.00402623,0.913581,1.31169,0.679478,3.68138,0.128535,0.452051,2.87009,0.942447,0.473775,0.990575,0.495187,0.543496,1.7609,0.644544,0.352128,2.7088,1.69498,2.96797,0.55632,0.00166314,0.037931,0.867557,2.67301,1.17689,0.590685,0.0582472,4.19662,0.179836,1.69836,1.31358,1.49582,0.406852,1.29723,1.39667,0.477145,0.447816,1.19232,2.0995,1.4763,0.0372801,0.558062,2.76918,3.73471,0.308949,1.57176,0.553801,0.365138,1.15407,2.44019,0.297839,0.90976,0.615724,0.813148,0.463448,0.654772,2.71199,0.237753,0.909471,1.26044,0.74914,1.39616,2.38322,0.167203,0.0126121,0.225377,1.5502,2.26877,2.24333,0.973906,0.709005,0.0451798,3.05034,0.189501,2.54107,1.02928,2.93998,1.85342,3.96938,2.19005,0.321954,0.162484,0.813151,2.76814,1.26534,2.03445,3.33698,0.651145,0.120352,1.04319,2.61028,0.264721,0.305187,0.897256,1.02103,0.58373,1.0037,1.71082,1.52767,0.668524,0.0208841,1.75831,1.83232,3.35193,4.0909,0.294792,0.394428,3.74717,3.01426,0.839781,1.40738,0.364597,0.526923,1.33506,0.342368,0.0127373,2.4178,2.62937,2.66246,0.298979,0.372313,0.149647,0.286726,2.90082,1.18625,0.520868,0.100791,3.63484,0.177705,1.7658,2.12389,0.915643,0.0530187,0.868138,1.60942,0.807279,0.734285,1.02882,2.66665,1.09878,0.207759,0.0152958,1.39496,2.98445,0.345051,1.99615,1.6004,0.905284,0.662348,2.13196,0.395938,2.09453,0.133119,2.15142,0.367175,0.262221,2.86103,0.118603,1.45757,0.690362,0.78626,2.20591,2.73903,0.103179,0.00699327,0.469433,1.13614,1.86629,2.94935,0.47088,0.0390566,0.00386581,4.85439,0.915615,1.80378,1.58185,2.70266,1.99958,3.65516,2.39768,0.321273,0.287234,0.443654,3.43264,0.878042,1.2525,4.4268,0.624393,0.499974,0.693653,3.4787,0.0673157,0.22458,1.16722,2.52149,0.631295,0.267162,1.50803,0.86045,0.36575,0.0676028,1.27223,0.49721,1.87069,4.34851,0.0029013,0.405826,3.7857,3.4135,0.674171,1.74218,0.42268,0.708406,1.79269,0.464328,0.139114,2.36464,1.41014,2.57096,0.355134,0.604991,0.391504,0.319167,2.55726,0.739781,1.02642,0.205901,3.9757,1.18295,1.24104,1.63107,1.51492,0.0216242,0.0775006,2.32664,2.68503,0.487947,1.85133,1.7975,1.55172,0.089643,0.0868945,2.00716,3.74157,0.401673,1.4517,2.23622,0.503565,0.864392,0.640068,0.360182,2.73337,0.0533497,0.796302,0.0789252,0.493384,3.57012,0.750832,0.837013,1.6331,0.361186,0.937966,3.20455,0.354754,0,1.34457,2.48876,3.36057,1.98883,0.920533,0.856483,0.0131143,3.56402,0.398756,3.64401,0.859795,2.60172,0.820675,3.51612,3.01152,0.256439,0.0793038,0.217291,2.4773,0.0733517,1.59288,2.78761,0.257391,0.0951554,0.812235,1.26376,0.108545,0.287873,1.1668,1.39608,1.13534,1.01982,1.83664,2.25875,0.785398,0.145803,0.978086,1.04681,3.51331,4.46406,0.338991,0.440636,2.46854,0.93062,0.387732,0.645399,2.80796,0.110147,2.04517,2.49583,0.257262,0.930653,0.876773,1.16229,1.00568,0.579815,0.02377,0.9081,1.58965,1.33536,0.390522,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\ndata = torch.Tensor(vue_clip_emb).view(1, -1, 100)\r\nlengths = torch.tensor([74])\r\noutput = ts_model_multihead.forward(data=data, lengths=lengths)\r\nprint(output)\r\n"
29992,"CUDNN_STATUS_NOT_SUPPORTED on Backward pass when input tensor was permuted (libtorch)## \U0001f41b Bug\r\n\r\nBetween 2460dced8f and 18bdf97dbb something broke in PyTorches internals, as I'm getting a CUDNN_STATUS_NOT_SUPPORTED error when trying to train my C++ implementation of Pointnet++.\r\n\r\nThis results from a permutation from BHWC to BCHW and not calling contiguous afterwards.\r\n\r\nThe forward pass works, but on the backward pass the following error is thrown:\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. (getWorkspaceSize at ../aten/src/ATen/native/cudnn/Conv.cpp:632)\r\n```\r\n\r\nThe exact same code works in 2460dced8f flawlessly.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```\r\n\r\nThread 1 ""qad_testing"" received signal SIGABRT, Aborted.\r\n0x00007fffe1aa1428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace\r\n#0  0x00007fffe1aa1428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fffe1aa302a in __GI_abort () at abort.c:89\r\n#2  0x00007fffe20db84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe20d96b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe20d9701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe20d96aa in std::rethrow_exception(std::__exception_ptr::exception_ptr) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe811d0c7 in torch::autograd::Engine::execute_with_graph_task(std::shared_ptr<torch::autograd::GraphTask>, std::shared_ptr<torch::autograd::Node>) () from /usr/local/torch/lib/libtorch.so\r\n#7  0x00007fffe81246aa in torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /usr/local/torch/lib/libtorch.so\r\n#8  0x00007fffe810fc7a in torch::autograd::run_backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool) () from /usr/local/torch/lib/libtorch.so\r\n#9  0x00007fffe81109f9 in torch::autograd::backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, c10::optional<bool>, bool) () from /usr/local/torch/lib/libtorch.so\r\n#10 0x00007fffe81533ea in torch::autograd::Variable::backward(at::Tensor const&, bool, bool) const () from /usr/local/torch/lib/libtorch.so\r\n#11 0x0000000000463759 in c10::KernelFunction::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x10eb4f0) at /usr/local/torch/include/ATen/core/boxing/KernelFunction.h:95\r\n#12 0x000000000045eb4e in void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}::operator()(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&) const (__closure=0x7fffffffd250, backendFallbackKernels=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:211\r\n#13 0x000000000046399c in c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > >::read<void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}>(void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}&&) const (this=0x7ffff7d8ad30 <c10::Dispatcher::singleton()::_singleton+144>,\r\n    readFunc=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xce266>) at /usr/local/torch/include/c10/util/LeftRight.h:74\r\n#14 0x000000000045ebfa in c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x7ffff7d8aca0 <c10::Dispatcher::singleton()::_singleton>, dispatchTable=..., backendFallbackKernels_=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:212\r\n#15 0x000000000045ad3e in void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}::operator()(c10::DispatchTable const&) const (__closure=0x7fffffffd390, dispatchTable=...) at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:201\r\n#16 0x0000000000463b08 in c10::LeftRight<c10::DispatchTable>::read<void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}>(void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}&&) const (this=0x10cbde8, readFunc=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xce362>)\r\n    at /usr/local/torch/include/c10/util/LeftRight.h:74\r\n#17 0x000000000045ec50 in c10::impl::OperatorEntry::readDispatchTable<void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}>(void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}&&) const (this=0x10cbd70, functor=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xc9482>)\r\n    at /usr/local/torch/include/ATen/core/dispatch/OperatorEntry.h:32\r\n#18 0x000000000045addf in c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x7ffff7d8aca0 <c10::Dispatcher::singleton()::_singleton>, op=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:202\r\n#19 0x000000000045567a in at::Tensor::backward (this=0x7fffffffd560, gradient=..., keep_graph=false, create_graph=false) at /usr/local/torch/include/ATen/core/TensorMethods.h:66\r\n#20 0x00000000004514ad in main (argc=1, argv=0x7fffffffd698) at /home/tobi/coar_ws/src/pointnetpp/src/qad_testing.cpp:46\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n## Expected behavior\r\n\r\nGetting no error.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+18bdf97\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.11.0\r\n[pip3] torch==1.4.0a0+2b5213d\r\n[conda] Could not collect\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0a0+18bdf\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): compiled from source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.5.2\r\n - CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.2\r\n - GPU models and configuration: GeForce GTX 1070\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168",high priority|module: cudnn|triaged,VitalyFedyunin,"## \U0001f41b Bug\r\n\r\nBetween 2460dced8f and 18bdf97dbb something broke in PyTorches internals, as I'm getting a CUDNN_STATUS_NOT_SUPPORTED error when trying to train my C++ implementation of Pointnet++.\r\n\r\nThis results from a permutation from BHWC to BCHW and not calling contiguous afterwards.\r\n\r\nThe forward pass works, but on the backward pass the following error is thrown:\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. (getWorkspaceSize at ../aten/src/ATen/native/cudnn/Conv.cpp:632)\r\n```\r\n\r\nThe exact same code works in 2460dced8f flawlessly.\r\n\r\n## To Reproduce\r\n\r\n```cpp\r\n#include <torch/torch.h>\r\n#include <boost/format.hpp>\r\n\r\nclass testModuleImpl : public torch::nn::Module {\r\n  public:\r\n    testModuleImpl(int in_channel, torch::IntArrayRef mlp) {\r\n      int last_channel = in_channel;\r\n      for(int i:mlp) {\r\n        std::string tmpNameLayer;\r\n        tmpNameLayer = (boost::format(""Layer_%i_conv"") % i).str();\r\n        auto tmp_conv = register_module(tmpNameLayer, torch::nn::Conv2d(last_channel, i, 1));\r\n        last_channel = i;\r\n        tmpNameLayer = (boost::format(""Layer_%i_bn"") % i).str();\r\n        auto tmp_bn = register_module(tmpNameLayer, torch::nn::BatchNorm2d(last_channel));\r\n        conv_layers_.push_back(tmp_conv);\r\n        bn_layers_.push_back(tmp_bn);\r\n\r\n      }\r\n\r\n    }\r\n    torch::Tensor forward(torch::Tensor x) {\r\n      for (int i=0;i<conv_layers_.size();i++){\r\n        x = bn_layers_[i](conv_layers_[i](x));\r\n      }\r\n      return x;\r\n\r\n    }\r\n\r\n  private:\r\n      std::vector<torch::nn::Conv2d> conv_layers_;\r\n      std::vector<torch::nn::BatchNorm2d> bn_layers_;\r\n\r\n};\r\nTORCH_MODULE(testModule);\r\n\r\nint main(int argc, char** argv){\r\n\r\n  testModule model(128, torch::IntArrayRef({256,512,32}));\r\n  auto a = torch::rand({16,32,16,128}).to(torch::kCUDA).permute({0,3,1,2});\r\n  model->to(torch::kCUDA);\r\n\r\n  auto b = model(a).sum();\r\n\r\n  std::cout << b << ""\\n"";\r\n\r\n  b.backward();\r\n  std::cout << ""done"" << ""\\n"";\r\n\r\n\r\n  return 1;\r\n}\r\n```\r\n\r\n\r\n<details><summary>Stack trace</summary>\r\n<p>\r\n\r\n```\r\n\r\nThread 1 ""qad_testing"" received signal SIGABRT, Aborted.\r\n0x00007fffe1aa1428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n54      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\r\n(gdb) backtrace\r\n#0  0x00007fffe1aa1428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\r\n#1  0x00007fffe1aa302a in __GI_abort () at abort.c:89\r\n#2  0x00007fffe20db84d in __gnu_cxx::__verbose_terminate_handler() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#3  0x00007fffe20d96b6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#4  0x00007fffe20d9701 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#5  0x00007fffe20d96aa in std::rethrow_exception(std::__exception_ptr::exception_ptr) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#6  0x00007fffe811d0c7 in torch::autograd::Engine::execute_with_graph_task(std::shared_ptr<torch::autograd::GraphTask>, std::shared_ptr<torch::autograd::Node>) () from /usr/local/torch/lib/libtorch.so\r\n#7  0x00007fffe81246aa in torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) () from /usr/local/torch/lib/libtorch.so\r\n#8  0x00007fffe810fc7a in torch::autograd::run_backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool, bool, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, bool) () from /usr/local/torch/lib/libtorch.so\r\n#9  0x00007fffe81109f9 in torch::autograd::backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, c10::optional<bool>, bool) () from /usr/local/torch/lib/libtorch.so\r\n#10 0x00007fffe81533ea in torch::autograd::Variable::backward(at::Tensor const&, bool, bool) const () from /usr/local/torch/lib/libtorch.so\r\n#11 0x0000000000463759 in c10::KernelFunction::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x10eb4f0) at /usr/local/torch/include/ATen/core/boxing/KernelFunction.h:95\r\n#12 0x000000000045eb4e in void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}::operator()(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&) const (__closure=0x7fffffffd250, backendFallbackKernels=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:211\r\n#13 0x000000000046399c in c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > >::read<void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}>(void c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::DispatchTable const&, c10::LeftRight<ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > > const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(ska::flat_hash_map<c10::TensorTypeId, c10::KernelFunction, std::hash<c10::TensorTypeId>, std::equal_to<c10::TensorTypeId>, std::allocator<std::pair<c10::TensorTypeId, c10::KernelFunction> > > const&)#1}&&) const (this=0x7ffff7d8ad30 <c10::Dispatcher::singleton()::_singleton+144>,\r\n    readFunc=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xce266>) at /usr/local/torch/include/c10/util/LeftRight.h:74\r\n#14 0x000000000045ebfa in c10::Dispatcher::doCallUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x7ffff7d8aca0 <c10::Dispatcher::singleton()::_singleton>, dispatchTable=..., backendFallbackKernels_=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:212\r\n#15 0x000000000045ad3e in void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}::operator()(c10::DispatchTable const&) const (__closure=0x7fffffffd390, dispatchTable=...) at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:201\r\n#16 0x0000000000463b08 in c10::LeftRight<c10::DispatchTable>::read<void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}>(void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}&&) const (this=0x10cbde8, readFunc=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xce362>)\r\n    at /usr/local/torch/include/c10/util/LeftRight.h:74\r\n#17 0x000000000045ec50 in c10::impl::OperatorEntry::readDispatchTable<void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}>(void c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&, bool, bool) const::{lambda(c10::DispatchTable const&)#1}&&) const (this=0x10cbd70, functor=<unknown type in /home/tobi/coar_ws/devel/.private/pointnetpp/lib/pointnetpp/qad_testing, CU 0x0, DIE 0xc9482>)\r\n    at /usr/local/torch/include/ATen/core/dispatch/OperatorEntry.h:32\r\n#18 0x000000000045addf in c10::Dispatcher::callUnboxedOnly<void, at::Tensor const&, at::Tensor const&, bool, bool> (this=0x7ffff7d8aca0 <c10::Dispatcher::singleton()::_singleton>, op=...)\r\n    at /usr/local/torch/include/ATen/core/dispatch/Dispatcher.h:202\r\n#19 0x000000000045567a in at::Tensor::backward (this=0x7fffffffd560, gradient=..., keep_graph=false, create_graph=false) at /usr/local/torch/include/ATen/core/TensorMethods.h:66\r\n#20 0x00000000004514ad in main (argc=1, argv=0x7fffffffd698) at /home/tobi/coar_ws/src/pointnetpp/src/qad_testing.cpp:46\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n## Expected behavior\r\n\r\nGetting no error.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+18bdf97\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.11.0\r\n[pip3] torch==1.4.0a0+2b5213d\r\n[conda] Could not collect\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0a0+18bdf\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): compiled from source\r\n - Build command you used (if compiling from source): ```CFLAGS=' -D_GLICXX_USE_CXX11_ABI ' USE_OPENCV=1 USE_CUDA=1 MAX_JOBS=7 BUILD_TEST=0 python3 setup.py install ```\r\n - Python version: 3.5.2\r\n - CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.2\r\n - GPU models and configuration: GeForce GTX 1070\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168","cpp\r\n#include <torch/torch.h>\r\n#include <boost/format.hpp>\r\n\r\nclass testModuleImpl : public torch::nn::Module {\r\n  public:\r\n    testModuleImpl(int in_channel, torch::IntArrayRef mlp) {\r\n      int last_channel = in_channel;\r\n      for(int i:mlp) {\r\n        std::string tmpNameLayer;\r\n        tmpNameLayer = (boost::format(""Layer_%i_conv"") % i).str();\r\n        auto tmp_conv = register_module(tmpNameLayer, torch::nn::Conv2d(last_channel, i, 1));\r\n        last_channel = i;\r\n        tmpNameLayer = (boost::format(""Layer_%i_bn"") % i).str();\r\n        auto tmp_bn = register_module(tmpNameLayer, torch::nn::BatchNorm2d(last_channel));\r\n        conv_layers_.push_back(tmp_conv);\r\n        bn_layers_.push_back(tmp_bn);\r\n\r\n      }\r\n\r\n    }\r\n    torch::Tensor forward(torch::Tensor x) {\r\n      for (int i=0;i<conv_layers_.size();i++){\r\n        x = bn_layers_[i](conv_layers_[i](x));\r\n      }\r\n      return x;\r\n\r\n    }\r\n\r\n  private:\r\n      std::vector<torch::nn::Conv2d> conv_layers_;\r\n      std::vector<torch::nn::BatchNorm2d> bn_layers_;\r\n\r\n};\r\nTORCH_MODULE(testModule);\r\n\r\nint main(int argc, char** argv){\r\n\r\n  testModule model(128, torch::IntArrayRef({256,512,32}));\r\n  auto a = torch::rand({16,32,16,128}).to(torch::kCUDA).permute({0,3,1,2});\r\n  model->to(torch::kCUDA);\r\n\r\n  auto b = model(a).sum();\r\n\r\n  std::cout << b << ""\\n"";\r\n\r\n  b.backward();\r\n  std::cout << ""done"" << ""\\n"";\r\n\r\n\r\n  return 1;\r\n}\r\n"
29984,"Some cublas functions don't handle inputs with zero strides## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n## Expected behavior\r\n\r\nNo exception is raised.\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.1+cu100\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.3.1+cu100\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.4.2+cu100\r\n[conda] Could not collect\r\n```\r\n\r\nThis code was run on Google Colab. The bug also reproduces on my server (Ubuntu 16.04LTS, 4x GTX1080Ti GPUs).\r\n\r\n## Additional context\r\n\r\nThe bug doesn't occur if:\r\n1. If I replace `sum()` with `mean()`\r\n\r\n\r\n2. If I use a different shape of the tensor\r\n\r\n\r\n3. If I run the code on CPU\r\n\r\nTherefore, I believe that this is not intended behavior.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen",high priority|triaged|module: cublas,zasdfgbnm,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ntorch.set_default_tensor_type('torch.cuda.FloatTensor')\r\nx = nn.Parameter(torch.ones(2, 2))\r\n(x @ torch.ones(2)).sum().backward()\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-11-c3b66f275e9a> in <module>()\r\n      1 x = nn.Parameter(torch.ones(2, 2))\r\n----> 2 (x @ torch.ones(2)).sum().backward()\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     97     Variable._execution_engine.run_backward(\r\n     98         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 99         allow_unreachable=True)  # allow_unreachable flag\r\n    100 \r\n    101 \r\n\r\nRuntimeError: cublas runtime error : an invalid numeric value was used as an argument at /pytorch/aten/src/THC/THCBlas.cu:120\r\n```\r\n## Expected behavior\r\n\r\nNo exception is raised.\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.1+cu100\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.3.1+cu100\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.4.2+cu100\r\n[conda] Could not collect\r\n```\r\n\r\nThis code was run on Google Colab. The bug also reproduces on my server (Ubuntu 16.04LTS, 4x GTX1080Ti GPUs).\r\n\r\n## Additional context\r\n\r\nThe bug doesn't occur if:\r\n1. If I replace `sum()` with `mean()`\r\n```python\r\n(x @ torch.ones(2)).mean().backward()\r\n```\r\n\r\n2. If I use a different shape of the tensor\r\n```python\r\n(x @ torch.ones(2, 1)).sum().backward()\r\n```\r\n\r\n3. If I run the code on CPU\r\n\r\nTherefore, I believe that this is not intended behavior.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ntorch.set_default_tensor_type('torch.cuda.FloatTensor')\r\nx = nn.Parameter(torch.ones(2, 2))\r\n(x @ torch.ones(2)).sum().backward()\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-11-c3b66f275e9a> in <module>()\r\n      1 x = nn.Parameter(torch.ones(2, 2))\r\n----> 2 (x @ torch.ones(2)).sum().backward()\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     97     Variable._execution_engine.run_backward(\r\n     98         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 99         allow_unreachable=True)  # allow_unreachable flag\r\n    100 \r\n    101 \r\n\r\nRuntimeError: cublas runtime error : an invalid numeric value was used as an argument at /pytorch/aten/src/THC/THCBlas.cu:120\r\n"
29804,"Parallel data loader performance degradation for IterableDataset with num_workers > 1 (but not for Dataset).## \U0001f41b Bug\r\n\r\nData loader performance degrades with > 1 data loader worker when using `IterableDataset` but not when using (map-style) `Dataset`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nMy experiment (code below) consisted of:\r\n\r\n1. Make a (Iterable)Dataset class. This synthesizes dummy data and possibly adds a time delay to simulate batch loading work.\r\n2. Make a Dataloader that consumes the above dataset instance. The number of workers varied from 0 (loading in the main process) to 4.\r\n3. Iterate through N=10 batches of data yielded by the dataloader, with possibly a time delay to simulate batch processing work (e.g. training).\r\n\r\nHere are my measurements for `IterableDataset`:\r\n\r\n```\r\n(num_workers=0, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 20.021s.\r\n(num_workers=1, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 11.433s.\r\n(num_workers=2, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 21.469s.\r\n(num_workers=3, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 31.517s.\r\n(num_workers=4, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 41.545s.\r\n\r\n(num_workers=0, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 11.014s.\r\n(num_workers=1, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.518s.\r\n(num_workers=2, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.643s.\r\n(num_workers=3, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.745s.\r\n(num_workers=4, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.862s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 11.013s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.515s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 20.541s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 30.617s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 40.610s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.004s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.543s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.553s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 3.593s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 4.649s.\r\n```\r\nThis is the code I used to run this experiment:\r\n\r\n```\r\nimport os\r\nimport time\r\nimport itertools\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\nclass TimedBlock:\r\n    """"""\r\n    Context manager to measure wall time.\r\n    """"""\r\n\r\n    def __init__(self, name):\r\n        self.name = name\r\n\r\n    def __enter__(self):\r\n        self.t_start = time.perf_counter()\r\n\r\n    def __exit__(self, *args):\r\n        print(""(%d) %s execution finished. Time elapsed = %.3fs."" % (os.getpid(), self.name,\r\n                                                                     time.perf_counter() - self.t_start))\r\n\r\n\r\nclass IterableDataset(torch.utils.data.IterableDataset):\r\n    """"""\r\n    Synthetic iterable dataset that simulates data loading work.\r\n    """"""\r\n\r\n    def __init__(self, n_items, shape, batch_loading_time):\r\n        """"""\r\n        Args:\r\n            n_items: number of items to return.\r\n            shape: shape of batch tensor to return.\r\n            batch_loading_time: each batch will take at least this many seconds to return.\r\n        """"""\r\n        self.n_items = n_items\r\n        self.shape = shape\r\n        self.batch_loading_time = batch_loading_time\r\n\r\n    def __iter__(self):\r\n        self.i = 0\r\n        while self.i < self.n_items:\r\n            t_start = time.perf_counter()\r\n            X = np.random.randn(*self.shape)\r\n            t_remaining =  self.batch_loading_time - (time.perf_counter() - t_start)\r\n            if t_remaining > 0:\r\n                time.sleep(t_remaining)\r\n            yield X\r\n            self.i += 1\r\n\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n    """"""\r\n    Synthetic map-style dataset that simulates data loading work.\r\n    """"""\r\n\r\n    def __init__(self, n_items, shape, batch_loading_time):\r\n        """"""\r\n        Args:\r\n            n_items: number of items to return.\r\n            shape: shape of batch tensor to return.\r\n            batch_loading_time: each batch will take at least this many seconds to return.\r\n        """"""\r\n        self.n_items = n_items\r\n        self.shape = shape\r\n        self.batch_loading_time = batch_loading_time\r\n\r\n    def __len__(self):\r\n        return self.n_items\r\n\r\n    def __getitem__(self, idx):\r\n        t_start = time.perf_counter()\r\n        X = np.random.randn(*self.shape)\r\n        t_remaining =  self.batch_loading_time - (time.perf_counter() - t_start)\r\n        if t_remaining > 0:\r\n            time.sleep(t_remaining)\r\n        return X\r\n\r\n\r\ndef test_simpleloader(n_iters=int(1e2), shape=(1,), num_workers=0, batch_loading_time=0, batch_process_time=0,\r\n                      exp_name=""simpleloader"", dataset_cls=IterableDataset):\r\n    """"""\r\n    Load and process n_iters batches of data using a dataloader and one of the above dataset classes. We simulate\r\n    training by waiting batch_process_time seconds per batch.\r\n    """"""\r\n\r\n    dataset = dataset_cls(n_iters, shape, batch_loading_time)\r\n    data_loader = torch.utils.data.DataLoader(dataset,\r\n                                             batch_size=None,\r\n                                             num_workers=num_workers,\r\n                                             collate_fn=None,\r\n                                             pin_memory=False,\r\n                                             worker_init_fn=None,\r\n                                             multiprocessing_context=None)\r\n    \r\n    with TimedBlock(exp_name):\r\n        for i, x in enumerate(data_loader):\r\n            time.sleep(batch_process_time)\r\n            pass\r\n\r\n\r\ndef simpleloader_grid(num_workers_grid=[0, 1, 2, 3, 4],\r\n                      batch_loading_time_grid=[10**i for i in range(0, -2, -1)],\r\n                      batch_process_time_grid=[10**i for i in range(0, -2, -1)],\r\n                      dataset_cls=IterableDataset):\r\n\r\n    n_iters = int(1e1)\r\n    shape = (1,)\r\n\r\n    params = itertools.product(num_workers_grid, batch_loading_time_grid, batch_process_time_grid)\r\n\r\n    for p in params:\r\n        test_simpleloader(n_iters, shape, p[0], p[1], p[2], str(p), dataset_cls=dataset_cls)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    torch.multiprocessing.set_start_method(""spawn"")\r\n    t_start = time.perf_counter()\r\n    simpleloader_grid(dataset_cls=IterableDataset)\r\n    print(""Main execution time = %.3fs"" % (time.perf_counter() - t_start))\r\n```\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nHere are my measurements for (map-style)`Dataset`, which I would also expect from `IterableDataset`:\r\n\r\n```\r\n(num_workers=0, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 20.022s.\r\n(num_workers=1, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.432s.\r\n(num_workers=2, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.455s.\r\n(num_workers=3, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.493s.\r\n(num_workers=4, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.479s.\r\n\r\n(num_workers=0, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 11.014s.\r\n(num_workers=1, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 10.529s.\r\n(num_workers=2, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 5.640s.\r\n(num_workers=3, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 4.537s.\r\n(num_workers=4, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 3.652s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 11.012s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.502s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.538s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.594s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.604s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.005s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.518s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.540s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.601s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.572s\r\n```\r\n\r\n## Environment\r\n\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @SsnL",module: dataloader|triaged,ssnl,"## \U0001f41b Bug\r\n\r\nData loader performance degrades with > 1 data loader worker when using `IterableDataset` but not when using (map-style) `Dataset`.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nMy experiment (code below) consisted of:\r\n\r\n1. Make a (Iterable)Dataset class. This synthesizes dummy data and possibly adds a time delay to simulate batch loading work.\r\n2. Make a Dataloader that consumes the above dataset instance. The number of workers varied from 0 (loading in the main process) to 4.\r\n3. Iterate through N=10 batches of data yielded by the dataloader, with possibly a time delay to simulate batch processing work (e.g. training).\r\n\r\nHere are my measurements for `IterableDataset`:\r\n\r\n```\r\n(num_workers=0, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 20.021s.\r\n(num_workers=1, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 11.433s.\r\n(num_workers=2, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 21.469s.\r\n(num_workers=3, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 31.517s.\r\n(num_workers=4, batch_loading_time=1,  batch_process_time=1) execution finished. Time elapsed = 41.545s.\r\n\r\n(num_workers=0, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 11.014s.\r\n(num_workers=1, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.518s.\r\n(num_workers=2, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.643s.\r\n(num_workers=3, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.745s.\r\n(num_workers=4, batch_loading_time=1,  batch_process_time=0.1) execution finished. Time elapsed = 10.862s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 11.013s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.515s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 20.541s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 30.617s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 40.610s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.004s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.543s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.553s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 3.593s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 4.649s.\r\n```\r\nThis is the code I used to run this experiment:\r\n\r\n```\r\nimport os\r\nimport time\r\nimport itertools\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\nclass TimedBlock:\r\n    """"""\r\n    Context manager to measure wall time.\r\n    """"""\r\n\r\n    def __init__(self, name):\r\n        self.name = name\r\n\r\n    def __enter__(self):\r\n        self.t_start = time.perf_counter()\r\n\r\n    def __exit__(self, *args):\r\n        print(""(%d) %s execution finished. Time elapsed = %.3fs."" % (os.getpid(), self.name,\r\n                                                                     time.perf_counter() - self.t_start))\r\n\r\n\r\nclass IterableDataset(torch.utils.data.IterableDataset):\r\n    """"""\r\n    Synthetic iterable dataset that simulates data loading work.\r\n    """"""\r\n\r\n    def __init__(self, n_items, shape, batch_loading_time):\r\n        """"""\r\n        Args:\r\n            n_items: number of items to return.\r\n            shape: shape of batch tensor to return.\r\n            batch_loading_time: each batch will take at least this many seconds to return.\r\n        """"""\r\n        self.n_items = n_items\r\n        self.shape = shape\r\n        self.batch_loading_time = batch_loading_time\r\n\r\n    def __iter__(self):\r\n        self.i = 0\r\n        while self.i < self.n_items:\r\n            t_start = time.perf_counter()\r\n            X = np.random.randn(*self.shape)\r\n            t_remaining =  self.batch_loading_time - (time.perf_counter() - t_start)\r\n            if t_remaining > 0:\r\n                time.sleep(t_remaining)\r\n            yield X\r\n            self.i += 1\r\n\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n    """"""\r\n    Synthetic map-style dataset that simulates data loading work.\r\n    """"""\r\n\r\n    def __init__(self, n_items, shape, batch_loading_time):\r\n        """"""\r\n        Args:\r\n            n_items: number of items to return.\r\n            shape: shape of batch tensor to return.\r\n            batch_loading_time: each batch will take at least this many seconds to return.\r\n        """"""\r\n        self.n_items = n_items\r\n        self.shape = shape\r\n        self.batch_loading_time = batch_loading_time\r\n\r\n    def __len__(self):\r\n        return self.n_items\r\n\r\n    def __getitem__(self, idx):\r\n        t_start = time.perf_counter()\r\n        X = np.random.randn(*self.shape)\r\n        t_remaining =  self.batch_loading_time - (time.perf_counter() - t_start)\r\n        if t_remaining > 0:\r\n            time.sleep(t_remaining)\r\n        return X\r\n\r\n\r\ndef test_simpleloader(n_iters=int(1e2), shape=(1,), num_workers=0, batch_loading_time=0, batch_process_time=0,\r\n                      exp_name=""simpleloader"", dataset_cls=IterableDataset):\r\n    """"""\r\n    Load and process n_iters batches of data using a dataloader and one of the above dataset classes. We simulate\r\n    training by waiting batch_process_time seconds per batch.\r\n    """"""\r\n\r\n    dataset = dataset_cls(n_iters, shape, batch_loading_time)\r\n    data_loader = torch.utils.data.DataLoader(dataset,\r\n                                             batch_size=None,\r\n                                             num_workers=num_workers,\r\n                                             collate_fn=None,\r\n                                             pin_memory=False,\r\n                                             worker_init_fn=None,\r\n                                             multiprocessing_context=None)\r\n    \r\n    with TimedBlock(exp_name):\r\n        for i, x in enumerate(data_loader):\r\n            time.sleep(batch_process_time)\r\n            pass\r\n\r\n\r\ndef simpleloader_grid(num_workers_grid=[0, 1, 2, 3, 4],\r\n                      batch_loading_time_grid=[10**i for i in range(0, -2, -1)],\r\n                      batch_process_time_grid=[10**i for i in range(0, -2, -1)],\r\n                      dataset_cls=IterableDataset):\r\n\r\n    n_iters = int(1e1)\r\n    shape = (1,)\r\n\r\n    params = itertools.product(num_workers_grid, batch_loading_time_grid, batch_process_time_grid)\r\n\r\n    for p in params:\r\n        test_simpleloader(n_iters, shape, p[0], p[1], p[2], str(p), dataset_cls=dataset_cls)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    torch.multiprocessing.set_start_method(""spawn"")\r\n    t_start = time.perf_counter()\r\n    simpleloader_grid(dataset_cls=IterableDataset)\r\n    print(""Main execution time = %.3fs"" % (time.perf_counter() - t_start))\r\n```\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nHere are my measurements for (map-style)`Dataset`, which I would also expect from `IterableDataset`:\r\n\r\n```\r\n(num_workers=0, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 20.022s.\r\n(num_workers=1, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.432s.\r\n(num_workers=2, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.455s.\r\n(num_workers=3, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.493s.\r\n(num_workers=4, batch_loading_time=1,   batch_process_time=1) execution finished. Time elapsed = 11.479s.\r\n\r\n(num_workers=0, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 11.014s.\r\n(num_workers=1, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 10.529s.\r\n(num_workers=2, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 5.640s.\r\n(num_workers=3, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 4.537s.\r\n(num_workers=4, batch_loading_time=1,   batch_process_time=0.1) execution finished. Time elapsed = 3.652s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 11.012s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.502s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.538s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.594s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=1) execution finished. Time elapsed = 10.604s.\r\n\r\n(num_workers=0, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 2.005s.\r\n(num_workers=1, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.518s.\r\n(num_workers=2, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.540s.\r\n(num_workers=3, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.601s.\r\n(num_workers=4, batch_loading_time=0.1, batch_process_time=0.1) execution finished. Time elapsed = 1.572s\r\n```\r\n\r\n## Environment\r\n\r\n```Collecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1060 6GB\r\nGPU 1: GeForce GTX 1060 6GB\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torch-cluster==1.4.3\r\n[pip] torch-geometric==1.3.0\r\n[pip] torch-scatter==1.4.0\r\n[pip] torch-sparse==0.4.3\r\n[pip] torch-spline-conv==1.1.0\r\n[pip] torchcontrib==0.0.2\r\n[pip] torchvision==0.2.2\r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] tensorflow                1.13.1          mkl_py37h54b294f_0  \r\n[conda] tensorflow-base           1.13.1          mkl_py37h7ce6ba3_0  \r\n[conda] torch                     1.3.0                    pypi_0    pypi\r\n[conda] torch-cluster             1.4.3                     dev_0    <develop>\r\n[conda] torch-geometric           1.3.0                    pypi_0    pypi\r\n[conda] torch-scatter             1.4.0                    pypi_0    pypi\r\n[conda] torch-sparse              0.4.3                    pypi_0    pypi\r\n[conda] torch-spline-conv         1.1.0                    pypi_0    pypi\r\n[conda] torchcontrib              0.0.2                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @SsnL",Collecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1060 6GB\r\nGPU 1: GeForce GTX 1060 6GB\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torch-cluster==1.4.3\r\n[pip] torch-geometric==1.3.0\r\n[pip] torch-scatter==1.4.0\r\n[pip] torch-sparse==0.4.3\r\n[pip] torch-spline-conv==1.1.0\r\n[pip] torchcontrib==0.0.2\r\n[pip] torchvision==0.2.2\r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] tensorflow                1.13.1          mkl_py37h54b294f_0  \r\n[conda] tensorflow-base           1.13.1          mkl_py37h7ce6ba3_0  \r\n[conda] torch                     1.3.0                    pypi_0    pypi\r\n[conda] torch-cluster             1.4.3                     dev_0    <develop>\r\n[conda] torch-geometric           1.3.0                    pypi_0    pypi\r\n[conda] torch-scatter             1.4.0                    pypi_0    pypi\r\n[conda] torch-sparse              0.4.3                    pypi_0    pypi\r\n[conda] torch-spline-conv         1.1.0                    pypi_0    pypi\r\n[conda] torchcontrib              0.0.2                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n
29648,"Calibration observer should be allowed to run on GPU## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nCalibration of the quantized model requires the observers to walk through the training data in floating point mode. This could be done on GPU. However, the obserrver's `min`/`max` computation does not respect the device setting. Specifically,\r\n\r\n\r\n\r\nrequires that both `x` and `min_val` be on the same device, but calling `cuda()` on thte oobserver doesn't set the `min_val` device.\r\n\r\n\r\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,hx89,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nCalibration of the quantized model requires the observers to walk through the training data in floating point mode. This could be done on GPU. However, the obserrver's `min`/`max` computation does not respect the device setting. Specifically,\r\n\r\n```python\r\nmin_val = torch.min(torch.min(x), min_val)\r\n```\r\n\r\nrequires that both `x` and `min_val` be on the same device, but calling `cuda()` on thte oobserver doesn't set the `min_val` device.\r\n\r\n\r\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nmin_val = torch.min(torch.min(x), min_val)\r\n"
29644,"`prepare` doesn't insert observers if given `qconfig` argument## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`prepare` function from the `quantize.py` is responsible for inserting the observers. It works well when the model already has a qconfig. However, according to the documentation, `prepare` can take the qconfig as an argument, in which case it will insert the observers even if the model doesn't have the qconfig member. This does not happen\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe observers must be inserted because the argument for qconofig is provided\r\n\r\n## Environment\r\n\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,z-a-f,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`prepare` function from the `quantize.py` is responsible for inserting the observers. It works well when the model already has a qconfig. However, according to the documentation, `prepare` can take the qconfig as an argument, in which case it will insert the observers even if the model doesn't have the qconfig member. This does not happen\r\n\r\n## To Reproduce\r\n\r\n```python\r\nfrom torch.quantization import default_qconfig, prepare\r\n\r\nmodel_with_qconfig.qconfig = default_qconfig\r\nmodel_prepared = prepare(model_with_qconfig, inplace=False)  # This will insert observers\r\n\r\nmodel_not_prepared = prepare(model_without_qconfig, qconfig=default_qconfig, inplace=False)  # This will NOT insert observers\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nThe observers must be inserted because the argument for qconofig is provided\r\n\r\n## Environment\r\n\r\n```console\r\n$> python ~/Git/pytorch/torch/utils/collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.4.0.dev20191110\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce GT 730\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.3\r\n[pip3] torch==1.4.0.dev20191110\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.4.0\r\n[pip3] torchvision==0.5.0.dev20191110\r\n[pip3] torchviz==0.0.1\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nfrom torch.quantization import default_qconfig, prepare\r\n\r\nmodel_with_qconfig.qconfig = default_qconfig\r\nmodel_prepared = prepare(model_with_qconfig, inplace=False)  # This will insert observers\r\n\r\nmodel_not_prepared = prepare(model_without_qconfig, qconfig=default_qconfig, inplace=False)  # This will NOT insert observers\r\n\r\n"
29578,"Floating point exception and segfault for empty tensors to BatchNorm2d## \U0001f41b Bug\r\n\r\nIntermediate empty tensors result in exceptions and segfault.\r\n\r\n## To Reproduce\r\n\r\n\r\n> Floating point exception\r\n\r\nIn the actual model `BatchNorm2d` is actually trained and then `.eval` is called. Segfault still happens.\r\n\r\n\r\n## Expected behavior\r\n\r\nNo segfault. Ideally please do [support empty tensors](https://github.com/pytorch/pytorch/issues/12013). \r\n\r\nI hit these issues with empty tensors during exporting/tracing, and cannot use `jit.script` because I want to export to ONNX.\r\n\r\nEven thought exceptions such as \r\n> RuntimeError: non-empty 3D or 4D input tensor expected but got ndim: 4\r\n\r\nare not that great, at least we can try-catch and work around it. \r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0.dev20191023\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] iristorch==0.0.43\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.4.0.dev20191023\r\n[pip] torchvision==0.5.0a0+558beab\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nThe empty tensor is an intermediate tensor, with dynamic batch size. Related to issue #15343 and #12013 \r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168",high priority|module: nn|triaged,ngimel,"## \U0001f41b Bug\r\n\r\nIntermediate empty tensors result in exceptions and segfault.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\ngg = nn.BatchNorm2d(64).eval()\r\nprob = torch.empty(size=(0, 64, 112, 112))\r\nprint(gg(prob))\r\n```\r\n> Floating point exception\r\n\r\nIn the actual model `BatchNorm2d` is actually trained and then `.eval` is called. Segfault still happens.\r\n\r\n\r\n## Expected behavior\r\n\r\nNo segfault. Ideally please do [support empty tensors](https://github.com/pytorch/pytorch/issues/12013). \r\n\r\nI hit these issues with empty tensors during exporting/tracing, and cannot use `jit.script` because I want to export to ONNX.\r\n\r\nEven thought exceptions such as \r\n> RuntimeError: non-empty 3D or 4D input tensor expected but got ndim: 4\r\n\r\nare not that great, at least we can try-catch and work around it. \r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0.dev20191023\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] iristorch==0.0.43\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.4.0.dev20191023\r\n[pip] torchvision==0.5.0a0+558beab\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nThe empty tensor is an intermediate tensor, with dynamic batch size. Related to issue #15343 and #12013 \r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168","python\r\nimport torch\r\nimport torch.nn as nn\r\ngg = nn.BatchNorm2d(64).eval()\r\nprob = torch.empty(size=(0, 64, 112, 112))\r\nprint(gg(prob))\r\n"
29538,"Warning when instantiating quantized model## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen instantiating a quantized model, a warning pops up saying\r\n\r\n![image](https://user-images.githubusercontent.com/4216323/68561371-21ae4d80-03fa-11ea-9a5e-68eff371b741.png)\r\n\r\n\r\n\r\n\r\nIf the model is already quantized, I don't think this warning serves any purpose but pollute the terminal and confuse the user\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nIf the model is already quantized (that is the `quantize=True` is set), there are no mode observers. This warning should be suppressed, and only be shown when the user actually  fails to run observers before converting the model\r\n\r\n## Environment\r\n\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,vkuzo,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen instantiating a quantized model, a warning pops up saying\r\n\r\n![image](https://user-images.githubusercontent.com/4216323/68561371-21ae4d80-03fa-11ea-9a5e-68eff371b741.png)\r\n\r\n\r\n```console\r\ntorch/quantization/observer.py:131: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \r\n```\r\n\r\nIf the model is already quantized, I don't think this warning serves any purpose but pollute the terminal and confuse the user\r\n\r\n## To Reproduce\r\n\r\n```python\r\n>>> from torchvision.models import quantization as models\r\n>>> q_model = models.mobilenet_v2(pretrained=True, progress=True, quantize=True)  \r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nIf the model is already quantized (that is the `quantize=True` is set), there are no mode observers. This warning should be suppressed, and only be shown when the user actually  fails to run observers before converting the model\r\n\r\n## Environment\r\n\r\n```console\r\n$> python ~/Git/pytorch/torch/utils/collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.4.0.dev20191110\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce GT 730\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.3\r\n[pip3] torch==1.4.0.dev20191110\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.4.0\r\n[pip3] torchvision==0.5.0.dev20191110\r\n[pip3] torchviz==0.0.1\r\n[conda] Could not collect\r\n\r\n```\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",console\r\ntorch/quantization/observer.py:131: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \r\n
29435,"quantized permute throws an error when trying to print/show the tensor## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`.permute` throws an error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    400                         if cls is not object \\\r\n    401                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 402                             return _repr_pprint(obj, self, cycle)\r\n    403\r\n    404             return _default_pprint(obj, self, cycle)\r\n\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    695     """"""A pprint that just redirects to the normal repr function.""""""\r\n    696     # Find newlines and replace them with p.break_()\r\n--> 697     output = repr(obj)\r\n    698     for idx,output_line in enumerate(output.splitlines()):\r\n    699         if idx:\r\n\r\n~/Git/pytorch-benchmarking/torch/tensor.py in __repr__(self)\r\n    157         # characters to replace unicode characters with.\r\n    158         if sys.version_info > (3,):\r\n--> 159             return torch._tensor_str._str(self)\r\n    160         else:\r\n    161             if hasattr(sys.stdout, 'encoding'):\r\n\r\n~/Git/pytorch-benchmarking/torch/_tensor_str.py in _str(self)\r\n    290             suffixes.append('zero_point=' + str(self.q_per_channel_zero_points()))\r\n    291             suffixes.append('axis=' + str(self.q_per_channel_axis()))\r\n--> 292         tensor_str = _tensor_str(self.dequantize(), indent)\r\n    293     else:\r\n    294         if self.numel() == 0 and not self.is_sparse:\r\n\r\nRuntimeError: Could not run 'aten::empty' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::empty' is only available for these backends: [SparseCPUTensorId, CPUTensorId, VariableTensorId, MkldnnCPUTensorId].\r\n```\r\n\r\n## To Reproduce\r\n\r\nThis is a better example:\r\n\r\n\r\n\r\nSteps to reproduce the behavior (OLD):\r\n\r\n\r\n\r\n## Environment\r\n\r\n\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",high priority|triage review|oncall: quantization|triaged,jerryzh168,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n`.permute` throws an error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    400                         if cls is not object \\\r\n    401                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 402                             return _repr_pprint(obj, self, cycle)\r\n    403\r\n    404             return _default_pprint(obj, self, cycle)\r\n\r\n~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    695     """"""A pprint that just redirects to the normal repr function.""""""\r\n    696     # Find newlines and replace them with p.break_()\r\n--> 697     output = repr(obj)\r\n    698     for idx,output_line in enumerate(output.splitlines()):\r\n    699         if idx:\r\n\r\n~/Git/pytorch-benchmarking/torch/tensor.py in __repr__(self)\r\n    157         # characters to replace unicode characters with.\r\n    158         if sys.version_info > (3,):\r\n--> 159             return torch._tensor_str._str(self)\r\n    160         else:\r\n    161             if hasattr(sys.stdout, 'encoding'):\r\n\r\n~/Git/pytorch-benchmarking/torch/_tensor_str.py in _str(self)\r\n    290             suffixes.append('zero_point=' + str(self.q_per_channel_zero_points()))\r\n    291             suffixes.append('axis=' + str(self.q_per_channel_axis()))\r\n--> 292         tensor_str = _tensor_str(self.dequantize(), indent)\r\n    293     else:\r\n    294         if self.numel() == 0 and not self.is_sparse:\r\n\r\nRuntimeError: Could not run 'aten::empty' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::empty' is only available for these backends: [SparseCPUTensorId, CPUTensorId, VariableTensorId, MkldnnCPUTensorId].\r\n```\r\n\r\n## To Reproduce\r\n\r\nThis is a better example:\r\n\r\n```python\r\nimport torch\r\nx = torch.randn(62, 59)\r\nqx = torch.quantize_per_tensor(x, 1.0, 0, torch.qint32)\r\nqy = qx.permute([1, 0])\r\nqy.dequantize()\r\n```\r\n\r\nSteps to reproduce the behavior (OLD):\r\n\r\n```python\r\nimport torch\r\nx = torch.randn(64, 64)\r\nqx = torch.quantize_per_tensor(x, 1.0, 0, torch.qint32)\r\nprint(qx.permute([1, 0]))\r\n```\r\n\r\n## Environment\r\n\r\n```console\r\n$> python torch/utils/collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+255505f\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15\r\nGCC version: Could not collect\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.4.0a0+255505f\r\n[pip3] torchsummary==1.5.1\r\n[conda] Could not collect\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nimport torch\r\nx = torch.randn(62, 59)\r\nqx = torch.quantize_per_tensor(x, 1.0, 0, torch.qint32)\r\nqy = qx.permute([1, 0])\r\nqy.dequantize()\r\n"
29234,"Cannot compile CPP extensions for benchmarking## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThird party `cpuinfo.h` is missing during compilation of the benchmarking cpp extensions.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nThis will throw an error:\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nIt should not throw an error :)\r\n\r\n## Environment\r\n\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",module: third_party,mingzhe09088,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThird party `cpuinfo.h` is missing during compilation of the benchmarking cpp extensions.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```console\r\npython setup.py develop\r\ncd benchmarks/operator_benchmark/pt_extension\r\npython setup.py install\r\n```\r\n\r\nThis will throw an error:\r\n\r\n```console\r\ncreating build/temp.macosx-10.14-x86_64-3.6\r\nclang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Applications/Xcode_10.1_fb.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include -I/Applications/Xcode_10.1_fb.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include -I/Users/zaf/.pyenv/versions/3.6.3/lib/python3.6/site-packages/torch/include -I/Users/zaf/.pyenv/versions/3.6.3/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/Users/zaf/.pyenv/versions/3.6.3/lib/python3.6/site-packages/torch/include/TH -I/Users/zaf/.pyenv/versions/3.6.3/lib/python3.6/site-packages/torch/include/THC -I/Users/zaf/.pyenv/versions/3.6.3/include/python3.6m -c extension.cpp -o build/temp.macosx-10.14-x86_64-3.6/extension.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=cpp_extension -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\r\nextension.cpp:1:10: fatal error: 'cpuinfo.h' file not found\r\n#include <cpuinfo.h>\r\n         ^~~~~~~~~~~\r\n1 error generated.\r\nerror: command 'clang' failed with exit status **1**\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nIt should not throw an error :)\r\n\r\n## Environment\r\n\r\n```console\r\n$> python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+b87b6fd\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15\r\nGCC version: Could not collect\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.4.0a0+b87b6fd\r\n[pip3] torchsummary==1.5.1\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",console\r\npython setup.py develop\r\ncd benchmarks/operator_benchmark/pt_extension\r\npython setup.py install\r\n
29108,"a retrained and saved  jit module could not be reload.I'm try to retain a JIT module from pytorch using c++ based on Libtorch libtorch-win-shared-with-deps-1.3.0 _cpu, and save retrained module for next usage.\r\n               the save and reload process will be like the following , but get erro while try reload.\t\t\r\n\r\n\r\nreported error:\r\n```\r\nCaught\r\nexpected newline but found 'number' here:\r\nat code/__torch__/modelv2.py:75:51\r\nimport __torch__.modelv2.features.16.conv.1\r\nimport __torch__.modelv2.features.17\r\nimport __torch__.modelv2.features.17.conv\r\nimport __torch__.modelv2.features.17.conv.0\r\nimport __torch__.modelv2.features.17.conv.1\r\nimport __torch__.modelv2.features.18\r\nclass features(Module):\r\n  __parameters__ = []\r\n  __annotations__ = []\r\n  __annotations__[""0""] = __torch__.modelv2.features.0\r\n                                                   ~~ <--- HERE\r\n  __annotations__[""1""] = __torch__.modelv2.features.1\r\n  __annotations__[""2""] = __torch__.modelv2.features.2\r\n  __annotations__[""3""] = __torch__.modelv2.features.3\r\n  __annotations__[""4""] = __torch__.modelv2.features.4\r\n  __annotations__[""5""] = __torch__.modelv2.features.5\r\n  __annotations__[""6""] = __torch__.modelv2.features.6\r\n  __annotations__[""7""] = __torch__.modelv2.features.7\r\n  __annotations__[""8""] = __torch__.modelv2.features.8\r\n  __annotations__[""9""] = __torch__.modelv2.features.9\r\nCompiled from code\r\nType class std::runtime_error\r\n```\r\nIs there any sugesstion:\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo",oncall: jit|triaged,suo,"I'm try to retain a JIT module from pytorch using c++ based on Libtorch libtorch-win-shared-with-deps-1.3.0 _cpu, and save retrained module for next usage.\r\n               the save and reload process will be like the following , but get erro while try reload.\t\t\r\n\r\n```cpp\r\n\t\ttorch::serialize::OutputArchive output_archive;\r\n\t\tauto presavedModule = torch::jit::load(""D:/localshare/data/model/model.pt"");\r\n\t\tpresavedModule.save(""D:/localshare/data/model/testmodel.pt"");\r\n\t\tauto savedModule = torch::jit::load(""D:/localshare/data/model/testmodel.pt"");\r\n\t\tsavedModule.dump(true, true, true);\r\n```\r\nreported error:\r\n```\r\nCaught\r\nexpected newline but found 'number' here:\r\nat code/__torch__/modelv2.py:75:51\r\nimport __torch__.modelv2.features.16.conv.1\r\nimport __torch__.modelv2.features.17\r\nimport __torch__.modelv2.features.17.conv\r\nimport __torch__.modelv2.features.17.conv.0\r\nimport __torch__.modelv2.features.17.conv.1\r\nimport __torch__.modelv2.features.18\r\nclass features(Module):\r\n  __parameters__ = []\r\n  __annotations__ = []\r\n  __annotations__[""0""] = __torch__.modelv2.features.0\r\n                                                   ~~ <--- HERE\r\n  __annotations__[""1""] = __torch__.modelv2.features.1\r\n  __annotations__[""2""] = __torch__.modelv2.features.2\r\n  __annotations__[""3""] = __torch__.modelv2.features.3\r\n  __annotations__[""4""] = __torch__.modelv2.features.4\r\n  __annotations__[""5""] = __torch__.modelv2.features.5\r\n  __annotations__[""6""] = __torch__.modelv2.features.6\r\n  __annotations__[""7""] = __torch__.modelv2.features.7\r\n  __annotations__[""8""] = __torch__.modelv2.features.8\r\n  __annotations__[""9""] = __torch__.modelv2.features.9\r\nCompiled from code\r\nType class std::runtime_error\r\n```\r\nIs there any sugesstion:\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo","cpp\r\n\t\ttorch::serialize::OutputArchive output_archive;\r\n\t\tauto presavedModule = torch::jit::load(""D:/localshare/data/model/model.pt"");\r\n\t\tpresavedModule.save(""D:/localshare/data/model/testmodel.pt"");\r\n\t\tauto savedModule = torch::jit::load(""D:/localshare/data/model/testmodel.pt"");\r\n\t\tsavedModule.dump(true, true, true);\r\n"
29102,"Indexed assignment of quantized Tensors yields unexpected results## \U0001f41b Bug\r\n\r\nIndexed assignment of quantized Tensors effectively copies over the `int_repr` of the source tensors. This makes no sense in terms of values when the scale / zero_point do not match.\r\n\r\n## To Reproduce\r\n\r\n\r\ngives\r\n```\r\n(tensor([0.0000, 1.0000, 2.0000, 2.5400], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.02, zero_point=0),\r\n tensor([0.0000, 2.0000, 4.0000, 5.0800], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.04, zero_point=0),\r\n tensor([0.0000, 1.0000, 2.0000, 2.5600], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.04, zero_point=0))\r\n```\r\n\r\n## Expected behavior\r\n\r\nI would expect q2 and q3 to be the same and close to q.\r\n\r\n## Environment\r\n\r\nmasterish PyTorch\r\n\r\n## Additional\r\n\r\nI would suspect that this could be fixed by changing `copy_` to re-quantize to the *target* quantization, which I would consider reasonable semantics, but of course it is different to the current one.\r\nIf that is an acceptable change, I'd be very happy to send a PR.\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,jerryzh168,"## \U0001f41b Bug\r\n\r\nIndexed assignment of quantized Tensors effectively copies over the `int_repr` of the source tensors. This makes no sense in terms of values when the scale / zero_point do not match.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nt = torch.arange(4.0)\r\nq = torch.quantize_per_tensor(t, 0.02, 0, torch.qint8)\r\nq2 = torch._empty_affine_quantized(q.shape, scale=0.04, zero_point=0, dtype=torch.qint8)\r\nq3 = torch._empty_affine_quantized(q.shape, scale=0.04, zero_point=0, dtype=torch.qint8)\r\nq2[:] = q\r\nq3[:] = q.dequantize()\r\nq, q2, q3\r\n```\r\ngives\r\n```\r\n(tensor([0.0000, 1.0000, 2.0000, 2.5400], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.02, zero_point=0),\r\n tensor([0.0000, 2.0000, 4.0000, 5.0800], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.04, zero_point=0),\r\n tensor([0.0000, 1.0000, 2.0000, 2.5600], size=(4,), dtype=torch.qint8,\r\n        quantization_scheme=torch.per_tensor_affine, scale=0.04, zero_point=0))\r\n```\r\n\r\n## Expected behavior\r\n\r\nI would expect q2 and q3 to be the same and close to q.\r\n\r\n## Environment\r\n\r\nmasterish PyTorch\r\n\r\n## Additional\r\n\r\nI would suspect that this could be fixed by changing `copy_` to re-quantize to the *target* quantization, which I would consider reasonable semantics, but of course it is different to the current one.\r\nIf that is an acceptable change, I'd be very happy to send a PR.\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\nt = torch.arange(4.0)\r\nq = torch.quantize_per_tensor(t, 0.02, 0, torch.qint8)\r\nq2 = torch._empty_affine_quantized(q.shape, scale=0.04, zero_point=0, dtype=torch.qint8)\r\nq3 = torch._empty_affine_quantized(q.shape, scale=0.04, zero_point=0, dtype=torch.qint8)\r\nq2[:] = q\r\nq3[:] = q.dequantize()\r\nq, q2, q3\r\n"
29035,[jit][script] Support namedtuples from other files## \U0001f41b Bug\r\n\r\n`torch.jit.script` seems to choke when using namedtuples imported from other modules. There are possibly two distinct bugs here:\r\n\r\n## To Reproduce\r\n\r\nSuppose I have three files:\r\n\r\na.py\r\n\r\n\r\nb.py:\r\n\r\n\r\nc.py:\r\n\r\n\r\nThen running `b.py` fails with:\r\n\r\n\r\nwhile running `c.py` fails with:\r\n\r\n\r\nUsing a namedtuple defined *within* the same file works just fine though.\r\n\r\n## Expected behavior\r\n\r\nBoth `b.py` and `c.py` run without errors\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 8.3.0-6ubuntu1~18.04.1) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.3\r\n[pip] torch==1.3.0\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo,oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\n`torch.jit.script` seems to choke when using namedtuples imported from other modules. There are possibly two distinct bugs here:\r\n\r\n## To Reproduce\r\n\r\nSuppose I have three files:\r\n\r\na.py\r\n```python\r\nfrom typing import NamedTuple\r\nclass Type(NamedTuple):\r\n    t: int\r\n```\r\n\r\nb.py:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nimport a\r\n\r\nclass M(nn.Module):\r\n    def forward(self) -> a.Type:\r\n        return a.Type(1)\r\n\r\ntorch.jit.script(M())\r\n```\r\n\r\nc.py:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nimport a\r\n\r\nclass M(nn.Module):\r\n    def forward(self):\r\n        return a.Type(1)\r\n\r\ntorch.jit.script(M())\r\n```\r\n\r\nThen running `b.py` fails with:\r\n```python\r\nTraceback (most recent call last):\r\n  File ""b.py"", line 10, in <module>\r\n    torch.jit.script(M())\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1203, in script\r\n    return torch.jit.torch.jit._recursive.recursive_script(obj)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script\r\n    return copy_to_script_module(mod, overload_stubs + stubs)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module\r\n    torch.jit._create_methods_from_stubs(script_module, stubs)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError: \r\nUnknown type name 'a.Type':\r\nat b.py:7:25\r\n    def forward(self) -> a.Type:\r\n                         ~~~~~~ <--- HERE\r\n        return a.Type(1)\r\n```\r\n\r\nwhile running `c.py` fails with:\r\n```python\r\nTraceback (most recent call last):\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/_utils_internal.py"", line 46, in get_source_lines_and_file\r\n    sourcelines, file_lineno = inspect.getsourcelines(obj)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/inspect.py"", line 955, in getsourcelines\r\n    lines, lnum = findsource(object)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/inspect.py"", line 786, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""c.py"", line 10, in <module>\r\n    torch.jit.script(M())\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1203, in script\r\n    return torch.jit.torch.jit._recursive.recursive_script(obj)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 173, in recursive_script\r\n    return copy_to_script_module(mod, overload_stubs + stubs)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 95, in copy_to_script_module\r\n    torch.jit._create_methods_from_stubs(script_module, stubs)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1423, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1075, in _compile_and_register_class\r\n    ast = get_jit_class_def(obj, obj.__name__)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/frontend.py"", line 148, in get_jit_class_def\r\n    self_name=self_name) for method in methods]\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/frontend.py"", line 148, in <listcomp>\r\n    self_name=self_name) for method in methods]\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/jit/frontend.py"", line 160, in get_jit_def\r\n    sourcelines, file_lineno, filename = get_source_lines_and_file(fn)\r\n  File ""/opt/anaconda/envs/test/lib/python3.7/site-packages/torch/_utils_internal.py"", line 50, in get_source_lines_and_file\r\n    ""Make sure original .py files are available. Original error: {}"").format(filename, e))\r\nOSError: Can't get source for None. TorchScript requires source access in order to carry out compilation. Make sure original .py files are available. Original error: could not get source code\r\n```\r\n\r\nUsing a namedtuple defined *within* the same file works just fine though.\r\n\r\n## Expected behavior\r\n\r\nBoth `b.py` and `c.py` run without errors\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 8.3.0-6ubuntu1~18.04.1) 8.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.3\r\n[pip] torch==1.3.0\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] mkl                       2019.4                      243  \r\n[conda] pytorch                   1.3.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @suo",python\r\nfrom typing import NamedTuple\r\nclass Type(NamedTuple):\r\n    t: int\r\n
29032,"_load_from_state_dict in Observers should check if key is in state_dict \r\nIf a model uses observers (for quantisation) and then this model is saved and used to initialise another model, if this second model has additional observers, `_load_from_state_dict` will raise a `key error` since it currently doesn't check if the key is present in the `state_dict`. I think this check should be enforced if `strict = False`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Define two models where the second has an additional `torch.quantization.Observer`\r\n1. Save the first model\r\n1. Load the first model and use it to initialise the second model.\r\n\r\n### A Concrete example\r\n\r\n\r\n\r\nIn `load_from_state_dict` it attempts to directly `.pop()` from the inputed `state_dict` (i.e. that from the first model). And, because it doesn't contain `observer_` it will trigger an error as:\r\n\r\n\r\n```\r\n  File ""{...}/miniconda3/envs/Pytorch1.3/lib/python3.7/site-packages/torch/quantization/observer.py"", line 238, in _load_from_state_dict\r\n    self.min_val = state_dict.pop(prefix + 'min_val')\r\nKeyError: 'observer_.min_val'\r\n```\r\n\r\n## Expected behavior\r\n\r\nSimply checking if the desired key/val to `.pop()` is in `state_dict` will solve this issue. I think this should be the default if `strict = False`.\r\n\r\n## Environment\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: version 3.11.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.0.0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      233  \r\n[conda] mkl-service               2.3.0            py37hfbe908c_0  \r\n[conda] mkl_fft                   1.0.14           py37h5e564d8_0  \r\n[conda] mkl_random                1.1.0            py37ha771720_0  \r\n[conda] pytorch                   1.3.0                   py3.7_0    pytorch\r\n[conda] torchvision               0.4.1                  py37_cpu    pytorch\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a",oncall: quantization|triaged,raghuramank100,"\r\nIf a model uses observers (for quantisation) and then this model is saved and used to initialise another model, if this second model has additional observers, `_load_from_state_dict` will raise a `key error` since it currently doesn't check if the key is present in the `state_dict`. I think this check should be enforced if `strict = False`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Define two models where the second has an additional `torch.quantization.Observer`\r\n1. Save the first model\r\n1. Load the first model and use it to initialise the second model.\r\n\r\n### A Concrete example\r\n\r\n```python\r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass modelA(nn.Module):\r\n    def __init__(self):\r\n        super(modelA, self).__init__()\r\n\r\n        self.observer = torch.quantization.MinMaxObserver()\r\n\r\n    def forward(self, input):\r\n        self.observer(input)\r\n\r\nclass modelB(modelA):\r\n    def __init__(self):\r\n        super(modelB, self).__init__()\r\n\r\n        self.observer_ = torch.quantization.MinMaxObserver()\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    # model A\r\n    a = modelA()\r\n\r\n    # do something\r\n    input = torch.randn(32)\r\n    a(input)\r\n\r\n    # save a\r\n    torch.save(a.state_dict(), ""a.pt"")\r\n\r\n    # define model B\r\n    b = modelB()\r\n    target_model_dict = b.state_dict()\r\n\r\n    # Now we want to initilize all params in B with those from A.\r\n    # Here I follow the steps as in https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/3\r\n\r\n    # 0. loading pre-trained dict\r\n    source_dict = torch.load(""a.pt"")\r\n\r\n    # 1. filter out unnecessary keys\r\n    source_dict = {k: v for k, v in source_dict.items() if k in target_model_dict}\r\n\r\n    # 2. overwrite entries in the existing state dict\r\n    target_model_dict.update(source_dict) \r\n\r\n    # 3. load the new state dict\r\n    not_found = b.load_state_dict(source_dict, strict=False) # <-- will crash\r\n\r\n    print(not_found)\r\n\r\n```\r\n\r\nIn `load_from_state_dict` it attempts to directly `.pop()` from the inputed `state_dict` (i.e. that from the first model). And, because it doesn't contain `observer_` it will trigger an error as:\r\n\r\n\r\n```\r\n  File ""{...}/miniconda3/envs/Pytorch1.3/lib/python3.7/site-packages/torch/quantization/observer.py"", line 238, in _load_from_state_dict\r\n    self.min_val = state_dict.pop(prefix + 'min_val')\r\nKeyError: 'observer_.min_val'\r\n```\r\n\r\n## Expected behavior\r\n\r\nSimply checking if the desired key/val to `.pop()` is in `state_dict` will solve this issue. I think this should be the default if `strict = False`.\r\n\r\n## Environment\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: version 3.11.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.0.0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      233  \r\n[conda] mkl-service               2.3.0            py37hfbe908c_0  \r\n[conda] mkl_fft                   1.0.14           py37h5e564d8_0  \r\n[conda] mkl_random                1.1.0            py37ha771720_0  \r\n[conda] pytorch                   1.3.0                   py3.7_0    pytorch\r\n[conda] torchvision               0.4.1                  py37_cpu    pytorch\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","python\r\n\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass modelA(nn.Module):\r\n    def __init__(self):\r\n        super(modelA, self).__init__()\r\n\r\n        self.observer = torch.quantization.MinMaxObserver()\r\n\r\n    def forward(self, input):\r\n        self.observer(input)\r\n\r\nclass modelB(modelA):\r\n    def __init__(self):\r\n        super(modelB, self).__init__()\r\n\r\n        self.observer_ = torch.quantization.MinMaxObserver()\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    # model A\r\n    a = modelA()\r\n\r\n    # do something\r\n    input = torch.randn(32)\r\n    a(input)\r\n\r\n    # save a\r\n    torch.save(a.state_dict(), ""a.pt"")\r\n\r\n    # define model B\r\n    b = modelB()\r\n    target_model_dict = b.state_dict()\r\n\r\n    # Now we want to initilize all params in B with those from A.\r\n    # Here I follow the steps as in https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/3\r\n\r\n    # 0. loading pre-trained dict\r\n    source_dict = torch.load(""a.pt"")\r\n\r\n    # 1. filter out unnecessary keys\r\n    source_dict = {k: v for k, v in source_dict.items() if k in target_model_dict}\r\n\r\n    # 2. overwrite entries in the existing state dict\r\n    target_model_dict.update(source_dict) \r\n\r\n    # 3. load the new state dict\r\n    not_found = b.load_state_dict(source_dict, strict=False) # <-- will crash\r\n\r\n    print(not_found)\r\n\r\n"
29013,"[feature request] Reinterpret tensor as different dtypeI'd like to read audio file by using a sox utility directly. Currently I can do it as:\r\n\r\n\r\nIs there a way to reinterpret a torch byte tensor (or python's `bytearray(...)`) as another type without invoking first NumPy? (or a generic numpy.view(dtype) functionality) If not, I think it'd be quite useful (mainly for integral types, but also maybe for some integer bit-tricks on float32 tensors, which can hopefully be fused by jit)\r\n\r\nAlso reported in https://discuss.pytorch.org/t/reinterpret-pytorch-array-as-a-different-dtype/24256\r\n\r\nThis probably can also be helpful for conversions between complex <-> pair or real/imag.\n\ncc @mruberry @rgommers",triaged|module: numpy|function request,kurtamohler,"I'd like to read audio file by using a sox utility directly. Currently I can do it as:\r\n```python\r\nsignal = torch.from_numpy(np.frombuffer(subprocess.check_output(['sox', '-V0', audio_path, '-b', '16', '-e', 'signed', '--endian', 'little', '-r', str(sample_rate), '-c', '1', '-t', 'raw', '-'], dtype = np.int16))\r\n```\r\n\r\nIs there a way to reinterpret a torch byte tensor (or python's `bytearray(...)`) as another type without invoking first NumPy? (or a generic numpy.view(dtype) functionality) If not, I think it'd be quite useful (mainly for integral types, but also maybe for some integer bit-tricks on float32 tensors, which can hopefully be fused by jit)\r\n\r\nAlso reported in https://discuss.pytorch.org/t/reinterpret-pytorch-array-as-a-different-dtype/24256\r\n\r\nThis probably can also be helpful for conversions between complex <-> pair or real/imag.\n\ncc @mruberry @rgommers","python\r\nsignal = torch.from_numpy(np.frombuffer(subprocess.check_output(['sox', '-V0', audio_path, '-b', '16', '-e', 'signed', '--endian', 'little', '-r', str(sample_rate), '-c', '1', '-t', 'raw', '-'], dtype = np.int16))\r\n"
28902,"C++ `torch::tensor` by default gives a double tensor, which is different from Python `torch.tensor` behaviorExample:\r\nIn C++:\r\n\r\n\r\nIn Python:\r\n\n\ncc @yf225",module: cpp|triaged,yf225,"Example:\r\nIn C++:\r\n```cpp\r\ntorch::tensor({1., 2., 3.}).dtype() -> double\r\n```\r\n\r\nIn Python:\r\n```python\r\n>>> torch.tensor([1., 2., 3.]).dtype\r\ntorch.float32\r\n```\n\ncc @yf225","cpp\r\ntorch::tensor({1., 2., 3.}).dtype() -> double\r\n"
28882,"Support RRef[T].__call__(*args) which invokes T.__call__(*args) on ownerThis is a necessary syntax sugar to for the following use case:\r\n\r\n\r\n\r\nWe can implement `RRef[T].__call__(*args)` by sending a message to the owner,\r\ntriggering the owner to run `T.__call__(*args)` locally, and returning immediately another RRef of the output.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",triaged|module: rpc,mrshenli,"This is a necessary syntax sugar to for the following use case:\r\n\r\n```python\r\nclass MyModel(nn.Module):\r\n   pass\r\n\r\nmodel = rpc.remote(""worker0"", MyModel)\r\n# The following line should result in an RPC call to worker0 to\r\n# run MyModel constructor\r\noutputs_rref = model(inputs)\r\n```\r\n\r\nWe can implement `RRef[T].__call__(*args)` by sending a message to the owner,\r\ntriggering the owner to run `T.__call__(*args)` locally, and returning immediately another RRef of the output.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528","python\r\nclass MyModel(nn.Module):\r\n   pass\r\n\r\nmodel = rpc.remote(""worker0"", MyModel)\r\n# The following line should result in an RPC call to worker0 to\r\n# run MyModel constructor\r\noutputs_rref = model(inputs)\r\n"
28786,"RPC couldn't match torch.ones with requires_grad=True## \U0001f41b Bug\r\n\r\nI was trying to use `rpc.remote` to create a tensor with `requires_grad=True`, but hit the following error:\r\n\r\n```\r\nCouldn't match schema: aten::ones(int[] size, *, int? dtype=None, int? layout=None, Device? device=None, bool? pin_memory=None) -> (Tensor) to args: ((3, 3),) and kwar\r\ngs: {'requires_grad': True}, reason: Unknown keyword argument 'requires_grad' for operator 'aten::ones'. Schema: aten::ones(int[] size, *, int? dtype=None, int? layout\r\n=None, Device? device=None, bool? pin_memory=None) -> (Tensor)\r\n```\r\n\r\n### To Reproduce\r\n\r\n\r\n\r\n### Work Around\r\n\r\nWrap `torch.ones` into a user function can pass that user function to remote, e.g.,\r\n\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @bhosmer @smessmer @ljk53 @ailzhang @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @xush6528",high priority|triage review|oncall: distributed|module: internals|triaged|better-engineering|module: rpc,pritamdamania87,"## \U0001f41b Bug\r\n\r\nI was trying to use `rpc.remote` to create a tensor with `requires_grad=True`, but hit the following error:\r\n\r\n```\r\nCouldn't match schema: aten::ones(int[] size, *, int? dtype=None, int? layout=None, Device? device=None, bool? pin_memory=None) -> (Tensor) to args: ((3, 3),) and kwar\r\ngs: {'requires_grad': True}, reason: Unknown keyword argument 'requires_grad' for operator 'aten::ones'. Schema: aten::ones(int[] size, *, int? dtype=None, int? layout\r\n=None, Device? device=None, bool? pin_memory=None) -> (Tensor)\r\n```\r\n\r\n### To Reproduce\r\n\r\n```python\r\nrpc.remote(dst, torch.ones, args=((3, 3),), kwargs={""requires_grad"":True})\r\n```\r\n\r\n### Work Around\r\n\r\nWrap `torch.ones` into a user function can pass that user function to remote, e.g.,\r\n\r\n```python\r\ndef _torch_ones(sizes, requires_grad=False):\r\n    return torch.ones(sizes, requires_grad=requires_grad)\r\n\r\nrpc.remote(dst, _torch_ones, args=((2, 2), ), kwargs={""requires_grad"":True})\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @bhosmer @smessmer @ljk53 @ailzhang @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @xush6528","python\r\nrpc.remote(dst, torch.ones, args=((3, 3),), kwargs={""requires_grad"":True})\r\n"
28739,"[JIT] `iterator expression` is expected to accept `Tensor`- PyTorch 1.3.0\r\n- Python 3.6.9\r\n- CUDA 10.1\r\n\r\n\r\nNow, this code will output\r\n```\r\niterator expression is expected to be a list, iterable, or range, found value of type 'Tensor'\r\n```\r\nHowever, this feature is useful for the framework lacks the function which is able to do the follow work:\r\n\r\nConvert\r\n```\r\nTensor([\r\n    [1, 2, 3]\r\n    [4, 5, 6]\r\n])\r\n```\r\nTo\r\n```\r\n[\r\n    Tensor([1, 2, 3])\r\n    Tensor([4, 5, 6])\r\n]\r\n```\r\n\r\nThanks!\n\ncc @suo",oncall: jit|triaged,eellison,"- PyTorch 1.3.0\r\n- Python 3.6.9\r\n- CUDA 10.1\r\n\r\n```python\r\nimport torch\r\n\r\n\r\n@torch.jit.script\r\ndef foo(input: torch.Tensor):\r\n    return [m for m in input]\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(foo(torch.ones(2, 3)))\r\n```\r\nNow, this code will output\r\n```\r\niterator expression is expected to be a list, iterable, or range, found value of type 'Tensor'\r\n```\r\nHowever, this feature is useful for the framework lacks the function which is able to do the follow work:\r\n\r\nConvert\r\n```\r\nTensor([\r\n    [1, 2, 3]\r\n    [4, 5, 6]\r\n])\r\n```\r\nTo\r\n```\r\n[\r\n    Tensor([1, 2, 3])\r\n    Tensor([4, 5, 6])\r\n]\r\n```\r\n\r\nThanks!\n\ncc @suo","python\r\nimport torch\r\n\r\n\r\n@torch.jit.script\r\ndef foo(input: torch.Tensor):\r\n    return [m for m in input]\r\n\r\n\r\nif __name__ == '__main__':\r\n    print(foo(torch.ones(2, 3)))\r\n"
28714,"Channels last max_pool2d fails with specific inputsRepro: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-616280bfd069> in <module>\r\n----> 1 torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\r\n\r\nRuntimeError: max_pool2d_with_indices_out_cuda_frame failed with error code 0\r\n```\n\ncc @VitalyFedyunin @jamesr66a",module: cuda|triaged|module: memory format,ifedan,"Repro: \r\n```python\r\nx = torch.randn([1, 1, 16777217, 2])                                                                                                                                                                                                                   \r\ninput = x.cuda().contiguous(memory_format=torch.channels_last)                                                                                                                                                                                         \r\nkernel_size, stride, padding, dilation, ceil_mode = 1, 1, 0, 1, False                                                                                                                                                                                 \r\ntorch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                                                                                                            \r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-616280bfd069> in <module>\r\n----> 1 torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\r\n\r\nRuntimeError: max_pool2d_with_indices_out_cuda_frame failed with error code 0\r\n```\n\ncc @VitalyFedyunin @jamesr66a","python\r\nx = torch.randn([1, 1, 16777217, 2])                                                                                                                                                                                                                   \r\ninput = x.cuda().contiguous(memory_format=torch.channels_last)                                                                                                                                                                                         \r\nkernel_size, stride, padding, dilation, ceil_mode = 1, 1, 0, 1, False                                                                                                                                                                                 \r\ntorch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)                                                                                                                                                                            \r\n"
28649,"error with detect_anomaly and SparseTensorWhen using the code snippet bellow, I get this error: `RuntimeError: No function is registered for schema aten::ne(Tensor self, Tensor other) -> Tensor on backend SparseCUDA`\r\n\r\n\r\n\r\nPyTorch version is 1.2.0\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen",high priority|module: sparse|module: autograd|triaged,prasunanand,"When using the code snippet bellow, I get this error: `RuntimeError: No function is registered for schema aten::ne(Tensor self, Tensor other) -> Tensor on backend SparseCUDA`\r\n\r\n```python\r\nimport torch\r\nfrom torch import autograd\r\n\r\n\r\nwith autograd.detect_anomaly():\r\n    idxs = torch.tensor([[0, 2, 3], [1, 1, 2], [2, 1, 4], [3, 5, 1]], device=0)\r\n    values = torch.tensor([1.0, 2.0, 3.0, 4.0], device=0, requires_grad=True)\r\n\r\n    sparse_tensor = torch.sparse_coo_tensor(indices=idxs.t(), values=values, size=(4, 6, 5))\r\n    dense_tensor = torch.sparse.sum(sparse_tensor, dim=2).to_dense()\r\n\r\n    dense_tensor.sum().backward()\r\n```\r\n\r\nPyTorch version is 1.2.0\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen","python\r\nimport torch\r\nfrom torch import autograd\r\n\r\n\r\nwith autograd.detect_anomaly():\r\n    idxs = torch.tensor([[0, 2, 3], [1, 1, 2], [2, 1, 4], [3, 5, 1]], device=0)\r\n    values = torch.tensor([1.0, 2.0, 3.0, 4.0], device=0, requires_grad=True)\r\n\r\n    sparse_tensor = torch.sparse_coo_tensor(indices=idxs.t(), values=values, size=(4, 6, 5))\r\n    dense_tensor = torch.sparse.sum(sparse_tensor, dim=2).to_dense()\r\n\r\n    dense_tensor.sum().backward()\r\n"
28559,"[jit] Function attributes skipping other attributes\r\n\r\noutputs\r\n\r\n```\r\nRuntimeError: \r\nModule 'N' has no attribute 'norm' :\r\nat ../test.py:79:11\r\n    def forward(self, src):\r\n        output = src\r\n        if self.norm is not None:\r\n           ~~~~~~~~~ <--- HERE\r\n            output = self.norm(output)\r\n        return output\r\n'N.forward' is being compiled since it was called from 'M.forward'\r\nat ../test.py:90:28\r\n    def forward(self, x):\r\n        return self.encoder(x)\r\n                            ~ <--- HERE\r\n```\r\n\r\nRemoving the `self.activation = ....` makes it work\n\ncc @suo",oncall: jit|triaged,suo,"```python\r\nclass N(nn.Module):\r\n    __constants__ = ['norm']\r\n\r\n    def __init__(self, norm=None):\r\n        super(N, self).__init__()\r\n        self.activation = torch.nn.functional.relu  # Commenting out this line makes it work\r\n        self.norm = norm\r\n\r\n    def forward(self, src):\r\n        output = src\r\n        if self.norm is not None:\r\n            output = self.norm(output)\r\n        return output\r\n\r\nclass M(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        encoder_norm = nn.ReLU()\r\n        self.encoder = N(encoder_norm)\r\n\r\n    def forward(self, x):\r\n        return self.encoder(x)\r\n\r\ntorch.jit.script(M())\r\n```\r\n\r\noutputs\r\n\r\n```\r\nRuntimeError: \r\nModule 'N' has no attribute 'norm' :\r\nat ../test.py:79:11\r\n    def forward(self, src):\r\n        output = src\r\n        if self.norm is not None:\r\n           ~~~~~~~~~ <--- HERE\r\n            output = self.norm(output)\r\n        return output\r\n'N.forward' is being compiled since it was called from 'M.forward'\r\nat ../test.py:90:28\r\n    def forward(self, x):\r\n        return self.encoder(x)\r\n                            ~ <--- HERE\r\n```\r\n\r\nRemoving the `self.activation = ....` makes it work\n\ncc @suo","python\r\nclass N(nn.Module):\r\n    __constants__ = ['norm']\r\n\r\n    def __init__(self, norm=None):\r\n        super(N, self).__init__()\r\n        self.activation = torch.nn.functional.relu  # Commenting out this line makes it work\r\n        self.norm = norm\r\n\r\n    def forward(self, src):\r\n        output = src\r\n        if self.norm is not None:\r\n            output = self.norm(output)\r\n        return output\r\n\r\nclass M(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        encoder_norm = nn.ReLU()\r\n        self.encoder = N(encoder_norm)\r\n\r\n    def forward(self, x):\r\n        return self.encoder(x)\r\n\r\ntorch.jit.script(M())\r\n"
28549,"Expose DifferentiableGraphBackward to python## \U0001f680 Feature\r\n\r\nCurrently, there is no easy way of visualizing the graph generated by autodiff in jit. We want something that could do\r\n\r\n\r\n\r\n## Additional context\r\n\r\nIn autograd, the `Node` for `grad_fn` of a JIT graph is defined as\r\n\r\n```\r\nstruct DifferentiableGraphBackward : public autograd::Node\r\n``` \r\nin `torch/csrc/jit/graph_executor.cpp`\r\n\r\nThis class does not have python correspondence, therefore python only views it as a general `CppFunction`. We need to create a Python class for it so that we could access additional information.\r\n\r\n**Please confirm if this feature sounds useful and if yes, assign me to this issue and I will work on it.**\n\ncc @suo @ezyang @SsnL @albanD @zou3519 @gqchen",oncall: jit|feature|module: autograd|triaged,zasdfgbnm,"## \U0001f680 Feature\r\n\r\nCurrently, there is no easy way of visualizing the graph generated by autodiff in jit. We want something that could do\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return (a + 100) * 5\r\n\r\na = torch.tensor(0., requires_grad=True)\r\nresult = f(a)\r\n\r\nprint(result.grad_fn.graph)\r\n```\r\n\r\n## Additional context\r\n\r\nIn autograd, the `Node` for `grad_fn` of a JIT graph is defined as\r\n\r\n```\r\nstruct DifferentiableGraphBackward : public autograd::Node\r\n``` \r\nin `torch/csrc/jit/graph_executor.cpp`\r\n\r\nThis class does not have python correspondence, therefore python only views it as a general `CppFunction`. We need to create a Python class for it so that we could access additional information.\r\n\r\n**Please confirm if this feature sounds useful and if yes, assign me to this issue and I will work on it.**\n\ncc @suo @ezyang @SsnL @albanD @zou3519 @gqchen","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return (a + 100) * 5\r\n\r\na = torch.tensor(0., requires_grad=True)\r\nresult = f(a)\r\n\r\nprint(result.grad_fn.graph)\r\n"
28502,"Boolean tensor transpose bug in 1.3.0## \U0001f41b Bug\r\n\r\nThere is problem when use a transpose a boolean tensor. It seems like that it hasn't been transposed.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nI got\r\n\r\n```\r\ntensor([[False, False],\r\n        [ True, False]])\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\ntensor([[False, True],\r\n        [False, False]])\r\n```\r\n\r\nHowever, when I try an int tensor, it doesn't happen.\r\n\r\n## Environment\r\n\r\npytorch 1.3.0\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @izdeby",high priority|triage review|module: type promotion|module: boolean tensor,nairbv,"## \U0001f41b Bug\r\n\r\nThere is problem when use a transpose a boolean tensor. It seems like that it hasn't been transposed.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.tensor([[True,  True],\r\n                  [False, True]])\r\nprint(a.t() == 0)\r\n```\r\n\r\nI got\r\n\r\n```\r\ntensor([[False, False],\r\n        [ True, False]])\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\ntensor([[False, True],\r\n        [False, False]])\r\n```\r\n\r\nHowever, when I try an int tensor, it doesn't happen.\r\n\r\n## Environment\r\n\r\npytorch 1.3.0\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @izdeby","python\r\nimport torch\r\n\r\na = torch.tensor([[True,  True],\r\n                  [False, True]])\r\nprint(a.t() == 0)\r\n"
28434,Quantized concatenation returns `-inf` for zero input## \U0001f41b Bug\r\n\r\n\r\n\r\nreturns the values that have `-inf` as a result.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100,oncall: quantization|triaged,z-a-f,"## \U0001f41b Bug\r\n\r\n```python\r\nqcat = torch.ops.quantized.cat\r\n\r\ns = 1.479488579387789e+37\r\nz = 23\r\n\r\n\r\nx = np.array([[[[0.]]]], dtype=np.float32)\r\nx = np.repeat(x, 70/x.shape[3], 3)\r\n\r\nx = torch.from_numpy(np.ascontiguousarray(x))\r\ny = x.clone()\r\n\r\nqx = torch.quantize_per_tensor(x, s, z, torch.quint8).permute([0, 3, 1, 2])\r\nqy = torch.quantize_per_tensor(y, s, z, torch.quint8).permute([0, 3, 1, 2])\r\n\r\nprint(qcat([qx, qy], scale=s, zero_point=z, dim=1))\r\n```\r\n\r\nreturns the values that have `-inf` as a result.\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jianyuh @dzhulgakov @raghuramank100","python\r\nqcat = torch.ops.quantized.cat\r\n\r\ns = 1.479488579387789e+37\r\nz = 23\r\n\r\n\r\nx = np.array([[[[0.]]]], dtype=np.float32)\r\nx = np.repeat(x, 70/x.shape[3], 3)\r\n\r\nx = torch.from_numpy(np.ascontiguousarray(x))\r\ny = x.clone()\r\n\r\nqx = torch.quantize_per_tensor(x, s, z, torch.quint8).permute([0, 3, 1, 2])\r\nqy = torch.quantize_per_tensor(y, s, z, torch.quint8).permute([0, 3, 1, 2])\r\n\r\nprint(qcat([qx, qy], scale=s, zero_point=z, dim=1))\r\n"
28370,`at::parallel_for` does not propagate thread-local variables to child threads in embedding_renorm_See repro below:\r\n\r\n\r\nOutputs:\r\n```\r\nRunning : 1000\r\nThis should be None:  None\r\nRunning : 1001\r\nThis should be None:  <CopySlices object at 0x7f02faa5e7a8>\r\n```\r\n\r\nCurrent supposition is that multi-threaded operations and thread local NoGradGuard could be problematic.\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel @mruberry,high priority|module: performance|module: internals|module: autograd|triaged|module: multithreading,albanD,"See repro below:\r\n\r\n```python\r\nimport torch\r\n\r\ndef run(val):\r\n    print(""Running :"", val)\r\n    weights = torch.rand(100, 64, requires_grad=True)\r\n    inp = torch.rand(val).mul(100).long()\r\n    with torch.no_grad():\r\n        torch.embedding_renorm_(weights, inp, 1.0, 2)\r\n    print(""This should be None: "", weights.grad_fn)\r\n\r\nrun(1000)\r\nrun(1001)\r\n```\r\nOutputs:\r\n```\r\nRunning : 1000\r\nThis should be None:  None\r\nRunning : 1001\r\nThis should be None:  <CopySlices object at 0x7f02faa5e7a8>\r\n```\r\n\r\nCurrent supposition is that multi-threaded operations and thread local NoGradGuard could be problematic.\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel @mruberry","python\r\nimport torch\r\n\r\ndef run(val):\r\n    print(""Running :"", val)\r\n    weights = torch.rand(100, 64, requires_grad=True)\r\n    inp = torch.rand(val).mul(100).long()\r\n    with torch.no_grad():\r\n        torch.embedding_renorm_(weights, inp, 1.0, 2)\r\n    print(""This should be None: "", weights.grad_fn)\r\n\r\nrun(1000)\r\nrun(1001)\r\n"
28361,"Improved Tensor subclassing support, preserving subclasses on function/method calls## \U0001f680 Feature\r\n\r\nRelated: #22402 \r\n\r\nThis feature proposes passing through `Tensor` subclasses via `__torch_function__`.\r\n\r\n### Desired Behaviour\r\nExample desired behavior would be:\r\n\r\n\r\n\r\n### Goals\r\n\r\nQuoting #22402 \r\n\r\n> These are _potential_ goals that have been collected from the above referenced PRs, other PyTorch issues (referenced in the relevant sections), as well as from discussions with mainly Edward Yang, and also other PyTorch and NumPy maintainers:\r\n> \r\n> 1. Support subclassing `torch.Tensor` in Python\r\n> 2. Preserve `Tensor` subclasses when calling `torch` functions on them\r\n> 3. Preserve `Tensor` subclasses when calling `numpy` functions on them\r\n> 4. Use the NumPy API with PyTorch tensors (i.e. NumPy API calls dispatch to `torch` functions)\r\n> 5. Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `Tensor` subclasses\r\n> 6. Reuse NumPy ufunc implementations directly from PyTorch\r\n> 7. Allow operations on mixed array types, e.g. `tensor + ndarray`\r\n\r\nAdditionally, from https://github.com/pytorch/pytorch/issues/28361#issuecomment-544520934:\r\n\r\n> * Preserve `Tensor` subclasses when calling `Tensor` methods \r\n> * Propagating subclass instances correctly also with operators, using views/slices/etc.\r\n\r\n### Rough Sketch of Implementation\r\nAnything with a type like a built-in tensor will bypass `__torch_function__` via their fast path (although they will have a default implementation) but anything else defined by an external library will have the option to allow it.\r\n\r\nThe following code snippet shows what the default `__torch_function__` on `TensorBase` would look like.\r\n\r\n\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jph00 @rgommers ",high priority|feature|triaged|module: numpy,hameerabbasi,"## \U0001f680 Feature\r\n\r\nRelated: #22402 \r\n\r\nThis feature proposes passing through `Tensor` subclasses via `__torch_function__`.\r\n\r\n### Desired Behaviour\r\nExample desired behavior would be:\r\n\r\n```python\r\nclass MyTensor(torch.Tensor):\r\n    _additional_attribute = ""Kartoffel""\r\n\r\na = MyTensor([0, 1, 2, 3])\r\n# b should be a MyTensor object, with all class attributes passed through.\r\nb = torch_function(a)\r\n```\r\n\r\n### Goals\r\n\r\nQuoting #22402 \r\n\r\n> These are _potential_ goals that have been collected from the above referenced PRs, other PyTorch issues (referenced in the relevant sections), as well as from discussions with mainly Edward Yang, and also other PyTorch and NumPy maintainers:\r\n> \r\n> 1. Support subclassing `torch.Tensor` in Python\r\n> 2. Preserve `Tensor` subclasses when calling `torch` functions on them\r\n> 3. Preserve `Tensor` subclasses when calling `numpy` functions on them\r\n> 4. Use the NumPy API with PyTorch tensors (i.e. NumPy API calls dispatch to `torch` functions)\r\n> 5. Use the PyTorch API with `torch.Tensor`-like objects that are _not_ `Tensor` subclasses\r\n> 6. Reuse NumPy ufunc implementations directly from PyTorch\r\n> 7. Allow operations on mixed array types, e.g. `tensor + ndarray`\r\n\r\nAdditionally, from https://github.com/pytorch/pytorch/issues/28361#issuecomment-544520934:\r\n\r\n> * Preserve `Tensor` subclasses when calling `Tensor` methods \r\n> * Propagating subclass instances correctly also with operators, using views/slices/etc.\r\n\r\n### Rough Sketch of Implementation\r\nAnything with a type like a built-in tensor will bypass `__torch_function__` via their fast path (although they will have a default implementation) but anything else defined by an external library will have the option to allow it.\r\n\r\nThe following code snippet shows what the default `__torch_function__` on `TensorBase` would look like.\r\n\r\n```python\r\nclass Tensor:\r\n    def __torch_function__(self, f, t, a, kw):\r\n        if not all(issubclass(ti, TensorBase) for ti in t):\r\n            return NotImplemented\r\n        result = f._wrapped(*a, **kw)\r\n        return type(self)(result)\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @jerryzh168 @jph00 @rgommers ","python\r\nclass MyTensor(torch.Tensor):\r\n    _additional_attribute = ""Kartoffel""\r\n\r\na = MyTensor([0, 1, 2, 3])\r\n# b should be a MyTensor object, with all class attributes passed through.\r\nb = torch_function(a)\r\n"
28347,"arange sometimes changes dimensionality of output tensor## \U0001f41b Bug\r\n\r\nUsing `torch.arange` with an `out=` target Tensor object, it will sometimes change the dimensionality of the target Tensor.  I'm creating a (1,N) tensor, and sometimes calling `arange` changes it to a (N+1) vector.  Same on cuda or cpu.\r\n\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nThis will output:\r\n```\r\nTrying 45...\r\n  Before arange shape is torch.Size([1, 45])\r\n  After arange shape is torch.Size([1, 45])\r\nTrying 46...\r\n  Before arange shape is torch.Size([1, 46])\r\n  After arange shape is torch.Size([1, 46])\r\nTrying 47...\r\n  Before arange shape is torch.Size([1, 47])\r\n  After arange shape is torch.Size([1, 47])\r\nTrying 48...\r\n  Before arange shape is torch.Size([1, 48])\r\n  After arange shape is torch.Size([1, 48])\r\nTrying 49...\r\n  Before arange shape is torch.Size([1, 49])\r\n  After arange shape is torch.Size([50])\r\nTraceback (most recent call last):\r\n  File ""shapebug.py"", line 10, in <module>\r\n    assert len(line.shape) == 2\r\nAssertionError\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\n`torch.arange` should not change the shape of the target Tensor.\r\n\r\n## Environment\r\n\r\nPyTorch 1.3, Ubuntu 18.04LTS, py 3.6.9 under Anaconda.  In detail (from `collect_env.py`):\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] facenet-pytorch==0.3.1\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] blas                      1.0                         mkl  \r\n[conda] facenet-pytorch           0.3.1                    pypi_0    pypi\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py36he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py36ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nI started doing it this way (out= Tensor) because I wanted to make sure the whole thing to happen on GPU, and that was the first thing I figured out.  I see another way to do this now -- `line=torch.arange(device='cuda')` which works.  But this seems like a bug.\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168",high priority|triaged|module: safe resize,prasunanand,"## \U0001f41b Bug\r\n\r\nUsing `torch.arange` with an `out=` target Tensor object, it will sometimes change the dimensionality of the target Tensor.  I'm creating a (1,N) tensor, and sometimes calling `arange` changes it to a (N+1) vector.  Same on cuda or cpu.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n  \r\nfor N in range(45,100):\r\n    print(f""Trying {N}..."")\r\n    line = torch.zeros(size=(1,N))\r\n    print(f""  Before arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n    torch.arange(-1, 1, 2/N, dtype=torch.float32, out=line)\r\n    print(f""  After arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n```\r\n\r\nThis will output:\r\n```\r\nTrying 45...\r\n  Before arange shape is torch.Size([1, 45])\r\n  After arange shape is torch.Size([1, 45])\r\nTrying 46...\r\n  Before arange shape is torch.Size([1, 46])\r\n  After arange shape is torch.Size([1, 46])\r\nTrying 47...\r\n  Before arange shape is torch.Size([1, 47])\r\n  After arange shape is torch.Size([1, 47])\r\nTrying 48...\r\n  Before arange shape is torch.Size([1, 48])\r\n  After arange shape is torch.Size([1, 48])\r\nTrying 49...\r\n  Before arange shape is torch.Size([1, 49])\r\n  After arange shape is torch.Size([50])\r\nTraceback (most recent call last):\r\n  File ""shapebug.py"", line 10, in <module>\r\n    assert len(line.shape) == 2\r\nAssertionError\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\n`torch.arange` should not change the shape of the target Tensor.\r\n\r\n## Environment\r\n\r\nPyTorch 1.3, Ubuntu 18.04LTS, py 3.6.9 under Anaconda.  In detail (from `collect_env.py`):\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 with Max-Q Design\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] facenet-pytorch==0.3.1\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.3.0\r\n[pip] torchvision==0.4.1a0+d94043a\r\n[conda] blas                      1.0                         mkl  \r\n[conda] facenet-pytorch           0.3.1                    pypi_0    pypi\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py36he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py36ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0  \r\n[conda] pytorch                   1.3.0           py3.6_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.1                py36_cu100    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nI started doing it this way (out= Tensor) because I wanted to make sure the whole thing to happen on GPU, and that was the first thing I figured out.  I see another way to do this now -- `line=torch.arange(device='cuda')` which works.  But this seems like a bug.\r\n\n\ncc @ezyang @gchanan @zou3519 @jerryzh168","python\r\nimport torch\r\n  \r\nfor N in range(45,100):\r\n    print(f""Trying {N}..."")\r\n    line = torch.zeros(size=(1,N))\r\n    print(f""  Before arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n    torch.arange(-1, 1, 2/N, dtype=torch.float32, out=line)\r\n    print(f""  After arange shape is {line.shape}"")\r\n    assert len(line.shape) == 2\r\n"
28280,"torch.jit.ignore messes with buffer device placement## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nRun this minimal example\r\n\r\n\r\n\r\nOutputs:\r\n```\r\ncuda:0\r\ncuda:0\r\ncpu\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-49-a9cb0d104a1f> in <module>\r\n     23 m = m.to('cuda')\r\n     24 print(m.a.device)  # prints cuda\r\n---> 25 m(torch.zeros(10).to('cuda'))  # fails because m.a is on cpu inside non_scriptable\r\n\r\n~/.conda/envs/MTMSF/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    543             result = self._slow_forward(*input, **kwargs)\r\n    544         else:\r\n--> 545             result = self.forward(*input, **kwargs)\r\n    546         for hook in self._forward_hooks.values():\r\n    547             hook_result = hook(self, input, result)\r\n\r\nRuntimeError: RuntimeError: expected device cuda:0 but got device cpu\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nScripted and non-scripted versions have identical behaviour.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.0.dev20190917\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX TITAN Black\r\nGPU 1: GeForce GTX TITAN Black\r\n\r\nNvidia driver version: 418.40.04\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] torch==1.3.0.dev20190917\r\n[pip] torchvision==0.5.0a0+e8b830f\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.3.0.dev20190917 py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch-nightly\r\n[conda] torchvision               0.5.0.dev20190917      py37_cu100    pytorch-nightly\r\n\r\ncc @suo",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nRun this minimal example\r\n\r\n```python\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.register_buffer('a', torch.zeros(10))\r\n        \r\n    def forward(self, inputs):\r\n        return self.non_scriptable(inputs)\r\n    \r\n    @torch.jit.ignore\r\n    def non_scriptable(self, inputs):\r\n        print(self.a.device)\r\n        return inputs * self.a\r\n\r\n\r\n# Not scripted: normal behaviour\r\nm = Model()\r\nm = m.to('cuda')\r\nm(torch.zeros(10).to('cuda'))\r\n\r\n# Scripted: fails\r\nm = Model()\r\nm = torch.jit.script(m)\r\nm = m.to('cuda')\r\nprint(m.a.device)  # prints cuda\r\nm(torch.zeros(10).to('cuda'))  # fails because m.a is on cpu inside non_scriptable\r\n```\r\n\r\nOutputs:\r\n```\r\ncuda:0\r\ncuda:0\r\ncpu\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-49-a9cb0d104a1f> in <module>\r\n     23 m = m.to('cuda')\r\n     24 print(m.a.device)  # prints cuda\r\n---> 25 m(torch.zeros(10).to('cuda'))  # fails because m.a is on cpu inside non_scriptable\r\n\r\n~/.conda/envs/MTMSF/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    543             result = self._slow_forward(*input, **kwargs)\r\n    544         else:\r\n--> 545             result = self.forward(*input, **kwargs)\r\n    546         for hook in self._forward_hooks.values():\r\n    547             hook_result = hook(self, input, result)\r\n\r\nRuntimeError: RuntimeError: expected device cuda:0 but got device cpu\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nScripted and non-scripted versions have identical behaviour.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.0.dev20190917\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX TITAN Black\r\nGPU 1: GeForce GTX TITAN Black\r\n\r\nNvidia driver version: 418.40.04\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] torch==1.3.0.dev20190917\r\n[pip] torchvision==0.5.0a0+e8b830f\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.3.0.dev20190917 py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch-nightly\r\n[conda] torchvision               0.5.0.dev20190917      py37_cu100    pytorch-nightly\r\n\r\ncc @suo","python\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.register_buffer('a', torch.zeros(10))\r\n        \r\n    def forward(self, inputs):\r\n        return self.non_scriptable(inputs)\r\n    \r\n    @torch.jit.ignore\r\n    def non_scriptable(self, inputs):\r\n        print(self.a.device)\r\n        return inputs * self.a\r\n\r\n\r\n# Not scripted: normal behaviour\r\nm = Model()\r\nm = m.to('cuda')\r\nm(torch.zeros(10).to('cuda'))\r\n\r\n# Scripted: fails\r\nm = Model()\r\nm = torch.jit.script(m)\r\nm = m.to('cuda')\r\nprint(m.a.device)  # prints cuda\r\nm(torch.zeros(10).to('cuda'))  # fails because m.a is on cpu inside non_scriptable\r\n"
28071,"net after quantize_qat can not improve accuracy## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\nI have a vgg net, and want to using training-aware quant function in pytorch-1.3, then i do as follows:\r\n\r\n\r\n\r\nin train_echo, the code is follows:\r\n\r\n\r\n\r\nI can see that parameters in net is modified every echo, but the Acc is always 10% which means randn in CIFAR10.\r\nSo I am very confused. Is any idea?\r\n\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: quantization|triaged,raghuramank100,"## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\nI have a vgg net, and want to using training-aware quant function in pytorch-1.3, then i do as follows:\r\n\r\n```python\r\nnet.eval()\r\n# Add quant config\r\nnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\r\n# merge conv+bn+relu\r\nfuse_list = [['features.0', 'features.1', 'features.2'], \\\r\n             ['features.3', 'features.4', 'features.5'], \\\r\n             ['features.7', 'features.8', 'features.9'], \\\r\n             ['features.10', 'features.11', 'features.12'], \\\r\n             ['features.14', 'features.15', 'features.16'], \\\r\n             ['features.17', 'features.18', 'features.19'], \\\r\n             ['features.20', 'features.21', 'features.22'], \\\r\n             ['features.24', 'features.25', 'features.26'], \\\r\n             ['features.27', 'features.28', 'features.29'], \\\r\n             ['features.30', 'features.31', 'features.32'], \\\r\n             ['features.34', 'features.35', 'features.36'], \\\r\n             ['features.37', 'features.38', 'features.39'], \\\r\n             ['features.40', 'features.41', 'features.42']]\r\nnet = torch.quantization.fuse_modules(net, fuse_list)\r\n# Add quant and dequant function\r\nnet = torch.quantization.QuantWrapper(net)\r\n# quant and train model\r\nnet = torch.quantization.quantize_qat(net, train_echo, trainloader)\r\nprint(net)\r\n```\r\n\r\nin train_echo, the code is follows:\r\n\r\n```python\r\ndef train_echo(net, data):\r\n    net.train()\r\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\r\n    for name, param in net.named_parameters():\r\n        if param.requires_grad:\r\n            print(name)\r\n    for echo in range(0, 1):\r\n        train_loss = 0\r\n        correct = 0\r\n        total = 0\r\n        for batch_idx, (inputs, targets) in enumerate(data):\r\n            print(net.state_dict()['module.features.40.weight'][0])\r\n            inputs, targets = inputs.to(device), targets.to(device)\r\n            optimizer.zero_grad()\r\n            outputs = net(inputs)\r\n            loss = criterion(outputs, targets)\r\n            loss.backward()\r\n            optimizer.step()\r\n            #print(net.state_dict()['module.features.40.weight'][0])\r\n            train_loss += loss.item()\r\n            _, predicted = outputs.max(1)\r\n            total += targets.size(0)\r\n            correct += predicted.eq(targets).sum().item()\r\n\r\n            progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\r\n                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\r\n```\r\n\r\nI can see that parameters in net is modified every echo, but the Acc is always 10% which means randn in CIFAR10.\r\nSo I am very confused. Is any idea?\r\n\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100","python\r\nnet.eval()\r\n# Add quant config\r\nnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\r\n# merge conv+bn+relu\r\nfuse_list = [['features.0', 'features.1', 'features.2'], \\\r\n             ['features.3', 'features.4', 'features.5'], \\\r\n             ['features.7', 'features.8', 'features.9'], \\\r\n             ['features.10', 'features.11', 'features.12'], \\\r\n             ['features.14', 'features.15', 'features.16'], \\\r\n             ['features.17', 'features.18', 'features.19'], \\\r\n             ['features.20', 'features.21', 'features.22'], \\\r\n             ['features.24', 'features.25', 'features.26'], \\\r\n             ['features.27', 'features.28', 'features.29'], \\\r\n             ['features.30', 'features.31', 'features.32'], \\\r\n             ['features.34', 'features.35', 'features.36'], \\\r\n             ['features.37', 'features.38', 'features.39'], \\\r\n             ['features.40', 'features.41', 'features.42']]\r\nnet = torch.quantization.fuse_modules(net, fuse_list)\r\n# Add quant and dequant function\r\nnet = torch.quantization.QuantWrapper(net)\r\n# quant and train model\r\nnet = torch.quantization.quantize_qat(net, train_echo, trainloader)\r\nprint(net)\r\n"
28070,"PyTorch1.3 cannot find kernel to dispatch when runing a Quantization Model## \u2753 PyTorch1.3 cannot find kernel to dispatch when running a Quantization Model\r\nWhen I tried to quantize mobilenet_v2, I followed the [tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html). I'm sure I have done `layer fusion`, `set quant config`, `calibration`, `model conversion`, and finally I got the quantized model with a smaller size. However, an error occurs when I tried to run this quantized model:\r\n``` python\r\ninput = torch.randn([1, 3, 224, 224], dtype=torch.float)\r\nout = quant_model(input)\r\n```\r\n\r\n\r\nIs there anyone who may help me? Thanks a lot!!!  :blue_heart:\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100",oncall: quantization|triaged,raghuramank100,"## \u2753 PyTorch1.3 cannot find kernel to dispatch when running a Quantization Model\r\nWhen I tried to quantize mobilenet_v2, I followed the [tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html). I'm sure I have done `layer fusion`, `set quant config`, `calibration`, `model conversion`, and finally I got the quantized model with a smaller size. However, an error occurs when I tried to run this quantized model:\r\n``` python\r\ninput = torch.randn([1, 3, 224, 224], dtype=torch.float)\r\nout = quant_model(input)\r\n```\r\n\r\n```bash\r\nRuntimeError: Didn't find kernel to dispatch to for operator 'quantized::conv2d_relu'. Tried to look up kernel for dispatch key 'CPUTensorId'. Registered dispatch keys are: [QuantizedCPUTensorId] (lookup_ at /pytorch/aten/src/ATen/core/dispatch/DispatchTable.h:249)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fdaafe91813 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1b46361 (0x7fda56bdc361 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch.so)\r\nframe #2: <unknown function> + 0x3e420a8 (0x7fda58ed80a8 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x4fd81c (0x7fdab502981c in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: <unknown function> + 0x4cc184 (0x7fdab4ff8184 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x210534 (0x7fdab4d3c534 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: python() [0x50682c]\r\n<omitting python frames>\r\nframe #8: python() [0x596285]\r\nframe #11: python() [0x504232]\r\nframe #12: python() [0x5966f8]\r\nframe #13: python() [0x54d4ee]\r\nframe #15: python() [0x5067b0]\r\nframe #17: python() [0x596285]\r\nframe #20: python() [0x504232]\r\nframe #21: python() [0x5966f8]\r\nframe #22: python() [0x54d4ee]\r\nframe #24: python() [0x5067b0]\r\nframe #26: python() [0x596285]\r\nframe #29: python() [0x504232]\r\nframe #30: python() [0x5966f8]\r\nframe #31: python() [0x54d4ee]\r\nframe #33: python() [0x5067b0]\r\nframe #35: python() [0x596285]\r\nframe #38: python() [0x504232]\r\nframe #39: python() [0x5966f8]\r\nframe #40: python() [0x54d4ee]\r\nframe #42: python() [0x5067b0]\r\nframe #44: python() [0x5058a4]\r\nframe #45: python() [0x5066f0]\r\nframe #47: python() [0x596285]\r\nframe #48: python() [0x548e73]\r\nframe #49: python() [0x53c229]\r\nframe #51: python() [0x5067b0]\r\nframe #53: python() [0x504232]\r\nframe #55: python() [0x647fa2]\r\nframe #60: __libc_start_main + 0xf0 (0x7fdab97bc830 in /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\nIs there anyone who may help me? Thanks a lot!!!  :blue_heart:\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100","bash\r\nRuntimeError: Didn't find kernel to dispatch to for operator 'quantized::conv2d_relu'. Tried to look up kernel for dispatch key 'CPUTensorId'. Registered dispatch keys are: [QuantizedCPUTensorId] (lookup_ at /pytorch/aten/src/ATen/core/dispatch/DispatchTable.h:249)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fdaafe91813 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x1b46361 (0x7fda56bdc361 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch.so)\r\nframe #2: <unknown function> + 0x3e420a8 (0x7fda58ed80a8 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x4fd81c (0x7fdab502981c in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: <unknown function> + 0x4cc184 (0x7fdab4ff8184 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x210534 (0x7fdab4d3c534 in /home/peter/envs/pt13/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: python() [0x50682c]\r\n<omitting python frames>\r\nframe #8: python() [0x596285]\r\nframe #11: python() [0x504232]\r\nframe #12: python() [0x5966f8]\r\nframe #13: python() [0x54d4ee]\r\nframe #15: python() [0x5067b0]\r\nframe #17: python() [0x596285]\r\nframe #20: python() [0x504232]\r\nframe #21: python() [0x5966f8]\r\nframe #22: python() [0x54d4ee]\r\nframe #24: python() [0x5067b0]\r\nframe #26: python() [0x596285]\r\nframe #29: python() [0x504232]\r\nframe #30: python() [0x5966f8]\r\nframe #31: python() [0x54d4ee]\r\nframe #33: python() [0x5067b0]\r\nframe #35: python() [0x596285]\r\nframe #38: python() [0x504232]\r\nframe #39: python() [0x5966f8]\r\nframe #40: python() [0x54d4ee]\r\nframe #42: python() [0x5067b0]\r\nframe #44: python() [0x5058a4]\r\nframe #45: python() [0x5066f0]\r\nframe #47: python() [0x596285]\r\nframe #48: python() [0x548e73]\r\nframe #49: python() [0x53c229]\r\nframe #51: python() [0x5067b0]\r\nframe #53: python() [0x504232]\r\nframe #55: python() [0x647fa2]\r\nframe #60: __libc_start_main + 0xf0 (0x7fdab97bc830 in /lib/x86_64-linux-gnu/libc.so.6\r\n"
27974,"backward pass slow when using Conv2d ## \U0001f41b Bug\r\n\r\nMy backward passes are roughly 300 times slower than my forward passes when using `nn.Conv2D` layers. For example, a forward pass using just a convolutional layer takes 0.003 seconds, while the backward pass takes more than one second.\r\n\r\n## To Reproduce\r\n\r\nHere is some code to reproduce the problem, which will print the time for a forward and a backward pass on a batch of 32 images with depth 8 and image size 32x32 using 5x5 kernels with a padding of 2 pixels.\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nI'd expect the backward pass to run at least 100 times faster. In fact, the performance is as expected when running the script on a different computer. Performance for `Linear` layers is also as expected.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 10 (buster)\r\nGCC version: (Debian 8.3.0-6) 8.3.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.0.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py37h7b6447c_0  \r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] torch                     1.0.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\nResults of `python -m torch.utils.bottleneck train.py`:\r\n\r\n```\r\n--------------------------------------------------------------------------------\r\n  Environment Summary\r\n--------------------------------------------------------------------------------\r\nPyTorch 1.0.0 compiled w/ CUDA 9.0.176\r\nRunning with Python 3.7 and \r\n\r\n`pip list` truncated output:\r\nUnable to fetch\r\n--------------------------------------------------------------------------------\r\n  cProfile output\r\n--------------------------------------------------------------------------------\r\n         5000 function calls (4940 primitive calls) in 109.082 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 81 to 15 due to restriction <15>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n      100  108.587    1.086  108.587    1.086 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n      100    0.393    0.004    0.393    0.004 {built-in method conv2d}\r\n      100    0.040    0.000    0.040    0.000 {built-in method torch._C._nn.mse_loss}\r\n      200    0.030    0.000    0.030    0.000 {built-in method numpy.copyto}\r\n        1    0.009    0.009  109.082  109.082 train.py:1(<module>)\r\n      300    0.004    0.000    0.004    0.000 {built-in method builtins.print}\r\n      200    0.003    0.000    0.441    0.002 /home/username/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py:483(__call__)\r\n      200    0.002    0.000    0.002    0.000 {built-in method numpy.empty}\r\n      200    0.002    0.000    0.002    0.000 {built-in method from_numpy}\r\n      100    0.001    0.000    0.001    0.000 {built-in method ones_like}\r\n      202    0.001    0.000    0.001    0.000 {method 'to' of 'torch._C._TensorBase' objects}\r\n      100    0.001    0.000    0.395    0.004 /home/username/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py:317(forward)\r\n      100    0.001    0.000    0.001    0.000 {built-in method broadcast_tensors}\r\n      200    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\r\n      100    0.001    0.000  108.590    1.086 /home/username/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py:38(backward)\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n  autograd profiler output (CPU mode)\r\n--------------------------------------------------------------------------------\r\n        top 15 events sorted by cpu_time_total\r\n\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                            CPU time        CUDA time            Calls        CPU total       CUDA total\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nMkldnnConvolutionBackward                  1308761.381us          0.000us                1    1308761.381us          0.000us\r\nmkldnn_convolution_backward                1308753.708us          0.000us                1    1308753.708us          0.000us\r\nmkldnn_convolution_backward_weights        1308749.338us          0.000us                1    1308749.338us          0.000us\r\nMkldnnConvolutionBackward                  1212529.170us          0.000us                1    1212529.170us          0.000us\r\nmkldnn_convolution_backward                1212521.603us          0.000us                1    1212521.603us          0.000us\r\nmkldnn_convolution_backward_weights        1212517.754us          0.000us                1    1212517.754us          0.000us\r\nMkldnnConvolutionBackward                  1198483.938us          0.000us                1    1198483.938us          0.000us\r\nmkldnn_convolution_backward                1198476.239us          0.000us                1    1198476.239us          0.000us\r\nmkldnn_convolution_backward_weights        1198471.536us          0.000us                1    1198471.536us          0.000us\r\nMkldnnConvolutionBackward                  1190918.954us          0.000us                1    1190918.954us          0.000us\r\nmkldnn_convolution_backward                1190911.341us          0.000us                1    1190911.341us          0.000us\r\nmkldnn_convolution_backward_weights        1190907.473us          0.000us                1    1190907.473us          0.000us\r\nMkldnnConvolutionBackward                  1164987.322us          0.000us                1    1164987.322us          0.000us\r\nmkldnn_convolution_backward                1164978.555us          0.000us                1    1164978.555us          0.000us\r\nmkldnn_convolution_backward_weights        1164973.806us          0.000us                1    1164973.806us          0.000us \r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",high priority|triage review|module: dependency bug|module: performance|module: cpu|triaged|module: mkldnn,XiaobingSuper,"## \U0001f41b Bug\r\n\r\nMy backward passes are roughly 300 times slower than my forward passes when using `nn.Conv2D` layers. For example, a forward pass using just a convolutional layer takes 0.003 seconds, while the backward pass takes more than one second.\r\n\r\n## To Reproduce\r\n\r\nHere is some code to reproduce the problem, which will print the time for a forward and a backward pass on a batch of 32 images with depth 8 and image size 32x32 using 5x5 kernels with a padding of 2 pixels.\r\n\r\n```python3\r\nimport time\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nn, c, h, w = (32, 8, 32, 32)\r\ndevice = torch.device(""cpu"")\r\nmodel = nn.Conv2d(c, c, 5, padding=2)\r\nmodel = model.to(device)\r\nmodel.train()\r\nloss_fn = nn.MSELoss()\r\n\r\nfor iteration in range(100):\r\n    t0 = time.perf_counter()\r\n    \r\n    x = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device)\r\n    y = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device)\r\n    loss = loss_fn(model(x), y)\r\n    \r\n    t1 = time.perf_counter()\r\n    \r\n    loss.backward()\r\n    \r\n    t2 = time.perf_counter()\r\n    print(""forward: %f seconds"" % (t1 - t0))\r\n    print(""backward: %f seconds"" % (t2 - t1))\r\n    print("""")\r\n```\r\n\r\n## Expected behavior\r\n\r\nI'd expect the backward pass to run at least 100 times faster. In fact, the performance is as expected when running the script on a different computer. Performance for `Linear` layers is also as expected.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 10 (buster)\r\nGCC version: (Debian 8.3.0-6) 8.3.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.0.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py37h7b6447c_0  \r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] torch                     1.0.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\nResults of `python -m torch.utils.bottleneck train.py`:\r\n\r\n```\r\n--------------------------------------------------------------------------------\r\n  Environment Summary\r\n--------------------------------------------------------------------------------\r\nPyTorch 1.0.0 compiled w/ CUDA 9.0.176\r\nRunning with Python 3.7 and \r\n\r\n`pip list` truncated output:\r\nUnable to fetch\r\n--------------------------------------------------------------------------------\r\n  cProfile output\r\n--------------------------------------------------------------------------------\r\n         5000 function calls (4940 primitive calls) in 109.082 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 81 to 15 due to restriction <15>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n      100  108.587    1.086  108.587    1.086 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n      100    0.393    0.004    0.393    0.004 {built-in method conv2d}\r\n      100    0.040    0.000    0.040    0.000 {built-in method torch._C._nn.mse_loss}\r\n      200    0.030    0.000    0.030    0.000 {built-in method numpy.copyto}\r\n        1    0.009    0.009  109.082  109.082 train.py:1(<module>)\r\n      300    0.004    0.000    0.004    0.000 {built-in method builtins.print}\r\n      200    0.003    0.000    0.441    0.002 /home/username/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py:483(__call__)\r\n      200    0.002    0.000    0.002    0.000 {built-in method numpy.empty}\r\n      200    0.002    0.000    0.002    0.000 {built-in method from_numpy}\r\n      100    0.001    0.000    0.001    0.000 {built-in method ones_like}\r\n      202    0.001    0.000    0.001    0.000 {method 'to' of 'torch._C._TensorBase' objects}\r\n      100    0.001    0.000    0.395    0.004 /home/username/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py:317(forward)\r\n      100    0.001    0.000    0.001    0.000 {built-in method broadcast_tensors}\r\n      200    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\r\n      100    0.001    0.000  108.590    1.086 /home/username/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py:38(backward)\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n  autograd profiler output (CPU mode)\r\n--------------------------------------------------------------------------------\r\n        top 15 events sorted by cpu_time_total\r\n\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                            CPU time        CUDA time            Calls        CPU total       CUDA total\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nMkldnnConvolutionBackward                  1308761.381us          0.000us                1    1308761.381us          0.000us\r\nmkldnn_convolution_backward                1308753.708us          0.000us                1    1308753.708us          0.000us\r\nmkldnn_convolution_backward_weights        1308749.338us          0.000us                1    1308749.338us          0.000us\r\nMkldnnConvolutionBackward                  1212529.170us          0.000us                1    1212529.170us          0.000us\r\nmkldnn_convolution_backward                1212521.603us          0.000us                1    1212521.603us          0.000us\r\nmkldnn_convolution_backward_weights        1212517.754us          0.000us                1    1212517.754us          0.000us\r\nMkldnnConvolutionBackward                  1198483.938us          0.000us                1    1198483.938us          0.000us\r\nmkldnn_convolution_backward                1198476.239us          0.000us                1    1198476.239us          0.000us\r\nmkldnn_convolution_backward_weights        1198471.536us          0.000us                1    1198471.536us          0.000us\r\nMkldnnConvolutionBackward                  1190918.954us          0.000us                1    1190918.954us          0.000us\r\nmkldnn_convolution_backward                1190911.341us          0.000us                1    1190911.341us          0.000us\r\nmkldnn_convolution_backward_weights        1190907.473us          0.000us                1    1190907.473us          0.000us\r\nMkldnnConvolutionBackward                  1164987.322us          0.000us                1    1164987.322us          0.000us\r\nmkldnn_convolution_backward                1164978.555us          0.000us                1    1164978.555us          0.000us\r\nmkldnn_convolution_backward_weights        1164973.806us          0.000us                1    1164973.806us          0.000us \r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh","python3\r\nimport time\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nn, c, h, w = (32, 8, 32, 32)\r\ndevice = torch.device(""cpu"")\r\nmodel = nn.Conv2d(c, c, 5, padding=2)\r\nmodel = model.to(device)\r\nmodel.train()\r\nloss_fn = nn.MSELoss()\r\n\r\nfor iteration in range(100):\r\n    t0 = time.perf_counter()\r\n    \r\n    x = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device)\r\n    y = torch.from_numpy(np.ones((n, c, h, w), np.float32)).to(device)\r\n    loss = loss_fn(model(x), y)\r\n    \r\n    t1 = time.perf_counter()\r\n    \r\n    loss.backward()\r\n    \r\n    t2 = time.perf_counter()\r\n    print(""forward: %f seconds"" % (t1 - t0))\r\n    print(""backward: %f seconds"" % (t2 - t1))\r\n    print("""")\r\n"
27968,"MultiheadAttention incorrectly handles mask## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nthe print is:\r\n\r\n         tensor([[[0.2822, 0.2024, 0.1926, 0.2565, 0.1463],\r\n         [0.2020, 0.1906, 0.1721, 0.2159, 0.2425],\r\n         [0.2074, 0.2127, 0.0882, 0.2422, 0.3333],\r\n         [0.2298, 0.1938, 0.2093, 0.1432, 0.2335],\r\n         [0.2531, 0.1772, 0.3045, 0.1246, 0.1881]],\r\n\r\n        [[0.3693, 0.3857, 0.3561, 0.0000, 0.0000],\r\n         [0.3655, 0.2712, 0.3918, 0.0000, 0.0000],\r\n         [0.2230, 0.3795, 0.2549, 0.0000, 0.0000],\r\n         [0.1698, 0.3805, 0.2318, 0.0000, 0.0000],\r\n         [0.2986, 0.3093, 0.4405, 0.0000, 0.0000]],\r\n\r\n        [[0.8889, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000]],\r\n\r\n        [[   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan]]], grad_fn=<DivBackward0>)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):  1.3.0\r\n - OS (e.g., Linux):  macos\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519",module: nn|triaged,zhangguanheng66,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nm = nn.MultiheadAttention(embed_dim=20, num_heads=5, dropout=0.1)\r\n\r\ni = torch.randn(4, 5, 20)\r\nq = k = v = i.transpose(0, 1)\r\nkey_padding_mask = seq_len_to_mask([5, 3, 1, 0], max_len=5)\r\n\r\nao, aow = m(q, k, v, key_padding_mask=key_padding_mask)\r\nao = ao.transpose(0, 1)\r\n\r\nprint(aow)\r\n```\r\n\r\nthe print is:\r\n\r\n         tensor([[[0.2822, 0.2024, 0.1926, 0.2565, 0.1463],\r\n         [0.2020, 0.1906, 0.1721, 0.2159, 0.2425],\r\n         [0.2074, 0.2127, 0.0882, 0.2422, 0.3333],\r\n         [0.2298, 0.1938, 0.2093, 0.1432, 0.2335],\r\n         [0.2531, 0.1772, 0.3045, 0.1246, 0.1881]],\r\n\r\n        [[0.3693, 0.3857, 0.3561, 0.0000, 0.0000],\r\n         [0.3655, 0.2712, 0.3918, 0.0000, 0.0000],\r\n         [0.2230, 0.3795, 0.2549, 0.0000, 0.0000],\r\n         [0.1698, 0.3805, 0.2318, 0.0000, 0.0000],\r\n         [0.2986, 0.3093, 0.4405, 0.0000, 0.0000]],\r\n\r\n        [[0.8889, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\r\n         [1.1111, 0.0000, 0.0000, 0.0000, 0.0000]],\r\n\r\n        [[   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan],\r\n         [   nan,    nan,    nan,    nan,    nan]]], grad_fn=<DivBackward0>)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):  1.3.0\r\n - OS (e.g., Linux):  macos\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519","python\r\nm = nn.MultiheadAttention(embed_dim=20, num_heads=5, dropout=0.1)\r\n\r\ni = torch.randn(4, 5, 20)\r\nq = k = v = i.transpose(0, 1)\r\nkey_padding_mask = seq_len_to_mask([5, 3, 1, 0], max_len=5)\r\n\r\nao, aow = m(q, k, v, key_padding_mask=key_padding_mask)\r\nao = ao.transpose(0, 1)\r\n\r\nprint(aow)\r\n"
27743,"Support dictionary outputs in TorchScript tracer## \U0001f680 Feature\r\n\r\n`torch.jit.trace()` currently requires traced functions to output plain tensors or tuples. It would be helpful if it also allowed functions that return dictionaries.\r\n\r\nMinimal failing example:\r\n\r\n\r\n## Motivation\r\n\r\nHaving modules take and return dictionaries makes it easier to define architectures with multiple inputs and outputs that are not always guaranteed to be present (e.g., for multi-task learning). Currently, such modules cannot be traced, e.g., for visualizing their structure.\r\n\r\n## Pitch\r\n\r\nFunctions returning dictionaries should be handled similarly to (named) tuples. For an OrderedDict, values can be taken in their stored order. For a general dict, the order should probably be made deterministic by sorting the keys, so it stays consistent across re-running the interpreter.\r\n(Disclaimer: I wanted to use this to add a graph to tensorboard; I don't know what the implications of this suggestion would be for TorchScript traces.)\r\n\r\n## Alternatives\r\n\r\nThe alternative would be for me to wrap the module/function in a function that converts the dictionary to a (named?) tuple before returning it, just for the `torch.jit.trace()` call. This would not give me a trace that creates a dictionary, but at least a trace useful for visualization.\r\n\r\n## Additional context\r\n\r\nThis was part of issue #16453, which also asked for dictionary inputs, which are possible by now:\r\n\n\ncc @suo",oncall: jit|triaged,wanchaol,"## \U0001f680 Feature\r\n\r\n`torch.jit.trace()` currently requires traced functions to output plain tensors or tuples. It would be helpful if it also allowed functions that return dictionaries.\r\n\r\nMinimal failing example:\r\n```python\r\nimport torch\r\ntorch.jit.trace(lambda x: {'out': x}, torch.arange(3))\r\n```\r\n\r\n## Motivation\r\n\r\nHaving modules take and return dictionaries makes it easier to define architectures with multiple inputs and outputs that are not always guaranteed to be present (e.g., for multi-task learning). Currently, such modules cannot be traced, e.g., for visualizing their structure.\r\n\r\n## Pitch\r\n\r\nFunctions returning dictionaries should be handled similarly to (named) tuples. For an OrderedDict, values can be taken in their stored order. For a general dict, the order should probably be made deterministic by sorting the keys, so it stays consistent across re-running the interpreter.\r\n(Disclaimer: I wanted to use this to add a graph to tensorboard; I don't know what the implications of this suggestion would be for TorchScript traces.)\r\n\r\n## Alternatives\r\n\r\nThe alternative would be for me to wrap the module/function in a function that converts the dictionary to a (named?) tuple before returning it, just for the `torch.jit.trace()` call. This would not give me a trace that creates a dictionary, but at least a trace useful for visualization.\r\n\r\n## Additional context\r\n\r\nThis was part of issue #16453, which also asked for dictionary inputs, which are possible by now:\r\n```python\r\nimport torch\r\ntorch.jit.trace(lambda x: x['input'], {'input': torch.arange(3)})\r\n```\n\ncc @suo","python\r\nimport torch\r\ntorch.jit.trace(lambda x: {'out': x}, torch.arange(3))\r\n"
27692,"nn.L1Loss works incorrectly with CPU## \U0001f41b Bug\r\n\r\nWhen a tensor is large, the L1 loss is not equal to `torch.abs(x - y).mean()` in CPU mode, but it works fine in GPU mode.\r\n\r\nI find this bug in PyTorch 1.2.0, and have verified that this problem exists in PyTorch 0.4.0 (on centos).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nJust create a large tensor, and test with the following code. I can reproduce it with `Intel i7-8750H` and `Intel E3-1230 v5`.\r\n\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch==1.2.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.4.0\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] mkl                       2018.0.1             h2108138_4    defaults\r\n[conda] mkl-service               1.1.2            py36h57e144c_4    defaults\r\n[conda] pytorch                   1.2.0           py3.6_cuda100_cudnn7_1    defaults\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.4.0                py36_cu100    defaults\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @VitalyFedyunin",high priority|module: cpu|triaged,XiaobingSuper,"## \U0001f41b Bug\r\n\r\nWhen a tensor is large, the L1 loss is not equal to `torch.abs(x - y).mean()` in CPU mode, but it works fine in GPU mode.\r\n\r\nI find this bug in PyTorch 1.2.0, and have verified that this problem exists in PyTorch 0.4.0 (on centos).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nJust create a large tensor, and test with the following code. I can reproduce it with `Intel i7-8750H` and `Intel E3-1230 v5`.\r\n\r\n```python\r\nimport torch\r\n# NOTE that in this situation, when N is less than 10, the results are usually\r\n# equal or similar, and the difference becomes larger when N is increased\r\na = torch.rand(12, 3, 1024, 1024)\r\nprint(torch.nn.L1Loss()(a, torch.zeros_like(a)))\r\nprint(a.abs().mean())\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] numpydoc==0.7.0\r\n[pip3] torch==1.2.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.4.0\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] mkl                       2018.0.1             h2108138_4    defaults\r\n[conda] mkl-service               1.1.2            py36h57e144c_4    defaults\r\n[conda] pytorch                   1.2.0           py3.6_cuda100_cudnn7_1    defaults\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.4.0                py36_cu100    defaults\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @VitalyFedyunin","python\r\nimport torch\r\n# NOTE that in this situation, when N is less than 10, the results are usually\r\n# equal or similar, and the difference becomes larger when N is increased\r\na = torch.rand(12, 3, 1024, 1024)\r\nprint(torch.nn.L1Loss()(a, torch.zeros_like(a)))\r\nprint(a.abs().mean())\r\n"
27690,"Multi-GPU Gather is much slower than Scatter## \U0001f41b Bug\r\n\r\nMulti-GPU gathering is much slower than scattering\r\n\r\n## To Reproduce\r\nCan run the following script on a Multi-GPU machine which should replicate the issue. It creates a large tensor on the CPU and scatters it to multiple GPUs, then also creates large I have tested with both pytorch 1.1.0 and pytorch 1.2.0.\r\n\r\n\r\nOutput: \r\nNote that when batch size (args.sz) = 128, the tensor to be scattered/gathered is 1GiB.\r\n\r\n| NGpus        | batch size           | Scatter Time (s)  |  Gather Time (s) |\r\n| ------------- |:-------------:| -----:|---:|\r\n| 2 | 64 |  0.07 | 0.35 |\r\n| 2 | 128 | 0.14 | 0.7 |\r\n| 2 | 256 | 0.27 | 1.4 |\r\n| 4     | 64   | 0.12 | 0.35 |\r\n| 4 | 128   |  0.16 | 0.7 |\r\n| 4 | 256 | 0.29 | 1.6 |\r\n\r\n## Expected behavior\r\n\r\nI would expect the scatter and gather timings to be a lot closer than this, not a factor of 5 out.\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\nGPU 4: GeForce GTX 1080\r\nGPU 5: GeForce GTX 1080\r\nGPU 6: GeForce GTX 1080\r\nGPU 7: GeForce GTX 1080\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n```\r\n",module: performance|module: multi-gpu|module: cuda|triaged,cpuhrsch,"## \U0001f41b Bug\r\n\r\nMulti-GPU gathering is much slower than scattering\r\n\r\n## To Reproduce\r\nCan run the following script on a Multi-GPU machine which should replicate the issue. It creates a large tensor on the CPU and scatters it to multiple GPUs, then also creates large I have tested with both pytorch 1.1.0 and pytorch 1.2.0.\r\n\r\n```python\r\n# coding: utf-8\r\nimport time\r\n# import py3nvml\r\nimport torch\r\nfrom torch.nn.parallel._functions import Scatter, Gather\r\nimport argparse\r\n\r\n\r\nN = 3\r\n\r\nparser = argparse.ArgumentParser(description='Test scatter gather')\r\nparser.add_argument('--sz', type=int, default=128, help='Batch size')\r\nparser.add_argument('--Ngpus', type=int, default=4, help='Batch size')\r\n\r\nargs = parser.parse_args()\r\n# Set CUDA_VISIBLE_DEVICES \r\n# py3nvml.grab_gpus(args.Ngpus)\r\n\r\n# Create data so all gpus have been used once\r\ncpu = -1\r\ndevs = [torch.device('cuda:{}'.format(i)) for i in range(args.Ngpus)]\r\nx_cpu = torch.randn(args.sz, 128, 128, 128)\r\nfor i in range(args.Ngpus):\r\n    y_gpu = torch.randn(args.sz, 128, 128, 128, device=devs[i])\r\n\r\ncpu_to_gpu = 0\r\nfor i in range(N):\r\n    x_cpu = torch.randn(args.sz, 128, 128, 128)\r\n\r\n    start = time.time()\r\n    out_list = Scatter.apply(devs, None, 0, x_cpu)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    cpu_to_gpu += (end-start)\r\n\r\nprint(""Scattering CPU to GPU takes {:.2f}s"".format(cpu_to_gpu/N))\r\ngpu_to_cpu = 0\r\nfor i in range(N):\r\n    y = [torch.randn(args.sz//args.Ngpus, 128, 128, 128, device=dev) for dev in devs]\r\n\r\n    start = time.time()\r\n    Gather.apply(cpu, 0, *y)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    gpu_to_cpu += (end-start)\r\n\r\nprint(""Gathering GPU to CPU takes {:.2f}s"".format(gpu_to_cpu/N))\r\n```\r\nOutput: \r\nNote that when batch size (args.sz) = 128, the tensor to be scattered/gathered is 1GiB.\r\n\r\n| NGpus        | batch size           | Scatter Time (s)  |  Gather Time (s) |\r\n| ------------- |:-------------:| -----:|---:|\r\n| 2 | 64 |  0.07 | 0.35 |\r\n| 2 | 128 | 0.14 | 0.7 |\r\n| 2 | 256 | 0.27 | 1.4 |\r\n| 4     | 64   | 0.12 | 0.35 |\r\n| 4 | 128   |  0.16 | 0.7 |\r\n| 4 | 256 | 0.29 | 1.6 |\r\n\r\n## Expected behavior\r\n\r\nI would expect the scatter and gather timings to be a lot closer than this, not a factor of 5 out.\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\nGPU 4: GeForce GTX 1080\r\nGPU 5: GeForce GTX 1080\r\nGPU 6: GeForce GTX 1080\r\nGPU 7: GeForce GTX 1080\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n```\r\n","python\r\n# coding: utf-8\r\nimport time\r\n# import py3nvml\r\nimport torch\r\nfrom torch.nn.parallel._functions import Scatter, Gather\r\nimport argparse\r\n\r\n\r\nN = 3\r\n\r\nparser = argparse.ArgumentParser(description='Test scatter gather')\r\nparser.add_argument('--sz', type=int, default=128, help='Batch size')\r\nparser.add_argument('--Ngpus', type=int, default=4, help='Batch size')\r\n\r\nargs = parser.parse_args()\r\n# Set CUDA_VISIBLE_DEVICES \r\n# py3nvml.grab_gpus(args.Ngpus)\r\n\r\n# Create data so all gpus have been used once\r\ncpu = -1\r\ndevs = [torch.device('cuda:{}'.format(i)) for i in range(args.Ngpus)]\r\nx_cpu = torch.randn(args.sz, 128, 128, 128)\r\nfor i in range(args.Ngpus):\r\n    y_gpu = torch.randn(args.sz, 128, 128, 128, device=devs[i])\r\n\r\ncpu_to_gpu = 0\r\nfor i in range(N):\r\n    x_cpu = torch.randn(args.sz, 128, 128, 128)\r\n\r\n    start = time.time()\r\n    out_list = Scatter.apply(devs, None, 0, x_cpu)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    cpu_to_gpu += (end-start)\r\n\r\nprint(""Scattering CPU to GPU takes {:.2f}s"".format(cpu_to_gpu/N))\r\ngpu_to_cpu = 0\r\nfor i in range(N):\r\n    y = [torch.randn(args.sz//args.Ngpus, 128, 128, 128, device=dev) for dev in devs]\r\n\r\n    start = time.time()\r\n    Gather.apply(cpu, 0, *y)\r\n    for i in range(args.Ngpus):\r\n        torch.cuda.synchronize(device=devs[i])\r\n    end = time.time()\r\n\r\n    gpu_to_cpu += (end-start)\r\n\r\nprint(""Gathering GPU to CPU takes {:.2f}s"".format(gpu_to_cpu/N))\r\n"
27623,"Not possible to use different key/value dimensionalities in nn.MultiheadAttention## \U0001f41b Bug\r\n\r\nThe [nn.MultiheadAttention constructor](https://pytorch.org/docs/stable/nn.html#torch.nn.MultiheadAttention) allows using _key_ and/or _value_ dimensionalities which are different from `embed_dim`, using the `kdim` and `vdim` positional arguments. However, when setting either `kdim` or `vdim` to a non-`None` value, [this assertion in `F.multi_head_attention_forward`](https://github.com/pytorch/pytorch/blob/v1.2.0/torch/nn/functional.py#L3103) unnecessarily blocks execution.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/modules/activation.py"", line 769, in forward\r\n    v_proj_weight=self.v_proj_weight)\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/functional.py"", line 3103, in multi_head_attention_forward\r\n    assert key.size() == value.size()\r\nAssertionError\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe above snippet should run without an AssertionError. The problematic assertion must only be checked if `kdim is None and vdim is None`, i.e., if `self._qkv_same_embed_dim is True`. The `forward()` function correctly checks the value of `self._qkv_same_embed_dim`, and calls `F.multi_head_attention_forward` with the appropriate arguments, including `use_separate_proj_weight=True` and separate query, key, and value projection weights. \r\n\r\nSo, in `F.multi_head_attention_forward`, this assertion should only be checked if `use_separate_proj_weight is False`.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0 (but problem remains in master too).",module: nn|triaged|oncall: transformer/mha,Nayef211,"## \U0001f41b Bug\r\n\r\nThe [nn.MultiheadAttention constructor](https://pytorch.org/docs/stable/nn.html#torch.nn.MultiheadAttention) allows using _key_ and/or _value_ dimensionalities which are different from `embed_dim`, using the `kdim` and `vdim` positional arguments. However, when setting either `kdim` or `vdim` to a non-`None` value, [this assertion in `F.multi_head_attention_forward`](https://github.com/pytorch/pytorch/blob/v1.2.0/torch/nn/functional.py#L3103) unnecessarily blocks execution.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nbatch_size = 5\r\nseq_len = 10\r\nembed_dim = 24\r\nvalue_dim = 12\r\nnum_heads = 1\r\n\r\nquery = torch.randn(seq_len, batch_size, embed_dim)\r\nkey = torch.randn(seq_len, batch_size, embed_dim)\r\nvalue = torch.randn(seq_len, batch_size, value_dim)\r\n\r\nmha = nn.MultiheadAttention(embed_dim, num_heads, vdim=value_dim)\r\nattn_out, attn_out_weights = mha(query, key, value)\r\n```\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/modules/activation.py"", line 769, in forward\r\n    v_proj_weight=self.v_proj_weight)\r\n  File ""/afs/inf.ed.ac.uk/user/s/sangelid/venv/pytorch_36/lib64/python3.6/site-packages/torch/nn/functional.py"", line 3103, in multi_head_attention_forward\r\n    assert key.size() == value.size()\r\nAssertionError\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe above snippet should run without an AssertionError. The problematic assertion must only be checked if `kdim is None and vdim is None`, i.e., if `self._qkv_same_embed_dim is True`. The `forward()` function correctly checks the value of `self._qkv_same_embed_dim`, and calls `F.multi_head_attention_forward` with the appropriate arguments, including `use_separate_proj_weight=True` and separate query, key, and value projection weights. \r\n\r\nSo, in `F.multi_head_attention_forward`, this assertion should only be checked if `use_separate_proj_weight is False`.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0 (but problem remains in master too).","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nbatch_size = 5\r\nseq_len = 10\r\nembed_dim = 24\r\nvalue_dim = 12\r\nnum_heads = 1\r\n\r\nquery = torch.randn(seq_len, batch_size, embed_dim)\r\nkey = torch.randn(seq_len, batch_size, embed_dim)\r\nvalue = torch.randn(seq_len, batch_size, value_dim)\r\n\r\nmha = nn.MultiheadAttention(embed_dim, num_heads, vdim=value_dim)\r\nattn_out, attn_out_weights = mha(query, key, value)\r\n"
27614,"scatter_add allows index tensor that doesn't match input size in forward pass but fails on backward pass## \U0001f41b Bug\r\n\r\nIf you have an index tensor whose shape does not match the other tensor's shape but is broadcast-compatible, the forward pass of `torch.scatter_add()` will succeed but the backward pass will throw a RuntimeError saying that the shapes do not match exactly on the non-indexed dimension.\r\n\r\n## To Reproduce\r\n\r\nRepro script:\r\n\r\n\r\nException (CPU):\r\n```\r\nRuntimeError: Expected tensor [8, 626304, 1], src [8, 105238, 3] and index [8, 626304, 1] to have the same size ap\r\nart from dimension 1\r\n```\r\n\r\nException (CUDA):\r\n```\r\nRuntimeError: invalid argument 2: Input tensor must have same size as output tensor apart from the specified dimen\r\nsion at /mnt/home/gbschwartz/pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:27\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo exceptions thrown.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 9.0 / 7.6.0\r\n - GPU models and configuration: V100\n\ncc @ezyang @gchanan @zou3519",high priority|module: crash|triaged,nikitaved,"## \U0001f41b Bug\r\n\r\nIf you have an index tensor whose shape does not match the other tensor's shape but is broadcast-compatible, the forward pass of `torch.scatter_add()` will succeed but the backward pass will throw a RuntimeError saying that the shapes do not match exactly on the non-indexed dimension.\r\n\r\n## To Reproduce\r\n\r\nRepro script:\r\n```python\r\nimport torch as th\r\n\r\nsrc = th.zeros(8, 105238, 3)\r\nind = th.zeros(8, 626304, 1).long()\r\nofs = th.zeros(8, 626304, 3)\r\nsrc.requires_grad_(True)\r\nofs.requires_grad_(True)\r\n\r\nout = src.scatter_add(1, ind, ofs)\r\n#out = src.scatter_add(1, ind.expand(-1, -1, 3), ofs) <<< this line works\r\n\r\nl = out.sum()\r\nl.backward()\r\n```\r\n\r\nException (CPU):\r\n```\r\nRuntimeError: Expected tensor [8, 626304, 1], src [8, 105238, 3] and index [8, 626304, 1] to have the same size ap\r\nart from dimension 1\r\n```\r\n\r\nException (CUDA):\r\n```\r\nRuntimeError: invalid argument 2: Input tensor must have same size as output tensor apart from the specified dimen\r\nsion at /mnt/home/gbschwartz/pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:27\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo exceptions thrown.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 9.0 / 7.6.0\r\n - GPU models and configuration: V100\n\ncc @ezyang @gchanan @zou3519","python\r\nimport torch as th\r\n\r\nsrc = th.zeros(8, 105238, 3)\r\nind = th.zeros(8, 626304, 1).long()\r\nofs = th.zeros(8, 626304, 3)\r\nsrc.requires_grad_(True)\r\nofs.requires_grad_(True)\r\n\r\nout = src.scatter_add(1, ind, ofs)\r\n#out = src.scatter_add(1, ind.expand(-1, -1, 3), ofs) <<< this line works\r\n\r\nl = out.sum()\r\nl.backward()\r\n"
27600,"Free Memory after CUDA out of memory error## \U0001f41b Bug\r\n\r\nSometimes, PyTorch does not free memory after a CUDA out of memory exception.\r\n\r\n## To Reproduce\r\n\r\n\r\nConsider the following function:\r\n\r\n\r\nExecuting it one time gives the expected out of memory error after some iterations:\r\n\r\n\r\nExecuting a second time gives a OOM error immediately after the first iteration, which means that the memory consumed by the scoped variables in x is still occupied (which is a little weird):\r\n\r\n\r\n\r\nCalling `gc.collect()` now sometimes (!!) leads to freeing the memory and sometimes it doesn't. \r\n\r\n\r\n## Expected behavior\r\n\r\nI expected a consistent behavior that frees the memory after the OOM exception occurred or at least after `gc.collect()` gets called.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.3.1611 (Core) \r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 387.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-wpe==0.0.0\r\n[pip] torch==1.0.0\r\n[pip] torch-complex==0.0.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] pytorch-wpe               0.0.0                    pypi_0    pypi\r\n[conda] torch-complex             0.0.1                    pypi_0    \r\n\r\n## Additional context\r\n\r\nI stumbled upon this because I tried to fallback to CPU for computation of a single batch after a OOM error. I noticed that after the computation on the CPU, still the GPU memory was blocked sometimes and caused all following batches to be computed on the CPU as well.\r\n\n\ncc @ezyang @gchanan @zou3519",module: cuda|module: memory usage|triaged,albanD,"## \U0001f41b Bug\r\n\r\nSometimes, PyTorch does not free memory after a CUDA out of memory exception.\r\n\r\n## To Reproduce\r\n\r\n\r\nConsider the following function:\r\n```python\r\nimport torch\r\n\r\ndef oom():\r\n    try:\r\n        x = torch.randn(100, 10000, device=1)\r\n        for i in range(100):\r\n            l = torch.nn.Linear(10000, 10000)\r\n            l.to(1)\r\n            x = l(x)\r\n    except RuntimeError as e:\r\n        print(e)\r\n        print('at iteration', i)\r\n```\r\n\r\nExecuting it one time gives the expected out of memory error after some iterations:\r\n```python\r\n>>> oom()\r\nCUDA out of memory. Tried to allocate 381.50 MiB (GPU 1; 7.92 GiB total capacity; 7.16 GiB already allocated; 231.00 MiB free; 452.50 KiB cached)\r\nat iteration 19\r\n```\r\n\r\nExecuting a second time gives a OOM error immediately after the first iteration, which means that the memory consumed by the scoped variables in x is still occupied (which is a little weird):\r\n\r\n```python\r\n>>> oom()\r\nCUDA out of memory. Tried to allocate 381.50 MiB (GPU 1; 7.92 GiB total capacity; 7.16 GiB already allocated; 231.00 MiB free; 452.50 KiB cached)\r\nat iteration 0\r\n```\r\n\r\nCalling `gc.collect()` now sometimes (!!) leads to freeing the memory and sometimes it doesn't. \r\n\r\n\r\n## Expected behavior\r\n\r\nI expected a consistent behavior that frees the memory after the OOM exception occurred or at least after `gc.collect()` gets called.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.3.1611 (Core) \r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 387.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-wpe==0.0.0\r\n[pip] torch==1.0.0\r\n[pip] torch-complex==0.0.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] pytorch-wpe               0.0.0                    pypi_0    pypi\r\n[conda] torch-complex             0.0.1                    pypi_0    \r\n\r\n## Additional context\r\n\r\nI stumbled upon this because I tried to fallback to CPU for computation of a single batch after a OOM error. I noticed that after the computation on the CPU, still the GPU memory was blocked sometimes and caused all following batches to be computed on the CPU as well.\r\n\n\ncc @ezyang @gchanan @zou3519","python\r\nimport torch\r\n\r\ndef oom():\r\n    try:\r\n        x = torch.randn(100, 10000, device=1)\r\n        for i in range(100):\r\n            l = torch.nn.Linear(10000, 10000)\r\n            l.to(1)\r\n            x = l(x)\r\n    except RuntimeError as e:\r\n        print(e)\r\n        print('at iteration', i)\r\n"
27598,"Conv2d crashes with `stride=0`## \U0001f41b Bug\r\n\r\nAs in the title: sanitization for the `stride` argument to convolutions doesn't check for `0` and crashes with core dump (note that negative strides are checked).\r\n\r\n## To Reproduce\r\n\r\n## Expected behavior\r\nA Python exception, ideally at `Conv2d` instantiation; alternatively at call site.\r\n## Environment\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] pytorch                   1.2.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n[conda] torchvision               0.4.0                  py37_cpu  [cpuonly]  pytorch\r\n\n\ncc @ezyang @gchanan @zou3519",high priority|module: crash|module: nn|module: error checking|triaged,anjali411,"## \U0001f41b Bug\r\n\r\nAs in the title: sanitization for the `stride` argument to convolutions doesn't check for `0` and crashes with core dump (note that negative strides are checked).\r\n\r\n## To Reproduce\r\n```python\r\ninput = torch.randn(1, 1, 32, 32)\r\nc = nn.Conv2d(1, 32, 3, stride=0, bias=False, padding=(0, 1), padding_mode='constant')\r\nc(input)\r\n```\r\n## Expected behavior\r\nA Python exception, ideally at `Conv2d` instantiation; alternatively at call site.\r\n## Environment\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cpuonly                   1.0                           0    pytorch\r\n[conda] pytorch                   1.2.0               py3.7_cpu_0  [cpuonly]  pytorch\r\n[conda] torchvision               0.4.0                  py37_cpu  [cpuonly]  pytorch\r\n\n\ncc @ezyang @gchanan @zou3519","python\r\ninput = torch.randn(1, 1, 32, 32)\r\nc = nn.Conv2d(1, 32, 3, stride=0, bias=False, padding=(0, 1), padding_mode='constant')\r\nc(input)\r\n"
27551,torch.split with tensor sizes fails in tracing## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nCode\r\n\r\n\r\nError:\r\n\r\n\r\n## Expected behavior\r\n\r\nTrace successfully\r\n\r\n## Additional context\r\n\r\nI think this is related to sizes being handled specially in tracing mode.\n\ncc @ezyang @gchanan @zou3519 @suo,high priority|oncall: jit,suo,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nCode\r\n```python\r\nimport torch\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def forward(self, x):\r\n        return torch.split(x, x.size(1))\r\n\r\n\r\nm = MyModule()\r\nx = torch.randn(1, 2, 3, 4)\r\n\r\n# ok to run the model\r\nm(x)\r\n\r\n# fail to trace it\r\nprint(torch.jit.trace(m, x))\r\n```\r\n\r\nError:\r\n\r\n```shell\r\n(tensor([[[[-0.8175, -0.3000,  0.0693, -1.0097],\r\n          [-0.3261,  0.6817, -0.8282, -1.8818],\r\n          [-1.1303,  1.8587,  0.0275, -0.2838]],\r\n\r\n         [[ 0.2386,  0.3672,  1.0122, -1.3396],\r\n          [ 0.6037, -0.8609,  0.6339,  0.8278],\r\n          [ 1.0117, -0.3204, -1.6440,  2.8207]]]]),)\r\n\r\nTraceback (most recent call last):\r\n  File ""/tmp/1.py"", line 12, in <module>\r\n    print(torch.jit.trace(m, x))\r\n  File ""torch/jit/__init__.py"", line 860, in trace\r\n    check_tolerance, _force_outplace, _module_class)\r\n  File ""torch/jit/__init__.py"", line 999, in trace_module\r\n    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)\r\n  File ""torch/nn/modules/module.py"", line 543, in __call__\r\n    result = self._slow_forward(*input, **kwargs)\r\n  File ""torch/nn/modules/module.py"", line 529, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/tmp/1.py"", line 6, in forward\r\n    return torch.split(x, x.size(1))\r\n  File ""torch/functional.py"", line 77, in split\r\n    return tensor.split(split_size_or_sections, dim)\r\n  File ""torch/tensor.py"", line 339, in split\r\n    return super(Tensor, self).split_with_sizes(split_size, dim)\r\nTypeError: split_with_sizes(): argument 'split_sizes' (position 1) must be tuple of ints, not Tensor\r\n```\r\n## Expected behavior\r\n\r\nTrace successfully\r\n\r\n## Additional context\r\n\r\nI think this is related to sizes being handled specially in tracing mode.\n\ncc @ezyang @gchanan @zou3519 @suo","python\r\nimport torch\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n    def forward(self, x):\r\n        return torch.split(x, x.size(1))\r\n\r\n\r\nm = MyModule()\r\nx = torch.randn(1, 2, 3, 4)\r\n\r\n# ok to run the model\r\nm(x)\r\n\r\n# fail to trace it\r\nprint(torch.jit.trace(m, x))\r\n"
27493,[jit] `index_put_` on a tensor slice with the element type does not workThis works in eager mode\r\n\r\n\n\ncc @suo,oncall: jit|triaged|small,jansel,"This works in eager mode\r\n\r\n```python\r\n# this works\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[torch.tensor(0)] = torch.tensor(2)\r\n    return a\r\n\r\n# this works\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[0] = 2\r\n    return a\r\n\r\n# this  doesn't work\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[torch.tensor(0)] = 2\r\n    return a\r\n\r\nx = torch.jit.script(fn)\r\n```\n\ncc @suo","python\r\n# this works\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[torch.tensor(0)] = torch.tensor(2)\r\n    return a\r\n\r\n# this works\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[0] = 2\r\n    return a\r\n\r\n# this  doesn't work\r\ndef fn(x):\r\n    a = zeros_like(x, dtype=torch.uint8)\r\n    a[torch.tensor(0)] = 2\r\n    return a\r\n\r\nx = torch.jit.script(fn)\r\n"
27368,"Improve error message if Python function is not found on callee instead of crashing@rohan-varma found that when the target Python function is missing on the callee side when using `rpc`/`remote`, the callee will hit segfault instead of sending the error message back to the caller. \r\n\r\n\r\n\r\n\r\n\r\nFor example, the above program will show the following error messages on the callee:\r\n\r\n```\r\nThread 6 ""python"" received signal SIGSEGV, Segmentation fault.\r\n[Switching to LWP 4177163]\r\nPyErr_Fetch (p_type=0x7fffb37fd740, p_value=0x7fffb37fd748, p_traceback=0x7fffb37fd750) at /tmp/build/80754af9/python_1553721932202/work/Python/errors.c:339          \r\n339     /tmp/build/80754af9/python_1553721932202/work/Python/errors.c: No such file or directory.                                                                     \r\n(gdb) backtrace\r\n#0  PyErr_Fetch (p_type=0x7fffb37fd740, p_value=0x7fffb37fd748, p_traceback=0x7fffb37fd750) at /tmp/build/80754af9/python_1553721932202/work/Python/errors.c:339      \r\n#1  0x00007fffe26bca53 in pybind11::error_already_set::~error_already_set() () from /data/users/shenli/pytorch/torch/lib/libtorch_python.so                           \r\n#2  0x00007fffe23ce0bc in __gxx_exception_cleanup (code=<optimized out>, exc=0x7fffa0000e40)                                                                          \r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:49                \r\n#3  __gxx_exception_cleanup (code=<optimized out>, exc=<optimized out>)\r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:33                \r\n#4  0x00007fffd2d2314b in torch::distributed::rpc::RequestCallback::operator()(torch::distributed::rpc::Message&) const ()                                            \r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#5  0x00007fffe2be5556 in torch::distributed::rpc::ProcessGroupAgent::enqueueRecv(torch::distributed::rpc::RecvWork)::{lambda(torch::distributed::rpc::RecvWork&)#1}::operator()(torch::distributed::rpc::RecvWork&) const () from /data/users/shenli/pytorch/torch/lib/libtorch_python.so                                                   \r\n#6  0x00007fffcc0cff13 in c10::ThreadPool::main_loop(unsigned long) () from /data/users/shenli/pytorch/torch/lib/libc10.so                                            \r\n#7  0x00007fffe23ea421 in std::execute_native_thread_routine_compat (__p=<optimized out>)                                                                             \r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94                  \r\n#8  0x00007ffff7bc6dd5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007ffff78efead in clone () from /lib64/libc.so.6\r\n```\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528",high priority|triaged|enhancement|better-engineering|module: rpc,rohan-varma,"@rohan-varma found that when the target Python function is missing on the callee side when using `rpc`/`remote`, the callee will hit segfault instead of sending the error message back to the caller. \r\n\r\n```python\r\n# On caller\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\ndist.init_process_group(""gloo"", init_method='tcp://127.0.0.1:12356', world_size=2, rank=1)\r\ndist.init_model_parallel(""worker0"")\r\n\r\ndef my_tensor_function(a, b):\r\n    return a + b\r\n\r\nret = dist.rpc_sync(""worker1"", my_tensor_function, args=(torch.ones(2, 2), torch.ones(2, 2)))\r\ndist.join_rpc()\r\n```\r\n\r\n```python\r\n# On callee\r\nimport torch.distributed as dist\r\n\r\n#def my_tensor_function(a, b):\r\n#        return a + b\r\n\r\ndist.init_process_group(""gloo"", init_method='tcp://127.0.0.1:12356', world_size=2, rank=0)\r\ndist.init_model_parallel(""worker1"")\r\ndist.join_rpc()\r\n```\r\n\r\nFor example, the above program will show the following error messages on the callee:\r\n\r\n```\r\nThread 6 ""python"" received signal SIGSEGV, Segmentation fault.\r\n[Switching to LWP 4177163]\r\nPyErr_Fetch (p_type=0x7fffb37fd740, p_value=0x7fffb37fd748, p_traceback=0x7fffb37fd750) at /tmp/build/80754af9/python_1553721932202/work/Python/errors.c:339          \r\n339     /tmp/build/80754af9/python_1553721932202/work/Python/errors.c: No such file or directory.                                                                     \r\n(gdb) backtrace\r\n#0  PyErr_Fetch (p_type=0x7fffb37fd740, p_value=0x7fffb37fd748, p_traceback=0x7fffb37fd750) at /tmp/build/80754af9/python_1553721932202/work/Python/errors.c:339      \r\n#1  0x00007fffe26bca53 in pybind11::error_already_set::~error_already_set() () from /data/users/shenli/pytorch/torch/lib/libtorch_python.so                           \r\n#2  0x00007fffe23ce0bc in __gxx_exception_cleanup (code=<optimized out>, exc=0x7fffa0000e40)                                                                          \r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:49                \r\n#3  __gxx_exception_cleanup (code=<optimized out>, exc=<optimized out>)\r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:33                \r\n#4  0x00007fffd2d2314b in torch::distributed::rpc::RequestCallback::operator()(torch::distributed::rpc::Message&) const ()                                            \r\n   from /data/users/shenli/pytorch/torch/lib/libtorch.so\r\n#5  0x00007fffe2be5556 in torch::distributed::rpc::ProcessGroupAgent::enqueueRecv(torch::distributed::rpc::RecvWork)::{lambda(torch::distributed::rpc::RecvWork&)#1}::operator()(torch::distributed::rpc::RecvWork&) const () from /data/users/shenli/pytorch/torch/lib/libtorch_python.so                                                   \r\n#6  0x00007fffcc0cff13 in c10::ThreadPool::main_loop(unsigned long) () from /data/users/shenli/pytorch/torch/lib/libc10.so                                            \r\n#7  0x00007fffe23ea421 in std::execute_native_thread_routine_compat (__p=<optimized out>)                                                                             \r\n    at /home/nwani/m3/conda-bld/compilers_linux-64_1560109574129/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:94                  \r\n#8  0x00007ffff7bc6dd5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007ffff78efead in clone () from /lib64/libc.so.6\r\n```\n\ncc @ezyang @gchanan @zou3519 @jerryzh168 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528","python\r\n# On caller\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\ndist.init_process_group(""gloo"", init_method='tcp://127.0.0.1:12356', world_size=2, rank=1)\r\ndist.init_model_parallel(""worker0"")\r\n\r\ndef my_tensor_function(a, b):\r\n    return a + b\r\n\r\nret = dist.rpc_sync(""worker1"", my_tensor_function, args=(torch.ones(2, 2), torch.ones(2, 2)))\r\ndist.join_rpc()\r\n"
27198,"[Contributor Welcome] Implement C++ API version of torch.nn.MultiMarginLoss## Context \r\n\r\nWe would like to add the following two APIs to the C++ frontend:\r\n\r\n*  `torch::nn::MultiMarginLoss` , which is the equivalent of Python API `torch.nn.MultiMarginLoss`.\r\n* `torch::nn::functional::multi_margin_loss`, which is the equivalent of Python API `torch.nn.functional.multi_margin_loss`.\r\n\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::MultiMarginLossOptions` to `torch/csrc/api/include/torch/nn/options/loss.h` (add this file if it doesn\u2019t exist), which should include the following parameters (based on https://pytorch.org/docs/stable/nn.html#torch.nn.MultiMarginLoss)\r\n    * `TORCH_ARG(int64_t, p) = 1`\r\n    * `TORCH_ARG(double, margin) = 1.0`\r\n    * `TORCH_ARG(Tensor, weight) = Tensor()`\r\n    * `TORCH_ARG(Reduction::Reduction, reduction) = Reduction::Mean`\r\n    * NOTE: please make sure to add the same comments for the parameters as in the Python version. For example, for parameter `p` we should say `Has a default value of 1. 1 and 2 are the only supported values`.\r\n* Add `torch::nn::functional::multi_margin_loss(...)` in  `torch/csrc/api/include/torch/nn/functional/loss.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n\r\n\r\n* Add `torch::nn::MultiMarginLoss` in `torch/csrc/api/include/torch/nn/modules/loss.h` (add this file if it doesn\u2019t exist). The module\u2019s constructor should take `torch::nn::MultiMarginLossOptions` as input (and store it in its internal options field), the module\u2019s reset() method should register `weight` as a buffer (same logic as https://github.com/pytorch/pytorch/blob/5cac7387135ccf8db5b4b1ed9f52f80c924c2bc8/torch/nn/modules/loss.py#L20), and the module\u2019s forward method should have the following signature:\r\n\r\n\r\n\r\n* Add test for `torch::nn::MultiMarginLoss` in test/cpp/api/modules.cpp. We can just check that the output value for some simple cases are as expected.\r\n\r\n* Add test for `torch::nn::functional::multi_margin_loss(...)` in test/cpp/api/functional.cpp, using essentially the same tests as `torch::nn::MultiMarginLoss` .\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\ncc @yf225",module: cpp|good first issue|triaged|enhancement,CarMiranda,"## Context \r\n\r\nWe would like to add the following two APIs to the C++ frontend:\r\n\r\n*  `torch::nn::MultiMarginLoss` , which is the equivalent of Python API `torch.nn.MultiMarginLoss`.\r\n* `torch::nn::functional::multi_margin_loss`, which is the equivalent of Python API `torch.nn.functional.multi_margin_loss`.\r\n\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::MultiMarginLossOptions` to `torch/csrc/api/include/torch/nn/options/loss.h` (add this file if it doesn\u2019t exist), which should include the following parameters (based on https://pytorch.org/docs/stable/nn.html#torch.nn.MultiMarginLoss)\r\n    * `TORCH_ARG(int64_t, p) = 1`\r\n    * `TORCH_ARG(double, margin) = 1.0`\r\n    * `TORCH_ARG(Tensor, weight) = Tensor()`\r\n    * `TORCH_ARG(Reduction::Reduction, reduction) = Reduction::Mean`\r\n    * NOTE: please make sure to add the same comments for the parameters as in the Python version. For example, for parameter `p` we should say `Has a default value of 1. 1 and 2 are the only supported values`.\r\n* Add `torch::nn::functional::multi_margin_loss(...)` in  `torch/csrc/api/include/torch/nn/functional/loss.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n```cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor multi_margin_loss(\r\n    const Tensor& input,\r\n    const Tensor& target,\r\n    const MultiMarginLossOptions& options = {}) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n```\r\n\r\n* Add `torch::nn::MultiMarginLoss` in `torch/csrc/api/include/torch/nn/modules/loss.h` (add this file if it doesn\u2019t exist). The module\u2019s constructor should take `torch::nn::MultiMarginLossOptions` as input (and store it in its internal options field), the module\u2019s reset() method should register `weight` as a buffer (same logic as https://github.com/pytorch/pytorch/blob/5cac7387135ccf8db5b4b1ed9f52f80c924c2bc8/torch/nn/modules/loss.py#L20), and the module\u2019s forward method should have the following signature:\r\n\r\n```cpp\r\nTensor forward(\r\n    const Tensor& input,\r\n    const Tensor& target) {\r\n  // Should call torch::nn::functional::multi_margin_loss with all inputs and the module's options\r\n}\r\n```\r\n\r\n* Add test for `torch::nn::MultiMarginLoss` in test/cpp/api/modules.cpp. We can just check that the output value for some simple cases are as expected.\r\n\r\n* Add test for `torch::nn::functional::multi_margin_loss(...)` in test/cpp/api/functional.cpp, using essentially the same tests as `torch::nn::MultiMarginLoss` .\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\ncc @yf225","cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor multi_margin_loss(\r\n    const Tensor& input,\r\n    const Tensor& target,\r\n    const MultiMarginLossOptions& options = {}) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n"
27196,"[Contributor Welcome] Implement C++ API version of torch.nn.functional.affine_grid## Context \r\n\r\nWe would like to add `torch::nn::functional::affine_grid` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.affine_grid`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::affine_grid(...)` in  `torch/csrc/api/include/torch/nn/functional/vision.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n\r\n\r\n* Add tests for `torch::nn::functional::affine_grid(...)` in test/cpp/api/functional.cpp. It would be great to provide different values for `theta` / `size` / `align_corners` , and check that the output values are as expected (aka. same as the output values from Python). Also we should add tests to check that all errors are thrown correctly, by using `ASSERT_THROWS_WITH` .\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\n\ncc @yf225",module: cpp|good first issue|triaged|enhancement,jon-tow,"## Context \r\n\r\nWe would like to add `torch::nn::functional::affine_grid` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.affine_grid`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::affine_grid(...)` in  `torch/csrc/api/include/torch/nn/functional/vision.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n```cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor affine_grid(\r\n    const Tensor& theta,\r\n    const IntArrayRef& size,\r\n    bool align_corners = false) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n```\r\n\r\n* Add tests for `torch::nn::functional::affine_grid(...)` in test/cpp/api/functional.cpp. It would be great to provide different values for `theta` / `size` / `align_corners` , and check that the output values are as expected (aka. same as the output values from Python). Also we should add tests to check that all errors are thrown correctly, by using `ASSERT_THROWS_WITH` .\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\n\ncc @yf225","cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor affine_grid(\r\n    const Tensor& theta,\r\n    const IntArrayRef& size,\r\n    bool align_corners = false) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n"
27096,Support deleting RRef in circular dependenciesThis was original mentioned by @aazzolini ([comment](https://github.com/pytorch/pytorch/pull/25499#discussion_r328411474))\r\n\r\nThe implementation in #25499 does not delete RRef correctly when it is inside some object with circular dependency. The following code would expose the leak.\r\n\r\n\r\n\r\nRef: \r\n\r\n- Python GC. https://rushter.com/blog/python-garbage-collector/\r\n- Python Ref Count C API. https://pythonextensionpatterns.readthedocs.io/\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528,triaged|better-engineering|module: rpc,mrshenli,"This was original mentioned by @aazzolini ([comment](https://github.com/pytorch/pytorch/pull/25499#discussion_r328411474))\r\n\r\nThe implementation in #25499 does not delete RRef correctly when it is inside some object with circular dependency. The following code would expose the leak.\r\n\r\n```python\r\n    @_wrap_with_rpc\r\n    def test_circular_dependency(self):\r\n        n = self.rank + 1\r\n        dst_rank = n % self.world_size\r\n        class EmptyClass:\r\n            pass\r\n        a = EmptyClass()\r\n        b = EmptyClass()\r\n        a.other = b\r\n        b.other = a\r\n        a.rref = dist.remote(\r\n            'worker{}'.format(dst_rank),\r\n            torch.add,\r\n            args=(torch.ones(n, n), 2)\r\n        )\r\n\r\n```\r\n\r\nRef: \r\n\r\n- Python GC. https://rushter.com/blog/python-garbage-collector/\r\n- Python Ref Count C API. https://pythonextensionpatterns.readthedocs.io/\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528","python\r\n    @_wrap_with_rpc\r\n    def test_circular_dependency(self):\r\n        n = self.rank + 1\r\n        dst_rank = n % self.world_size\r\n        class EmptyClass:\r\n            pass\r\n        a = EmptyClass()\r\n        b = EmptyClass()\r\n        a.other = b\r\n        b.other = a\r\n        a.rref = dist.remote(\r\n            'worker{}'.format(dst_rank),\r\n            torch.add,\r\n            args=(torch.ones(n, n), 2)\r\n        )\r\n\r\n"
27082,"[Contributor Welcome] Implement C++ API version of torch.nn.functional.pdist## Context \r\n\r\nWe would like to add `torch::nn::functional::pdist` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.pdist`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::pdist(...)` in  `torch/csrc/api/include/torch/nn/functional/distance.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::pdist(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for some simple cases.\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\r\n\r\n\r\ncc @yf225",module: cpp|good first issue|triaged,jon-tow,"## Context \r\n\r\nWe would like to add `torch::nn::functional::pdist` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.pdist`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::pdist(...)` in  `torch/csrc/api/include/torch/nn/functional/distance.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n```cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor pdist(\r\n    const Tensor& input,\r\n    int64_t p = 2) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n```\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::pdist(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for some simple cases.\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\r\n\r\n\r\ncc @yf225","cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor pdist(\r\n    const Tensor& input,\r\n    int64_t p = 2) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n"
27081,"[Contributor Welcome] Implement C++ API version of torch.nn.functional.one_hot## Context \r\n\r\nWe would like to add `torch::nn::functional::one_hot` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.one_hot`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::one_hot(...)` in  `torch/csrc/api/include/torch/nn/functional/embedding.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::one_hot(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for the following cases (expressed in Python):\r\n\r\n\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\r\n\n\ncc @yf225",module: cpp|good first issue|triaged,bzinodev,"## Context \r\n\r\nWe would like to add `torch::nn::functional::one_hot` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.one_hot`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::functional::one_hot(...)` in  `torch/csrc/api/include/torch/nn/functional/embedding.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n```cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor one_hot(\r\n    const Tensor& tensor,\r\n    int64_t num_classes = -1) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n```\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::one_hot(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for the following cases (expressed in Python):\r\n\r\n```python\r\n>>> F.one_hot(torch.arange(0, 5) % 3)\r\ntensor([[1, 0, 0],\r\n        [0, 1, 0],\r\n        [0, 0, 1],\r\n        [1, 0, 0],\r\n        [0, 1, 0]])\r\n>>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\r\ntensor([[1, 0, 0, 0, 0],\r\n        [0, 1, 0, 0, 0],\r\n        [0, 0, 1, 0, 0],\r\n        [1, 0, 0, 0, 0],\r\n        [0, 1, 0, 0, 0]])\r\n>>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\r\ntensor([[[1, 0, 0],\r\n         [0, 1, 0]],\r\n        [[0, 0, 1],\r\n         [1, 0, 0]],\r\n        [[0, 1, 0],\r\n         [0, 0, 1]]])\r\n```\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\r\n\n\ncc @yf225","cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor one_hot(\r\n    const Tensor& tensor,\r\n    int64_t num_classes = -1) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n"
27078,"[Contributor Welcome] Implement C++ API version of torch.nn.functional.gumbel_softmax## Context \r\n\r\nWe would like to add `torch::nn::functional::gumbel_softmax` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.gumbel_softmax`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::GumbelSoftmaxOptions` to `torch/csrc/api/include/torch/nn/options/activation.h` (add this file if it doesn\u2019t exist), which should include the following parameters (based on https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.gumbel_softmax)\r\n    * `TORCH_ARG(double, tau) = 1.0`\r\n    * `TORCH_ARG(bool, hard) = false`\r\n    * `TORCH_ARG(double, eps) = 1e-10`\r\n    * `TORCH_ARG(int64_t, dim) = -1`\r\n    * NOTE: please make sure to add the same comments for the parameters as in the Python version. For example, for parameter `tau` we should say `non-negative scalar temperature`\r\n* Add `torch::nn::functional::gumbel_softmax(...)` in  `torch/csrc/api/include/torch/nn/functional/activation.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::gumbel_softmax(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for the following cases (expressed in Python):\r\n\r\n\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\n\ncc @yf225",module: cpp|good first issue|triaged,Naresh1318,"## Context \r\n\r\nWe would like to add `torch::nn::functional::gumbel_softmax` to the C++ API, so that C++ users can easily find the equivalent of Python API `torch.nn.functional.gumbel_softmax`.\r\n\r\n\r\n## Steps\r\n\r\n* Add `torch::nn::GumbelSoftmaxOptions` to `torch/csrc/api/include/torch/nn/options/activation.h` (add this file if it doesn\u2019t exist), which should include the following parameters (based on https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.gumbel_softmax)\r\n    * `TORCH_ARG(double, tau) = 1.0`\r\n    * `TORCH_ARG(bool, hard) = false`\r\n    * `TORCH_ARG(double, eps) = 1e-10`\r\n    * `TORCH_ARG(int64_t, dim) = -1`\r\n    * NOTE: please make sure to add the same comments for the parameters as in the Python version. For example, for parameter `tau` we should say `non-negative scalar temperature`\r\n* Add `torch::nn::functional::gumbel_softmax(...)` in  `torch/csrc/api/include/torch/nn/functional/activation.h` (add this file if it doesn\u2019t exist). The function should have the following signature:\r\n\r\n```cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor gumbel_softmax(\r\n    const Tensor& logits,\r\n    const GumbelSoftmaxOptions& options = {}) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n```\r\n\r\n\r\n\r\n* Add test for `torch::nn::functional::gumbel_softmax(...)` in test/cpp/api/functional.cpp. It can just check whether the function output is as expected for the following cases (expressed in Python):\r\n\r\n```python\r\n>>> logits = torch.randn(20, 32)\r\n>>> # Sample soft categorical using reparametrization trick:\r\n>>> F.gumbel_softmax(logits, tau=1, hard=False)\r\n>>> # Sample hard categorical using ""Straight-through"" trick:\r\n>>> F.gumbel_softmax(logits, tau=1, hard=True)\r\n```\r\n\r\n## Helpful Resources\r\n\r\nThere are quite a few PRs for adding new functionals / new modules for the C++ API (the list of PRs is in https://github.com/pytorch/pytorch/issues/25883), which can serve as great references. Also please ping @yf225 on this issue if you encounter any problems.\r\n\r\n\r\n## How do I claim this feature request?\r\n\r\nPlease comment in this issue if you are interested in working on it.\r\n\r\n\r\n\n\ncc @yf225","cpp\r\nnamespace torch {\r\nnamespace nn {\r\nnamespace functional {\r\n\r\ninline Tensor gumbel_softmax(\r\n    const Tensor& logits,\r\n    const GumbelSoftmaxOptions& options = {}) {\r\n  ...\r\n}\r\n\r\n} // namespace functional\r\n} // namespace nn\r\n} // namespace torch\r\n"
26944,"ProcessGroupAgent failed to join with long async forward chainApart from the deadlock issue mentioned in #26362, there could be two other reasons that causes `join()` to occasionally throw timeout errors. \r\n\r\n1. There is a period of time that the sender has finished sending and the receiver has finished receiving, but the received message is still in the listenLoop() and hasn't been enqueued into the ThreadPool yet. It means that the following `sync`/`join` code could exit before all communications finish. \r\n\r\nhttps://github.com/pytorch/pytorch/blob/7e95439e9f4dd1aa3d343250cb418cbc356a72ff/torch/csrc/distributed/rpc/process_group_agent.cpp#L162-L173\r\n\r\nAs a result, the `SHUTDOWN` message could be sent before regular messages, leading to the timeout. \r\n\r\n2. #24074 and #25499 attempts to address the above problem by waiting for all futures to settle on each individual worker. However, this might not be sufficient. When we have many layers of nested rpc_async/remote calls, received messages could trigger more sends. It means even if one worker didn't see any send task in the thread pool and no unsettled futures at that time, it doesn't mean the messages it received later won't create new sends. \r\n\r\nThese might be one reason for the flakiness @rohan-varma saw earlier today in #26570. \r\n\r\n#### Proposed Solution\r\n\r\nHow about using `ProcessGroup::allreduce(MAX)` to globally check if there is any unfinished future on any worker, and use a while loop to ensure that `SHUTDOWN` message is only sent after that? Something like:\r\n\r\n\r\n\r\nHowever, future p2p `RpcAgent` implementations does not have the luxury to use collective communication APIs. We might eventually need to build a more complex termination detection algorithm on top of `RpcAgent`.\r\n\r\n@xush6528 @satgera @aazzolini @rohan-varma @pietern Does this make sense?\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini",oncall: distributed|triaged|module: rpc,mrshenli,"Apart from the deadlock issue mentioned in #26362, there could be two other reasons that causes `join()` to occasionally throw timeout errors. \r\n\r\n1. There is a period of time that the sender has finished sending and the receiver has finished receiving, but the received message is still in the listenLoop() and hasn't been enqueued into the ThreadPool yet. It means that the following `sync`/`join` code could exit before all communications finish. \r\n\r\nhttps://github.com/pytorch/pytorch/blob/7e95439e9f4dd1aa3d343250cb418cbc356a72ff/torch/csrc/distributed/rpc/process_group_agent.cpp#L162-L173\r\n\r\nAs a result, the `SHUTDOWN` message could be sent before regular messages, leading to the timeout. \r\n\r\n2. #24074 and #25499 attempts to address the above problem by waiting for all futures to settle on each individual worker. However, this might not be sufficient. When we have many layers of nested rpc_async/remote calls, received messages could trigger more sends. It means even if one worker didn't see any send task in the thread pool and no unsettled futures at that time, it doesn't mean the messages it received later won't create new sends. \r\n\r\nThese might be one reason for the flakiness @rohan-varma saw earlier today in #26570. \r\n\r\n#### Proposed Solution\r\n\r\nHow about using `ProcessGroup::allreduce(MAX)` to globally check if there is any unfinished future on any worker, and use a while loop to ensure that `SHUTDOWN` message is only sent after that? Something like:\r\n\r\n```C++\r\nwhile(true) {\r\n  auto t = torch::tensor({futures_.size()});\r\n  pg_->allreduce(t, ReduceOp::MAX)->wait();\r\n  if (t.storage().data<int64_t>()[0] == 0) {\r\n    break;\r\n  }\r\n  std::unique_lock<std::mutex> lock(futureMutex_);\r\n  futureCV_.wait(lock, [this] {\r\n    return futures_.empty();\r\n  });\r\n  lock.unlock();\r\n}\r\n```\r\n\r\nHowever, future p2p `RpcAgent` implementations does not have the luxury to use collective communication APIs. We might eventually need to build a more complex termination detection algorithm on top of `RpcAgent`.\r\n\r\n@xush6528 @satgera @aazzolini @rohan-varma @pietern Does this make sense?\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini","C++\r\nwhile(true) {\r\n  auto t = torch::tensor({futures_.size()});\r\n  pg_->allreduce(t, ReduceOp::MAX)->wait();\r\n  if (t.storage().data<int64_t>()[0] == 0) {\r\n    break;\r\n  }\r\n  std::unique_lock<std::mutex> lock(futureMutex_);\r\n  futureCV_.wait(lock, [this] {\r\n    return futures_.empty();\r\n  });\r\n  lock.unlock();\r\n}\r\n"
26838,"test_cuda_kernel_loop_overflow_large fails on current build## \U0001f41b Bug\r\n\r\n`test_cuda_kernel_loop_overflow_large` from `TestCuda`: fails with `RuntimeError: avg_pool2d_out_cuda_frame failed with error code 0` using the current PyTorch master and nightly binaries tested on:\r\n* TitanV, CUDA10.0\r\n* P100, CUDA10.1\r\n* V100, CUDA10.1\r\n\r\nRelated PR: [link](https://github.com/pytorch/pytorch/pull/24818)\r\nCC @skrah, @ezyang \r\nIs this test being skipped due to:\r\n`@unittest.skipIf(not TEST_LARGE_TENSOR, ""not enough memory"")`?\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nTest should pass.\r\n\r\n## Environment\r\n\r\nFailing on `3f72bcfc`, `24ae9b5`, `1.3.0.dev20190923`, and `1.3.0.dev20190925`.\r\n\r\n\r\nCC @ngimel\n\ncc @ezyang @gchanan @zou3519",high priority|module: cuda|module: ci|triaged,pbelevich,"## \U0001f41b Bug\r\n\r\n`test_cuda_kernel_loop_overflow_large` from `TestCuda`: fails with `RuntimeError: avg_pool2d_out_cuda_frame failed with error code 0` using the current PyTorch master and nightly binaries tested on:\r\n* TitanV, CUDA10.0\r\n* P100, CUDA10.1\r\n* V100, CUDA10.1\r\n\r\nRelated PR: [link](https://github.com/pytorch/pytorch/pull/24818)\r\nCC @skrah, @ezyang \r\nIs this test being skipped due to:\r\n`@unittest.skipIf(not TEST_LARGE_TENSOR, ""not enough memory"")`?\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nx = torch.randn(1, 1, 1, 2**31 - 1, dtype=torch.float16, device=""cuda"")\r\nexpected = x[0, 0, 0, 2**31 - 2]\r\ny = torch.nn.functional.avg_pool2d(x, kernel_size=1)\r\ntorch.cuda.synchronize()\r\n\r\n> RuntimeError: avg_pool2d_out_cuda_frame failed with error code 0\r\n```\r\n\r\n## Expected behavior\r\n\r\nTest should pass.\r\n\r\n## Environment\r\n\r\nFailing on `3f72bcfc`, `24ae9b5`, `1.3.0.dev20190923`, and `1.3.0.dev20190925`.\r\n\r\n\r\nCC @ngimel\n\ncc @ezyang @gchanan @zou3519","python\r\nimport torch\r\n\r\nx = torch.randn(1, 1, 1, 2**31 - 1, dtype=torch.float16, device=""cuda"")\r\nexpected = x[0, 0, 0, 2**31 - 2]\r\ny = torch.nn.functional.avg_pool2d(x, kernel_size=1)\r\ntorch.cuda.synchronize()\r\n\r\n> RuntimeError: avg_pool2d_out_cuda_frame failed with error code 0\r\n"
26790,"Importing tensorboard jams CUDA device selection## \U0001f41b Bug\r\n\r\nOn a multi-GPU environment, it is common to set `os.environ['CUDA_VISIBLE_DEVICES']` to select one GPU. However I have found that importing `torch.utils.tensorboard` leads to an unexpected behavior in this context.\r\n\r\nI have two Titan Xp GPUs on an Ubuntu 16.04.5 server. Setting `os.environ['CUDA_VISIBLE_DEVICES'] = '1'` had all memory allocations and computations happen precisely on the second Titan GPU, until I integrated tensorboard into my code today. Now with the same code, memory allocation and computation happens on the first Titan GPU.\r\n\r\nBelow are the screenshots I took. The first Titan GPU is running something, which allocates 8742MB is GPU memory. The second Titan GPU has 10MB allocated by default (I have no idea why). Refer to the reproduction steps below. \r\n\r\n(Screenshot 1)\r\n<img width=""1440"" alt=""\uc2a4\ud06c\ub9b0\uc0f7 2019-09-25 \uc624\ud6c4 3 58 17"" src=""https://user-images.githubusercontent.com/29395896/65578067-102af680-dfb0-11e9-8ddf-21eb126c60c2.png"">\r\n\r\n(Screenshot 2)\r\n<img width=""1440"" alt=""\uc2a4\ud06c\ub9b0\uc0f7 2019-09-25 \uc624\ud6c4 3 58 55"" src=""https://user-images.githubusercontent.com/29395896/65578069-11f4ba00-dfb0-11e9-9941-1edb5fd3950d.png"">\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run `python reproduction_script.py`.\r\n1. Check that memory is allocated on the second GPU. (Screenshot 1)\r\n1. Uncomment the fourth line `import torch.utils.tensorboard`.\r\n1. Run `python reproduction_script.py` again.\r\n1. Check that memory is allocated on the *first* GPU. (Screenshot 2)\r\n\r\nBelow is the `reproduction_script.py` code:\r\n\r\n\r\n## Expected behavior\r\n\r\nI expect that setting `os.environ['CUDA_VISIBLE_DEVICES']` to the index of a specific GPU and allocating tensors by the `device` name returned calling `torch.cuda.current_device()` have all memory allocations and computations happen on that specific GPU.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0a0+6b959ee\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchvision               0.4.0                py36_cu100    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\n`conda list tb-nightly` prints\r\n```\r\n# packages in environment at /home/jaywonchung/anaconda3/envs/meta:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntb-nightly                2.1.0a20190924           pypi_0    pypi\r\n```\r\n",triaged|module: tensorboard,sanekmelnikov,"## \U0001f41b Bug\r\n\r\nOn a multi-GPU environment, it is common to set `os.environ['CUDA_VISIBLE_DEVICES']` to select one GPU. However I have found that importing `torch.utils.tensorboard` leads to an unexpected behavior in this context.\r\n\r\nI have two Titan Xp GPUs on an Ubuntu 16.04.5 server. Setting `os.environ['CUDA_VISIBLE_DEVICES'] = '1'` had all memory allocations and computations happen precisely on the second Titan GPU, until I integrated tensorboard into my code today. Now with the same code, memory allocation and computation happens on the first Titan GPU.\r\n\r\nBelow are the screenshots I took. The first Titan GPU is running something, which allocates 8742MB is GPU memory. The second Titan GPU has 10MB allocated by default (I have no idea why). Refer to the reproduction steps below. \r\n\r\n(Screenshot 1)\r\n<img width=""1440"" alt=""\uc2a4\ud06c\ub9b0\uc0f7 2019-09-25 \uc624\ud6c4 3 58 17"" src=""https://user-images.githubusercontent.com/29395896/65578067-102af680-dfb0-11e9-8ddf-21eb126c60c2.png"">\r\n\r\n(Screenshot 2)\r\n<img width=""1440"" alt=""\uc2a4\ud06c\ub9b0\uc0f7 2019-09-25 \uc624\ud6c4 3 58 55"" src=""https://user-images.githubusercontent.com/29395896/65578069-11f4ba00-dfb0-11e9-9941-1edb5fd3950d.png"">\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run `python reproduction_script.py`.\r\n1. Check that memory is allocated on the second GPU. (Screenshot 1)\r\n1. Uncomment the fourth line `import torch.utils.tensorboard`.\r\n1. Run `python reproduction_script.py` again.\r\n1. Check that memory is allocated on the *first* GPU. (Screenshot 2)\r\n\r\nBelow is the `reproduction_script.py` code:\r\n```python\r\nimport os\r\n\r\nimport torch\r\n# import torch.utils.tensorboard\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\ndevice = torch.cuda.current_device()\r\n\r\ngpu_tensor = torch.ones((200, 200, 200), device=device)\r\n\r\ninput('Enter anything to terminate script.')\r\n```\r\n\r\n## Expected behavior\r\n\r\nI expect that setting `os.environ['CUDA_VISIBLE_DEVICES']` to the index of a specific GPU and allocating tensors by the `device` name returned calling `torch.cuda.current_device()` have all memory allocations and computations happen on that specific GPU.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0a0+6b959ee\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchvision               0.4.0                py36_cu100    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\n`conda list tb-nightly` prints\r\n```\r\n# packages in environment at /home/jaywonchung/anaconda3/envs/meta:\r\n#\r\n# Name                    Version                   Build  Channel\r\ntb-nightly                2.1.0a20190924           pypi_0    pypi\r\n```\r\n","python\r\nimport os\r\n\r\nimport torch\r\n# import torch.utils.tensorboard\r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '1'\r\ndevice = torch.cuda.current_device()\r\n\r\ngpu_tensor = torch.ones((200, 200, 200), device=device)\r\n\r\ninput('Enter anything to terminate script.')\r\n"
26747,[jit] `dict()` with kwargs doesn't work\n\ncc @suo,oncall: jit|low priority|triaged|jit-backlog,eellison,"```python\r\ndict(a=2, b=3, c=4)\r\n```\n\ncc @suo","python\r\ndict(a=2, b=3, c=4)\r\n"
26729,[jit] Cannot access tensor layout\n\ncc @suo,oncall: jit|triaged,jamesr66a,```python\r\n@torch.jit.script\r\ndef fn(x):\r\n\treturn x.layout\r\n```\n\ncc @suo,python\r\n@torch.jit.script\r\ndef fn(x):\r\n\treturn x.layout\r\n
26727,"[jit] `zeros_like` needs full Tensor options to work\r\n\r\noutputs\r\n\r\n\r\n```\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::zeros_like(Tensor self) -> (Tensor):\r\n  Keyword argument dtype unknown.\r\n  \r\n  aten::zeros_like.dtype(Tensor self, *, int dtype, int layout, Device device, bool pin_memory=False) -> (Tensor):\r\n  Argument layout not provided.\r\n```\n\ncc @suo",oncall: jit|triaged|jit-backlog,Krovatkin,"```python\r\npos_idx_per_image_mask = torch.zeros_like(\r\n    matched_idxs_per_image, dtype=torch.uint8\r\n)\r\n```\r\n\r\noutputs\r\n\r\n\r\n```\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::zeros_like(Tensor self) -> (Tensor):\r\n  Keyword argument dtype unknown.\r\n  \r\n  aten::zeros_like.dtype(Tensor self, *, int dtype, int layout, Device device, bool pin_memory=False) -> (Tensor):\r\n  Argument layout not provided.\r\n```\n\ncc @suo","python\r\npos_idx_per_image_mask = torch.zeros_like(\r\n    matched_idxs_per_image, dtype=torch.uint8\r\n)\r\n"
26726,[jit] Recursive script doesn't pick up TorchScript class attributes\n\ncc @suo,oncall: jit|triaged|jit-backlog,jamesr66a,"```python\r\n@torch.jit.script\r\nclass X(object)\r\n\t...\r\n\r\nclass M(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\r\n\t\t# The type is not inferred correctly here, so this doesn't get added as an attribute\r\n\t\tself.x = X()\r\n```\n\ncc @suo","python\r\n@torch.jit.script\r\nclass X(object)\r\n\t...\r\n\r\nclass M(nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper().__init__()\r\n\r\n\t\t# The type is not inferred correctly here, so this doesn't get added as an attribute\r\n\t\tself.x = X()\r\n"
26673,"Annotation Problem *.pyi## \U0001f680 Feature\r\n\r\nIt is difficult to control the validity of function arguments when they are in different files.\r\nVery often the information in *.pyi does not match *.py.\r\nhttps://github.com/pytorch/pytorch/blob/fcd13549f923342a7474993928d86181b75734d3/torch/utils/data/dataloader.pyi#L35\r\nI suggest using Python 2 Annotations:\r\n\r\nYou can expand this format to indicate in which file to place annotations.\r\nAfter that, all *.pyi can be completely assembled in one script. Information will always be up to date.\n\ncc @ezyang @gchanan @zou3519",high priority|module: typing|triaged,ngoldbaum,"## \U0001f680 Feature\r\n\r\nIt is difficult to control the validity of function arguments when they are in different files.\r\nVery often the information in *.pyi does not match *.py.\r\nhttps://github.com/pytorch/pytorch/blob/fcd13549f923342a7474993928d86181b75734d3/torch/utils/data/dataloader.pyi#L35\r\nI suggest using Python 2 Annotations:\r\n```python\r\ndef optional_unwrap (self, x, y):\r\n\xa0\xa0 # type: (Optional [int], Optional [int]) -> int\r\n```\r\nYou can expand this format to indicate in which file to place annotations.\r\nAfter that, all *.pyi can be completely assembled in one script. Information will always be up to date.\n\ncc @ezyang @gchanan @zou3519","python\r\ndef optional_unwrap (self, x, y):\r\n\xa0\xa0 # type: (Optional [int], Optional [int]) -> int\r\n"
26578,"[jit] Fix future type annotation in pythonWe should fix the `Future` type annotation to make it also work in python, fork will immediately execute the function single threaded and returns a Future of the result, which wait just unpacks.(the same way that tracing did). \r\n\n\ncc @ezyang @gchanan @zou3519 @suo",oncall: jit|triaged|jit-backlog,wanchaol,"We should fix the `Future` type annotation to make it also work in python, fork will immediately execute the function single threaded and returns a Future of the result, which wait just unpacks.(the same way that tracing did). \r\n```python\r\nfrom torch import nn\r\nfrom typing import List\r\nimport torch\r\n\r\nclass Encoder(nn.Module):\r\n    def forward(self, x):\r\n        return x\r\n\r\nclass EncoderEnsemble2(nn.Module):\r\n    def __init__(self, encoders : List[nn.Module]):\r\n        super().__init__()\r\n        self.encoders = nn.ModuleList(encoders)\r\n\r\n    def forward(self, x):\r\n        futures = torch.jit.annotate(\r\n            List[Future[Tensor]], []\r\n        )\r\n        for encoder in self.encoders:\r\n            futures.append(\r\n                torch.jit._fork(encoder, x)\r\n            )\r\n\r\n        all_outputs = []\r\n        for future in futures:\r\n            all_outputs.append(torch.jit._wait(future))\r\n        return all_outputs\r\n\r\nmodel =EncoderEnsemble2([Encoder()])\r\noutput = model(torch.randn(3, 4))\r\n```\n\ncc @ezyang @gchanan @zou3519 @suo","python\r\nfrom torch import nn\r\nfrom typing import List\r\nimport torch\r\n\r\nclass Encoder(nn.Module):\r\n    def forward(self, x):\r\n        return x\r\n\r\nclass EncoderEnsemble2(nn.Module):\r\n    def __init__(self, encoders : List[nn.Module]):\r\n        super().__init__()\r\n        self.encoders = nn.ModuleList(encoders)\r\n\r\n    def forward(self, x):\r\n        futures = torch.jit.annotate(\r\n            List[Future[Tensor]], []\r\n        )\r\n        for encoder in self.encoders:\r\n            futures.append(\r\n                torch.jit._fork(encoder, x)\r\n            )\r\n\r\n        all_outputs = []\r\n        for future in futures:\r\n            all_outputs.append(torch.jit._wait(future))\r\n        return all_outputs\r\n\r\nmodel =EncoderEnsemble2([Encoder()])\r\noutput = model(torch.randn(3, 4))\r\n"
26472,"[ONNX] support scater_add with dynamic tensor shapes## \U0001f41b Bug\r\n\r\nI was getting this error\r\n\r\n> RuntimeError: Method run failed due to: [ONNXRuntimeError] : 1 : GENERAL ERROR : /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:341 void onnxruntime::BroadcastIterator::Init(int64_t, int64_t) axis == 1 \\|\\| axis == largest was false. Attempting to  broadcast an axis by a dimension other than 1. 0 by 6\r\n\r\nAn could not figure out where `6` was coming from. `6` is `shape[0]` of an intermediate tensor when `torch.onnx.export` was called, but it is dynamic.\r\n\r\n`scatter_add` creates a constant with a fixed size:\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nUse scatter_add along with a dynamic tensor input\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\n`scatter_add` should not assume fixed tensor sizes\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nWorkaround is to avoid `scatter_add` and use `where`\r\n\r\nUsed this workaround:\r\n\r\n",module: onnx|triaged,spandantiwari|houseroad,"## \U0001f41b Bug\r\n\r\nI was getting this error\r\n\r\n> RuntimeError: Method run failed due to: [ONNXRuntimeError] : 1 : GENERAL ERROR : /onnxruntime_src/onnxruntime/core/providers/cpu/math/element_wise_ops.h:341 void onnxruntime::BroadcastIterator::Init(int64_t, int64_t) axis == 1 \\|\\| axis == largest was false. Attempting to  broadcast an axis by a dimension other than 1. 0 by 6\r\n\r\nAn could not figure out where `6` was coming from. `6` is `shape[0]` of an intermediate tensor when `torch.onnx.export` was called, but it is dynamic.\r\n\r\n`scatter_add` creates a constant with a fixed size:\r\n\r\n```python\r\n@parse_args('v', 'i', 'v', 'v')\r\ndef scatter_add(g, self, dim, index, src):\r\n    if self.type().kind() != ""CompleteTensorType"":\r\n        return _unimplemented(""scatter_add"", ""input size not accessible"")\r\n    dtype = self.type().scalarType()\r\n    dtype = sym_help.scalar_type_to_onnx.index(sym_help.cast_pytorch_to_onnx[dtype])\r\n    dims = self.type().sizes()\r\n    to_add = torch.zeros(dims)\r\n    to_add = g.op(""Constant"", value_t=to_add)\r\n    to_add = scatter(g, to_add, dim, index, src)\r\n    return add(g, self, to_add)\r\n```\r\n\r\n\r\n## To Reproduce\r\n\r\nUse scatter_add along with a dynamic tensor input\r\n\r\n```python\r\n    cond = input_x < 0\r\n    idx = torch.nonzero(cond).view(-1)  # idx is dynamic shape\r\n    w = torch.scatter_add(w, 0, idx, input_x[idx])  # w[idx] += input_x[idx]\r\n    input_x = torch.where(cond, zero, input_x)  # input_x[idx] = 0\r\n```\r\n\r\n## Expected behavior\r\n\r\n`scatter_add` should not assume fixed tensor sizes\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nWorkaround is to avoid `scatter_add` and use `where`\r\n\r\nUsed this workaround:\r\n\r\n```python\r\nw = torch.scatter(w, 0, idx, w[idx] + input_x[idx])  # w[idx] += input_x[idx]\r\n```","python\r\n@parse_args('v', 'i', 'v', 'v')\r\ndef scatter_add(g, self, dim, index, src):\r\n    if self.type().kind() != ""CompleteTensorType"":\r\n        return _unimplemented(""scatter_add"", ""input size not accessible"")\r\n    dtype = self.type().scalarType()\r\n    dtype = sym_help.scalar_type_to_onnx.index(sym_help.cast_pytorch_to_onnx[dtype])\r\n    dims = self.type().sizes()\r\n    to_add = torch.zeros(dims)\r\n    to_add = g.op(""Constant"", value_t=to_add)\r\n    to_add = scatter(g, to_add, dim, index, src)\r\n    return add(g, self, to_add)\r\n"
26287,"problem with mkldnn and march=native on Intel Sandybridge processors## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch with -march=native on an Intel Sandybridge processor\r\n1. Run the test case below. This test case is a simplification of https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_mkldnn.py#L336\r\n\r\nCode of:\r\n\r\n\r\nProduces error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/activation.py"", line 982, in forward\r\n    return F.softmax(input, self.dim, _stacklevel=5)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/functional.py"", line 1230, in softmax\r\n    ret = input.softmax(dim)\r\nRuntimeError: basic_string::append\r\n```\r\n\r\n## Expected behavior\r\n\r\nThat it should not fail.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.7.2\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPyTorch built from source\r\nBuild command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON /rds/bear-apps/devel/2019a/branfosj-eb-4/EL7/EL7-haswell/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build\r\nCFLAGS and CXXFLAGS are set to -O2 -ftree-vectorize -march=native -fno-math-errno\r\n\r\n\r\nNote that the problem does not appear:\r\n1.  if I remove the `-march=native`\r\n1. nor is it seen on Haswell or Cascadelake processors.\n\ncc @ezyang @gchanan @zou3519 @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",high priority|triaged|module: mkldnn,albanD,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch with -march=native on an Intel Sandybridge processor\r\n1. Run the test case below. This test case is a simplification of https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_mkldnn.py#L336\r\n\r\nCode of:\r\n```python\r\nx = torch.randn(2, dtype=torch.float32)\r\nsoftmax = torch.nn.Softmax(dim=0)\r\nsoftmax(x.to_mkldnn())\r\n```\r\n\r\nProduces error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/modules/activation.py"", line 982, in forward\r\n    return F.softmax(input, self.dim, _stacklevel=5)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-1/EL7/EL7-sandybridge/software/PyTorch/1.2.0-foss-2019a-Python-3.7.2/lib/python3.7/site-packages/torch/nn/functional.py"", line 1230, in softmax\r\n    ret = input.softmax(dim)\r\nRuntimeError: basic_string::append\r\n```\r\n\r\n## Expected behavior\r\n\r\nThat it should not fail.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.7.2\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPyTorch built from source\r\nBuild command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON /rds/bear-apps/devel/2019a/branfosj-eb-4/EL7/EL7-haswell/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build\r\nCFLAGS and CXXFLAGS are set to -O2 -ftree-vectorize -march=native -fno-math-errno\r\n\r\n\r\nNote that the problem does not appear:\r\n1.  if I remove the `-march=native`\r\n1. nor is it seen on Haswell or Cascadelake processors.\n\ncc @ezyang @gchanan @zou3519 @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh","python\r\nx = torch.randn(2, dtype=torch.float32)\r\nsoftmax = torch.nn.Softmax(dim=0)\r\nsoftmax(x.to_mkldnn())\r\n"
26212,"[torchscript] Support requires_grad_ op in TorchScript## \U0001f680 Feature\r\nThe ability to set the `requires_grad` attribute a tensor in TorchScript.\r\n\r\n## Motivation\r\n\r\nWe are looking to use TorchScript to JIT compile a large algorithm. A part of the algorithm requires computing the gradient of  a certain callable w.r.t. parameters computed within the function being scripted. It is not possible or desirable to set `requires_grad_` at the outset because we only need to record the gradients at a very specific part of the larger algorithm.\r\n\r\ne.g.\r\n\r\n\r\ncurrently errors out with the master branch:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/Users/npradhan/workspace/pyro_dev/pyro/examples/pytorch_hackathon/example.py"", line 49, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/npradhan/miniconda3/envs/pytorch-dev/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1211, in script\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\nUnknown builtin op: aten::requires_grad_.\r\nCould not find any similar ops to aten::requires_grad_. This op may not exist or may not be currently supported in TorchScript.\r\n:\r\nat /Users/npradhan/workspace/pyro_dev/pyro/examples/pytorch_hackathon/example.py:51:4\r\n@torch.jit.script\r\ndef scriptfn(t):\r\n    t.requires_grad_(True)\r\n    ~~~~~~~~~~~~~~~~ <--- HERE\r\n    t_sq = t ** 2\r\n    grads = torch.autograd.grad([t_sq], [t])\r\n    t.requires_grad_(False)\r\n    return grads\r\n```\r\n\r\n## Pitch\r\n\r\nExpose the `requires_grad_` method for Tensors in TorchScript so as to enable usage patterns like above. cc. @zdevito, @bwasti who mentioned that this is already being worked on, but I couldn't find the relevant PR.\r\n\r\n\r\ncc @suo",oncall: jit,eellison,"## \U0001f680 Feature\r\nThe ability to set the `requires_grad` attribute a tensor in TorchScript.\r\n\r\n## Motivation\r\n\r\nWe are looking to use TorchScript to JIT compile a large algorithm. A part of the algorithm requires computing the gradient of  a certain callable w.r.t. parameters computed within the function being scripted. It is not possible or desirable to set `requires_grad_` at the outset because we only need to record the gradients at a very specific part of the larger algorithm.\r\n\r\ne.g.\r\n```python\r\n@torch.jit.script\r\ndef scriptfn(t):\r\n    t.requires_grad_(True)\r\n    t_sq = t ** 2\r\n    grads = torch.autograd.grad([t_sq], [t])\r\n    t.requires_grad_(False)\r\n    return grads\r\n\r\n\r\nu = torch.tensor(2.)\r\nprint(scriptfn(u))\r\n```\r\n\r\ncurrently errors out with the master branch:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/Users/npradhan/workspace/pyro_dev/pyro/examples/pytorch_hackathon/example.py"", line 49, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/npradhan/miniconda3/envs/pytorch-dev/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1211, in script\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\nUnknown builtin op: aten::requires_grad_.\r\nCould not find any similar ops to aten::requires_grad_. This op may not exist or may not be currently supported in TorchScript.\r\n:\r\nat /Users/npradhan/workspace/pyro_dev/pyro/examples/pytorch_hackathon/example.py:51:4\r\n@torch.jit.script\r\ndef scriptfn(t):\r\n    t.requires_grad_(True)\r\n    ~~~~~~~~~~~~~~~~ <--- HERE\r\n    t_sq = t ** 2\r\n    grads = torch.autograd.grad([t_sq], [t])\r\n    t.requires_grad_(False)\r\n    return grads\r\n```\r\n\r\n## Pitch\r\n\r\nExpose the `requires_grad_` method for Tensors in TorchScript so as to enable usage patterns like above. cc. @zdevito, @bwasti who mentioned that this is already being worked on, but I couldn't find the relevant PR.\r\n\r\n\r\ncc @suo","python\r\n@torch.jit.script\r\ndef scriptfn(t):\r\n    t.requires_grad_(True)\r\n    t_sq = t ** 2\r\n    grads = torch.autograd.grad([t_sq], [t])\r\n    t.requires_grad_(False)\r\n    return grads\r\n\r\n\r\nu = torch.tensor(2.)\r\nprint(scriptfn(u))\r\n"
26201,"Port fused layer_norm from APEX to ATenOriginally implemented in:\r\nhttps://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu\r\n\r\n# Changes:\r\n- Copy-paste the implementation from APEX to ATen. Codes are modified a bit to fit into ATen (for example: add support for the case `grad_out` is undefined, and to make it compatible with ROCm).\r\n- API change: `at::layer_norm` no longer requires the `bool cudnn_enabled` as an argument. (do I need to keep this argument for back compatibility? It is no longer used.)\r\n- `layer_norm` is now infinitely differentiable, but if higher order derivatives are asked (`create_graph=True` when computing backward), the performance of backwards is no longer optimized. (described below)\r\n- Some minor maintainability thing:\r\n  - The variables currently are named quite inconsistent, such as `X` vs `input`, `M` vs `n1`, `gamma` vs `weight`, I renamed a lot of them to make them consistent.\r\n  - Also inconsistency in code style: `)\\n{` vs `) {`, I changed all of them to `)\\n{`\r\n\r\n# About differentiability:\r\n**Current implementation:**\r\n- CPU: twice differentiable, optimized forward, backward and double backward code.\r\n- GPU: infinitely differentiable, not optimized for performance\r\n\r\n**This PR:**\r\nBoth CPU and GPU code now becomes infinitely differentiable and has optimized forward. And for backward, if `create_graph` set to false, then autograd will use the optimized code for computing backwards. But if `create_graph` set to true, then autograd will use a fallback infinitely differentiable implementation of the backward using ATen operators, which is not optimized for performance. This behavior is similar to what is done in WeightNorm (See: #10842).\r\n\r\nDue to this change, the optimized CPU code for double backward becomes a dead code and is removed.\r\n\r\n# Benchmark\r\nCode (Jupyter Notebook):\r\n\r\nResult on torch-nightly installed by pip:\r\n```\r\nPyTorch version: 1.3.0.dev20190920\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 100)\r\nElement size 4\r\nSize of the input tensor is 40000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n146 \xb5s \xb1 16 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n55.1 \xb5s \xb1 5.51 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n392 \xb5s \xb1 66.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n104 \xb5s \xb1 15.7 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.01 ms \xb1 135 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n146 \xb5s \xb1 15.4 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (1000, 100)\r\nElement size 4\r\nSize of the input tensor is 400000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n116 \xb5s \xb1 29.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n191 \xb5s \xb1 383 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n210 \xb5s \xb1 7.03 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n298 \xb5s \xb1 1.24 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n735 \xb5s \xb1 53.4 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n341 \xb5s \xb1 1.73 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 500)\r\nElement size 4\r\nSize of the input tensor is 200000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n82 \xb5s \xb1 958 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n101 \xb5s \xb1 200 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n209 \xb5s \xb1 10.2 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n168 \xb5s \xb1 2.45 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n980 \xb5s \xb1 337 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n7.18 ms \xb1 1.84 ms per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n```\r\nResult for this PR:\r\n```\r\nPyTorch version: 1.3.0a0+77e6902\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 100)\r\nElement size 4\r\nSize of the input tensor is 40000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n33.7 \xb5s \xb1 97.6 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n26.3 \xb5s \xb1 66.8 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n112 \xb5s \xb1 3.06 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n57.7 \xb5s \xb1 1.5 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.15 ms \xb1 68 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n369 \xb5s \xb1 7.04 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (1000, 100)\r\nElement size 4\r\nSize of the input tensor is 400000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n37.4 \xb5s \xb1 95.1 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n136 \xb5s \xb1 49.3 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n116 \xb5s \xb1 1.83 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n201 \xb5s \xb1 7.78 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\nThe slowest run took 4.15 times longer than the fastest. This could mean that an intermediate result is being cached.\r\n1.84 ms \xb1 1.16 ms per loop (mean \xb1 std. dev. of 7 runs, 100 loops each)\r\ncpu backward, create_graph=True:\r\n592 \xb5s \xb1 27.1 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 500)\r\nElement size 4\r\nSize of the input tensor is 200000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n34.5 \xb5s \xb1 91.5 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n70.4 \xb5s \xb1 25.2 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n118 \xb5s \xb1 5.52 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n131 \xb5s \xb1 7.7 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.72 ms \xb1 125 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n471 \xb5s \xb1 45.1 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n```",oncall: jit|module: onnx|module: internals|module: nn|module: cuda|module: cpu|triaged|open source,zasdfgbnm,"Originally implemented in:\r\nhttps://github.com/NVIDIA/apex/blob/master/csrc/layer_norm_cuda_kernel.cu\r\n\r\n# Changes:\r\n- Copy-paste the implementation from APEX to ATen. Codes are modified a bit to fit into ATen (for example: add support for the case `grad_out` is undefined, and to make it compatible with ROCm).\r\n- API change: `at::layer_norm` no longer requires the `bool cudnn_enabled` as an argument. (do I need to keep this argument for back compatibility? It is no longer used.)\r\n- `layer_norm` is now infinitely differentiable, but if higher order derivatives are asked (`create_graph=True` when computing backward), the performance of backwards is no longer optimized. (described below)\r\n- Some minor maintainability thing:\r\n  - The variables currently are named quite inconsistent, such as `X` vs `input`, `M` vs `n1`, `gamma` vs `weight`, I renamed a lot of them to make them consistent.\r\n  - Also inconsistency in code style: `)\\n{` vs `) {`, I changed all of them to `)\\n{`\r\n\r\n# About differentiability:\r\n**Current implementation:**\r\n- CPU: twice differentiable, optimized forward, backward and double backward code.\r\n- GPU: infinitely differentiable, not optimized for performance\r\n\r\n**This PR:**\r\nBoth CPU and GPU code now becomes infinitely differentiable and has optimized forward. And for backward, if `create_graph` set to false, then autograd will use the optimized code for computing backwards. But if `create_graph` set to true, then autograd will use a fallback infinitely differentiable implementation of the backward using ATen operators, which is not optimized for performance. This behavior is similar to what is done in WeightNorm (See: #10842).\r\n\r\nDue to this change, the optimized CPU code for double backward becomes a dead code and is removed.\r\n\r\n# Benchmark\r\nCode (Jupyter Notebook):\r\n```python\r\nimport torch\r\nfrom torch.nn import LayerNorm\r\nimport warnings\r\nimport gc\r\n\r\n\r\nLINE_WIDTH = 80\r\nwarnings.filterwarnings('ignore')\r\nprint('PyTorch version:', torch.__version__)\r\nprint()\r\n\r\n\r\ndef benchmark(*sizes):\r\n    print('=' * LINE_WIDTH)\r\n    print(""Benchmarking input shape"", sizes)\r\n    normalized_shape = sizes[1:]\r\n    layer_norm_cuda = LayerNorm(normalized_shape).cuda()\r\n    layer_norm_cpu = LayerNorm(normalized_shape).cpu()\r\n    \r\n    input_cuda = torch.randn(*sizes, device='cuda', requires_grad=True)\r\n    input_cpu = torch.randn(*sizes, device='cpu', requires_grad=True)\r\n\r\n    input_bytes = input_cuda.numel() * input_cuda.element_size()\r\n    print(""Element size"", input_cuda.element_size())\r\n    print(""Size of the input tensor is"", input_bytes, ""bytes"")\r\n\r\n    print('-' * LINE_WIDTH)\r\n\r\n    print(""cuda forward:"")\r\n    %timeit layer_norm_cuda(input_cuda); torch.cuda.synchronize()\r\n    print(""cpu forward:"")\r\n    %timeit layer_norm_cpu(input_cpu)\r\n\r\n    print('-' * LINE_WIDTH)\r\n    \r\n    out_cuda = layer_norm_cuda(input_cuda)\r\n    out_cpu = layer_norm_cpu(input_cpu)\r\n    upstream_grad_cuda = torch.randn_like(out_cuda)\r\n    upstream_grad_cpu = torch.randn_like(out_cpu)\r\n    \r\n    print('cuda backward, create_graph=False:')\r\n    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True); torch.cuda.synchronize()\r\n    gc.collect()\r\n    print('cpu backward, create_graph=False:')\r\n    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True)\r\n    gc.collect()\r\n\r\n    print('-' * LINE_WIDTH)\r\n    \r\n    print('cuda backward, create_graph=True:')\r\n    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True, create_graph=True); torch.cuda.synchronize()\r\n    gc.collect()\r\n    print('cpu backward, create_graph=True:')\r\n    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True, create_graph=True)\r\n    gc.collect()\r\n\r\n    print('=' * LINE_WIDTH)\r\n    print()\r\n\r\n\r\nbenchmark(100, 100)\r\nbenchmark(1000, 100)\r\nbenchmark(100, 500)\r\n```\r\nResult on torch-nightly installed by pip:\r\n```\r\nPyTorch version: 1.3.0.dev20190920\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 100)\r\nElement size 4\r\nSize of the input tensor is 40000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n146 \xb5s \xb1 16 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n55.1 \xb5s \xb1 5.51 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n392 \xb5s \xb1 66.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n104 \xb5s \xb1 15.7 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.01 ms \xb1 135 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n146 \xb5s \xb1 15.4 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (1000, 100)\r\nElement size 4\r\nSize of the input tensor is 400000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n116 \xb5s \xb1 29.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n191 \xb5s \xb1 383 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n210 \xb5s \xb1 7.03 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n298 \xb5s \xb1 1.24 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n735 \xb5s \xb1 53.4 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n341 \xb5s \xb1 1.73 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 500)\r\nElement size 4\r\nSize of the input tensor is 200000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n82 \xb5s \xb1 958 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n101 \xb5s \xb1 200 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n209 \xb5s \xb1 10.2 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=False:\r\n168 \xb5s \xb1 2.45 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n980 \xb5s \xb1 337 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n7.18 ms \xb1 1.84 ms per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n```\r\nResult for this PR:\r\n```\r\nPyTorch version: 1.3.0a0+77e6902\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 100)\r\nElement size 4\r\nSize of the input tensor is 40000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n33.7 \xb5s \xb1 97.6 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n26.3 \xb5s \xb1 66.8 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n112 \xb5s \xb1 3.06 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n57.7 \xb5s \xb1 1.5 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.15 ms \xb1 68 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n369 \xb5s \xb1 7.04 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (1000, 100)\r\nElement size 4\r\nSize of the input tensor is 400000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n37.4 \xb5s \xb1 95.1 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n136 \xb5s \xb1 49.3 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n116 \xb5s \xb1 1.83 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n201 \xb5s \xb1 7.78 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\nThe slowest run took 4.15 times longer than the fastest. This could mean that an intermediate result is being cached.\r\n1.84 ms \xb1 1.16 ms per loop (mean \xb1 std. dev. of 7 runs, 100 loops each)\r\ncpu backward, create_graph=True:\r\n592 \xb5s \xb1 27.1 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n\r\n================================================================================\r\nBenchmarking input shape (100, 500)\r\nElement size 4\r\nSize of the input tensor is 200000 bytes\r\n--------------------------------------------------------------------------------\r\ncuda forward:\r\n34.5 \xb5s \xb1 91.5 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu forward:\r\n70.4 \xb5s \xb1 25.2 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=False:\r\n118 \xb5s \xb1 5.52 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\ncpu backward, create_graph=False:\r\n131 \xb5s \xb1 7.7 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n--------------------------------------------------------------------------------\r\ncuda backward, create_graph=True:\r\n1.72 ms \xb1 125 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\ncpu backward, create_graph=True:\r\n471 \xb5s \xb1 45.1 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n================================================================================\r\n```","python\r\nimport torch\r\nfrom torch.nn import LayerNorm\r\nimport warnings\r\nimport gc\r\n\r\n\r\nLINE_WIDTH = 80\r\nwarnings.filterwarnings('ignore')\r\nprint('PyTorch version:', torch.__version__)\r\nprint()\r\n\r\n\r\ndef benchmark(*sizes):\r\n    print('=' * LINE_WIDTH)\r\n    print(""Benchmarking input shape"", sizes)\r\n    normalized_shape = sizes[1:]\r\n    layer_norm_cuda = LayerNorm(normalized_shape).cuda()\r\n    layer_norm_cpu = LayerNorm(normalized_shape).cpu()\r\n    \r\n    input_cuda = torch.randn(*sizes, device='cuda', requires_grad=True)\r\n    input_cpu = torch.randn(*sizes, device='cpu', requires_grad=True)\r\n\r\n    input_bytes = input_cuda.numel() * input_cuda.element_size()\r\n    print(""Element size"", input_cuda.element_size())\r\n    print(""Size of the input tensor is"", input_bytes, ""bytes"")\r\n\r\n    print('-' * LINE_WIDTH)\r\n\r\n    print(""cuda forward:"")\r\n    %timeit layer_norm_cuda(input_cuda); torch.cuda.synchronize()\r\n    print(""cpu forward:"")\r\n    %timeit layer_norm_cpu(input_cpu)\r\n\r\n    print('-' * LINE_WIDTH)\r\n    \r\n    out_cuda = layer_norm_cuda(input_cuda)\r\n    out_cpu = layer_norm_cpu(input_cpu)\r\n    upstream_grad_cuda = torch.randn_like(out_cuda)\r\n    upstream_grad_cpu = torch.randn_like(out_cpu)\r\n    \r\n    print('cuda backward, create_graph=False:')\r\n    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True); torch.cuda.synchronize()\r\n    gc.collect()\r\n    print('cpu backward, create_graph=False:')\r\n    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True)\r\n    gc.collect()\r\n\r\n    print('-' * LINE_WIDTH)\r\n    \r\n    print('cuda backward, create_graph=True:')\r\n    %timeit out_cuda.backward(upstream_grad_cuda, retain_graph=True, create_graph=True); torch.cuda.synchronize()\r\n    gc.collect()\r\n    print('cpu backward, create_graph=True:')\r\n    %timeit out_cpu.backward(upstream_grad_cpu, retain_graph=True, create_graph=True)\r\n    gc.collect()\r\n\r\n    print('=' * LINE_WIDTH)\r\n    print()\r\n\r\n\r\nbenchmark(100, 100)\r\nbenchmark(1000, 100)\r\nbenchmark(100, 500)\r\n"
26191,[ONNX] scatter support for broadcast## \U0001f41b Bug\r\n\r\nScatter `src` is not broadcasted\r\n\r\n> RuntimeError: Method run failed due to: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Node: rand_node_name_266 Status Message: Indices and updates must have the same rank\r\n\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nscatter `src` should be broadcasted\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nAm not sure if this is ORT or exporter or ONNX issue!\r\n,module: onnx|triaged,rgommers,"## \U0001f41b Bug\r\n\r\nScatter `src` is not broadcasted\r\n\r\n> RuntimeError: Method run failed due to: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Node: rand_node_name_266 Status Message: Indices and updates must have the same rank\r\n\r\n\r\n## To Reproduce\r\n\r\n```python\r\na = torch.as_tensor([-0.2, 1.0, 2.3])\r\nb = a + 1\r\nidx = torch.nonzero(a> 0).view(-1)\r\nb.scatter_(0, idx, 0)\r\n```\r\n\r\n## Expected behavior\r\n\r\nscatter `src` should be broadcasted\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nAm not sure if this is ORT or exporter or ONNX issue!\r\n","python\r\na = torch.as_tensor([-0.2, 1.0, 2.3])\r\nb = a + 1\r\nidx = torch.nonzero(a> 0).view(-1)\r\nb.scatter_(0, idx, 0)\r\n"
26067,"TorchScript doesn't handle submodule being None## \U0001f41b Bug\r\n\r\nIf one of the submodules is None, TorchScript doesn't recognize it as an attribute, unless explicitly specified in `__constants__` lists.\r\n\r\nObviously, if it's just a plain attribute, it might be necessary to additionally annotate it. But it doesn't work even if explicit `add_module` call is made.\r\n\r\n## To Reproduce\r\n\r\n\r\nError:\r\n```\r\nRuntimeError: \r\nmodule has no attribute 'sub':\r\nat <ipython-input-33-82f6eee81215>:12:11\r\n    def forward(self, x):\r\n        x = x.relu()\r\n        if self.sub is not None:\r\n           ~~~~~~~~ <--- HERE\r\n            x = self.sub(x)\r\n        return x+1\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Additional context\r\n\r\nIt's a pretty common pattern, e.g. in TorchVision: https://github.com/pytorch/vision/blob/a129b6b86a75f2d1fc80055f2cd0fae63efc0d2d/torchvision/models/resnet.py#L110\n\ncc @suo",oncall: jit|jit-backlog,driazati,"## \U0001f41b Bug\r\n\r\nIf one of the submodules is None, TorchScript doesn't recognize it as an attribute, unless explicitly specified in `__constants__` lists.\r\n\r\nObviously, if it's just a plain attribute, it might be necessary to additionally annotate it. But it doesn't work even if explicit `add_module` call is made.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nclass MyModule(torch.nn.Module):\r\n    # __constants__ = ['sub'] # adding just this line works\r\n    def __init__(self, sub):\r\n        super(MyModule, self).__init__()\r\n        # either of the following lines fails\r\n        self.add_module('sub', sub)\r\n        # or\r\n        self.sub = sub\r\n    \r\n    def forward(self, x):\r\n        x = x.relu()\r\n        if self.sub is not None:\r\n            x = self.sub(x)\r\n        return x+1\r\n\r\nm1 = MyModule(torch.nn.ReLU())\r\nm2 = MyModule(None)\r\nprint(m1(torch.rand(5)))\r\nprint(m2(torch.rand(5)))\r\nprint(torch.jit.script(m1).code) # succeeds\r\nprint(torch.jit.script(m2).code) # fails\r\n```\r\nError:\r\n```\r\nRuntimeError: \r\nmodule has no attribute 'sub':\r\nat <ipython-input-33-82f6eee81215>:12:11\r\n    def forward(self, x):\r\n        x = x.relu()\r\n        if self.sub is not None:\r\n           ~~~~~~~~ <--- HERE\r\n            x = self.sub(x)\r\n        return x+1\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Additional context\r\n\r\nIt's a pretty common pattern, e.g. in TorchVision: https://github.com/pytorch/vision/blob/a129b6b86a75f2d1fc80055f2cd0fae63efc0d2d/torchvision/models/resnet.py#L110\n\ncc @suo","python\r\nclass MyModule(torch.nn.Module):\r\n    # __constants__ = ['sub'] # adding just this line works\r\n    def __init__(self, sub):\r\n        super(MyModule, self).__init__()\r\n        # either of the following lines fails\r\n        self.add_module('sub', sub)\r\n        # or\r\n        self.sub = sub\r\n    \r\n    def forward(self, x):\r\n        x = x.relu()\r\n        if self.sub is not None:\r\n            x = self.sub(x)\r\n        return x+1\r\n\r\nm1 = MyModule(torch.nn.ReLU())\r\nm2 = MyModule(None)\r\nprint(m1(torch.rand(5)))\r\nprint(m2(torch.rand(5)))\r\nprint(torch.jit.script(m1).code) # succeeds\r\nprint(torch.jit.script(m2).code) # fails\r\n"
26036,"[jit] Bool list isn't being created in python -> script conversionWe should probably just delete `BoolList` entirely since it's not used\r\n\r\n\r\n\r\n```\r\nRuntimeError: isBoolList() INTERNAL ASSERT FAILED at ../aten/src/ATen/core/ivalue_inl.h:551, please report a bug to PyTorch. Expected BoolList but got GenericList\r\nThe above operation failed in interpreter, with the following stack trace:\r\nat <string>:3:4\r\ndef bool_list(x):\r\n    # type: (List[bool])\r\n    x.clear()\r\n    ~~~~~~~ <--- HERE\r\n```\r\n\r\ncc @suo",oncall: jit|triaged,smessmer,"We should probably just delete `BoolList` entirely since it's not used\r\n\r\n```python\r\n@torch.jit.script\r\ndef fn(x: List[bool]):\r\n    x.clear()\r\n\r\nfn([True, False])\r\n```\r\n\r\n```\r\nRuntimeError: isBoolList() INTERNAL ASSERT FAILED at ../aten/src/ATen/core/ivalue_inl.h:551, please report a bug to PyTorch. Expected BoolList but got GenericList\r\nThe above operation failed in interpreter, with the following stack trace:\r\nat <string>:3:4\r\ndef bool_list(x):\r\n    # type: (List[bool])\r\n    x.clear()\r\n    ~~~~~~~ <--- HERE\r\n```\r\n\r\ncc @suo","python\r\n@torch.jit.script\r\ndef fn(x: List[bool]):\r\n    x.clear()\r\n\r\nfn([True, False])\r\n"
25992,"[jit] builtin max doesn't handle lists of numbers## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nBuilt-in `max` in Python doesn't work with lists (or tuples) of numbers (both `int` and `float`).\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n\r\nGives\r\n```\r\nRuntimeError:\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n\r\n  prim::max(int a, int b) -> (int):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, float b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(int a, float b) -> (float):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, int b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\nThe original call is:\r\nat <ipython-input-9-ac85f2fa7d2c>:2:11\r\ndef f(a:int, b:int):\r\n    return max([a, b])\r\n           ~~~ <--- HERE\r\n```\r\n\r\nFunnily, using `min` works without a problem.\r\n\r\n\r\n## Environment\r\nPyTorch nightly from today.\r\n\r\n\r\n\n\ncc @suo",oncall: jit|triaged,Krovatkin,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nBuilt-in `max` in Python doesn't work with lists (or tuples) of numbers (both `int` and `float`).\r\n\r\n## To Reproduce\r\n\r\n\r\n```python\r\n@torch.jit.script\r\ndef f(a:int, b:int):\r\n    return max([a, b])\r\n```\r\n\r\nGives\r\n```\r\nRuntimeError:\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n\r\n  prim::max(int a, int b) -> (int):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, float b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(int a, float b) -> (float):\r\n  Expected a value of type 'int' for argument 'a' but instead found type 'List[int]'.\r\n\r\n  prim::max(float a, int b) -> (float):\r\n  Expected a value of type 'float' for argument 'a' but instead found type 'List[int]'.\r\n\r\nThe original call is:\r\nat <ipython-input-9-ac85f2fa7d2c>:2:11\r\ndef f(a:int, b:int):\r\n    return max([a, b])\r\n           ~~~ <--- HERE\r\n```\r\n\r\nFunnily, using `min` works without a problem.\r\n\r\n\r\n## Environment\r\nPyTorch nightly from today.\r\n\r\n```python\r\nIn [13]: torch.__version__\r\nOut[13]: '1.3.0.dev20190911'\r\n```\r\n\n\ncc @suo","python\r\n@torch.jit.script\r\ndef f(a:int, b:int):\r\n    return max([a, b])\r\n"
25874,"Python/C++ API Parity: tensor autograd API## \U0001f680 Feature\r\nWe would like to implement the following autograd APIs for C++ tensors, with appropriate tests:\r\n(left: Python API, right: C++ API)\r\n\r\n- [x] `torch.Tensor.detach` -> `at::Tensor::detach` (https://github.com/pytorch/pytorch/pull/26251)\r\n- [x] `torch.Tensor.detach_` -> `at::Tensor::detach_` (https://github.com/pytorch/pytorch/pull/26251)\r\n- [x] `torch.Tensor.backward` -> `at::Tensor::backward` (https://github.com/pytorch/pytorch/pull/26150)\r\n- [x] `torch.Tensor.set_data` -> `at::Tensor::set_data` (https://github.com/pytorch/pytorch/pull/26647)\r\n- [x] `torch.Tensor.data` -> `at::Tensor::data` (https://github.com/pytorch/pytorch/pull/26008)\r\n- [x] `torch.Tensor.grad` -> `at::Tensor::grad` (https://github.com/pytorch/pytorch/pull/26150)\r\n- [x] `torch.Tensor.grad_fn` -> `at::Tensor::grad_fn` (https://github.com/pytorch/pytorch/pull/28287)\r\n- [x] `torch.Tensor.output_nr` -> `at::Tensor::output_nr` (https://github.com/pytorch/pytorch/pull/26216)\r\n- [x] `torch.Tensor.is_leaf` -> `at::Tensor::is_leaf` (https://github.com/pytorch/pytorch/pull/26186)\r\n- [x] `torch.Tensor._version` -> `at::Tensor::_version` (https://github.com/pytorch/pytorch/pull/26561)\r\n- [x] `torch.Tensor.register_hook` -> `at::Tensor::register_hook` (https://github.com/pytorch/pytorch/pull/26568, https://github.com/pytorch/pytorch/pull/28287)\r\n  - Ideally, `register_hook` should return a `RemovableHandle` object, which we can call `.remove()` on, similar to the Python version https://github.com/pytorch/pytorch/blob/3b43cfde80d828b39abfd988eb64efcbdb5b61a1/torch/tensor.py#L197-L235\r\n- [x] `torch.Tensor.requires_grad_` -> `at::Tensor::requires_grad_` (https://github.com/pytorch/pytorch/pull/26332)\r\n- [x] `torch.Tensor.retain_grad` -> `at::Tensor::retain_grad` (https://github.com/pytorch/pytorch/pull/33349)\r\n- [x] `torch.Tensor._base` -> `at::Tensor::_base` (https://github.com/pytorch/pytorch/pull/33316)\r\n\r\nThe current situation is that all these APIs exist in `torch::autograd::Variable` class but not `at::Tensor` class, and the user needs to explicitly cast the C++ tensor to a `torch::autograd::Variable` object in order to use those APIs. What we want to do is to add those APIs in `at::Tensor`, and have the VariableType dispatcher route those API calls to the `torch::autograd::Variable` class internally, so that the user doesn't need to do the explicit casting.\r\n\r\nEssentially, we want to following to work (using `data()` as an example):\r\n\r\n\r\nBut currently, only this works:\r\n\r\n\r\nNote that some of the APIs (e.g. `backward`/ `set_data` / `grad`) might already exist in `at::Tensor`, but they might still not have appropriate tests in `test/cpp/api/autograd.cpp`. To make sure things work, we would want to add tests for all of the APIs above.\r\n\r\n## Motivation\r\nAs part of the Python / C++ API parity work, we would like to provide an identical set of Tensor autograd APIs in C++ as we do in Python. This can address user complaints such as:\r\n- https://github.com/pytorch/pytorch/issues/21899\r\n- https://github.com/yf225/pytorch-cpp-issue-tracker/issues/351\r\n- https://github.com/yf225/pytorch-cpp-issue-tracker/issues/194\r\n\r\n## Possible Steps\r\n1. Add entries for the Tensor autograd APIs in `native_functions.yaml` and `Tensor.h`\r\n2. Provide stubs in `aten/src/ATen/native/VariableMethodStubs.cpp` to throw error when the the new API is called on a non-Variable\r\n3. Provide implementation for the new API in `torch/csrc/autograd/VariableTypeManual.cpp`, which should just be calling `as_variable_ref(self).some_api_name(...)` which calls into the Variable methods. (Note that you would be able to find current Python tensor autograd API bindings in `torch/csrc/autograd/python_variable.cpp` (e.g. `THPVariable_get_data`), which may help with creating the right implementation in `VariableTypeManual.cpp`)\r\n\r\n`backward()` and `set_data()` are good examples of how the dispatching logic should work, and looking into their implementations is highly recommended.\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @yf225",module: cpp|module: autograd|triaged,yf225|glaringlee,"## \U0001f680 Feature\r\nWe would like to implement the following autograd APIs for C++ tensors, with appropriate tests:\r\n(left: Python API, right: C++ API)\r\n\r\n- [x] `torch.Tensor.detach` -> `at::Tensor::detach` (https://github.com/pytorch/pytorch/pull/26251)\r\n- [x] `torch.Tensor.detach_` -> `at::Tensor::detach_` (https://github.com/pytorch/pytorch/pull/26251)\r\n- [x] `torch.Tensor.backward` -> `at::Tensor::backward` (https://github.com/pytorch/pytorch/pull/26150)\r\n- [x] `torch.Tensor.set_data` -> `at::Tensor::set_data` (https://github.com/pytorch/pytorch/pull/26647)\r\n- [x] `torch.Tensor.data` -> `at::Tensor::data` (https://github.com/pytorch/pytorch/pull/26008)\r\n- [x] `torch.Tensor.grad` -> `at::Tensor::grad` (https://github.com/pytorch/pytorch/pull/26150)\r\n- [x] `torch.Tensor.grad_fn` -> `at::Tensor::grad_fn` (https://github.com/pytorch/pytorch/pull/28287)\r\n- [x] `torch.Tensor.output_nr` -> `at::Tensor::output_nr` (https://github.com/pytorch/pytorch/pull/26216)\r\n- [x] `torch.Tensor.is_leaf` -> `at::Tensor::is_leaf` (https://github.com/pytorch/pytorch/pull/26186)\r\n- [x] `torch.Tensor._version` -> `at::Tensor::_version` (https://github.com/pytorch/pytorch/pull/26561)\r\n- [x] `torch.Tensor.register_hook` -> `at::Tensor::register_hook` (https://github.com/pytorch/pytorch/pull/26568, https://github.com/pytorch/pytorch/pull/28287)\r\n  - Ideally, `register_hook` should return a `RemovableHandle` object, which we can call `.remove()` on, similar to the Python version https://github.com/pytorch/pytorch/blob/3b43cfde80d828b39abfd988eb64efcbdb5b61a1/torch/tensor.py#L197-L235\r\n- [x] `torch.Tensor.requires_grad_` -> `at::Tensor::requires_grad_` (https://github.com/pytorch/pytorch/pull/26332)\r\n- [x] `torch.Tensor.retain_grad` -> `at::Tensor::retain_grad` (https://github.com/pytorch/pytorch/pull/33349)\r\n- [x] `torch.Tensor._base` -> `at::Tensor::_base` (https://github.com/pytorch/pytorch/pull/33316)\r\n\r\nThe current situation is that all these APIs exist in `torch::autograd::Variable` class but not `at::Tensor` class, and the user needs to explicitly cast the C++ tensor to a `torch::autograd::Variable` object in order to use those APIs. What we want to do is to add those APIs in `at::Tensor`, and have the VariableType dispatcher route those API calls to the `torch::autograd::Variable` class internally, so that the user doesn't need to do the explicit casting.\r\n\r\nEssentially, we want to following to work (using `data()` as an example):\r\n```cpp\r\n#include <torch/torch.h>\r\n\r\nint main()\r\n{\r\n  torch::Tensor t = torch::randn({3, 4});\r\n  torch::Tensor t2 = t.data();\r\n  return 0;\r\n}\r\n```\r\n\r\nBut currently, only this works:\r\n```cpp\r\n#include <torch/torch.h>\r\n#include <torch/csrc/autograd/variable.h>\r\n\r\nint main()\r\n{\r\n  torch::Tensor t = torch::randn({3, 4});\r\n  torch::Tensor t2 = torch::autograd::as_variable_ref(t).variable_data();\r\n  return 0;\r\n}\r\n```\r\n\r\nNote that some of the APIs (e.g. `backward`/ `set_data` / `grad`) might already exist in `at::Tensor`, but they might still not have appropriate tests in `test/cpp/api/autograd.cpp`. To make sure things work, we would want to add tests for all of the APIs above.\r\n\r\n## Motivation\r\nAs part of the Python / C++ API parity work, we would like to provide an identical set of Tensor autograd APIs in C++ as we do in Python. This can address user complaints such as:\r\n- https://github.com/pytorch/pytorch/issues/21899\r\n- https://github.com/yf225/pytorch-cpp-issue-tracker/issues/351\r\n- https://github.com/yf225/pytorch-cpp-issue-tracker/issues/194\r\n\r\n## Possible Steps\r\n1. Add entries for the Tensor autograd APIs in `native_functions.yaml` and `Tensor.h`\r\n2. Provide stubs in `aten/src/ATen/native/VariableMethodStubs.cpp` to throw error when the the new API is called on a non-Variable\r\n3. Provide implementation for the new API in `torch/csrc/autograd/VariableTypeManual.cpp`, which should just be calling `as_variable_ref(self).some_api_name(...)` which calls into the Variable methods. (Note that you would be able to find current Python tensor autograd API bindings in `torch/csrc/autograd/python_variable.cpp` (e.g. `THPVariable_get_data`), which may help with creating the right implementation in `VariableTypeManual.cpp`)\r\n\r\n`backward()` and `set_data()` are good examples of how the dispatching logic should work, and looking into their implementations is highly recommended.\r\n\r\ncc @ezyang @SsnL @albanD @zou3519 @yf225","cpp\r\n#include <torch/torch.h>\r\n\r\nint main()\r\n{\r\n  torch::Tensor t = torch::randn({3, 4});\r\n  torch::Tensor t2 = t.data();\r\n  return 0;\r\n}\r\n"
25831,Add c++ api support for new_zeros and new_empty.Something like torch::Tensor::new_zeros is useful when using with libtorch's c++ frontend.\r\n\r\nBut now there's only an api for python.\r\n\n\ncc @yf225,module: cpp|triaged|small,v0dro,"Something like torch::Tensor::new_zeros is useful when using with libtorch's c++ frontend.\r\n\r\nBut now there's only an api for python.\r\n```c++\r\nTensor new_zeros(c10::TensorTypeId type_id, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs)\r\n```\n\ncc @yf225","c++\r\nTensor new_zeros(c10::TensorTypeId type_id, at::ScalarType scalar_type, PyObject* args, PyObject* kwargs)\r\n"
25804,"[jit] String default args get printed as ascii valuese.g. passing the wrong types to `interpolate`\r\n\r\n```\r\n  aten::__interpolate(Tensor input, int? size=None, float[]? scale_factor=None, str\r\n mode='\\156\\145\\141\\162\\145\\163\\164', bool? align_corners=None) -> (Tensor):       \r\n  Expected a value of type 'Optional[int]' for argument 'size' but instead found ty\r\npe 'Tensor'.                                                                       \r\n```\r\n\r\ncc @suo",oncall: jit|good first issue|jit-backlog,Krovatkin,"e.g. passing the wrong types to `interpolate`\r\n```python\r\n@torch.jit.script\r\ndef bad_error(x):\r\n    return torch.nn.functional.interpolate(x, 'bad')\r\n```\r\n```\r\n  aten::__interpolate(Tensor input, int? size=None, float[]? scale_factor=None, str\r\n mode='\\156\\145\\141\\162\\145\\163\\164', bool? align_corners=None) -> (Tensor):       \r\n  Expected a value of type 'Optional[int]' for argument 'size' but instead found ty\r\npe 'Tensor'.                                                                       \r\n```\r\n\r\ncc @suo","python\r\n@torch.jit.script\r\ndef bad_error(x):\r\n    return torch.nn.functional.interpolate(x, 'bad')\r\n"
25758,"[jit] Implement `in` for list and tuple## \U0001f41b Bug\r\nThe *in* operator is bond to Dict when I tried to use it on List or Tuple typle.  I can bypass this with a for loop. But I think this one still should be reported.\r\n\r\n\r\n## To Reproduce\r\nuncomment to see the bug\r\n\r\n## Expected behavior\r\n```\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::__contains__(Dict(str, t) dict, str key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[str, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(int, t) dict, int key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[int, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(float, t) dict, float key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[float, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(Tensor, t) dict, Tensor key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[Tensor, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n\r\nThe original call is:\r\nat slice.py:20:7\r\n@torch.jit.script\r\ndef foo2(v:Tuple[int], k: int):\r\n    if k in v:\r\n       ~~~~~~ <--- HERE\r\n        return True\r\n    else:\r\n        return False\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.3.0.dev20190905\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190905\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\n\ncc @suo",oncall: jit|triaged|jit-backlog,wconstab,"## \U0001f41b Bug\r\nThe *in* operator is bond to Dict when I tried to use it on List or Tuple typle.  I can bypass this with a for loop. But I think this one still should be reported.\r\n\r\n\r\n## To Reproduce\r\nuncomment to see the bug\r\n```python\r\nimport torch\r\nfrom typing import Dict, Tuple, List\r\n\r\n@torch.jit.script\r\ndef foo(v: Dict[int, int], k: int):\r\n    if k in v:\r\n        return True\r\n    else:\r\n        return False\r\n\r\n# @torch.jit.script\r\n# def foo1(v:List[int], k: int):\r\n#     if k in v:\r\n#         return True\r\n#     else:\r\n#         return False\r\n\r\n# @torch.jit.script\r\n# def foo2(v:Tuple[int], k: int):\r\n#     if k in v:\r\n#         return True\r\n#     else:\r\n#         return False\r\n\r\n\r\ndef python(v, k):\r\n    if k in v:\r\n        return True\r\n    else:\r\n        return False\r\n\r\nif __name__ == ""__main__"":\r\n    a = [1,2,3,4,5,6]\r\n    b = 3\r\n    print(python(a,b))\r\n```\r\n## Expected behavior\r\n```\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::__contains__(Dict(str, t) dict, str key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[str, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(int, t) dict, int key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[int, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(float, t) dict, float key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[float, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n  \r\n  aten::__contains__(Dict(Tensor, t) dict, Tensor key) -> (bool):\r\n  Could not match type Tuple[int] to Dict[Tensor, t] in argument 'dict': Cannot match a dict to Tuple[int].\r\n\r\nThe original call is:\r\nat slice.py:20:7\r\n@torch.jit.script\r\ndef foo2(v:Tuple[int], k: int):\r\n    if k in v:\r\n       ~~~~~~ <--- HERE\r\n        return True\r\n    else:\r\n        return False\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.3.0.dev20190905\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190905\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\n\ncc @suo","python\r\nimport torch\r\nfrom typing import Dict, Tuple, List\r\n\r\n@torch.jit.script\r\ndef foo(v: Dict[int, int], k: int):\r\n    if k in v:\r\n        return True\r\n    else:\r\n        return False\r\n\r\n# @torch.jit.script\r\n# def foo1(v:List[int], k: int):\r\n#     if k in v:\r\n#         return True\r\n#     else:\r\n#         return False\r\n\r\n# @torch.jit.script\r\n# def foo2(v:Tuple[int], k: int):\r\n#     if k in v:\r\n#         return True\r\n#     else:\r\n#         return False\r\n\r\n\r\ndef python(v, k):\r\n    if k in v:\r\n        return True\r\n    else:\r\n        return False\r\n\r\nif __name__ == ""__main__"":\r\n    a = [1,2,3,4,5,6]\r\n    b = 3\r\n    print(python(a,b))\r\n"
25682,"JIT tracing class wrapped variable fails## \U0001f41b Bug\r\nIf a tensor is wrapped in class, the tracing fails.\r\n## To Reproduce\r\nRun the script.\r\n## Demo code\r\n\r\n## Expected behavior\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\nCollecting environment information...\r\nPyTorch version: 1.3.0.dev20190902\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190902\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\ncc @suo",oncall: jit|triaged,wanchaol,"## \U0001f41b Bug\r\nIf a tensor is wrapped in class, the tracing fails.\r\n## To Reproduce\r\nRun the script.\r\n## Demo code\r\n```python\r\nimport torch\r\n\r\nclass Module1(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Module1, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x, value:int):\r\n        return x + value\r\n\r\n@torch.jit.script\r\nclass Wrapper(object):\r\n    def __init__(self, value):\r\n        self.value = value\r\n\r\nclass Module2(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Module2, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x: Wrapper, value:int):\r\n        return x.value + value\r\n\r\n\r\n# from demo_class import make_module1\r\nclass Module3(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Module3, self).__init__()\r\n        self.module1 = Module1()\r\n        self.module2 = Module2()\r\n\r\n    def forward(self, x):\r\n        #  tmp = self.module1(x,3)\r\n        x_wrapped = Wrapper(x)\r\n        tmp = self.module2(x_wrapped,3)\r\n        y = tmp+1\r\n        return y\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    module = Module3()\r\n    print(module)\r\n    dummy = torch.zeros(1,3,200,200)\r\n    torch.jit.trace(module, dummy)\r\n```\r\n## Expected behavior\r\n```bash\r\nModule3(\r\n  (module1): Module1()\r\n  (module2): Module2()\r\n)\r\n# ...\r\n\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: Tried to trace <__torch__.Wrapper object at 0x371a810> but it is not part of the active trace. Modules that are called during a trace must be registered as submodules of the thing being traced.\r\n```\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\nCollecting environment information...\r\nPyTorch version: 1.3.0.dev20190902\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190902\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\ncc @suo","python\r\nimport torch\r\n\r\nclass Module1(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Module1, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x, value:int):\r\n        return x + value\r\n\r\n@torch.jit.script\r\nclass Wrapper(object):\r\n    def __init__(self, value):\r\n        self.value = value\r\n\r\nclass Module2(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Module2, self).__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x: Wrapper, value:int):\r\n        return x.value + value\r\n\r\n\r\n# from demo_class import make_module1\r\nclass Module3(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Module3, self).__init__()\r\n        self.module1 = Module1()\r\n        self.module2 = Module2()\r\n\r\n    def forward(self, x):\r\n        #  tmp = self.module1(x,3)\r\n        x_wrapped = Wrapper(x)\r\n        tmp = self.module2(x_wrapped,3)\r\n        y = tmp+1\r\n        return y\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    module = Module3()\r\n    print(module)\r\n    dummy = torch.zeros(1,3,200,200)\r\n    torch.jit.trace(module, dummy)\r\n"
25644,"""Using PyTorch C++ Frontend"" TorchModule generator architecture is erroneous## \U0001f4da Documentation\r\n\r\nI followed the DCGAN tutorial on the page [""Using the PyTorch C++ Frontend""](https://pytorch.org/tutorials/advanced/cpp_frontend.html).\r\n\r\nInstead of using the sequential version of the generator I used the TorchModule one, and I realized that the two do not match. Additionally, the TorchModule version of the generator does not work with the discriminator.\r\n\r\nHere is the sequential version:\r\n\r\n\r\nThe erroneous TorchModule version:\r\n\r\n\r\nAnd the fixed version I used:\r\n```cpp\r\nstruct GeneratorImpl : nn::Module {\r\n  GeneratorImpl(int kNoiseSize)\r\n      : conv1(nn::Conv2dOptions(kNoiseSize, 256, 4)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm1(256),\r\n        conv2(nn::Conv2dOptions(256, 128, 3)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm2(128),\r\n        conv3(nn::Conv2dOptions(128, 64, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm3(64),\r\n        conv4(nn::Conv2dOptions(64, 1, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)) \r\n  {\r\n    // register_module() is needed if we want to use the parameters() method later on\r\n    register_module(""conv1"", conv1);\r\n    register_module(""conv2"", conv2);\r\n    register_module(""conv3"", conv3);\r\n    register_module(""conv4"", conv4);\r\n    register_module(""batch_norm1"", batch_norm1);\r\n    register_module(""batch_norm2"", batch_norm1);\r\n    register_module(""batch_norm3"", batch_norm1);\r\n  }\r\n\r\n  torch::Tensor forward(torch::Tensor x) {\r\n    x = torch::relu(batch_norm1(conv1(x)));\r\n    x = torch::relu(batch_norm2(conv2(x)));\r\n    x = torch::relu(batch_norm3(conv3(x)));\r\n    x = torch::tanh(conv4(x));\r\n    return x;\r\n  }\r\n\r\n  nn::Conv2d conv1, conv2, conv3, conv4;\r\n  nn::BatchNorm batch_norm1, batch_norm2, batch_norm3;\r\n};\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225",module: cpp|triaged,yf225,"## \U0001f4da Documentation\r\n\r\nI followed the DCGAN tutorial on the page [""Using the PyTorch C++ Frontend""](https://pytorch.org/tutorials/advanced/cpp_frontend.html).\r\n\r\nInstead of using the sequential version of the generator I used the TorchModule one, and I realized that the two do not match. Additionally, the TorchModule version of the generator does not work with the discriminator.\r\n\r\nHere is the sequential version:\r\n```cpp\r\nnn::Sequential generator(\r\n    // Layer 1\r\n    nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(256),\r\n    nn::Functional(torch::relu),\r\n    // Layer 2\r\n    nn::Conv2d(nn::Conv2dOptions(256, 128, 3)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(128),\r\n    nn::Functional(torch::relu),\r\n    // Layer 3\r\n    nn::Conv2d(nn::Conv2dOptions(128, 64, 4)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(64),\r\n    nn::Functional(torch::relu),\r\n    // Layer 4\r\n    nn::Conv2d(nn::Conv2dOptions(64, 1, 4)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::Functional(torch::tanh));\r\n```\r\n\r\nThe erroneous TorchModule version:\r\n```cpp\r\nstruct GeneratorImpl : nn::Module {\r\n  GeneratorImpl()\r\n      : conv1(nn::Conv2dOptions(kNoiseSize, 512, 4)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm1(512),\r\n        conv2(nn::Conv2dOptions(512, 256, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm2(256),\r\n        conv3(nn::Conv2dOptions(256, 128, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm3(128),\r\n        conv4(nn::Conv2dOptions(128, 64, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm4(64),\r\n        conv5(nn::Conv2dOptions(64, 1, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)) {}\r\n\r\n  torch::Tensor forward(torch::Tensor x) {\r\n    x = torch::relu(batch_norm1(conv1(x)));\r\n    x = torch::relu(batch_norm2(conv2(x)));\r\n    x = torch::relu(batch_norm3(conv3(x)));\r\n    x = torch::relu(batch_norm4(conv4(x)));\r\n    x = torch::tanh(conv5(x));\r\n    return x;\r\n  }\r\n\r\n  nn::Conv2d conv1, conv2, conv3, conv4, conv5;\r\n  nn::BatchNorm batch_norm1, batch_norm2, batch_norm3, batch_norm4;\r\n};\r\n```\r\n\r\nAnd the fixed version I used:\r\n```cpp\r\nstruct GeneratorImpl : nn::Module {\r\n  GeneratorImpl(int kNoiseSize)\r\n      : conv1(nn::Conv2dOptions(kNoiseSize, 256, 4)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm1(256),\r\n        conv2(nn::Conv2dOptions(256, 128, 3)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm2(128),\r\n        conv3(nn::Conv2dOptions(128, 64, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)),\r\n        batch_norm3(64),\r\n        conv4(nn::Conv2dOptions(64, 1, 4)\r\n                  .stride(2)\r\n                  .padding(1)\r\n                  .with_bias(false)\r\n                  .transposed(true)) \r\n  {\r\n    // register_module() is needed if we want to use the parameters() method later on\r\n    register_module(""conv1"", conv1);\r\n    register_module(""conv2"", conv2);\r\n    register_module(""conv3"", conv3);\r\n    register_module(""conv4"", conv4);\r\n    register_module(""batch_norm1"", batch_norm1);\r\n    register_module(""batch_norm2"", batch_norm1);\r\n    register_module(""batch_norm3"", batch_norm1);\r\n  }\r\n\r\n  torch::Tensor forward(torch::Tensor x) {\r\n    x = torch::relu(batch_norm1(conv1(x)));\r\n    x = torch::relu(batch_norm2(conv2(x)));\r\n    x = torch::relu(batch_norm3(conv3(x)));\r\n    x = torch::tanh(conv4(x));\r\n    return x;\r\n  }\r\n\r\n  nn::Conv2d conv1, conv2, conv3, conv4;\r\n  nn::BatchNorm batch_norm1, batch_norm2, batch_norm3;\r\n};\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225","cpp\r\nnn::Sequential generator(\r\n    // Layer 1\r\n    nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(256),\r\n    nn::Functional(torch::relu),\r\n    // Layer 2\r\n    nn::Conv2d(nn::Conv2dOptions(256, 128, 3)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(128),\r\n    nn::Functional(torch::relu),\r\n    // Layer 3\r\n    nn::Conv2d(nn::Conv2dOptions(128, 64, 4)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::BatchNorm(64),\r\n    nn::Functional(torch::relu),\r\n    // Layer 4\r\n    nn::Conv2d(nn::Conv2dOptions(64, 1, 4)\r\n                   .stride(2)\r\n                   .padding(1)\r\n                   .with_bias(false)\r\n                   .transposed(true)),\r\n    nn::Functional(torch::tanh));\r\n"
25637,"Unable to  subscirpt self-defined class in torchscript function## \U0001f41b Bug\r\nUnable to call \\_\\_getitem\\_\\_ of self defined class in torchscript function\r\n## To Reproduce\r\n\r\n\r\n## Expected behavior\r\n```\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\n'__torch__.Foo' object is not subscriptable:\r\nat demo_class.py:14:11\r\n@torch.jit.script\r\ndef bar(v: Foo, index: torch.Tensor):\r\n    return v[index]\r\n           ~~~~~~~ <--- HERE\r\n```\r\n## Environment\r\nPyTorch version: 1.3.0.dev20190902\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190902\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\ncc @suo",oncall: jit|triaged,eellison,"## \U0001f41b Bug\r\nUnable to call \\_\\_getitem\\_\\_ of self defined class in torchscript function\r\n## To Reproduce\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass Foo(object):\r\n    def __init__(self, value: torch.Tensor):\r\n        self.value = value\r\n\r\n    def __getitem__(self, item: torch.Tensor):\r\n        updated_value = self.value[item]\r\n        return Foo(updated_value)\r\n\r\n@torch.jit.script \r\ndef bar(v: Foo, index: torch.Tensor):\r\n    return v[index]\r\n\r\nif __name__ == ""__main__"":\r\n    a = torch.tensor([0,1,2,3,4,5])\r\n    slice_index = torch.tensor([0,2,5])\r\n    b = Foo(a)\r\n    b_slice = b[slice_index]  # this one works\r\n    print(b_slice.value)\r\n    print(bar(b, slice_index).value) # this one does not work with @torch.jit.script\r\n```\r\n\r\n## Expected behavior\r\n```\r\n    fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\n'__torch__.Foo' object is not subscriptable:\r\nat demo_class.py:14:11\r\n@torch.jit.script\r\ndef bar(v: Foo, index: torch.Tensor):\r\n    return v[index]\r\n           ~~~~~~~ <--- HERE\r\n```\r\n## Environment\r\nPyTorch version: 1.3.0.dev20190902\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.3.0.dev20190902\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\ncc @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass Foo(object):\r\n    def __init__(self, value: torch.Tensor):\r\n        self.value = value\r\n\r\n    def __getitem__(self, item: torch.Tensor):\r\n        updated_value = self.value[item]\r\n        return Foo(updated_value)\r\n\r\n@torch.jit.script \r\ndef bar(v: Foo, index: torch.Tensor):\r\n    return v[index]\r\n\r\nif __name__ == ""__main__"":\r\n    a = torch.tensor([0,1,2,3,4,5])\r\n    slice_index = torch.tensor([0,2,5])\r\n    b = Foo(a)\r\n    b_slice = b[slice_index]  # this one works\r\n    print(b_slice.value)\r\n    print(bar(b, slice_index).value) # this one does not work with @torch.jit.script\r\n"
25605,"Reference cycle in _LRScheduler## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nIn the following code introduced in version 1.2.0 by #20124, when constructing the `_LRScheduler`, the `step` method of `optimizer` is replaced with a new closure that captures the `optimizer` itself.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/9cb9f15989b08d71fcf0ad0ddda84299c0854c2d/torch/optim/lr_scheduler.py#L29-L40\r\n\r\nThis makes the optimizer reference itself, thus creating a reference cycle that prevents it from being garbage collected.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nTo verify this, run the following snippet:\r\n\r\n\r\nThe output is:\r\n```\r\n[<bound method Adam.step of Adam (\r\nParameter Group 0\r\n    amsgrad: False\r\n    betas: (0.9, 0.999)\r\n    eps: 1e-08\r\n    initial_lr: 0.001\r\n    lr: 0.001\r\n    weight_decay: 0\r\n)>, <cell at 0x109354948: Adam object at 0x10940d8d0>]\r\n12\r\n```\r\nwhich shows that `optim.step` is a referrer of `optim`, and it's not collected automatically.\r\n\r\nIf the `scheduler` is not created, the output becomes:\r\n```\r\n[]\r\n0\r\n```\r\nwhich shows that no referrers of `optim` exist and it's GC-ed properly.\r\n\r\n## Expected behavior\r\n\r\nThe self-reference could be easily prevented by using the `self` argument and wrapping the created function with `types.MethodType(wrapper, opt)`.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.2.0\r\n - Other information probably irrelevant.\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @vincentqb",high priority|module: optimizer|triaged,vincentqb,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nIn the following code introduced in version 1.2.0 by #20124, when constructing the `_LRScheduler`, the `step` method of `optimizer` is replaced with a new closure that captures the `optimizer` itself.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/9cb9f15989b08d71fcf0ad0ddda84299c0854c2d/torch/optim/lr_scheduler.py#L29-L40\r\n\r\nThis makes the optimizer reference itself, thus creating a reference cycle that prevents it from being garbage collected.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nTo verify this, run the following snippet:\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nimport gc\r\n\r\ndef main():\r\n    param = nn.Parameter(torch.randn(10))\r\n\r\n    optim = torch.optim.Adam([param])\r\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lambda epoch: 1.0)\r\n    del scheduler\r\n\r\n    print(gc.get_referrers(optim))\r\n    \r\n    gc.collect()\r\n    del optim\r\n    print(gc.collect())\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThe output is:\r\n```\r\n[<bound method Adam.step of Adam (\r\nParameter Group 0\r\n    amsgrad: False\r\n    betas: (0.9, 0.999)\r\n    eps: 1e-08\r\n    initial_lr: 0.001\r\n    lr: 0.001\r\n    weight_decay: 0\r\n)>, <cell at 0x109354948: Adam object at 0x10940d8d0>]\r\n12\r\n```\r\nwhich shows that `optim.step` is a referrer of `optim`, and it's not collected automatically.\r\n\r\nIf the `scheduler` is not created, the output becomes:\r\n```\r\n[]\r\n0\r\n```\r\nwhich shows that no referrers of `optim` exist and it's GC-ed properly.\r\n\r\n## Expected behavior\r\n\r\nThe self-reference could be easily prevented by using the `self` argument and wrapping the created function with `types.MethodType(wrapper, opt)`.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.2.0\r\n - Other information probably irrelevant.\r\n\r\n\n\ncc @ezyang @gchanan @zou3519 @vincentqb","python\r\nimport torch\r\nfrom torch import nn\r\n\r\nimport gc\r\n\r\ndef main():\r\n    param = nn.Parameter(torch.randn(10))\r\n\r\n    optim = torch.optim.Adam([param])\r\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lambda epoch: 1.0)\r\n    del scheduler\r\n\r\n    print(gc.get_referrers(optim))\r\n    \r\n    gc.collect()\r\n    del optim\r\n    print(gc.collect())\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"
25573,"assignment bug of advanced indexing## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe output should be:\r\n\r\n\r\n\r\nwhile got:\r\n\r\n\r\nI suspect that `a[8]` was assigned with 7 first and then its value was indexed and assigned to `a[9]`.\r\n\r\nThe behavior of `numpy` lib is correct:\r\n\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): Mac OSX 10.13.3\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: No CUDA\r\n - GPU models and configuration: No CUDA\r\n - Any other relevant information:\r\n",triaged|module: advanced indexing|module: partial aliasing,pbelevich,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\n\r\na = torch.arange(10)\r\nprint(a)\r\na[1:] = a[:9]\r\nprint(a)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe output should be:\r\n\r\n```python\r\n\r\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\ntensor([0, 0, 1, 2, 3, 4, 5, 6, 7, 8])\r\n```\r\n\r\nwhile got:\r\n\r\n```python\r\n\r\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\ntensor([0, 0, 1, 2, 3, 4, 5, 6, 7, 7])\r\n\r\n```\r\nI suspect that `a[8]` was assigned with 7 first and then its value was indexed and assigned to `a[9]`.\r\n\r\nThe behavior of `numpy` lib is correct:\r\n\r\n```python\r\n\r\nimport numpy as np\r\na = np.arange(10)\r\nprint(a) # [0 1 2 3 4 5 6 7 8 9]\r\na[1:] = a[:9]\r\nprint(a) # [0 0 1 2 3 4 5 6 7 8]\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): Mac OSX 10.13.3\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: No CUDA\r\n - GPU models and configuration: No CUDA\r\n - Any other relevant information:\r\n",python\r\n\r\na = torch.arange(10)\r\nprint(a)\r\na[1:] = a[:9]\r\nprint(a)\r\n
25547,"nonexistent attribute __class__ when defining class with jit## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n```\r\nRuntimeError: \r\nTried to access nonexistent attribute __class__. Did you forget to initialize it in __init__()?:\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.2.0\r\n[pip3] torch-nightly==1.2.0.dev20190530\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collecttion:\r\n\r\n## Additional context\r\n\r\n\n\ncc @suo",oncall: jit|triaged|jit-backlog,suo,"## \U0001f41b Bug\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass Foo(object):\r\n    def __init__(self):\r\n        self.tmp = 1\r\n    def __repr__(self):\r\n        s = self.__class__.__name__ +  ""tmp={}"".format(self.tmp)\r\n        return\r\n\r\nif __name__ == ""__main__"":\r\n    foo = Foo()\r\n    print(foo)\r\n```\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n```\r\nRuntimeError: \r\nTried to access nonexistent attribute __class__. Did you forget to initialize it in __init__()?:\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.2.0\r\n[pip3] torch-nightly==1.2.0.dev20190530\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collecttion:\r\n\r\n## Additional context\r\n\r\n\n\ncc @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass Foo(object):\r\n    def __init__(self):\r\n        self.tmp = 1\r\n    def __repr__(self):\r\n        s = self.__class__.__name__ +  ""tmp={}"".format(self.tmp)\r\n        return\r\n\r\nif __name__ == ""__main__"":\r\n    foo = Foo()\r\n    print(foo)\r\n"
25498,"[jit] torch.jit.script range() input type raises isInt() when used with int tensor values## \U0001f41b Bug\r\n\r\nI have a `Tensor[int]` passed to `torch.jit.script` when I do\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun this script:\r\n\r\n\r\n\r\n```text\r\n\r\n>RuntimeError: isInt() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:187, please report a bug to PyTorch.                                                                                                                                                       > The above operation failed in interpreter, with the following stack trace:\r\n> for sg in range(size):\r\n> ~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n```\r\n\r\nEven if I change it to \r\n\r\n\r\n\r\nI get the same assertion about `isInt`\r\n\r\n## Expected behavior\r\n\r\n`range()` to accept input from a tensor of int\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nI am going to replace that with a while loop and see if that succeeds \r\n\n\ncc @suo",oncall: jit|triaged,wanchaol,"## \U0001f41b Bug\r\n\r\nI have a `Tensor[int]` passed to `torch.jit.script` when I do\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun this script:\r\n\r\n```python\r\n@torch.jit.script\r\ndef smt_pred(confs, child, child_sizes, group_offsets,\r\n             threshold, obj,\r\n             height, width, b, a):\r\n    # type: (List[Tensor],List[Tensor],List[Tensor],List[int],Tensor,Tensor,int,int,int,int) -> Tuple[Tensor,Tensor,Tensor]\r\n    size = child_sizes[0][1]\r\n    ch = child[0][1]\r\n    for sg in range(size):\r\n        pass\r\n \r\n```\r\n\r\n```text\r\n\r\n>RuntimeError: isInt() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:187, please report a bug to PyTorch.                                                                                                                                                       > The above operation failed in interpreter, with the following stack trace:\r\n> for sg in range(size):\r\n> ~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE\r\n```\r\n\r\nEven if I change it to \r\n\r\n```python\r\nfor sg in range(size.int()):\r\n    pass\r\n```\r\n\r\nI get the same assertion about `isInt`\r\n\r\n## Expected behavior\r\n\r\n`range()` to accept input from a tensor of int\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.14.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 418.40.04\r\ncuDNN version: /usr/local/lib/libcudnn.so.5.1.10\r\n\r\nVersions of relevant libraries:\r\n[pip] mictorch==0.0.1\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nI am going to replace that with a while loop and see if that succeeds \r\n\n\ncc @suo","python\r\n@torch.jit.script\r\ndef smt_pred(confs, child, child_sizes, group_offsets,\r\n             threshold, obj,\r\n             height, width, b, a):\r\n    # type: (List[Tensor],List[Tensor],List[Tensor],List[int],Tensor,Tensor,int,int,int,int) -> Tuple[Tensor,Tensor,Tensor]\r\n    size = child_sizes[0][1]\r\n    ch = child[0][1]\r\n    for sg in range(size):\r\n        pass\r\n \r\n"
25462,"Cannot use forward references for types## \U0001f41b Bug\r\nThe JIT can't pass following code when I tried to passing another instance with same class type. \r\n## To Reproduce\r\n### not working in python3.6\r\n\r\n### working in python3.6\r\n\r\n``` python\r\nclass A:\r\n    def __init__(self):\r\n        self.v = 100\r\n    \r\n    def copy(self, b: 'A'):\r\n        self.v = b.v\r\n```\r\n## Expected behavior\r\nRuntimeError: \r\nExpression of type string_literal cannot be used in a type expression:\r\nat demo_class.py:8:23\r\n    def copy(self, b: 'A'):\r\n                       ~ <--- HERE\r\n        self.v = b.v\r\n\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.2.0\r\n[pip3] torch-nightly==1.2.0.dev20190530\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n[Reference](https://www.python.org/dev/peps/pep-0484/#forward-references)\r\n[Stack Overflow](https://stackoverflow.com/questions/33533148/how-do-i-specify-that-the-return-type-of-a-method-is-the-same-as-the-class-itsel)\r\n\n\ncc @suo",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\nThe JIT can't pass following code when I tried to passing another instance with same class type. \r\n## To Reproduce\r\n### not working in python3.6\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass A:\r\n    def __init__(self):\r\n        self.v = 100\r\n    \r\n    def copy(self, b: 'A'):\r\n        self.v = b.v\r\n```\r\n### working in python3.6\r\n\r\n``` python\r\nclass A:\r\n    def __init__(self):\r\n        self.v = 100\r\n    \r\n    def copy(self, b: 'A'):\r\n        self.v = b.v\r\n```\r\n## Expected behavior\r\nRuntimeError: \r\nExpression of type string_literal cannot be used in a type expression:\r\nat demo_class.py:8:23\r\n    def copy(self, b: 'A'):\r\n                       ~ <--- HERE\r\n        self.v = b.v\r\n\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.0\r\n[pip3] torch==1.2.0\r\n[pip3] torch-nightly==1.2.0.dev20190530\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n[Reference](https://www.python.org/dev/peps/pep-0484/#forward-references)\r\n[Stack Overflow](https://stackoverflow.com/questions/33533148/how-do-i-specify-that-the-return-type-of-a-method-is-the-same-as-the-class-itsel)\r\n\n\ncc @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\nclass A:\r\n    def __init__(self):\r\n        self.v = 100\r\n    \r\n    def copy(self, b: 'A'):\r\n        self.v = b.v\r\n"
25301,"CUDA IPC garbage collection hangs when disposing of LSTMs ## \U0001f41b Bug\r\n\r\nCopying a LSTM's state dict to another process via a torch Queue will hang when the source process tries to dispose of the underlying tensors.\r\n\r\n## To Reproduce\r\nBelow we have two processes. The learner process passes an LSTM's state dict to the actor process via a Queue. \r\n\r\nWhen run, the actor and learner will each loop a few times an then the learner process will hang. The output will be something like\r\n\r\n```\r\nLearner started on PID #1193\r\nActor started on PID #1192\r\nLearner stepped\r\nLearner stepped\r\nActor stepped\r\nActor stepped\r\nLearner stepped\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe expected behaviour is that the learner doesn't hang, and the actor/learner stepped messages carry on forever. You can verify the script itself isn't broken by removing the `.cuda()` call - it works fine with a CPU network.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\nNvidia driver version: 430.40\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchfile==0.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py37h7b6447c_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\nThe bug goes away when using a CPU LSTM rather than a CUDA LSTM\r\n\r\nThe bug goes away when using a `nn.Linear(1, 1)` rather than a LSTM\r\n\r\nIn my research code, the bug takes longer to turn up when the `deepcopy` is removed, but it still appears eventually.\r\n\r\nAttaching GDB to the learner process after the hang, `info threads` will show that some thread is sitting at `__lll_lock_wait ()`, and checking that thread's backtrace gets\r\n\r\n```\r\n#0  0x00007fa14e96b10d in __lll_lock_wait () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#1  0x00007fa14e964023 in pthread_mutex_lock () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#2  0x00007fa13fe19307 in torch::(anonymous namespace)::CudaIPCSentDataLimbo::collect() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#3  0x00007fa13fe195ec in torch::(anonymous namespace)::CudaIPCSentDataDelete(void*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#4  0x00007fa13fe17131 in torch::CudaIPCSentData::~CudaIPCSentData() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#5  0x00007fa13fe19365 in torch::(anonymous namespace)::CudaIPCSentDataLimbo::collect() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#6  0x00007fa13fe195d8 in torch::(anonymous namespace)::CudaIPCSentDataDelete(void*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#7  0x00007fa10ebb9fa4 in c10::TensorImpl::release_resources() [clone .localalias.182] () from /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so\r\n#8  0x00007fa13fcba014 in c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::reset_() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#9  0x00007fa13ff0042b in THPVariable_clear(THPVariable*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#10 0x00007fa13ff00461 in THPVariable_dealloc(THPVariable*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#11 0x0000562bd5e2698f in subtype_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/typeobject.c:1256\r\n#12 0x0000562bd5d88a0c in free_keys_object (keys=0x7fa0fb6e24c8) at /tmp/build/80754af9/python_1553721932202/work/Objects/dictobject.c:559\r\n#13 dict_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/dictobject.c:1913\r\n#14 0x0000562bd5e90521 in odict_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/odictobject.c:1376\r\n```\r\n\r\nso the issue seems to arise from the locks taken by `CudaIPCSentDataLimbo::collect()`.",module: multiprocessing|module: cuda|triaged|module: deadlock,VitalyFedyunin,"## \U0001f41b Bug\r\n\r\nCopying a LSTM's state dict to another process via a torch Queue will hang when the source process tries to dispose of the underlying tensors.\r\n\r\n## To Reproduce\r\nBelow we have two processes. The learner process passes an LSTM's state dict to the actor process via a Queue. \r\n```python\r\nimport os\r\nfrom torch import nn\r\nfrom torch.multiprocessing import Queue, Process, set_start_method\r\nfrom time import sleep\r\nfrom copy import deepcopy\r\n\r\ndef actor(queue):\r\n    print(f'Actor started on PID #{os.getpid()}')\r\n    while True:\r\n        if not queue.empty():\r\n            queue.get()\r\n            print('Actor stepped')\r\n        sleep(.01)\r\n    \r\ndef learner(queue):\r\n    print(f'Learner started on PID #{os.getpid()}')\r\n    net = nn.LSTM(1, 1).cuda()\r\n    while True:\r\n        if not queue.full():\r\n            queue.put(deepcopy(net.state_dict()))\r\n            print('Learner stepped')\r\n        sleep(.01)\r\n\r\ndef run():\r\n    queue = Queue(1)\r\n\r\n    processes = dict(\r\n        a=Process(target=actor, args=(queue,)),\r\n        l=Process(target=learner, args=(queue,)))\r\n\r\n    for p in processes.values():\r\n        p.start()\r\n\r\n    for p in processes.values():\r\n        p.join()\r\n\r\nif __name__ == '__main__':\r\n    set_start_method('spawn')\r\n    run()\r\n```\r\nWhen run, the actor and learner will each loop a few times an then the learner process will hang. The output will be something like\r\n\r\n```\r\nLearner started on PID #1193\r\nActor started on PID #1192\r\nLearner stepped\r\nLearner stepped\r\nActor stepped\r\nActor stepped\r\nLearner stepped\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe expected behaviour is that the learner doesn't hang, and the actor/learner stepped messages carry on forever. You can verify the script itself isn't broken by removing the `.cuda()` call - it works fine with a CPU network.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\nNvidia driver version: 430.40\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchfile==0.1.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.0.2            py37h7b6447c_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\nThe bug goes away when using a CPU LSTM rather than a CUDA LSTM\r\n\r\nThe bug goes away when using a `nn.Linear(1, 1)` rather than a LSTM\r\n\r\nIn my research code, the bug takes longer to turn up when the `deepcopy` is removed, but it still appears eventually.\r\n\r\nAttaching GDB to the learner process after the hang, `info threads` will show that some thread is sitting at `__lll_lock_wait ()`, and checking that thread's backtrace gets\r\n\r\n```\r\n#0  0x00007fa14e96b10d in __lll_lock_wait () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#1  0x00007fa14e964023 in pthread_mutex_lock () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#2  0x00007fa13fe19307 in torch::(anonymous namespace)::CudaIPCSentDataLimbo::collect() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#3  0x00007fa13fe195ec in torch::(anonymous namespace)::CudaIPCSentDataDelete(void*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#4  0x00007fa13fe17131 in torch::CudaIPCSentData::~CudaIPCSentData() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#5  0x00007fa13fe19365 in torch::(anonymous namespace)::CudaIPCSentDataLimbo::collect() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#6  0x00007fa13fe195d8 in torch::(anonymous namespace)::CudaIPCSentDataDelete(void*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#7  0x00007fa10ebb9fa4 in c10::TensorImpl::release_resources() [clone .localalias.182] () from /opt/conda/lib/python3.7/site-packages/torch/lib/libc10.so\r\n#8  0x00007fa13fcba014 in c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::reset_() () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#9  0x00007fa13ff0042b in THPVariable_clear(THPVariable*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#10 0x00007fa13ff00461 in THPVariable_dealloc(THPVariable*) () from /opt/conda/lib/python3.7/site-packages/torch/lib/libtorch_python.so\r\n#11 0x0000562bd5e2698f in subtype_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/typeobject.c:1256\r\n#12 0x0000562bd5d88a0c in free_keys_object (keys=0x7fa0fb6e24c8) at /tmp/build/80754af9/python_1553721932202/work/Objects/dictobject.c:559\r\n#13 dict_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/dictobject.c:1913\r\n#14 0x0000562bd5e90521 in odict_dealloc () at /tmp/build/80754af9/python_1553721932202/work/Objects/odictobject.c:1376\r\n```\r\n\r\nso the issue seems to arise from the locks taken by `CudaIPCSentDataLimbo::collect()`.","python\r\nimport os\r\nfrom torch import nn\r\nfrom torch.multiprocessing import Queue, Process, set_start_method\r\nfrom time import sleep\r\nfrom copy import deepcopy\r\n\r\ndef actor(queue):\r\n    print(f'Actor started on PID #{os.getpid()}')\r\n    while True:\r\n        if not queue.empty():\r\n            queue.get()\r\n            print('Actor stepped')\r\n        sleep(.01)\r\n    \r\ndef learner(queue):\r\n    print(f'Learner started on PID #{os.getpid()}')\r\n    net = nn.LSTM(1, 1).cuda()\r\n    while True:\r\n        if not queue.full():\r\n            queue.put(deepcopy(net.state_dict()))\r\n            print('Learner stepped')\r\n        sleep(.01)\r\n\r\ndef run():\r\n    queue = Queue(1)\r\n\r\n    processes = dict(\r\n        a=Process(target=actor, args=(queue,)),\r\n        l=Process(target=learner, args=(queue,)))\r\n\r\n    for p in processes.values():\r\n        p.start()\r\n\r\n    for p in processes.values():\r\n        p.join()\r\n\r\nif __name__ == '__main__':\r\n    set_start_method('spawn')\r\n    run()\r\n"
25242,"problem with mkldnn and march=native## \U0001f41b Bug\r\n\r\nThe mkldnn multiplication goes wrong when compiled with `-march=native` and the tensors are sufficiently large (>=1024 items) on both Haswell and Cascadelake architecture CPUs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch with `-march=native`\r\n1. Run the test case below. This test case is a simplification of https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_mkldnn.py#L227 (`common_utils.py` and `expecttest.py` are as per the v1.2.0 branch).\r\n\r\n\r\n\r\nFailure is:\r\n```\r\n======================================================================\r\nFAIL: test_mul (__main__.TestMkldnn)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test_mul.py"", line 13, in test_mul\r\n    self.assertEqual((x * y), (mx * my).to_dense())\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-4/src/easybuild-easyconfigs/easybuild/easyconfigs/p/PyTorch/common_utils.py"", line 676, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-4/src/easybuild-easyconfigs/easybuild/easyconfigs/p/PyTorch/common_utils.py"", line 656, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: tensor(6.6931e+34, dtype=torch.float32) not less than or equal to 1e-05 : \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nFurther testing:\r\n1. The test case passes if `size = 1023`\r\n1. It is adding `-march=native` that causes the problem - removing that from the CFLAGS / CXXFLAGS and all is fine.\r\n1. Adding debug printing of `x`, `mx`, `x * x`, `mx * mx`, and `(mx * mx).to_dense()` shows clearly that the problem occurs at the `mx * mx` stage:\r\n\r\n```\r\ntensor([-0.1117, -0.4966,  0.1631,  ..., -1.6426, -1.0094, -0.3527],\r\n       dtype=torch.float32)\r\ntensor([-0.1117, -0.4966,  0.1631,  ..., -1.6426, -1.0094, -0.3527],\r\n       dtype=torch.float32, layout=torch._mkldnn)\r\ntensor([0.0125, 0.2466, 0.0266,  ..., 2.6983, 1.0190, 0.1244],\r\n       dtype=torch.float32)\r\ntensor([-2.5416e-29,  1.5521e-41, -2.5416e-29,  ...,  0.0000e+00,\r\n         0.0000e+00,  0.0000e+00], dtype=torch.float32, layout=torch._mkldnn)\r\ntensor([-2.5416e-29,  1.5521e-41, -2.5416e-29,  ...,  0.0000e+00,\r\n         0.0000e+00,  0.0000e+00], dtype=torch.float32)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThat the multiplication should be done correctly.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPyTorch built from source\r\nBuild command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON  /rds/bear-apps/devel/2019a/branfosj-eb-4/EL7/EL7-haswell/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build\r\nCFLAGS and CXXFLAGS are set to -O2 -ftree-vectorize -march=native -fno-math-errno\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",module: cpu|triaged|module: mkldnn,pbelevich,"## \U0001f41b Bug\r\n\r\nThe mkldnn multiplication goes wrong when compiled with `-march=native` and the tensors are sufficiently large (>=1024 items) on both Haswell and Cascadelake architecture CPUs.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build PyTorch with `-march=native`\r\n1. Run the test case below. This test case is a simplification of https://github.com/pytorch/pytorch/blob/v1.2.0/test/test_mkldnn.py#L227 (`common_utils.py` and `expecttest.py` are as per the v1.2.0 branch).\r\n\r\n```python\r\nimport torch\r\nimport unittest\r\nfrom common_utils import TestCase\r\n\r\nclass TestMkldnn(TestCase):\r\n    def test_mul(self):\r\n        size = 1024\r\n        x = torch.randn(size, dtype=torch.float32)\r\n        mx = x.to_mkldnn()\r\n        self.assertEqual((x * x), (mx * mx).to_dense())\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n```\r\n\r\nFailure is:\r\n```\r\n======================================================================\r\nFAIL: test_mul (__main__.TestMkldnn)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""test_mul.py"", line 13, in test_mul\r\n    self.assertEqual((x * y), (mx * my).to_dense())\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-4/src/easybuild-easyconfigs/easybuild/easyconfigs/p/PyTorch/common_utils.py"", line 676, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File ""/rds/bear-apps/devel/2019a/branfosj-eb-4/src/easybuild-easyconfigs/easybuild/easyconfigs/p/PyTorch/common_utils.py"", line 656, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: tensor(6.6931e+34, dtype=torch.float32) not less than or equal to 1e-05 : \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nFurther testing:\r\n1. The test case passes if `size = 1023`\r\n1. It is adding `-march=native` that causes the problem - removing that from the CFLAGS / CXXFLAGS and all is fine.\r\n1. Adding debug printing of `x`, `mx`, `x * x`, `mx * mx`, and `(mx * mx).to_dense()` shows clearly that the problem occurs at the `mx * mx` stage:\r\n\r\n```\r\ntensor([-0.1117, -0.4966,  0.1631,  ..., -1.6426, -1.0094, -0.3527],\r\n       dtype=torch.float32)\r\ntensor([-0.1117, -0.4966,  0.1631,  ..., -1.6426, -1.0094, -0.3527],\r\n       dtype=torch.float32, layout=torch._mkldnn)\r\ntensor([0.0125, 0.2466, 0.0266,  ..., 2.6983, 1.0190, 0.1244],\r\n       dtype=torch.float32)\r\ntensor([-2.5416e-29,  1.5521e-41, -2.5416e-29,  ...,  0.0000e+00,\r\n         0.0000e+00,  0.0000e+00], dtype=torch.float32, layout=torch._mkldnn)\r\ntensor([-2.5416e-29,  1.5521e-41, -2.5416e-29,  ...,  0.0000e+00,\r\n         0.0000e+00,  0.0000e+00], dtype=torch.float32)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThat the multiplication should be done correctly.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 8.2.0\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPyTorch built from source\r\nBuild command: PYTORCH_BUILD_VERSION=1.2.0 PYTORCH_BUILD_NUMBER=1 VERBOSE=1 LDFLAGS=""$LDFLAGS -ldl"" USE_FFMPEG=ON USE_GLOO_IBVERBS=1 USE_GFLAGS=ON USE_GLOG=ON  /rds/bear-apps/devel/2019a/branfosj-eb-4/EL7/EL7-haswell/software/Python/3.7.2-GCCcore-8.2.0/bin/python setup.py build\r\nCFLAGS and CXXFLAGS are set to -O2 -ftree-vectorize -march=native -fno-math-errno\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh","python\r\nimport torch\r\nimport unittest\r\nfrom common_utils import TestCase\r\n\r\nclass TestMkldnn(TestCase):\r\n    def test_mul(self):\r\n        size = 1024\r\n        x = torch.randn(size, dtype=torch.float32)\r\n        mx = x.to_mkldnn()\r\n        self.assertEqual((x * x), (mx * mx).to_dense())\r\n\r\nif __name__ == '__main__':\r\n    unittest.main()\r\n"
25241,"torch.bmm produces incorrect results for GPU fp16 tensors with batch size > 65535## \U0001f41b Bug\r\n\r\nWhen using `torch.bmm` to multiply GPU matrices in FP16 with a batch size > 65535, the results are only correct for the first 65535 batch entries, after that is garbage.\r\n\r\n## To Reproduce\r\n\r\nMinimal example:\r\n\r\n\r\n\r\nIn my case, the output is:\r\n```\r\ntensor([13.3080, 16.3155, 15.8803, 14.1044, 14.4463, 14.3881, 12.6462, 15.0877],\r\n       device='cuda:0')\r\ntensor([13.3047, 16.3125, 15.8828, 14.1016, 14.4453, 14.3906, 12.6484, 15.0859],\r\n       device='cuda:0', dtype=torch.float16)\r\n--------------------------------------------------------------------------------\r\ntensor([16.6753, 16.2456, 15.2091, 16.5250, 15.8137, 14.3729, 15.5661, 16.0661],\r\n       device='cuda:0')\r\ntensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16)\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 20, in <module>\r\n    assert torch.allclose(res, res_half.float(), atol=1e-1)\r\nAssertionError\r\n```\r\n\r\nNote that the incorrect entries were not always zero in all my tests.\r\n\r\n## Expected behavior\r\n\r\nThe output tensors should be the same (to within floating point accuracy) after the index 65535.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: TITAN V\r\nGPU 5: TITAN V\r\nGPU 6: TITAN V\r\nGPU 7: TITAN V\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.3.0\r\n[pip] xnmtorch==0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.0.2            py37h7b6447c_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchtext                 0.4.0                     dev_0    <develop>\r\n[conda] torchvision               0.3.0           py37_cu10.0.130_1    pytorch\r\n[conda] xnmtorch                  0.1                       dev_0    <develop>\r\n```\r\n\r\nThe test was run while restricting to one GPU (using `CUDA_VISIBLE_DEVICES`) and was tested on both the 2080 Ti and the Titan V GPUs\r\n\n\ncc @ezyang @gchanan",high priority|module: cuda|triaged,pbelevich,"## \U0001f41b Bug\r\n\r\nWhen using `torch.bmm` to multiply GPU matrices in FP16 with a batch size > 65535, the results are only correct for the first 65535 batch entries, after that is garbage.\r\n\r\n## To Reproduce\r\n\r\nMinimal example:\r\n\r\n```python\r\nimport torch\r\n\r\ntorch.manual_seed(1234)\r\n\r\nmat_a = torch.rand(65536, 8, 64).cuda()\r\nmat_b = torch.rand(65536, 64, 8).cuda()\r\n\r\nmat_a_half = mat_a.half()\r\nmat_b_half = mat_b.half()\r\n\r\nres = torch.bmm(mat_a, mat_b)\r\nres_half = torch.bmm(mat_a_half, mat_b_half)\r\n\r\nprint(res[65534, 0])\r\nprint(res_half[65534, 0])\r\nprint(""-"" * 80)\r\nprint(res[65535, 0])\r\nprint(res_half[65535, 0])\r\n\r\nassert torch.allclose(res, res_half.float(), atol=1e-1)\r\n```\r\n\r\nIn my case, the output is:\r\n```\r\ntensor([13.3080, 16.3155, 15.8803, 14.1044, 14.4463, 14.3881, 12.6462, 15.0877],\r\n       device='cuda:0')\r\ntensor([13.3047, 16.3125, 15.8828, 14.1016, 14.4453, 14.3906, 12.6484, 15.0859],\r\n       device='cuda:0', dtype=torch.float16)\r\n--------------------------------------------------------------------------------\r\ntensor([16.6753, 16.2456, 15.2091, 16.5250, 15.8137, 14.3729, 15.5661, 16.0661],\r\n       device='cuda:0')\r\ntensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16)\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 20, in <module>\r\n    assert torch.allclose(res, res_half.float(), atol=1e-1)\r\nAssertionError\r\n```\r\n\r\nNote that the incorrect entries were not always zero in all my tests.\r\n\r\n## Expected behavior\r\n\r\nThe output tensors should be the same (to within floating point accuracy) after the index 65535.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: TITAN V\r\nGPU 5: TITAN V\r\nGPU 6: TITAN V\r\nGPU 7: TITAN V\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.3.0\r\n[pip] xnmtorch==0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.0.2            py37h7b6447c_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch                   1.2.0           py3.7_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] torchtext                 0.4.0                     dev_0    <develop>\r\n[conda] torchvision               0.3.0           py37_cu10.0.130_1    pytorch\r\n[conda] xnmtorch                  0.1                       dev_0    <develop>\r\n```\r\n\r\nThe test was run while restricting to one GPU (using `CUDA_VISIBLE_DEVICES`) and was tested on both the 2080 Ti and the Titan V GPUs\r\n\n\ncc @ezyang @gchanan","python\r\nimport torch\r\n\r\ntorch.manual_seed(1234)\r\n\r\nmat_a = torch.rand(65536, 8, 64).cuda()\r\nmat_b = torch.rand(65536, 64, 8).cuda()\r\n\r\nmat_a_half = mat_a.half()\r\nmat_b_half = mat_b.half()\r\n\r\nres = torch.bmm(mat_a, mat_b)\r\nres_half = torch.bmm(mat_a_half, mat_b_half)\r\n\r\nprint(res[65534, 0])\r\nprint(res_half[65534, 0])\r\nprint(""-"" * 80)\r\nprint(res[65535, 0])\r\nprint(res_half[65535, 0])\r\n\r\nassert torch.allclose(res, res_half.float(), atol=1e-1)\r\n"
25176,"Tensor slicing with boolean numpy mask wrong## \U0001f41b Bug\r\nNumpy arrays of dtype `np.bool` should be interpreted as masks when slicing torch arrays, just like tensors of dtype `torch.bool` are. Insead, such arrays are interpreted as indices.\r\n\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\n\r\n\r\nOutput\r\n\r\n## Expected behavior\r\nThe expected behaviour is that the result is the same as for slicing with torch tensors with the same dtype, where for the input\r\n\r\nwe get the expected output\r\n\r\n\r\n## Environment\r\n - PyTorch Version: 1.2.0a0+0885dd2\r\n - OS: Ubuntu 16.04.6 LTS\r\n - How you installed PyTorch: Nvidia docker image (`FROM nvcr.io/nvidia/pytorch:19.06-py3`)\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: Nvidia driver version: 384.183, ` /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0`\r\n - GPU models and configuration: Tesla V100\r\n",triaged|module: numpy|module: boolean tensor,izdeby,"## \U0001f41b Bug\r\nNumpy arrays of dtype `np.bool` should be interpreted as masks when slicing torch arrays, just like tensors of dtype `torch.bool` are. Insead, such arrays are interpreted as indices.\r\n\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nt = torch.tensor(range(5))\r\nmask = np.array([False, True, True, False, True])\r\n\r\nprint(t[mask.astype(np.bool)])\r\nprint(t[mask.astype(np.uint8)])\r\n```\r\nOutput\r\n```bash\r\ntensor([0, 1, 1, 0, 1])\r\ntensor([1, 2, 4])\r\n```\r\n## Expected behavior\r\nThe expected behaviour is that the result is the same as for slicing with torch tensors with the same dtype, where for the input\r\n```python\r\nt = torch.tensor(range(5))\r\nmask = torch.tensor([False, True, True, False, True])\r\n\r\nprint(t[mask.to(torch.bool)])\r\nprint(t[mask.to(torch.uint8)])\r\n```\r\nwe get the expected output\r\n```bash\r\ntensor([1, 2, 4])\r\ntensor([1, 2, 4])\r\n```\r\n\r\n## Environment\r\n - PyTorch Version: 1.2.0a0+0885dd2\r\n - OS: Ubuntu 16.04.6 LTS\r\n - How you installed PyTorch: Nvidia docker image (`FROM nvcr.io/nvidia/pytorch:19.06-py3`)\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: Nvidia driver version: 384.183, ` /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0`\r\n - GPU models and configuration: Tesla V100\r\n","python\r\nt = torch.tensor(range(5))\r\nmask = np.array([False, True, True, False, True])\r\n\r\nprint(t[mask.astype(np.bool)])\r\nprint(t[mask.astype(np.uint8)])\r\n"
25162,"Incorrect Validation Accuracy Due to Distributed Sampler## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThe new `DistributedSampler` pads the number of samples to make it divisible by the number of processes, which is a bit similar to `drop_last` in `DataLoader` while here `DsitributedSampler` duplicates the last few samples instead of dropping them.\r\n\r\nThis will definitely cause incorrect validation accuracy when you want to use distributed validation.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n## Environment\r\n - PyTorch Version (1.2.0):\r\n\r\n## Feature Request\r\nAdd `drop_last` or `duplicate_last` option in `DistributedSampler`.\r\n\r\ncc @SsnL @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera",oncall: distributed|module: dataloader|triaged,rohan-varma,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThe new `DistributedSampler` pads the number of samples to make it divisible by the number of processes, which is a bit similar to `drop_last` in `DataLoader` while here `DsitributedSampler` duplicates the last few samples instead of dropping them.\r\n\r\nThis will definitely cause incorrect validation accuracy when you want to use distributed validation.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```Python\r\n    def __iter__(self):\r\n        # deterministically shuffle based on epoch\r\n        g = torch.Generator()\r\n        g.manual_seed(self.epoch)\r\n        if self.shuffle:\r\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\r\n        else:\r\n            indices = list(range(len(self.dataset)))\r\n\r\n\r\n        # add extra samples to make it evenly divisible\r\n        indices += indices[:(self.total_size - len(indices))]\r\n        assert len(indices) == self.total_size\r\n\r\n        # subsample\r\n        indices = indices[self.rank:self.total_size:self.num_replicas]\r\n        assert len(indices) == self.num_samples\r\n\r\n        return iter(indices)\r\n```\r\n\r\n\r\n## Environment\r\n - PyTorch Version (1.2.0):\r\n\r\n## Feature Request\r\nAdd `drop_last` or `duplicate_last` option in `DistributedSampler`.\r\n\r\ncc @SsnL @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera","Python\r\n    def __iter__(self):\r\n        # deterministically shuffle based on epoch\r\n        g = torch.Generator()\r\n        g.manual_seed(self.epoch)\r\n        if self.shuffle:\r\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\r\n        else:\r\n            indices = list(range(len(self.dataset)))\r\n\r\n\r\n        # add extra samples to make it evenly divisible\r\n        indices += indices[:(self.total_size - len(indices))]\r\n        assert len(indices) == self.total_size\r\n\r\n        # subsample\r\n        indices = indices[self.rank:self.total_size:self.num_replicas]\r\n        assert len(indices) == self.num_samples\r\n\r\n        return iter(indices)\r\n"
25081,"Automatic mixed precision for Pytorch# \U0001f680 Feature\r\n\r\nWe would like Pytorch to support the automatic mixed precision training recipe:  auto-casting of Cuda operations to FP16 or FP32 based on a whitelist-blacklist model of what precision is best for each operation, as well as gradient scaling to avoid underflow during the backward pass.\r\n\r\n# Work in progress\r\n\r\n* [PR #33366](https://github.com/pytorch/pytorch/pull/33366) for the gradient scaling API (merged!)\r\n* [PR #35102](https://github.com/pytorch/pytorch/pull/35102) for the autocasting API and eager-mode backend (merged!)\r\n* [Dedicated issue tracking jit support](https://github.com/pytorch/pytorch/issues/25387) (@jjsjann123 is driving this on our side)\r\n* [Jit scripting for context managers (`with` statements)](https://github.com/pytorch/pytorch/issues/28762 )\r\n\r\n# Motivation\r\n\r\nMixed precision is essential to achieve good performance on Tensor Core GPUs (Volta + Turing).  We've been trying to educate and evangelize for it since Volta's release.  We have [Apex](https://github.com/nvidia/apex), our own repository of tools for mixed precision training, which has seen moderate adoption.  However, forcing users to install a separate toolkit is burdensome, especially if their environments aren't set up to build extensions, and we'll never be able to achieve the same performance as a native integration.  Native, well documented support in Pytorch core is certainly the most convenient way to enable mixed precision training for the largest number of users.\r\n\r\n# Pitch\r\n\r\nAfter initial discussions with @mruberry, @zdevito, @jamesr66a, @gchanan, and @jjsjann123 we believe the UX should permit auto-casting and gradient scaling as modular and independent components.\r\n\r\n## Auto-casting\r\n\r\n### Background\r\n\r\nThe current philosophy of Apex's Automatic Mixed Precision (Amp, in recommended mode ""O1"") is that when training with mixed precision, the user never needs to manually alter the precision of their model or data.  The user declares their model in default (FP32) precision.  **The model parameters (leaves) are and remain FP32 for the duration of training.  These leaves are also directly owned and stepped by the optimizer,** which is identical to the ordinary behavior of Pytorch in the absence of mixed precision.\r\n\r\nTo ensure that FP16 is used for operations that benefit from it, `Tensor.*` methods and `torch.*` and `torch.nn.functional.*` functions are patched to cast data to a certain type before running the actual op.  Which type depends on what precision is best for that op.  For example, `torch.mm` is patched to cast the incoming input and weight to FP16, which enables Tensor Cores.  `torch.log` is patched to cast input to fp32, because log's forward and backward may require a large dynamic range.  This casting-as-data-flows-through-functions is the strategy used by Amp in Apex today, and achieves accuracy comparable to pure FP32 training on a wide range of networks.  It is also the strategy used by MXNet and Tensorflow's Amp integrations.  However, Apex's Amp is implemented by Python-side monkey-patching of `torch.*` and `torch.nn.functional.*` functions, which is invasive and not ideal for performance.\r\n\r\n### Proposed Implementation\r\n\r\nFor eager execution, we propose to integrate the same casting-as-data-flows-through-functions strategy that Apex's Amp uses.  We propose to implement this by inserting the casts in some of the autogenerated C++ functions on the Cuda dispatch path.  Each such function will be given a few additional lines by the autogeneration script.  For example, a whilelist function with 1 argument would be given something like\r\n\r\nThese casts should be autograd-exposed, so they will be reversed in backward().  They should also precede the autograd-exposed call of the whitelist or blacklist op itself, so if the op saves its inputs for backward, the inputs are saved as the correct (post-cast) type.\r\n\r\nOn the Python side, the user shall request auto-casting by running the forward pass (or any portions of the forward pass where auto-casting is desired) under a nestable context manager, for example\r\n\r\n`torch.get_` and `set_autocasting_state` will get/set a backend state that is queryable within C++ dispatch functions.\r\n\r\n`whitelist_type` can be changed to request that whitelist functions be autocast to types other than FP16.  The `enabled` argument can be used to locally disable autocasting if the user wishes to have manual control over the types that are used in some regions of their model, while permitting autocasting in others.\r\n\r\nMy initial thought is that `with autocast()` may be used to wrap any regions of code where a graph is being constructed via explicit Python invocations of Pytorch ops (ie, the forward pass), but shall not wrap any region where a previously constructed graph is being backwarded through.  All desired casting will have been recorded as part of the graph construction, and will be correctly reversed by a bare backward call without needing to be under an `autocast` context.  Backward passes with `create_graph=True` also belong to the latter category (ie, they should not be under a `with autocast()` context). [Gradient Penalty](#gradient-penalty) under End to End Examples below shows this philosophy more clearly.\r\n\r\nIt's possible that running both forward and backward under the context manager won't do any harm (ie, any resulting casts requested during backward will be no-ops, because the type flow is already being properly/faithfully reversed by autograd) and it can be permissible (but not required) to also allow the backward pass to take place under the context manager.\r\n\r\nWhen training with FP16, gradient scaling and auto-casting must both be requested by the user.  We envision that when autocasting to formats other than FP16, gradient scaling will not be necessary, and the auto-casting context manager can be used without any gradient scaling.\r\n\r\n### Example Usage\r\n\r\nWithin `model.forward`, if the user has regions where they wish explicit control over the precision, they may nest an invocation of `with autocast(enabled=False)`:\r\n\r\n\r\n### Gotchas/Challenges\r\n\r\nThe Amp casts need to be recorded by autograd, so they'll be properly reversed in backward.  Unfortunately, for many ops, dispatch into an autograd-disabled region currently occurs in `VariableType*.cpp`, at a higher level of the call chain than the Cuda-specific dispatch functions.  `VariableType*.cpp` is also the level at which necessary data is saved for backward.  In other words, by the time we've reached the Cuda-specific dispatch functions, it's too late to invoke autograd-exposed casts.  The alternative is to insert the\r\n\r\nsnippets at the level of `VariableType*.cpp`, before we dive into autograd-disabled regions, but then these if statements will be on the hot path taken by all dispatches (Cuda and non-Cuda).  I'd like to avoid having the if statement on any non-Cuda code path.  This is a tricky problem and I need to think hard about it.\r\n\r\n## Gradient scaling\r\n\r\n### Background\r\n\r\nLate in training, FP16 gradients can underflow, halting convergence and in some cases causing destabilization.  Apex's Amp mitigates underflow via ""dynamic gradient scaling.""  The implementation creates scaled gradients by handing the user `scaled_loss  =  loss*scale_factor`, then requiring that the user invoke `scaled_loss.backward()`. By the chain rule, all gradients flowing backward through the network are then scaled by `scale_factor`.  Apex's Amp attempts to maximize use of FP16's full dynamic range by choosing the highest `scale_factor` that can be used without incurring inf/nan gradients, which is accomplished as follows:  Initially, a high `scale_factor` is chosen.  Each iteration, after backward() returns, gradients are checked for infs/nans.  If any infs/nans are found, the optimizer skips the step, and the scale factor is reduced.  The scale factor is also periodically increased if a streak of successful (inf/nan free) iterations occurs.  Gradients are unscaled in FP32 before being applied to FP32 model parameters.\r\n\r\n### Proposed API\r\n\r\nUser scripts will implement gradient scaling with an instance of a helper class.  Typical use would look like\r\n\r\n\r\nCreating scaled gradients with `torch.autograd.backward` would look like\r\n\r\n\r\nCreating scaled gradients with `torch.autograd.grad` would look like\r\n\r\n\r\n[The best explanation for `AmpScaler` is the class itself, as found in the gradient scaling PR](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L6).  It's lightweight and each function is documented.\r\n\r\n`scaler` internally initializes and maintains the scale value, and updates it each iteration based on whether `optimizer`'s gradients contained infs or NaNs.  If infs/NaN gradients are encountered in a given iteration, `scaler.update` reduces the scale.  If no inf/NaN gradients are encountered, `scaler.update` increases the scale slightly.  This approach achieves what dynamic gradient scaling intends:  over time, riding the edge of the highest gradient scale that can be used without incurring overflow.\r\n\r\nThe user also has the freedom to manually reset the scale value at the end of any iteration, by passing a new scale to `scaler.update` as a Tensor or Python float.\r\n\r\n`AmpScaler` instances contain at most a few Tensors and Python scalars.  When checkpointing, the user can easily save an `AmpScaler` instance as part of the checkpoint, alongside `model.state_dict()` and `optimizer.state_dict()`.  The model and optimizer instance(s) are not affected by AmpScaler, and remain exactly what they would be in the absence of mixed precision.  Therefore, behaviors and invocations of `model.state_dict()` and `optimizer.state_dict()` themselves may remain unaltered.\r\n\r\n### Interaction with Existing Optimizers\r\n\r\nAs long as training scripts adhere to the proposed API, existing optimizers (whether native or custom) will immediately work with mixed precision.  In particular, the gradient scaling API does not rely on changes to the Python source of `step()` in any existing optimizers.\r\n\r\n`scaler.step(optimizer)` [wraps `optimizer.step()` with logic to make it scaling-safe](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L146-L192).  Specifically, `scaler.step(optimizer)`\r\n- makes sure gradients are unscaled before `optimizer.step()`\r\n- skips `optimizer.step()` if inf/nan gradients are found.\r\n\r\n`optimizer.step()` itself is untouched, and again, does not need to change for existing optimizers.\r\n\r\n### Interaction with New Custom Optimizers\r\n\r\n`AmpScaler` defines a contract such that custom optimizers _may_ implement their own scaling-safe `step` methods if they choose to.  If an optimizer obeys this contract, [`AmpScaler.step` will call the optimizer's `step` method directly](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L179-L184).  This gives custom optimizer authors a control point to implement ninja optimizations like sync-free dynamic gradient scaling.\r\n\r\n### Gotchas/Challenges\r\n\r\nWhen the user invokes a backward pass with a scale factor, all gradients produced by this backward pass (leaf gradients produced by `loss.backward` or `torch.autograd.backward`, or out-of-place gradients produced by `torch.autograd.grad`) will be scaled.  Therefore, anything that manipulates the gradients between `scaler.scale(loss).backward()` and the `scaler.step(optimizer)` will require proper awareness of the scale factor.  Examples of operations that require scale-factor-aware treatment are \r\n- gradient clipping\r\n- gradient penalty computations.\r\n\r\nIn our opinion, requiring the user be aware of the scale factor when making direct use of the gradients is not terribly burdensome.  The user has explicitly requested gradient scaling; they should not be surprised when they end up with scaled gradients on their hands.  The treatment of such cases is also not difficult from a code-change perspective, as long as it is clearly documented.\r\n\r\nClosures are also a challenge.  Do we need to support explicit closure use?  Based on the scripts and issues I've seen, closure use is not terribly common, and LBFGS is the only native optimizer that requires a closure.  However, I think I can torture the proposed API into supporting closures if it turns out to be in high demand.\r\n\r\n## End to End Examples (Auto-Casting + Gradient Scaling)\r\n\r\n### Typical Use (1 loss, 1 optimizer)\r\n\r\n\r\n### Gradient Clipping\r\n\r\nGradient clipping requires awareness that the gradients resulting from `scaler.scale(loss).backward()` are scaled. One simple way to account for the scale factor is by clipping to `max_norm*scaler.get_scale()` instead of max_norm:\r\n\r\n\r\nHere the scaled gradients are clipped. `scaler.step(optimizer)` is aware that gradients have not yet been unscaled, and unscales them under the hood before calling `optimizer.step()`.\r\n\r\n### Gradient Clipping with Explicit Unscaling\r\n\r\nThe specific case of clipping scaled gradients isn\u2019t so hard (all you have to do is clip to `max_norm*scaler.get_scale()`). However, in general, between the backward pass and the optimizer step you may wish to manipulate gradients in some way that\u2019s not so easy to translate to scaled gradients. In such cases, you can unscale and step separately. Here\u2019s how that looks, using gradient clipping as an example once more:\r\n\r\n\r\n\r\n### Gradient Penalty\r\n(based on **Higher order gradients** from the [release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0))\r\n\r\nGradient penalty also requires awareness that the gradients are scaled in certain places. Additionally, gradient penalty demonstrates:\r\n- `torch.autograd.grad`\r\n- Double-backward\r\n- Some clarification of what counts as a ""forward pass"" for the purpose of using `with autocast` (in other words, when exactly it's appropriate to use `with autocast`).\r\n\r\nThe following shows an implementation of gradient penalty under the proposed API.\r\n\r\n\r\nGradient penalty is a tricky case to think about but writing the code is simple once the right pattern is established.  Compared to the [example in the release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0), the only extra line for gradient penalty computation is the unscaling `grad_params = [p*(1./scaler.get_scale()) for p in grad_params]`.  I think this can be considered a documentation problem, and addressed by providing clear examples.\r\n\r\n### Multiple Models/Optimizers/Losses\r\n\r\nNetworks must use the same `AmpScaler` instance (and therefore the same scale) to create gradients for all backward passes in a given iteration, otherwise we open the door to nasty corner cases.  For example, if two different losses, with different gradient scales, accumulate into the same parameters' .grads, the accumulation math breaks.  If two different losses, with different gradient scales, accumulate into different parameters owned by the same optimizer, then when you invoke `scaler.unscale(optimizer)`, there's no single correct value that can be used to unscale all the gradients owned by that optimizer, and handling multiple scale factors for different parameters within the same optimizer would get ugly fast.  Requiring that networks use the same `AmpScaler` instance for all backward passes avoids all such control flow difficulties, while still achieving what loss scaling is meant to achieve.\r\n\r\n`scaler.update()` must be called only at the end of the iteration, after `scaler.step(optimizer)` has been called for all optimizers used this iteration.  This requirement allows `update` to account for infs/nans found among any of the optimizers' gradients.\r\n\r\n\r\n\r\nI had to write Apex's Amp to handle arbitrary combinations of multiple models/optimizers/losses.  I'm painfully aware of the complicated combinations of models/optimizers/losses people want to implement.  In my opinion, the proposed interface permits a great deal of flexibility in network design.\r\n\r\n### Gradient accumulation\r\n\r\nGradient accumulation across iterations (between steps) is a common use case.  The proposed API accommodates gradient accumulation without trouble:\r\n\r\n\r\n### Switching automatic mixed precision on and off\r\n\r\nIf users want to run with or without autocasting+gradient scaling, they shouldn't have to litter their code with if statements.  The API should allow one code path that accommodates easily switching autocasting+gradient scaling on and off.\r\n\r\nThe autocasting context manager and `AmpScaler` constructor provide such convenience by accepting an `enabled=False` argument.\r\n\r\nIn the following example, autocasting and gradient scaling can be switched on and off by flipping `args.use_mixed_precision` with no additional control flow required.\r\n\r\n\r\n\r\n### Batch replay\r\n\r\nSometimes every iteration/batch is valuable enough that users don't want to skip any.  Instead, it's preferable to replay the batch with a reduced loss scale until gradients do not contain infs/nans.  Batch replay control flow is not provided by the API alone, but with the proposed gradient scaling PR, it would be easy to rig:\r\n\r\n\r\n\r\n# Alternatives\r\n\r\n### Python-side alternatives for gradient scaling and unscaling\r\n\r\nThe supplementary information contains an in-depth discussion of some alternatives I considered for the [gradient scaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-scaling_apis_considered-md) and [gradient unscaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-unscaling_apis_considered-md) API. \r\n\r\n### Gradient scaling in the autograd backend\r\n\r\nI recently submitted [a PR](https://github.com/pytorch/pytorch/pull/24893) that implemented gradient scaling directly in the autograd engine (Engine::execute).\r\n\r\nBenefits:\r\n - It automatically enabled gradient scaling for all typical single- and double-backward use cases (including complex cases like gradient penalties) without requiring any change to user scripts.\r\n- Gradients returned visibly to the user, either in-place via `.grad` attributes or out-of-place via a call to `torch.autograd.grad`, were only ever unscaled, eliminating the need to change gradient clipping or manually unscale before computing gradient penalties.\r\n\r\nDrawbacks:\r\n- Modifying engine.cpp, especially for a GPU-specific purpose, is not to be done lightly (ie, not until alternatives are exhausted).\r\n- It's unclear what the user-facing API would look like.  I figured the implementation was general enough to permit many options, and the exact API could be nailed down later.  It certainly requires maintaining a global ""amp state.""\r\n- Altering the backend to do black-box gradient scaling and require no change to user scripts is a double-edged sword.  Explicit Python-side control and visibility of gradient scaling, as we propose above, is not a bad thing.  @cbcase, this seems like an instance of the https://www.jwz.org/doc/worse-is-better.html thing you told me about...\r\n\r\ncc @ezyang @gchanan @vincentqb\r\n\r\n",high priority|feature|module: optimizer|module: cuda|triaged|module: half,mcarilli,"# \U0001f680 Feature\r\n\r\nWe would like Pytorch to support the automatic mixed precision training recipe:  auto-casting of Cuda operations to FP16 or FP32 based on a whitelist-blacklist model of what precision is best for each operation, as well as gradient scaling to avoid underflow during the backward pass.\r\n\r\n# Work in progress\r\n\r\n* [PR #33366](https://github.com/pytorch/pytorch/pull/33366) for the gradient scaling API (merged!)\r\n* [PR #35102](https://github.com/pytorch/pytorch/pull/35102) for the autocasting API and eager-mode backend (merged!)\r\n* [Dedicated issue tracking jit support](https://github.com/pytorch/pytorch/issues/25387) (@jjsjann123 is driving this on our side)\r\n* [Jit scripting for context managers (`with` statements)](https://github.com/pytorch/pytorch/issues/28762 )\r\n\r\n# Motivation\r\n\r\nMixed precision is essential to achieve good performance on Tensor Core GPUs (Volta + Turing).  We've been trying to educate and evangelize for it since Volta's release.  We have [Apex](https://github.com/nvidia/apex), our own repository of tools for mixed precision training, which has seen moderate adoption.  However, forcing users to install a separate toolkit is burdensome, especially if their environments aren't set up to build extensions, and we'll never be able to achieve the same performance as a native integration.  Native, well documented support in Pytorch core is certainly the most convenient way to enable mixed precision training for the largest number of users.\r\n\r\n# Pitch\r\n\r\nAfter initial discussions with @mruberry, @zdevito, @jamesr66a, @gchanan, and @jjsjann123 we believe the UX should permit auto-casting and gradient scaling as modular and independent components.\r\n\r\n## Auto-casting\r\n\r\n### Background\r\n\r\nThe current philosophy of Apex's Automatic Mixed Precision (Amp, in recommended mode ""O1"") is that when training with mixed precision, the user never needs to manually alter the precision of their model or data.  The user declares their model in default (FP32) precision.  **The model parameters (leaves) are and remain FP32 for the duration of training.  These leaves are also directly owned and stepped by the optimizer,** which is identical to the ordinary behavior of Pytorch in the absence of mixed precision.\r\n\r\nTo ensure that FP16 is used for operations that benefit from it, `Tensor.*` methods and `torch.*` and `torch.nn.functional.*` functions are patched to cast data to a certain type before running the actual op.  Which type depends on what precision is best for that op.  For example, `torch.mm` is patched to cast the incoming input and weight to FP16, which enables Tensor Cores.  `torch.log` is patched to cast input to fp32, because log's forward and backward may require a large dynamic range.  This casting-as-data-flows-through-functions is the strategy used by Amp in Apex today, and achieves accuracy comparable to pure FP32 training on a wide range of networks.  It is also the strategy used by MXNet and Tensorflow's Amp integrations.  However, Apex's Amp is implemented by Python-side monkey-patching of `torch.*` and `torch.nn.functional.*` functions, which is invasive and not ideal for performance.\r\n\r\n### Proposed Implementation\r\n\r\nFor eager execution, we propose to integrate the same casting-as-data-flows-through-functions strategy that Apex's Amp uses.  We propose to implement this by inserting the casts in some of the autogenerated C++ functions on the Cuda dispatch path.  Each such function will be given a few additional lines by the autogeneration script.  For example, a whilelist function with 1 argument would be given something like\r\n```python\r\nif(autocasting_is_enabled())\r\n  input = input.half()\r\n```\r\nThese casts should be autograd-exposed, so they will be reversed in backward().  They should also precede the autograd-exposed call of the whitelist or blacklist op itself, so if the op saves its inputs for backward, the inputs are saved as the correct (post-cast) type.\r\n\r\nOn the Python side, the user shall request auto-casting by running the forward pass (or any portions of the forward pass where auto-casting is desired) under a nestable context manager, for example\r\n```python\r\n@contextlib.contextmanager\r\ndef autocast(whitelist_type=torch.float16, enabled=True):\r\n    old_whitelist_type, old_status = torch.get_autocasting_state()\r\n    torch.set_autocasting_state(whitelist_type, enabled)\r\n    try:\r\n        yield\r\n    finally:\r\n        torch.set_autocasting_state(original_whitelist_type, old_status)\r\n```\r\n`torch.get_` and `set_autocasting_state` will get/set a backend state that is queryable within C++ dispatch functions.\r\n\r\n`whitelist_type` can be changed to request that whitelist functions be autocast to types other than FP16.  The `enabled` argument can be used to locally disable autocasting if the user wishes to have manual control over the types that are used in some regions of their model, while permitting autocasting in others.\r\n\r\nMy initial thought is that `with autocast()` may be used to wrap any regions of code where a graph is being constructed via explicit Python invocations of Pytorch ops (ie, the forward pass), but shall not wrap any region where a previously constructed graph is being backwarded through.  All desired casting will have been recorded as part of the graph construction, and will be correctly reversed by a bare backward call without needing to be under an `autocast` context.  Backward passes with `create_graph=True` also belong to the latter category (ie, they should not be under a `with autocast()` context). [Gradient Penalty](#gradient-penalty) under End to End Examples below shows this philosophy more clearly.\r\n\r\nIt's possible that running both forward and backward under the context manager won't do any harm (ie, any resulting casts requested during backward will be no-ops, because the type flow is already being properly/faithfully reversed by autograd) and it can be permissible (but not required) to also allow the backward pass to take place under the context manager.\r\n\r\nWhen training with FP16, gradient scaling and auto-casting must both be requested by the user.  We envision that when autocasting to formats other than FP16, gradient scaling will not be necessary, and the auto-casting context manager can be used without any gradient scaling.\r\n\r\n### Example Usage\r\n```python\r\nwith autocast():\r\n    output = model(input)\r\n    loss = loss_fn(output, target)\r\n# The backward pass should be invoked outside the context manager.  All casting has been appropriately recorded as part of the forward pass.\r\n```\r\nWithin `model.forward`, if the user has regions where they wish explicit control over the precision, they may nest an invocation of `with autocast(enabled=False)`:\r\n```python\r\ndef forward(self, x):\r\n    x = self.layer_permitting_autocasting(x)\r\n    with autocast(enabled=False):\r\n        x = x.float()\r\n        x = self.explicitly_float_layer(x)\r\n    x = self.another_layer_permitting_autocasting(x)\r\n    return x\r\n```\r\n\r\n### Gotchas/Challenges\r\n\r\nThe Amp casts need to be recorded by autograd, so they'll be properly reversed in backward.  Unfortunately, for many ops, dispatch into an autograd-disabled region currently occurs in `VariableType*.cpp`, at a higher level of the call chain than the Cuda-specific dispatch functions.  `VariableType*.cpp` is also the level at which necessary data is saved for backward.  In other words, by the time we've reached the Cuda-specific dispatch functions, it's too late to invoke autograd-exposed casts.  The alternative is to insert the\r\n```python\r\nif(autocasting_is_enabled())\r\n  input = input.half() or float()\r\n```\r\nsnippets at the level of `VariableType*.cpp`, before we dive into autograd-disabled regions, but then these if statements will be on the hot path taken by all dispatches (Cuda and non-Cuda).  I'd like to avoid having the if statement on any non-Cuda code path.  This is a tricky problem and I need to think hard about it.\r\n\r\n## Gradient scaling\r\n\r\n### Background\r\n\r\nLate in training, FP16 gradients can underflow, halting convergence and in some cases causing destabilization.  Apex's Amp mitigates underflow via ""dynamic gradient scaling.""  The implementation creates scaled gradients by handing the user `scaled_loss  =  loss*scale_factor`, then requiring that the user invoke `scaled_loss.backward()`. By the chain rule, all gradients flowing backward through the network are then scaled by `scale_factor`.  Apex's Amp attempts to maximize use of FP16's full dynamic range by choosing the highest `scale_factor` that can be used without incurring inf/nan gradients, which is accomplished as follows:  Initially, a high `scale_factor` is chosen.  Each iteration, after backward() returns, gradients are checked for infs/nans.  If any infs/nans are found, the optimizer skips the step, and the scale factor is reduced.  The scale factor is also periodically increased if a streak of successful (inf/nan free) iterations occurs.  Gradients are unscaled in FP32 before being applied to FP32 model parameters.\r\n\r\n### Proposed API\r\n\r\nUser scripts will implement gradient scaling with an instance of a helper class.  Typical use would look like\r\n```python\r\nscaler = torch.cuda.amp.AmpScaler()\r\n\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    output = model(input)\r\n    loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\nCreating scaled gradients with `torch.autograd.backward` would look like\r\n```python\r\ntorch.autograd.backward(scaler.scale((output0, output1)), grad_tensors=(grad0, grad1))\r\n```\r\n\r\nCreating scaled gradients with `torch.autograd.grad` would look like\r\n```python\r\ntorch.autograd.grad(scaler.scale((output0, output1)), model.parameters(), grad_outputs=(grad0, grad1))\r\n```\r\n\r\n[The best explanation for `AmpScaler` is the class itself, as found in the gradient scaling PR](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L6).  It's lightweight and each function is documented.\r\n\r\n`scaler` internally initializes and maintains the scale value, and updates it each iteration based on whether `optimizer`'s gradients contained infs or NaNs.  If infs/NaN gradients are encountered in a given iteration, `scaler.update` reduces the scale.  If no inf/NaN gradients are encountered, `scaler.update` increases the scale slightly.  This approach achieves what dynamic gradient scaling intends:  over time, riding the edge of the highest gradient scale that can be used without incurring overflow.\r\n\r\nThe user also has the freedom to manually reset the scale value at the end of any iteration, by passing a new scale to `scaler.update` as a Tensor or Python float.\r\n\r\n`AmpScaler` instances contain at most a few Tensors and Python scalars.  When checkpointing, the user can easily save an `AmpScaler` instance as part of the checkpoint, alongside `model.state_dict()` and `optimizer.state_dict()`.  The model and optimizer instance(s) are not affected by AmpScaler, and remain exactly what they would be in the absence of mixed precision.  Therefore, behaviors and invocations of `model.state_dict()` and `optimizer.state_dict()` themselves may remain unaltered.\r\n\r\n### Interaction with Existing Optimizers\r\n\r\nAs long as training scripts adhere to the proposed API, existing optimizers (whether native or custom) will immediately work with mixed precision.  In particular, the gradient scaling API does not rely on changes to the Python source of `step()` in any existing optimizers.\r\n\r\n`scaler.step(optimizer)` [wraps `optimizer.step()` with logic to make it scaling-safe](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L146-L192).  Specifically, `scaler.step(optimizer)`\r\n- makes sure gradients are unscaled before `optimizer.step()`\r\n- skips `optimizer.step()` if inf/nan gradients are found.\r\n\r\n`optimizer.step()` itself is untouched, and again, does not need to change for existing optimizers.\r\n\r\n### Interaction with New Custom Optimizers\r\n\r\n`AmpScaler` defines a contract such that custom optimizers _may_ implement their own scaling-safe `step` methods if they choose to.  If an optimizer obeys this contract, [`AmpScaler.step` will call the optimizer's `step` method directly](https://github.com/pytorch/pytorch/blob/35829f24ef2a71eacedcd6307b2fe9325e4a6a94/torch/cuda/amp/amp_scaler.py#L179-L184).  This gives custom optimizer authors a control point to implement ninja optimizations like sync-free dynamic gradient scaling.\r\n\r\n### Gotchas/Challenges\r\n\r\nWhen the user invokes a backward pass with a scale factor, all gradients produced by this backward pass (leaf gradients produced by `loss.backward` or `torch.autograd.backward`, or out-of-place gradients produced by `torch.autograd.grad`) will be scaled.  Therefore, anything that manipulates the gradients between `scaler.scale(loss).backward()` and the `scaler.step(optimizer)` will require proper awareness of the scale factor.  Examples of operations that require scale-factor-aware treatment are \r\n- gradient clipping\r\n- gradient penalty computations.\r\n\r\nIn our opinion, requiring the user be aware of the scale factor when making direct use of the gradients is not terribly burdensome.  The user has explicitly requested gradient scaling; they should not be surprised when they end up with scaled gradients on their hands.  The treatment of such cases is also not difficult from a code-change perspective, as long as it is clearly documented.\r\n\r\nClosures are also a challenge.  Do we need to support explicit closure use?  Based on the scripts and issues I've seen, closure use is not terribly common, and LBFGS is the only native optimizer that requires a closure.  However, I think I can torture the proposed API into supporting closures if it turns out to be in high demand.\r\n\r\n## End to End Examples (Auto-Casting + Gradient Scaling)\r\n\r\n### Typical Use (1 loss, 1 optimizer)\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Gradient Clipping\r\n\r\nGradient clipping requires awareness that the gradients resulting from `scaler.scale(loss).backward()` are scaled. One simple way to account for the scale factor is by clipping to `max_norm*scaler.get_scale()` instead of max_norm:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n\r\n    # Gradients are scaled, so we clip to max_norm*scale\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm*scaler.get_scale())\r\n\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\nHere the scaled gradients are clipped. `scaler.step(optimizer)` is aware that gradients have not yet been unscaled, and unscales them under the hood before calling `optimizer.step()`.\r\n\r\n### Gradient Clipping with Explicit Unscaling\r\n\r\nThe specific case of clipping scaled gradients isn\u2019t so hard (all you have to do is clip to `max_norm*scaler.get_scale()`). However, in general, between the backward pass and the optimizer step you may wish to manipulate gradients in some way that\u2019s not so easy to translate to scaled gradients. In such cases, you can unscale and step separately. Here\u2019s how that looks, using gradient clipping as an example once more:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n\r\n    scaler.unscale(optimizer)\r\n    # Since the optimizer's owned gradients are unscaled, we can clip to max_norm directly:\r\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\r\n\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Gradient Penalty\r\n(based on **Higher order gradients** from the [release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0))\r\n\r\nGradient penalty also requires awareness that the gradients are scaled in certain places. Additionally, gradient penalty demonstrates:\r\n- `torch.autograd.grad`\r\n- Double-backward\r\n- Some clarification of what counts as a ""forward pass"" for the purpose of using `with autocast` (in other words, when exactly it's appropriate to use `with autocast`).\r\n\r\nThe following shows an implementation of gradient penalty under the proposed API.\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n\r\n    # We should scale outputs for the out-of-place backward pass\r\n    grad_params = torch.autograd.grad(scaler.scale(loss), model.parameters(), create_graph=True)\r\n\r\n    # In general, the penalty term may depend nonlinearly on the out-of-place gradients, so to be safe,\r\n    # manually unscale them before computing the penalty.  This unscale should be autograd-exposed.\r\n    grad_params = [p*(1./scaler.get_scale()) for p in grad_params]\r\n\r\n    # Compute the penalty term and add it to the loss.\r\n    # The penalty term computation is effectively another snippet of forward pass, so it makes\r\n    # sense to enable autocasting for this section as well:\r\n    with autocast():\r\n        grad_norm = 0\r\n        for grad in grad_params:\r\n            grad_norm += grad.pow(2).sum()\r\n        grad_norm = grad_norm.sqrt()\r\n        loss = loss + grad_norm\r\n\r\n    # The usual scaling for backward will now accumulate leaf gradients that are appropriately scaled.\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\nGradient penalty is a tricky case to think about but writing the code is simple once the right pattern is established.  Compared to the [example in the release notes](https://github.com/pytorch/pytorch/releases?after=v0.3.0), the only extra line for gradient penalty computation is the unscaling `grad_params = [p*(1./scaler.get_scale()) for p in grad_params]`.  I think this can be considered a documentation problem, and addressed by providing clear examples.\r\n\r\n### Multiple Models/Optimizers/Losses\r\n\r\nNetworks must use the same `AmpScaler` instance (and therefore the same scale) to create gradients for all backward passes in a given iteration, otherwise we open the door to nasty corner cases.  For example, if two different losses, with different gradient scales, accumulate into the same parameters' .grads, the accumulation math breaks.  If two different losses, with different gradient scales, accumulate into different parameters owned by the same optimizer, then when you invoke `scaler.unscale(optimizer)`, there's no single correct value that can be used to unscale all the gradients owned by that optimizer, and handling multiple scale factors for different parameters within the same optimizer would get ugly fast.  Requiring that networks use the same `AmpScaler` instance for all backward passes avoids all such control flow difficulties, while still achieving what loss scaling is meant to achieve.\r\n\r\n`scaler.update()` must be called only at the end of the iteration, after `scaler.step(optimizer)` has been called for all optimizers used this iteration.  This requirement allows `update` to account for infs/nans found among any of the optimizers' gradients.\r\n\r\n```python\r\nscaler = torch.cuda.amp.AmpScaler()\r\n...\r\nfor input, target in data:\r\n    optimizer0.zero_grad()\r\n    optimizer1.zero_grad()\r\n    with autocast():\r\n        output0 = model0(input)\r\n        output1 = model1(input)\r\n        loss0 = loss_fn(2 * output0 + 3 * output1, target)\r\n        loss1 = loss_fn(3 * output0 - 5 * output1, target)\r\n\r\n    scaler.scale(loss0).backward(retain_graph=True)\r\n    scaler.scale(loss1).backward()\r\n\r\n    # Users can choose which optimizers receive explicit unscaling\r\n    scaler.unscale(optimizer0)\r\n\r\n    scaler.step(optimizer0)\r\n    scaler.step(optimizer1)\r\n    scaler.update()\r\n```\r\n\r\nI had to write Apex's Amp to handle arbitrary combinations of multiple models/optimizers/losses.  I'm painfully aware of the complicated combinations of models/optimizers/losses people want to implement.  In my opinion, the proposed interface permits a great deal of flexibility in network design.\r\n\r\n### Gradient accumulation\r\n\r\nGradient accumulation across iterations (between steps) is a common use case.  The proposed API accommodates gradient accumulation without trouble:\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor i, (input, target) in enumerate(data):\r\n    with autocast():\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n        loss = loss/iters_to_accumulate\r\n    scaler.scale(loss).backward()\r\n    if (i + 1) % iters_to_accumulate == 0:\r\n        scaler.step(optimizer)\r\n        scaler.update()\r\n        optimizer.zero_grad()\r\n```\r\n\r\n### Switching automatic mixed precision on and off\r\n\r\nIf users want to run with or without autocasting+gradient scaling, they shouldn't have to litter their code with if statements.  The API should allow one code path that accommodates easily switching autocasting+gradient scaling on and off.\r\n\r\nThe autocasting context manager and `AmpScaler` constructor provide such convenience by accepting an `enabled=False` argument.\r\n\r\nIn the following example, autocasting and gradient scaling can be switched on and off by flipping `args.use_mixed_precision` with no additional control flow required.\r\n\r\n```python\r\nscaler = AmpScaler(enabled=args.use_mixed_precision)\r\n...\r\nfor input, target in data:\r\n    optimizer.zero_grad()\r\n    with autocast(enabled=args.use_mixed_precision):\r\n        output = model(input)\r\n        loss = loss_fn(output, target)\r\n    scaler.scale(loss).backward()\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n### Batch replay\r\n\r\nSometimes every iteration/batch is valuable enough that users don't want to skip any.  Instead, it's preferable to replay the batch with a reduced loss scale until gradients do not contain infs/nans.  Batch replay control flow is not provided by the API alone, but with the proposed gradient scaling PR, it would be easy to rig:\r\n\r\n```python\r\nscaler = AmpScaler()\r\n...\r\nfor input, target in data:\r\n    # Replay the batch, updating the scale if necessary, until we receive gradients that aren't inf/nan.\r\n    while True:\r\n        optimizer.zero_grad()\r\n        with autocast():\r\n            output = model(input)\r\n            loss = loss_fn(output, target)\r\n        scaler.scale(loss).backward()\r\n        scaler.unscale(optimizer)\r\n        if scaler._found_inf(optimizer).item():\r\n            scaler.update()\r\n        else:\r\n            break\r\n    scaler.step(optimizer)\r\n    scaler.update()\r\n```\r\n\r\n# Alternatives\r\n\r\n### Python-side alternatives for gradient scaling and unscaling\r\n\r\nThe supplementary information contains an in-depth discussion of some alternatives I considered for the [gradient scaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-scaling_apis_considered-md) and [gradient unscaling](https://gist.github.com/mcarilli/43445260404c8d7cd79d84439808250e#file-unscaling_apis_considered-md) API. \r\n\r\n### Gradient scaling in the autograd backend\r\n\r\nI recently submitted [a PR](https://github.com/pytorch/pytorch/pull/24893) that implemented gradient scaling directly in the autograd engine (Engine::execute).\r\n\r\nBenefits:\r\n - It automatically enabled gradient scaling for all typical single- and double-backward use cases (including complex cases like gradient penalties) without requiring any change to user scripts.\r\n- Gradients returned visibly to the user, either in-place via `.grad` attributes or out-of-place via a call to `torch.autograd.grad`, were only ever unscaled, eliminating the need to change gradient clipping or manually unscale before computing gradient penalties.\r\n\r\nDrawbacks:\r\n- Modifying engine.cpp, especially for a GPU-specific purpose, is not to be done lightly (ie, not until alternatives are exhausted).\r\n- It's unclear what the user-facing API would look like.  I figured the implementation was general enough to permit many options, and the exact API could be nailed down later.  It certainly requires maintaining a global ""amp state.""\r\n- Altering the backend to do black-box gradient scaling and require no change to user scripts is a double-edged sword.  Explicit Python-side control and visibility of gradient scaling, as we propose above, is not a bad thing.  @cbcase, this seems like an instance of the https://www.jwz.org/doc/worse-is-better.html thing you told me about...\r\n\r\ncc @ezyang @gchanan @vincentqb\r\n\r\n",python\r\nif(autocasting_is_enabled())\r\n  input = input.half()\r\n
25030,"torch.multinomial samples same elements multiple times when replacement=False## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSometimes a sampled index is drawn multiple time in a row even if replacement=False on `torch.multinomial`. This happens when an element of `input` is very small but not zero.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nHere is a minimal working example:\r\n\r\n.\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nRaise an error because one element of `input` is almost zero.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] torch==1.2.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.3.0\r\n[conda] Could not collect\r\n\n\ncc @ezyang @gchanan @zou3519 @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @nikitaved",high priority|module: distributions|triaged|module: correctness (silent),zou3519,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nSometimes a sampled index is drawn multiple time in a row even if replacement=False on `torch.multinomial`. This happens when an element of `input` is very small but not zero.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nHere is a minimal working example:\r\n```python \r\nimport torch\r\ntorch.manual_seed(0)\r\nw = torch.tensor( [1.2899e-01, 6.2532e-01, 3.6483e-02, 1.5196e-01, 2.9675e-03, \r\n4.9773e-03,4.5881e-02, 2.9019e-03, 5.2139e-04, 1.5281e-17] )\r\nprint(torch.multinomial(w, 10, replacement=False))\r\n```\r\n.\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nRaise an error because one element of `input` is almost zero.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] torch==1.2.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.3.0\r\n[conda] Could not collect\r\n\n\ncc @ezyang @gchanan @zou3519 @vincentqb @fritzo @neerajprad @alicanb @vishwakftw @nikitaved","python \r\nimport torch\r\ntorch.manual_seed(0)\r\nw = torch.tensor( [1.2899e-01, 6.2532e-01, 3.6483e-02, 1.5196e-01, 2.9675e-03, \r\n4.9773e-03,4.5881e-02, 2.9019e-03, 5.2139e-04, 1.5281e-17] )\r\nprint(torch.multinomial(w, 10, replacement=False))\r\n"
24839,"ScriptModule construction error when PYTORCH_JIT=0## \U0001f41b Bug\r\n\r\nIn order to debug code I usually set the environment variable `PYTORCH_JIT=0`. This works as long as I don't have any `ScriptModule`. As soon as a `ScriptModule` is constructed, I get the following error:\r\n\r\n```\r\nRuntimeError: _construct is not supported on ScriptModules\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nExecute the following command:\r\n\r\n\r\n## Expected behavior\r\n\r\nNo error should be thrown and the code should run without JIT\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-ignite==0.2.0\r\n[pip3] torch==1.2.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] Could not collect\r\n```\r\n\n\ncc @suo",oncall: jit|triaged,suo,"## \U0001f41b Bug\r\n\r\nIn order to debug code I usually set the environment variable `PYTORCH_JIT=0`. This works as long as I don't have any `ScriptModule`. As soon as a `ScriptModule` is constructed, I get the following error:\r\n\r\n```\r\nRuntimeError: _construct is not supported on ScriptModules\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nExecute the following command:\r\n\r\n```bash\r\nPYTORCH_JIT=0 python <<EOF\r\nfrom torch.jit import ScriptModule\r\n\r\nclass AModule(ScriptModule):\r\n    def forward(self, *input):\r\n        pass\r\n\r\nAModule()\r\nEOF\r\n```\r\n## Expected behavior\r\n\r\nNo error should be thrown and the code should run without JIT\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-ignite==0.2.0\r\n[pip3] torch==1.2.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] Could not collect\r\n```\r\n\n\ncc @suo","bash\r\nPYTORCH_JIT=0 python <<EOF\r\nfrom torch.jit import ScriptModule\r\n\r\nclass AModule(ScriptModule):\r\n    def forward(self, *input):\r\n        pass\r\n\r\nAModule()\r\nEOF\r\n"
24823,"Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results_This issue is an expansion of the issue reported in https://github.com/pytorch/pytorch/issues/19826.\r\nThe discussion there diagnoses the segfault that occurs in the vectorized 2D CPU kernel. This issue covers the wider problematic handling of `NaN` and `inf` in all versions of `grid_sample` kernels. For details on `inf`, see the comment below._\r\n\r\n### Summary\r\n\r\nThe `grid_sample` function does not have proper handling of `NaN` values for in its grid input.\r\n\r\nThe 2D CPU version segfaults under certain conditions and parameters, as described in https://github.com/pytorch/pytorch/issues/19826, and with simplified examples below.\r\n\r\nThe other `grid_sample` kernels (3D CPU, and 2D/3D CUDA) do not segfault, but produce incorrect results under certain conditions when the grid contains a `NaN` value.\r\n\r\nProper handling would place a `NaN` in the output for every grid location that has a `NaN`.\r\n\r\n### Segmentation fault in the CPU 2D kernel\r\n\r\nThis is covered and diagnosed by @SsnL at https://github.com/pytorch/pytorch/issues/19826, but I want to provide a simple example to reproduce the segfault behavior, and expand on the exact conditions in which it occurs.\r\n\r\n- Here is a simple example to reproduce the segmentation fault:\r\n\r\n\r\n- This segfault does not, however, happen if both components of a grid point are `NaN`.\r\nExample:\r\n\r\nwhich is, in fact, the correct and desired behavior.\r\n\r\n- The segfault occurs for padding modes `border` and `reflection`, but not for `zeros` (where it works correctly).\r\n\r\n\r\n### The CUDA kernels (both 2D and 3D)\r\n\r\nThe CUDA kernel does not segfault. However, in `border` padding mode, it produces an incorrect result as if the `NaN` value were a `-1`.\r\nExample:\r\n\r\nNotice the result at the central output pixel. It behaves as if the `NaN` values of the grid were actually `-1`. Unlike the `border` padding mode, however, the `zeros` and `reflection` modes work correctly (produce a `NaN` in that pixel).\r\n\r\n\r\n### The 3D CPU kernel\r\n\r\n- The 3D CPU implementation also does not segfault, but unlike in the CUDA version, a `NaN` value is effectively treated as if it were a `+1` under the `border` padding mode.\r\n\r\nNotice the result in the central output pixel is `13.` in the first case (as if the grid there is `[0,0,+1]`), and `1.` in the second case (as if the grid is `[+1,+1,+1]`). As mentioned above, the same thing on CUDA results in a `15.` and a `27.` (as if the grid at that point were `[0,0,-1]` and `[-1,-1,-1]`, respectively).\r\nThat output pixel should just be a `NaN`.\r\n\r\n- The `zeros` and `reflection` padding modes on 3D CPU always produce a `0.` result in the output wherever there is a `NaN` in the grid. I am not yet sure how to explain this last one.\r\n\r\n### Desired behavior\r\n\r\nEvery pixel in the output for which the corresponding grid point has a `NaN` in one of its components should come out to be a `NaN`.\r\n\r\nAn alternative behavior (not advocating for this - I'm just presenting it as an option, for completeness) is to fill in a border value wherever there is a `NaN`. For the `zero` padding mode, this would fill in a `0.`. For the `border` and `reflection` padding modes, it's not clear how this would work.\r\n\r\nIn any case, the behaviors should be standardized across the different kernels.\r\n\r\n### (Partial) Diagnoses\r\n\r\nThe 2D CPU segfault issue is diagnosed in https://github.com/pytorch/pytorch/issues/19826, and I think I have a decent idea of what's going on in the CUDA and 3D CPU `border` mode. The 3D CPU `zeros` and `reflection` modes might need another look at the code to diagnose.\r\n\r\nI thought it would be good to write out all these cases explicitly, as I did above, since it helps for reproducing these errors and fixing them.\r\nOnce they're fixed, this will also be a good list of test cases to verify that the issues are indeed fixed.\r\n\r\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel",high priority|module: crash|module: cuda|triaged|module: interpolation,izdeby,"_This issue is an expansion of the issue reported in https://github.com/pytorch/pytorch/issues/19826.\r\nThe discussion there diagnoses the segfault that occurs in the vectorized 2D CPU kernel. This issue covers the wider problematic handling of `NaN` and `inf` in all versions of `grid_sample` kernels. For details on `inf`, see the comment below._\r\n\r\n### Summary\r\n\r\nThe `grid_sample` function does not have proper handling of `NaN` values for in its grid input.\r\n\r\nThe 2D CPU version segfaults under certain conditions and parameters, as described in https://github.com/pytorch/pytorch/issues/19826, and with simplified examples below.\r\n\r\nThe other `grid_sample` kernels (3D CPU, and 2D/3D CUDA) do not segfault, but produce incorrect results under certain conditions when the grid contains a `NaN` value.\r\n\r\nProper handling would place a `NaN` in the output for every grid location that has a `NaN`.\r\n\r\n### Segmentation fault in the CPU 2D kernel\r\n\r\nThis is covered and diagnosed by @SsnL at https://github.com/pytorch/pytorch/issues/19826, but I want to provide a simple example to reproduce the segfault behavior, and expand on the exact conditions in which it occurs.\r\n\r\n- Here is a simple example to reproduce the segmentation fault:\r\n```python\r\n>>> image = torch.rand(1, 1, 3, 3, device='cpu')\r\n>>> grid = torch.rand(1, 3, 3, 2, device='cpu')\r\n>>> grid[:,1,1,0] = float('nan')\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border')\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n- This segfault does not, however, happen if both components of a grid point are `NaN`.\r\nExample:\r\n```python\r\n>>> image = torch.rand(1, 1, 3, 3, device='cpu')\r\n>>> grid = torch.rand(1, 3, 3, 2, device='cpu')\r\n>>> grid[:,1,1,:] = float('nan')\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border')\r\ntensor([[[[0.2587, 0.1807, 0.2114],\r\n          [0.1993,    nan, 0.2673],\r\n          [0.2065, 0.1258, 0.2002]]]])\r\n```\r\nwhich is, in fact, the correct and desired behavior.\r\n\r\n- The segfault occurs for padding modes `border` and `reflection`, but not for `zeros` (where it works correctly).\r\n\r\n\r\n### The CUDA kernels (both 2D and 3D)\r\n\r\nThe CUDA kernel does not segfault. However, in `border` padding mode, it produces an incorrect result as if the `NaN` value were a `-1`.\r\nExample:\r\n```python\r\n>>> image = torch.arange(9, 0, -1, dtype=torch.float, device='cuda').view(1,1,3,3)\r\ntensor([[[[9., 8., 7.],\r\n          [6., 5., 4.],\r\n          [3., 2., 1.]]]], device='cuda:0')\r\n# set grid to identity. Note: for old versions, drop the align_corners option\r\n>>> grid = torch.nn.functional.affine_grid(torch.tensor([[[1.,0.,0.],[0.,1.,0.]]], device='cuda'), (1,1,3,3), align_corners=True)\r\n>>> grid[:,1,1,0] = float('nan')  # set the x-coordinate of the central grid point to NaN\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border', align_corners=True)\r\ntensor([[[[9., 8., 7.],\r\n          [6., 6., 4.],\r\n          [3., 2., 1.]]]], device='cuda:0')\r\n>>> grid[:,1,1,:] = float('nan')  # set both coordinates of the central grid point to NaN\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border', align_corners=True)\r\ntensor([[[[9., 8., 7.],\r\n          [6., 9., 4.],\r\n          [3., 2., 1.]]]], device='cuda:0')\r\n```\r\nNotice the result at the central output pixel. It behaves as if the `NaN` values of the grid were actually `-1`. Unlike the `border` padding mode, however, the `zeros` and `reflection` modes work correctly (produce a `NaN` in that pixel).\r\n\r\n\r\n### The 3D CPU kernel\r\n\r\n- The 3D CPU implementation also does not segfault, but unlike in the CUDA version, a `NaN` value is effectively treated as if it were a `+1` under the `border` padding mode.\r\n```python\r\n>>> image = torch.arange(27, 0, -1, dtype=torch.float, device='cpu').view(1,1,3,3,3)\r\ntensor([[[[[27., 26., 25.],\r\n           [24., 23., 22.],\r\n           [21., 20., 19.]],\r\n\r\n          [[18., 17., 16.],\r\n           [15., 14., 13.],\r\n           [12., 11., 10.]],\r\n\r\n          [[ 9.,  8.,  7.],\r\n           [ 6.,  5.,  4.],\r\n           [ 3.,  2.,  1.]]]]])\r\n# set grid to identity. Note: for old versions, drop the align_corners option\r\n>>> grid = torch.nn.functional.affine_grid(torch.tensor([[[1.,0.,0.,0.],[0.,1.,0.,0.],[0.,0.,1.,0.]]], device='cpu'), (1,1,3,3,3), align_corners=True)\r\n>>> grid[:,1,1,1,0] = float('nan')  # set the x-coordinate of the central grid point to NaN\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border', align_corners=True)\r\ntensor([[[[[27., 26., 25.],\r\n           [24., 23., 22.],\r\n           [21., 20., 19.]],\r\n\r\n          [[18., 17., 16.],\r\n           [15., 13., 13.],\r\n           [12., 11., 10.]],\r\n\r\n          [[ 9.,  8.,  7.],\r\n           [ 6.,  5.,  4.],\r\n           [ 3.,  2.,  1.]]]]])\r\n>>> grid[:,1,1,1,:] = float('nan')  # set all 3 coordinates of the central grid point to NaN\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border', align_corners=True)\r\ntensor([[[[[27., 26., 25.],\r\n           [24., 23., 22.],\r\n           [21., 20., 19.]],\r\n\r\n          [[18., 17., 16.],\r\n           [15.,  1., 13.],\r\n           [12., 11., 10.]],\r\n\r\n          [[ 9.,  8.,  7.],\r\n           [ 6.,  5.,  4.],\r\n           [ 3.,  2.,  1.]]]]])\r\n```\r\nNotice the result in the central output pixel is `13.` in the first case (as if the grid there is `[0,0,+1]`), and `1.` in the second case (as if the grid is `[+1,+1,+1]`). As mentioned above, the same thing on CUDA results in a `15.` and a `27.` (as if the grid at that point were `[0,0,-1]` and `[-1,-1,-1]`, respectively).\r\nThat output pixel should just be a `NaN`.\r\n\r\n- The `zeros` and `reflection` padding modes on 3D CPU always produce a `0.` result in the output wherever there is a `NaN` in the grid. I am not yet sure how to explain this last one.\r\n\r\n### Desired behavior\r\n\r\nEvery pixel in the output for which the corresponding grid point has a `NaN` in one of its components should come out to be a `NaN`.\r\n\r\nAn alternative behavior (not advocating for this - I'm just presenting it as an option, for completeness) is to fill in a border value wherever there is a `NaN`. For the `zero` padding mode, this would fill in a `0.`. For the `border` and `reflection` padding modes, it's not clear how this would work.\r\n\r\nIn any case, the behaviors should be standardized across the different kernels.\r\n\r\n### (Partial) Diagnoses\r\n\r\nThe 2D CPU segfault issue is diagnosed in https://github.com/pytorch/pytorch/issues/19826, and I think I have a decent idea of what's going on in the CUDA and 3D CPU `border` mode. The 3D CPU `zeros` and `reflection` modes might need another look at the code to diagnose.\r\n\r\nI thought it would be good to write out all these cases explicitly, as I did above, since it helps for reproducing these errors and fixing them.\r\nOnce they're fixed, this will also be a good list of test cases to verify that the issues are indeed fixed.\r\n\r\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @ngimel","python\r\n>>> image = torch.rand(1, 1, 3, 3, device='cpu')\r\n>>> grid = torch.rand(1, 3, 3, 2, device='cpu')\r\n>>> grid[:,1,1,0] = float('nan')\r\n>>> torch.nn.functional.grid_sample(image, grid, padding_mode='border')\r\nSegmentation fault (core dumped)\r\n"
24807,Pylint Error `torch.tensor is not callable`## \U0001f41b Bug\r\n\r\nPylint returns the error `torch.tensor is not callable`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n```\r\n>>> pipenv run pylint test.py\r\ntest.py:4:8: E1102: torch.tensor is not callable (not-callable)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis shall not raise pylint error\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPytorch installed in Pipenv Environment\r\npylint==2.3.1\r\n```\r\n\r\n## Additional context\r\n\r\nNone\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @bhosmer @smessmer @ljk53 @ailzhang @malfet @rgommers @xuzhao9 @gramster,high priority|module: internals|module: typing|triaged|small,pmeier,"## \U0001f41b Bug\r\n\r\nPylint returns the error `torch.tensor is not callable`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\n\r\nif __name__ == ""__main__"":\r\n    t = torch.tensor([1, 2, 3], dtype=torch.float64)\r\n```\r\n\r\n```\r\n>>> pipenv run pylint test.py\r\ntest.py:4:8: E1102: torch.tensor is not callable (not-callable)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis shall not raise pylint error\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPytorch installed in Pipenv Environment\r\npylint==2.3.1\r\n```\r\n\r\n## Additional context\r\n\r\nNone\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @bhosmer @smessmer @ljk53 @ailzhang @malfet @rgommers @xuzhao9 @gramster","python\r\nimport torch\r\n\r\nif __name__ == ""__main__"":\r\n    t = torch.tensor([1, 2, 3], dtype=torch.float64)\r\n"
24806,"Typing Error for ConstantPad## \U0001f41b Bug\r\n\r\nThere is a Typing Error for Argument 1 of ConstantPad classes when using different paddings for different sides.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n```\r\n>>> pipenv run mypy test.py\r\n\r\ntest.py:4: error: Argument 1 to ""ConstantPad1d"" has incompatible type ""Tuple[int, int]""; expected ""Union[int, Tuple[int]]""\r\ntest.py:5: error: Argument 1 to ""ConstantPad2d"" has incompatible type ""Tuple[int, int, int, int]""; expected ""Union[int, Tuple[int, int]]""\r\ntest.py:6: error: Argument 1 to ""ConstantPad3d"" has incompatible type ""Tuple[int, int, int, int, int, int]""; expected ""Union[int, Tuple[int, int, int]]""\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis shall not raise mypy error\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPytorch installed in Pipenv Environment\r\nmypy==0.720\r\n```\r\n\r\n## Additional context\r\n\r\nNone\n\ncc @ezyang",module: typing|triaged|small,ezyang,"## \U0001f41b Bug\r\n\r\nThere is a Typing Error for Argument 1 of ConstantPad classes when using different paddings for different sides.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch.nn as nn\r\n\r\nif __name__ == ""__main__"":\r\n    t1 = nn.ConstantPad1d((3, 1), 3.5)\r\n    t2 = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\r\n    t3 = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)\r\n```\r\n\r\n```\r\n>>> pipenv run mypy test.py\r\n\r\ntest.py:4: error: Argument 1 to ""ConstantPad1d"" has incompatible type ""Tuple[int, int]""; expected ""Union[int, Tuple[int]]""\r\ntest.py:5: error: Argument 1 to ""ConstantPad2d"" has incompatible type ""Tuple[int, int, int, int]""; expected ""Union[int, Tuple[int, int]]""\r\ntest.py:6: error: Argument 1 to ""ConstantPad3d"" has incompatible type ""Tuple[int, int, int, int, int, int]""; expected ""Union[int, Tuple[int, int, int]]""\r\n```\r\n\r\n## Expected behavior\r\n\r\nThis shall not raise mypy error\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.2.0\r\n[conda] Could not collect\r\n\r\nPytorch installed in Pipenv Environment\r\nmypy==0.720\r\n```\r\n\r\n## Additional context\r\n\r\nNone\n\ncc @ezyang","python\r\nimport torch.nn as nn\r\n\r\nif __name__ == ""__main__"":\r\n    t1 = nn.ConstantPad1d((3, 1), 3.5)\r\n    t2 = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\r\n    t3 = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)\r\n"
24429,"Example in torchscript documentation does not work## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to [doc](https://pytorch.org/docs/stable/jit.html#torch.jit.trace) and check for code example\r\n1. Copy/paste into fresh install venv woth pytorch 1.2\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\nError message\r\n\r\n\r\n## Expected behavior\r\n\r\nDoc example to work :) \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 430.14\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.0a0+6b959ee\r\n[conda] Could not collect\r\n",oncall: jit,driazati,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Go to [doc](https://pytorch.org/docs/stable/jit.html#torch.jit.trace) and check for code example\r\n1. Copy/paste into fresh install venv woth pytorch 1.2\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv = nn.Conv2d(1, 1, 3)\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n    def weighted_kernel_sum(self, weight):\r\n        return weight * self.conv.weight\r\n\r\nexample_weight = torch.rand(1, 1, 3, 3)\r\nexample_forward_input = torch.rand(1, 1, 3, 3)\r\nn = Net()\r\n# the following two calls are equivalent\r\nmodule = torch.jit.trace_module(n, example_forward_input)\r\n```\r\n\r\nError message\r\n```bash\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-bd56084c5306> in <module>\r\n     14 n = Net()\r\n     15 # the following two calls are equivalent\r\n---> 16 module = torch.jit.trace_module(n, example_forward_input)\r\n     17 module = torch.jit.trace_module(n.forward, example_forward_input)\r\n\r\n~/miniconda3/envs/ai/lib/python3.7/site-packages/torch/jit/__init__.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\r\n    894 \r\n    895     if not isinstance(inputs, dict):\r\n--> 896         raise AttributeError(""expected a dictionary of (method_name, input) pairs"")\r\n    897 \r\n    898     module = make_module(mod, _module_class, _compilation_unit)\r\n\r\nAttributeError: expected a dictionary of (method_name, input) pairs\r\n```\r\n\r\n## Expected behavior\r\n\r\nDoc example to work :) \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 430.14\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.0a0+6b959ee\r\n[conda] Could not collect\r\n","python\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv = nn.Conv2d(1, 1, 3)\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\n    def weighted_kernel_sum(self, weight):\r\n        return weight * self.conv.weight\r\n\r\nexample_weight = torch.rand(1, 1, 3, 3)\r\nexample_forward_input = torch.rand(1, 1, 3, 3)\r\nn = Net()\r\n# the following two calls are equivalent\r\nmodule = torch.jit.trace_module(n, example_forward_input)\r\n"
24413,"In-place modification detection does not work if a Tensor does not own its storagePyTorch checks if tensors have been modified in-place to ensure correctness of the backward operation, with `ctx.save_for_backward` and `ctx.saved_tensors` in the `forward` and `backward` methods of subclasses of `torch.autograd.Function`. This check does not appear to function when the saved tensor is not the 'owner' of its storage, e.g. when it is the result of a `view` or `transpose` operation.\r\n\r\nTo reproduce:\r\n\r\n\r\nExpected behaviour is for an exception to be raised on the line `y.backward(torch.rand_like(y))`, in the same manner as an exception is raised on the line `yy.backward(torch.rand_like(yy))`.\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7.4\n\ncc @ezyang @SsnL @albanD",module: autograd|triaged,albanD,"PyTorch checks if tensors have been modified in-place to ensure correctness of the backward operation, with `ctx.save_for_backward` and `ctx.saved_tensors` in the `forward` and `backward` methods of subclasses of `torch.autograd.Function`. This check does not appear to function when the saved tensor is not the 'owner' of its storage, e.g. when it is the result of a `view` or `transpose` operation.\r\n\r\nTo reproduce:\r\n```python\r\nimport torch\r\n\r\nclass f(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, tensor1):\r\n        # make sure we don't share storage with our input\r\n        # (just to be clear that that's not what's going wrong)\r\n        tensor2 = tensor1.clone()\r\n        # tensor3 shares storage with tensor2\r\n        # tensor3 = tensor2.view(-1) etc. would also demonstrate the problem\r\n        tensor3 = tensor2.t()\r\n        ctx.save_for_backward(tensor3)\r\n        return tensor3\r\n    @staticmethod\r\n    def backward(ctx, grad):\r\n        # trigger correctness check\r\n        _ = ctx.saved_tensors\r\n        return grad.t().clone()\r\n\r\n\r\nx = torch.rand(2, 3, requires_grad=True)\r\ny = f.apply(x)\r\ny += 1\r\n# Does not raise an error; this is the bug.\r\ny.backward(torch.rand_like(y))\r\n\r\n\r\n# In constrast, the following code correctly raises an error.\r\nclass ff(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, tensor1):\r\n        tensor2 = tensor1.clone()\r\n        ctx.save_for_backward(tensor2)\r\n        return tensor2\r\n    @staticmethod\r\n    def backward(ctx, grad):\r\n        _ = ctx.saved_tensors\r\n        return grad.clone()\r\n\r\nxx = torch.rand(2, 3, requires_grad=True)\r\nyy = ff.apply(xx)\r\nyy += 1\r\nyy.backward(torch.rand_like(yy))\r\n```\r\n\r\nExpected behaviour is for an exception to be raised on the line `y.backward(torch.rand_like(y))`, in the same manner as an exception is raised on the line `yy.backward(torch.rand_like(yy))`.\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7.4\n\ncc @ezyang @SsnL @albanD","python\r\nimport torch\r\n\r\nclass f(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, tensor1):\r\n        # make sure we don't share storage with our input\r\n        # (just to be clear that that's not what's going wrong)\r\n        tensor2 = tensor1.clone()\r\n        # tensor3 shares storage with tensor2\r\n        # tensor3 = tensor2.view(-1) etc. would also demonstrate the problem\r\n        tensor3 = tensor2.t()\r\n        ctx.save_for_backward(tensor3)\r\n        return tensor3\r\n    @staticmethod\r\n    def backward(ctx, grad):\r\n        # trigger correctness check\r\n        _ = ctx.saved_tensors\r\n        return grad.t().clone()\r\n\r\n\r\nx = torch.rand(2, 3, requires_grad=True)\r\ny = f.apply(x)\r\ny += 1\r\n# Does not raise an error; this is the bug.\r\ny.backward(torch.rand_like(y))\r\n\r\n\r\n# In constrast, the following code correctly raises an error.\r\nclass ff(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, tensor1):\r\n        tensor2 = tensor1.clone()\r\n        ctx.save_for_backward(tensor2)\r\n        return tensor2\r\n    @staticmethod\r\n    def backward(ctx, grad):\r\n        _ = ctx.saved_tensors\r\n        return grad.clone()\r\n\r\nxx = torch.rand(2, 3, requires_grad=True)\r\nyy = ff.apply(xx)\r\nyy += 1\r\nyy.backward(torch.rand_like(yy))\r\n"
24239,"JIT infers List[Tensor] inccorectly to List[int]## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nThis code throws an error below:\r\n\r\n\r\n\r\nThe equivalent code below works as expected\r\n\r\n`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n`[channel(torch.randn(1, 3, 3) > 0) for _ in [0, 1, 2]]` is expected to be `List[Tensor]` but is inferred as `List[int]` by JIT.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n* Ubuntu 16.04/MacOS Mojave\r\n* Python 3.7.4 (from conda)\r\n* PyTorch 1.2.0 (from conda)\r\n",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n```python\r\ndef channel(tensor: torch.Tensor) -> torch.Tensor:\r\n    a = torch.randn(2)\r\n    return a[tensor.long()]\r\n\r\n@torch.jit.script\r\ndef full():\r\n    torch.stack([channel(torch.randn(1, 3, 3) > 0) for _ in [0, 1, 2]])\r\n```\r\n\r\nThis code throws an error below:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 8, in <module>\r\n    @torch.jit.script\r\n  File ""/opt/.miniconda/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1181, in script\r\n    return _compile_function(fn=obj, qualified_name=qualified_name, _frames_up=_frames_up + 1, _rcb=_rcb)\r\n  File ""/opt/.miniconda/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1077, in _compile_function\r\n    script_fn = torch._C._jit_script_compile(qualified_name, ast, _rcb, get_default_args(fn))\r\nRuntimeError:\r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n\r\n  aten::stack(Tensor[] tensors, int dim=0) -> (Tensor):\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[int]'.\r\n\r\n  aten::stack(Tensor[] tensors, int dim=0, *, Tensor(a) out) -> (Tensor(a)):\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[int]'.\r\n\r\nThe original call is:\r\nat test.py:10:5\r\n@torch.jit.script\r\ndef full():\r\n    torch.stack([channel(torch.randn(1, 3, 3) > 0) for _ in [0, 1, 2]])\r\n    ~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nThe equivalent code below works as expected\r\n\r\n```python\r\ndef channel(tensor: torch.Tensor) -> torch.Tensor:\r\n    a = torch.randn(2)\r\n    return a[tensor.long()]\r\n\r\n@torch.jit.script\r\ndef full():\r\n    l = []\r\n    for _ in range(3):\r\n        l.append(channel(torch.randn(1, 3, 3) > 0))\r\n    torch.stack(l)\r\n````\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n`[channel(torch.randn(1, 3, 3) > 0) for _ in [0, 1, 2]]` is expected to be `List[Tensor]` but is inferred as `List[int]` by JIT.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n* Ubuntu 16.04/MacOS Mojave\r\n* Python 3.7.4 (from conda)\r\n* PyTorch 1.2.0 (from conda)\r\n","python\r\ndef channel(tensor: torch.Tensor) -> torch.Tensor:\r\n    a = torch.randn(2)\r\n    return a[tensor.long()]\r\n\r\n@torch.jit.script\r\ndef full():\r\n    torch.stack([channel(torch.randn(1, 3, 3) > 0) for _ in [0, 1, 2]])\r\n"
24229,"Confusing error message for Custom Class type mismatch## \U0001f41b Bug\r\n\r\nWhen using user-defined classes, it's easy to miss type annotation of members of the class (and Python won't error out). The only error occurs when the instance of the class is passed to one of the torchscript functions and it's pretty confusing by itself.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nProduces:\r\n```\r\n     17\r\n     18 p2 = Pair(1, 2)\r\n---> 19 print(sum_pair(p2))\r\n\r\nRuntimeError: sum_pair() Expected a value of type '__torch__.Pair' for argument 'p' but instead found type 'Pair'.\r\nPosition: 0\r\nValue: <__main__.Pair object at 0x7fb985f6f3c8>\r\nDeclaration: sum_pair(ClassType<Pair> p) -> (Tensor)\r\n```\r\n\r\n## Expected behavior\r\n\r\nBetter error message (ideally with individual fields names that mismatch) would be nice.\r\n\r\n\n\ncc @suo",oncall: jit|triaged|jit-backlog,suo,"## \U0001f41b Bug\r\n\r\nWhen using user-defined classes, it's easy to miss type annotation of members of the class (and Python won't error out). The only error occurs when the instance of the class is passed to one of the torchscript functions and it's pretty confusing by itself.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch import Tensor\r\n\r\n@torch.jit.script\r\nclass Pair:\r\n  def __init__(self, first, second):\r\n    self.first = first\r\n    self.second = second\r\n\r\n@torch.jit.script\r\ndef sum_pair(p):\r\n  # type: (Pair) -> Tensor\r\n  return p.first + p.second\r\n\r\n# works\r\np = Pair(torch.tensor([1]), torch.tensor([2]))\r\nprint(sum_pair(p))\r\n\r\n# errors\r\np2 = Pair(1, 2)\r\nprint(sum_pair(p2))\r\n```\r\n\r\nProduces:\r\n```\r\n     17\r\n     18 p2 = Pair(1, 2)\r\n---> 19 print(sum_pair(p2))\r\n\r\nRuntimeError: sum_pair() Expected a value of type '__torch__.Pair' for argument 'p' but instead found type 'Pair'.\r\nPosition: 0\r\nValue: <__main__.Pair object at 0x7fb985f6f3c8>\r\nDeclaration: sum_pair(ClassType<Pair> p) -> (Tensor)\r\n```\r\n\r\n## Expected behavior\r\n\r\nBetter error message (ideally with individual fields names that mismatch) would be nice.\r\n\r\n\n\ncc @suo","python\r\nimport torch\r\nfrom torch import Tensor\r\n\r\n@torch.jit.script\r\nclass Pair:\r\n  def __init__(self, first, second):\r\n    self.first = first\r\n    self.second = second\r\n\r\n@torch.jit.script\r\ndef sum_pair(p):\r\n  # type: (Pair) -> Tensor\r\n  return p.first + p.second\r\n\r\n# works\r\np = Pair(torch.tensor([1]), torch.tensor([2]))\r\nprint(sum_pair(p))\r\n\r\n# errors\r\np2 = Pair(1, 2)\r\nprint(sum_pair(p2))\r\n"
24173,Transformer model seems not supported in TorchScript?## \U0001f41b Bug\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torchtext==0.3.1\r\n[conda] _tflow_select             2.3.0                       mkl    defaults\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.4                      243    defaults\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0    defaults\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0    defaults\r\n[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] tensorflow                1.14.0          mkl_py36h2526735_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] tensorflow-base           1.14.0          mkl_py36h7ce6ba3_0    defaults\r\n,oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n```python\r\nPython builtin <built-in function hasattr> is currently not supported in Torchscript:\r\nat /home/chejin/anaconda3/envs/pytorch-gpu/lib/python3.6/site-packages/torch/nn/modules/activation.py:759:12\r\n        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\r\n        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\r\n\r\n        - Outputs:\r\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\r\n          E is the embedding dimension.\r\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\r\n          L is the target sequence length, S is the source sequence length.\r\n        """"""\r\n        if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:\r\n           ~~~~~~~ <--- HERE\r\n            return F.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n                self.in_proj_weight, self.in_proj_bias,\r\n                self.bias_k, self.bias_v, self.add_zero_attn,\r\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n                training=self.training,\r\n                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n                attn_mask=attn_mask, use_separate_proj_weight=True,\r\n                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,'__module__.__torch__.torch.nn.modules.activation.MultiheadAttention.forward' is being compiled since it was called from '__modul\r\ne__.__torch__.torch.nn.modules.transformer.TransformerEncoderLayer.forward'\r\n```\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 430.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torchtext==0.3.1\r\n[conda] _tflow_select             2.3.0                       mkl    defaults\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] cuda100                   1.0                           0    pytorch\r\n[conda] mkl                       2019.4                      243    defaults\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0    defaults\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0    defaults\r\n[conda] pytorch                   1.2.0           py3.6_cuda10.0.130_cudnn7.6.2_0    pytorch\r\n[conda] tensorflow                1.14.0          mkl_py36h2526735_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] tensorflow-base           1.14.0          mkl_py36h7ce6ba3_0    defaults\r\n","python\r\nPython builtin <built-in function hasattr> is currently not supported in Torchscript:\r\nat /home/chejin/anaconda3/envs/pytorch-gpu/lib/python3.6/site-packages/torch/nn/modules/activation.py:759:12\r\n        - key_padding_mask: :math:`(N, S)`, ByteTensor, where N is the batch size, S is the source sequence length.\r\n        - attn_mask: :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\r\n\r\n        - Outputs:\r\n        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\r\n          E is the embedding dimension.\r\n        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\r\n          L is the target sequence length, S is the source sequence length.\r\n        """"""\r\n        if hasattr(self, '_qkv_same_embed_dim') and self._qkv_same_embed_dim is False:\r\n           ~~~~~~~ <--- HERE\r\n            return F.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n                self.in_proj_weight, self.in_proj_bias,\r\n                self.bias_k, self.bias_v, self.add_zero_attn,\r\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n                training=self.training,\r\n                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n                attn_mask=attn_mask, use_separate_proj_weight=True,\r\n                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,'__module__.__torch__.torch.nn.modules.activation.MultiheadAttention.forward' is being compiled since it was called from '__modul\r\ne__.__torch__.torch.nn.modules.transformer.TransformerEncoderLayer.forward'\r\n"
24005,"Using  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel`## \U0001f41b Bug\r\n\r\nUsing  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel` resulting in the following stacktrace\r\n\r\n```\r\nTraceback (most recent call last):                                                                    \r\n  File ""minimal_buggy_2.py"", line 198, in <module>                                                    \r\n    train(hps)                                                                                        \r\n  File ""minimal_buggy_2.py"", line 179, in train                                                       \r\n    loss.backward()                                                                                   \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/tensor.py"", line 107, in backward                \r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)                               \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/function.py"", line 77, in apply         \r\n    return self._forward_cls.backward(self, *args)                                                    \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py"", line 99, in backward       \r\n    torch.autograd.backward(outputs, args)                                                            \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\nRuntimeError: has_marked_unused_parameters_ ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:181, please report a bug to PyTorch.\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py` works\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py --parallel` breaks\r\n\r\nThe code for `minimal_buggy.py` is here:\r\n\r\n\r\n## Expected behavior\r\n\r\nExpect either command to work independent of using `DistributedDataParallel` or not.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] pytorch-memlab==0.0.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] pytorch-memlab            0.0.3                    pypi_0    pypi\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.3.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski",oncall: distributed|module: checkpoint|feature|triaged,mrshenli,"## \U0001f41b Bug\r\n\r\nUsing  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel` resulting in the following stacktrace\r\n\r\n```\r\nTraceback (most recent call last):                                                                    \r\n  File ""minimal_buggy_2.py"", line 198, in <module>                                                    \r\n    train(hps)                                                                                        \r\n  File ""minimal_buggy_2.py"", line 179, in train                                                       \r\n    loss.backward()                                                                                   \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/tensor.py"", line 107, in backward                \r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)                               \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/function.py"", line 77, in apply         \r\n    return self._forward_cls.backward(self, *args)                                                    \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/utils/checkpoint.py"", line 99, in backward       \r\n    torch.autograd.backward(outputs, args)                                                            \r\n  File ""/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward      \r\n    allow_unreachable=True)  # allow_unreachable flag                                                 \r\nRuntimeError: has_marked_unused_parameters_ ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:181, please report a bug to PyTorch.\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py` works\r\n1. Running `python -m torch.distributed.launch --nproc_per_node=2 minimal_buggy.py --parallel` breaks\r\n\r\nThe code for `minimal_buggy.py` is here:\r\n```python\r\nfrom argparse import ArgumentParser, Namespace\r\nimport numpy as np\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.distributed as dist\r\nfrom torch.utils.checkpoint import checkpoint_sequential\r\n\r\nimport torchvision.datasets as ds\r\nfrom torchvision import transforms\r\n\r\n\r\nclass GradientStep(nn.Module):\r\n\r\n    def __init__(self, energy):\r\n        super(GradientStep, self).__init__()\r\n        self._energy = energy\r\n        self.step_size = 0.1\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        with torch.enable_grad():\r\n            x.requires_grad_()\r\n            omega = self._energy(x).sum()\r\n            grad_out = torch.ones_like(omega).to(x.device)\r\n            dx = torch.autograd.grad(outputs=omega, inputs=x, grad_outputs=grad_out,\r\n                                     create_graph=True, retain_graph=True,\r\n                                     allow_unused=True)[0]\r\n            dx.requires_grad_()\r\n            x = (x - self.step_size ** 2 * dx)\r\n        return x\r\n\r\n\r\nclass FFN(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(FFN, self).__init__()\r\n        self.n_hidden = 1024\r\n        self._energy = nn.Sequential(\r\n            nn.Linear(28**2, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, 1))\r\n        self.L = 10\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        y = x.clone().to(x.device)\r\n        y.requires_grad_()\r\n        fwd = nn.Sequential(*[GradientStep(self._energy) for _ in range(self.L)])\r\n        y = checkpoint_sequential(fwd, self.L, y)\r\n        return y\r\n\r\n\r\ndef get_distributed_mnist_iterators(batch_size, **kwargs):\r\n    def _worker_init_fn(worker_id):\r\n        np.random.seed(np.random.get_state()[1][0] + worker_id)\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=train_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=test_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    return train_loader, test_loader, train_sampler, test_sampler\r\n\r\n\r\ndef get_mnist_iterators(batch_size, **kwargs):\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    return train_loader, test_loader\r\n\r\n\r\ndef parse_args() -> Namespace:\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--local_rank',\r\n                            type=int,\r\n                            default=0,\r\n                            help=""Is being set by the pytorch distributed launcher"")\r\n\r\n    parser.add_argument('--parallel',\r\n                        default=False,\r\n                        action='store_true')\r\n\r\n    hps = parser.parse_args()\r\n    return hps\r\n\r\n\r\ndef train(hps: Namespace):\r\n\r\n    if hps.parallel:\r\n        train_loader, test_loader, _, _ = get_distributed_mnist_iterators(\r\n            batch_size=32)\r\n    else:\r\n        train_loader, test_loader = get_mnist_iterators(\r\n            batch_size=32)\r\n\r\n    model = FFN()\r\n    model.cuda()\r\n    if hps.parallel:\r\n        model = nn.parallel.DistributedDataParallel(model)\r\n    crit = nn.MSELoss()\r\n\r\n    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    for epoch in range(100):\r\n        for step, (b, lbl) in enumerate(train_loader):\r\n\r\n            model.train()\r\n            opt.zero_grad()\r\n            model.zero_grad()\r\n            corrupt_b = (b + 0.3 * torch.randn_like(b)).cuda()\r\n            recons = model(corrupt_b)\r\n            loss = crit(recons, b.cuda())\r\n            loss.backward()\r\n            opt.step()\r\n\r\n            if dist.get_rank() == 0:\r\n                if step % 10 == 0:\r\n                    print(f'epoch {epoch}, batch: {step}, loss: {float(loss.cpu())}')\r\n\r\n\r\ndef setup():\r\n    hps = parse_args()\r\n    dist.init_process_group(backend='nccl', init_method=f'env://', rank=hps.local_rank)\r\n\r\n    size = dist.get_world_size()\r\n    group = torch.distributed.new_group(ranks=list(range(size)))\r\n    return group, hps\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    group, hps = setup()\r\n    train(hps)\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpect either command to work independent of using `DistributedDataParallel` or not.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] pytorch-memlab==0.0.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.3.0\r\n[conda] pytorch-memlab            0.0.3                    pypi_0    pypi\r\n[conda] torch                     1.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.3.0                    pypi_0    pypi\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","python\r\nfrom argparse import ArgumentParser, Namespace\r\nimport numpy as np\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.distributed as dist\r\nfrom torch.utils.checkpoint import checkpoint_sequential\r\n\r\nimport torchvision.datasets as ds\r\nfrom torchvision import transforms\r\n\r\n\r\nclass GradientStep(nn.Module):\r\n\r\n    def __init__(self, energy):\r\n        super(GradientStep, self).__init__()\r\n        self._energy = energy\r\n        self.step_size = 0.1\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        with torch.enable_grad():\r\n            x.requires_grad_()\r\n            omega = self._energy(x).sum()\r\n            grad_out = torch.ones_like(omega).to(x.device)\r\n            dx = torch.autograd.grad(outputs=omega, inputs=x, grad_outputs=grad_out,\r\n                                     create_graph=True, retain_graph=True,\r\n                                     allow_unused=True)[0]\r\n            dx.requires_grad_()\r\n            x = (x - self.step_size ** 2 * dx)\r\n        return x\r\n\r\n\r\nclass FFN(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(FFN, self).__init__()\r\n        self.n_hidden = 1024\r\n        self._energy = nn.Sequential(\r\n            nn.Linear(28**2, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, self.n_hidden),\r\n            nn.LeakyReLU(),\r\n            nn.Linear(self.n_hidden, 1))\r\n        self.L = 10\r\n\r\n    def forward(self, x: torch.Tensor):\r\n        y = x.clone().to(x.device)\r\n        y.requires_grad_()\r\n        fwd = nn.Sequential(*[GradientStep(self._energy) for _ in range(self.L)])\r\n        y = checkpoint_sequential(fwd, self.L, y)\r\n        return y\r\n\r\n\r\ndef get_distributed_mnist_iterators(batch_size, **kwargs):\r\n    def _worker_init_fn(worker_id):\r\n        np.random.seed(np.random.get_state()[1][0] + worker_id)\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n    test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=train_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=False,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=test_sampler,\r\n        worker_init_fn=_worker_init_fn)\r\n\r\n    return train_loader, test_loader, train_sampler, test_sampler\r\n\r\n\r\ndef get_mnist_iterators(batch_size, **kwargs):\r\n\r\n    base_transforms = [\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Lambda(lambda x: x.reshape(-1, ))]\r\n\r\n    train_dataset = ds.MNIST(\r\n        '/tmp/mnist/train',\r\n        train=True,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    test_dataset = ds.MNIST(\r\n        '/tmp/mnist/test',\r\n        train=False,\r\n        download=True,\r\n        transform=transforms.Compose(base_transforms))\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset,\r\n        batch_size=batch_size,\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset,\r\n        batch_size=kwargs.get('test_batch_size', 100),\r\n        shuffle=True,\r\n        num_workers=kwargs.get('workers', 4),\r\n        pin_memory=True,\r\n        sampler=None)\r\n\r\n    return train_loader, test_loader\r\n\r\n\r\ndef parse_args() -> Namespace:\r\n    parser = ArgumentParser()\r\n    parser.add_argument('--local_rank',\r\n                            type=int,\r\n                            default=0,\r\n                            help=""Is being set by the pytorch distributed launcher"")\r\n\r\n    parser.add_argument('--parallel',\r\n                        default=False,\r\n                        action='store_true')\r\n\r\n    hps = parser.parse_args()\r\n    return hps\r\n\r\n\r\ndef train(hps: Namespace):\r\n\r\n    if hps.parallel:\r\n        train_loader, test_loader, _, _ = get_distributed_mnist_iterators(\r\n            batch_size=32)\r\n    else:\r\n        train_loader, test_loader = get_mnist_iterators(\r\n            batch_size=32)\r\n\r\n    model = FFN()\r\n    model.cuda()\r\n    if hps.parallel:\r\n        model = nn.parallel.DistributedDataParallel(model)\r\n    crit = nn.MSELoss()\r\n\r\n    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\r\n    for epoch in range(100):\r\n        for step, (b, lbl) in enumerate(train_loader):\r\n\r\n            model.train()\r\n            opt.zero_grad()\r\n            model.zero_grad()\r\n            corrupt_b = (b + 0.3 * torch.randn_like(b)).cuda()\r\n            recons = model(corrupt_b)\r\n            loss = crit(recons, b.cuda())\r\n            loss.backward()\r\n            opt.step()\r\n\r\n            if dist.get_rank() == 0:\r\n                if step % 10 == 0:\r\n                    print(f'epoch {epoch}, batch: {step}, loss: {float(loss.cpu())}')\r\n\r\n\r\ndef setup():\r\n    hps = parse_args()\r\n    dist.init_process_group(backend='nccl', init_method=f'env://', rank=hps.local_rank)\r\n\r\n    size = dist.get_world_size()\r\n    group = torch.distributed.new_group(ranks=list(range(size)))\r\n    return group, hps\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    group, hps = setup()\r\n    train(hps)\r\n"
23999,"TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nConnected to pydev debugger (build 182.4505.26)\r\ntensor([102.,  55.,  80.,  13.], device='cuda:0')\r\ntensor([ 3., 55., 77.,  9.], device='cuda:0')\r\nE:/Company/Models/test_freeze_pytorch/test_np.py:8: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\r\n  aa_ts = torch.Tensor(aa).cuda().float()\r\nD:\\SoftInstall\\Aconda\\envs\\tf\\lib\\site-packages\\torch\\jit\\__init__.py:642: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\r\nNot within tolerance rtol=1e-05 atol=1e-05 at input[1] (6.0 vs. 55.0) and 0 other locations (25.00%)\r\n  _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)\r\ntensor([102.,   6.,  80.,  13.], device='cuda:0')\r\ntensor([ 3.,  4., 77.,  9.], device='cuda:0')\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.0.0\r\n - OS (e.g., Linux):win10\r\n - How you installed PyTorch (`conda`, `pip`, source):pip\r\n - Python version:3.5\r\n - CUDA/cuDNN version:9.0\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: jit|triaged,wanchaol,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\ndef model(mu):\r\n    aa = [3, 4, 77, 9]\r\n    aa = np.asarray(aa)\r\n    aa_ts = torch.Tensor(aa).cuda().float()\r\n    mu = mu + aa_ts\r\n    # need to reset some idx of the valve but it not work in trace.jit\r\n    mu[1] = 55\r\n    return mu\r\n\r\n\r\nx1 = torch.tensor([99, 2, 3, 4]).cuda().float()\r\nx2 = torch.tensor([0, 0, 0, 0]).cuda().float()\r\n\r\nprint(model(x1))\r\nprint(model(x2))\r\n\r\nfn = torch.jit.trace(model, x1)\r\nfn.save(""test_model_freeze.pt"")\r\nmodel_new = torch.jit.load('test_model_freeze.pt')\r\ny1 = model_new(x1)\r\ny2 = model_new(x2)\r\nprint(y1)\r\nprint(y2)\r\n```\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nConnected to pydev debugger (build 182.4505.26)\r\ntensor([102.,  55.,  80.,  13.], device='cuda:0')\r\ntensor([ 3., 55., 77.,  9.], device='cuda:0')\r\nE:/Company/Models/test_freeze_pytorch/test_np.py:8: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\r\n  aa_ts = torch.Tensor(aa).cuda().float()\r\nD:\\SoftInstall\\Aconda\\envs\\tf\\lib\\site-packages\\torch\\jit\\__init__.py:642: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\r\nNot within tolerance rtol=1e-05 atol=1e-05 at input[1] (6.0 vs. 55.0) and 0 other locations (25.00%)\r\n  _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)\r\ntensor([102.,   6.,  80.,  13.], device='cuda:0')\r\ntensor([ 3.,  4., 77.,  9.], device='cuda:0')\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0):1.0.0\r\n - OS (e.g., Linux):win10\r\n - How you installed PyTorch (`conda`, `pip`, source):pip\r\n - Python version:3.5\r\n - CUDA/cuDNN version:9.0\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\ndef model(mu):\r\n    aa = [3, 4, 77, 9]\r\n    aa = np.asarray(aa)\r\n    aa_ts = torch.Tensor(aa).cuda().float()\r\n    mu = mu + aa_ts\r\n    # need to reset some idx of the valve but it not work in trace.jit\r\n    mu[1] = 55\r\n    return mu\r\n\r\n\r\nx1 = torch.tensor([99, 2, 3, 4]).cuda().float()\r\nx2 = torch.tensor([0, 0, 0, 0]).cuda().float()\r\n\r\nprint(model(x1))\r\nprint(model(x2))\r\n\r\nfn = torch.jit.trace(model, x1)\r\nfn.save(""test_model_freeze.pt"")\r\nmodel_new = torch.jit.load('test_model_freeze.pt')\r\ny1 = model_new(x1)\r\ny2 = model_new(x2)\r\nprint(y1)\r\nprint(y2)\r\n"
23948,"[torch.distributed][RPC]  An RPC callee could crash on `RpcAgent::join()`, if the caller terminates with unresolved future.## \U0001f41b Bug\r\n\r\nThis is a followup for #23228 \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. A peer sends out an async RPC without waiting for the response.\r\n1. Every peer join.\r\n\r\n## Expected behavior\r\n\r\nNo peer process crashes\r\n\r\n## Reason\r\n\r\nThis error happens because when a callee is sending out a response (pulled from it's send queue) to the caller, the caller has already terminated it's recvLoop thread and destructed itself.\r\n\r\nThe condition to terminate the recvLoop thread on rank[i] is that rank[i-1] sends , while peers of ranks other than rank[i] and rank[i - 1] could still has pending requests to send out to rank[i].\r\n\r\n![image](https://user-images.githubusercontent.com/7608630/62644298-a458d580-b8fe-11e9-9dd4-7273ca0b138f.png)\r\n\r\n\r\n# Solution\r\n\r\n## 1. This should work\r\n\r\n1. ProcessGroupAgent::join, needs to have all futures resolved before sending out termination message. i.e. put the first barrier in ProcessGroupAgent::join after waiting for all futures resolved.\r\n\r\n1. recvLoop thread terminates only if it has received termination messages from all peers.\r\n\r\n## 2. Maybe work\r\n\r\ncall a second barrier before ::join() returns.\r\n\r\nIt's actually better, because if a user doesn't wait on the unresolved future, it means leaving them unresolved doesn't matter. But the system could log a warning to remind users about it.\r\n\r\nI have tried out this solution. I will lead to a deadlock, as described in, https://github.com/pytorch/pytorch/issues/23975.\r\n\r\n![image](https://user-images.githubusercontent.com/7608630/62644970-2f869b00-b900-11e9-8ea9-ac7fa95ca3d8.png)\r\n\r\n## Error message\r\n\r\n\r\n\r\n# Caveat\r\n\r\nThe problem is solvable, but it's hard to add a test case for it, because the problem only manifests in a certain execution order.\r\n\r\nKeep this issue open until we found a concurrency testing framework/approach, which can reproduce any desired deterministic execution order.\r\n\r\nReference, to a Python time simulation (stepping) framework. [SimPy](https://simpy.readthedocs.io/en/latest/).\r\n\r\nThis issue could emerge again when implementing other communication backend other than ProcessGroup.",oncall: distributed|module: tests|triaged,xush6528,"## \U0001f41b Bug\r\n\r\nThis is a followup for #23228 \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. A peer sends out an async RPC without waiting for the response.\r\n1. Every peer join.\r\n\r\n## Expected behavior\r\n\r\nNo peer process crashes\r\n\r\n## Reason\r\n\r\nThis error happens because when a callee is sending out a response (pulled from it's send queue) to the caller, the caller has already terminated it's recvLoop thread and destructed itself.\r\n\r\nThe condition to terminate the recvLoop thread on rank[i] is that rank[i-1] sends , while peers of ranks other than rank[i] and rank[i - 1] could still has pending requests to send out to rank[i].\r\n\r\n![image](https://user-images.githubusercontent.com/7608630/62644298-a458d580-b8fe-11e9-9dd4-7273ca0b138f.png)\r\n\r\n\r\n# Solution\r\n\r\n## 1. This should work\r\n\r\n1. ProcessGroupAgent::join, needs to have all futures resolved before sending out termination message. i.e. put the first barrier in ProcessGroupAgent::join after waiting for all futures resolved.\r\n\r\n1. recvLoop thread terminates only if it has received termination messages from all peers.\r\n\r\n## 2. Maybe work\r\n\r\ncall a second barrier before ::join() returns.\r\n\r\nIt's actually better, because if a user doesn't wait on the unresolved future, it means leaving them unresolved doesn't matter. But the system could log a warning to remind users about it.\r\n\r\nI have tried out this solution. I will lead to a deadlock, as described in, https://github.com/pytorch/pytorch/issues/23975.\r\n\r\n![image](https://user-images.githubusercontent.com/7608630/62644970-2f869b00-b900-11e9-8ea9-ac7fa95ca3d8.png)\r\n\r\n## Error message\r\n\r\n```Bash\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:38334\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:30850\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:21379\r\nERROR\r\n\r\n======================================================================\r\nERROR: test_rpc_no_op (caffe2.torch.fb.modelparallel.prototype.pytorch.tests.test_rpc.TestRPC)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/tests/test_rpc.py"", line 210, in test_rpc_no_op\r\n    TestRPC.launch_rpc_test(app_rpc_no_op)\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/tests/test_rpc.py"", line 206, in launch_rpc_test\r\n    exit_on_worker_done=""w:1"",\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/rpc.py"", line 1301, in rpc_main\r\n    nprocs=len(worker_names),\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/torch/multiprocessing/spaw\r\nn.py"", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/torch/multiprocessing/spaw\r\nn.py"", line 107, in join\r\n    (error_index, name)\r\nException: process 3 terminated with signal SIGABRT\r\n```\r\n\r\n# Caveat\r\n\r\nThe problem is solvable, but it's hard to add a test case for it, because the problem only manifests in a certain execution order.\r\n\r\nKeep this issue open until we found a concurrency testing framework/approach, which can reproduce any desired deterministic execution order.\r\n\r\nReference, to a Python time simulation (stepping) framework. [SimPy](https://simpy.readthedocs.io/en/latest/).\r\n\r\nThis issue could emerge again when implementing other communication backend other than ProcessGroup.","Bash\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:38334\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:30850\r\nterminate called after throwing an instance of 'gloo::IoException'\r\n  what():  [gloo/transport/tcp/pair.cc:572] Connection closed by peer [2401:db00:30:503a:face:0:75:0]:21379\r\nERROR\r\n\r\n======================================================================\r\nERROR: test_rpc_no_op (caffe2.torch.fb.modelparallel.prototype.pytorch.tests.test_rpc.TestRPC)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/tests/test_rpc.py"", line 210, in test_rpc_no_op\r\n    TestRPC.launch_rpc_test(app_rpc_no_op)\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/tests/test_rpc.py"", line 206, in launch_rpc_test\r\n    exit_on_worker_done=""w:1"",\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/caffe2/torch/fb/modelparal\r\nlel/prototype/pytorch/rpc.py"", line 1301, in rpc_main\r\n    nprocs=len(worker_names),\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/torch/multiprocessing/spaw\r\nn.py"", line 171, in spawn\r\n    while not spawn_context.join():\r\n  File ""/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/torch/fb/modelparallel/prototype/pytorch/tests/test_rpc#binary,link-tree/torch/multiprocessing/spaw\r\nn.py"", line 107, in join\r\n    (error_index, name)\r\nException: process 3 terminated with signal SIGABRT\r\n"
23925,"'border' and 'reflection\u2019 modes of grid_sample have incorrect gradients at border## \U0001f41b Bug\r\n\r\nWhen `padding_mode='border'` in `grid_sample`, and a grid point falls exactly on the high boundary of the image (`size - 1`), the gradient should be based on the border padding scheme, which should give either the gradient from just inside the boundary, or zero from just outside the boundary (either could be valid, since it\u2019s a non differentiable point). Instead, the gradient is currently based on zero padding the image, which gives wacky results.\r\n\r\nSame problem occurs with `padding_mode='reflection'` for 2D `grid_sample` on CPU.\r\nReflection modes of both the cuda version and the 3D CPU version also have this problem, but it\u2019s arguably worse, since the incorrect gradient is also negated. Furthermore, this is an inconsistency between the behavior of CPU and CUDA kernels.\r\n\r\n## Example:\r\n\r\n\r\n\r\nNotice the wacky last row and last column. This is because the gradient there is currently calculated as if the image was zero-padded.\r\n\r\nThe result should ideally look like\r\n\r\n\r\n\r\nwhich finds the gradient using the in-bounds neighbor.\r\n\r\nA less ideal, but still palatable result would be\r\n\r\n\r\n\r\nwhich finds the gradient using the out-of-bounds, border-padded neighbor.\r\n\r\nReflection mode on cpu (for instance, try using these same commands, but with `padding_mode='reflection'`) gives the exact same problematic result.\r\nWhen using reflection mode on cuda, however, (as well as for 3D grid_sample on cpu) the problematic gradients are negated!\r\n\r\n\r\n\r\nThis is also problematic, of course, but even more so because of the mismatch between the cpu and cuda behaviors.\r\n\r\nFor `reflection` mode, I think it makes sense to set the gradient in such cases to zero, since it\u2019s sort of at the apex of a symmetric hill. But setting it to take the gradient of one side or the other might also be acceptable for most practical purposes.\r\n\r\nFor `border` mode, by contrast, I think it makes more sense to always take the non-zero gradient from the inner side, since the outer side gradient will be zero and so effectively stop training (see the related discussion for `clamp` at #7002 and #7049).\r\n\r\n\r\n\r\n\r\nPyTorch Version: tested on commit https://github.com/pytorch/pytorch/commit/0539462ca2966aa29657b58aeb17a85c21524d31",high priority|triaged,fmassa,"## \U0001f41b Bug\r\n\r\nWhen `padding_mode='border'` in `grid_sample`, and a grid point falls exactly on the high boundary of the image (`size - 1`), the gradient should be based on the border padding scheme, which should give either the gradient from just inside the boundary, or zero from just outside the boundary (either could be valid, since it\u2019s a non differentiable point). Instead, the gradient is currently based on zero padding the image, which gives wacky results.\r\n\r\nSame problem occurs with `padding_mode='reflection'` for 2D `grid_sample` on CPU.\r\nReflection modes of both the cuda version and the 3D CPU version also have this problem, but it\u2019s arguably worse, since the incorrect gradient is also negated. Furthermore, this is an inconsistency between the behavior of CPU and CUDA kernels.\r\n\r\n## Example:\r\n```python\r\nimage = torch.arange(0, 5, dtype=torch.float).expand((1,1,5,5)).requires_grad_()\r\n\r\nid_grid = torch.nn.functional.affine_grid(\r\n    torch.tensor([[[1,0,0],[0,1,0.]]]), (1,1,5,5), align_corners=True).requires_grad_()\r\n\r\ntorch.nn.functional.grid_sample(image, id_grid, padding_mode='border',\r\n                                align_corners=True).sum().backward()\r\n\r\nprint(id_grid.grad.permute(0,3,1,2))\r\n```\r\n```python\r\ntensor([[[[ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.],\r\n          [ 2.,  2.,  2.,  2., -8.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0., -2., -4., -6., -8.]]]])\r\n```\r\n\r\nNotice the wacky last row and last column. This is because the gradient there is currently calculated as if the image was zero-padded.\r\n\r\nThe result should ideally look like\r\n\r\n```python\r\ntensor([[[[ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.],\r\n          [ 2.,  2.,  2.,  2.,  2.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.]]]])\r\n```\r\n\r\nwhich finds the gradient using the in-bounds neighbor.\r\n\r\nA less ideal, but still palatable result would be\r\n\r\n```python\r\ntensor([[[[ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.],\r\n          [ 2.,  2.,  2.,  2.,  0.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.]]]])\r\n```\r\n\r\nwhich finds the gradient using the out-of-bounds, border-padded neighbor.\r\n\r\nReflection mode on cpu (for instance, try using these same commands, but with `padding_mode='reflection'`) gives the exact same problematic result.\r\nWhen using reflection mode on cuda, however, (as well as for 3D grid_sample on cpu) the problematic gradients are negated!\r\n\r\n```python\r\ntensor([[[[2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.],\r\n          [2., 2., 2., 2., 8.]],\r\n\r\n         [[0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0.],\r\n          [-0., 2., 4., 6., 8.]]]])\r\n```\r\n\r\nThis is also problematic, of course, but even more so because of the mismatch between the cpu and cuda behaviors.\r\n\r\nFor `reflection` mode, I think it makes sense to set the gradient in such cases to zero, since it\u2019s sort of at the apex of a symmetric hill. But setting it to take the gradient of one side or the other might also be acceptable for most practical purposes.\r\n\r\nFor `border` mode, by contrast, I think it makes more sense to always take the non-zero gradient from the inner side, since the outer side gradient will be zero and so effectively stop training (see the related discussion for `clamp` at #7002 and #7049).\r\n\r\n\r\n\r\n\r\nPyTorch Version: tested on commit https://github.com/pytorch/pytorch/commit/0539462ca2966aa29657b58aeb17a85c21524d31","python\r\nimage = torch.arange(0, 5, dtype=torch.float).expand((1,1,5,5)).requires_grad_()\r\n\r\nid_grid = torch.nn.functional.affine_grid(\r\n    torch.tensor([[[1,0,0],[0,1,0.]]]), (1,1,5,5), align_corners=True).requires_grad_()\r\n\r\ntorch.nn.functional.grid_sample(image, id_grid, padding_mode='border',\r\n                                align_corners=True).sum().backward()\r\n\r\nprint(id_grid.grad.permute(0,3,1,2))\r\n"
23854,"Fan out calculation broken for group (depthwise) convolution## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n1. Should return `(9,9)` instead of `(9, 36)`\r\n1. Should return `(18, 9)` instead of `(18, 18)`\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pacman\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: None\r\n - Any other relevant information: None\n\ncc @ezyang @gchanan @zou3519",module: convolution|triaged|module: initialization,albanD,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\n# depthwise, should be (9, 9)\r\nm = torch.nn.Conv2d(4, 4, 3, groups=4)\r\nprint(torch.nn.init._calculate_fan_in_and_fan_out(m.weight))\r\n\r\n# groupwise, should be (18, 9)\r\nm = torch.nn.Conv2d(4, 2, 3, groups=2)\r\nprint(torch.nn.init._calculate_fan_in_and_fan_out(m.weight))\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n1. Should return `(9,9)` instead of `(9, 36)`\r\n1. Should return `(18, 9)` instead of `(18, 18)`\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pacman\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration: None\r\n - Any other relevant information: None\n\ncc @ezyang @gchanan @zou3519","python\r\n# depthwise, should be (9, 9)\r\nm = torch.nn.Conv2d(4, 4, 3, groups=4)\r\nprint(torch.nn.init._calculate_fan_in_and_fan_out(m.weight))\r\n\r\n# groupwise, should be (18, 9)\r\nm = torch.nn.Conv2d(4, 2, 3, groups=2)\r\nprint(torch.nn.init._calculate_fan_in_and_fan_out(m.weight))\r\n"
23781,"Torch hub does not check dependencies before importing attributes## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nEnsure `tqdm` is not installed in your environment, verify that `python -c 'import tqdm'` **raises an exception.**\r\n\r\n\r\n([repo here](https://github.com/willprice/pytorch-hub-bug))\r\nThis errors with \r\n\r\n```\r\nUsing cache found in /home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.7/site-packages/torch/hub.py"", line 336, in load\r\n    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)\r\n  File ""/usr/lib/python3.7/site-packages/torch/hub.py"", line 70, in import_module\r\n    spec.loader.exec_module(module)\r\n  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\r\n  File ""/home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master/hubconf.py"", line 3, in <module>\r\n    from entry import entry_point\r\n  File ""/home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master/entry.py"", line 1, in <module>\r\n    import tqdm\r\nModuleNotFoundError: No module named 'tqdm'\r\n```\r\n\r\n## Expected behavior\r\nI'd expect `torch` to raise an exception saying a dependency isn't installed, rather than an `ImportError`.\r\n\r\nI would expect `torch.hub.load` to check `dependencies` in `hubconf.py` before executing `hubconf.py` and actually loading in the entrypoints. It's not clear from examples of `hubconf.py` that to avoid this behaviour you'd have to wrap all your entrypoints in functions that import the dependencies within the function body and not at the top level. This is a pain as it involves adding a wrapper for every entrypoint and duplicating all the docstrings.\r\n\r\n## Suggested fix\r\n\r\nRather than evaluating the module, it'd make more sense to pull out the AST node corresponding to the `dependencies` variable and evaluate that only, perform the dependency check and only then import the full module. It might be that people have complicated `hubconf.py` files that prevent this from happening. In that case things get a bit more tricky and it might make sense to fallback on the current implementation as at least the user still gets ImportError exceptions that are *useful* if not the most user friendly.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.1.0\r\nCMake version: version 3.15.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-ignite==0.1.2\r\n[pip3] torch==1.1.0\r\n[pip3] torchaudio==0.2\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.2.1\r\n[pip3] torchviz==0.0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch-cpu               1.1.0               py3.6_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py36_cuNone_1    pytorch\r\n```",triaged|module: hub,ailzhang,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nEnsure `tqdm` is not installed in your environment, verify that `python -c 'import tqdm'` **raises an exception.**\r\n\r\n```python\r\nimport torch\r\ntorch.hub.load('willprice/pytorch-hub-bug', 'entry_point')\r\n```\r\n([repo here](https://github.com/willprice/pytorch-hub-bug))\r\nThis errors with \r\n\r\n```\r\nUsing cache found in /home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.7/site-packages/torch/hub.py"", line 336, in load\r\n    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)\r\n  File ""/usr/lib/python3.7/site-packages/torch/hub.py"", line 70, in import_module\r\n    spec.loader.exec_module(module)\r\n  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\r\n  File ""/home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master/hubconf.py"", line 3, in <module>\r\n    from entry import entry_point\r\n  File ""/home/will/.cache/torch/hub/willprice_pytorch-hub-bug_master/entry.py"", line 1, in <module>\r\n    import tqdm\r\nModuleNotFoundError: No module named 'tqdm'\r\n```\r\n\r\n## Expected behavior\r\nI'd expect `torch` to raise an exception saying a dependency isn't installed, rather than an `ImportError`.\r\n\r\nI would expect `torch.hub.load` to check `dependencies` in `hubconf.py` before executing `hubconf.py` and actually loading in the entrypoints. It's not clear from examples of `hubconf.py` that to avoid this behaviour you'd have to wrap all your entrypoints in functions that import the dependencies within the function body and not at the top level. This is a pain as it involves adding a wrapper for every entrypoint and duplicating all the docstrings.\r\n\r\n## Suggested fix\r\n\r\nRather than evaluating the module, it'd make more sense to pull out the AST node corresponding to the `dependencies` variable and evaluate that only, perform the dependency check and only then import the full module. It might be that people have complicated `hubconf.py` files that prevent this from happening. In that case things get a bit more tricky and it might make sense to fallback on the current implementation as at least the user still gets ImportError exceptions that are *useful* if not the most user friendly.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.1.0\r\nCMake version: version 3.15.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-ignite==0.1.2\r\n[pip3] torch==1.1.0\r\n[pip3] torchaudio==0.2\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.2.1\r\n[pip3] torchviz==0.0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch-cpu               1.1.0               py3.6_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py36_cuNone_1    pytorch\r\n```","python\r\nimport torch\r\ntorch.hub.load('willprice/pytorch-hub-bug', 'entry_point')\r\n"
23777,"Inconsistent model behavior when loading model using load_state_dict in different ways## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI have a model which has two embedding layers. And I initialize the model, train it on GPU, and save its state dict to disk using `torch.save(model.state_dict, 'tmp.pt')`. \r\n\r\nAnd now I want to use previous model to do inference jobs, I find two ways below can cause different results:\r\n1. initialize model on cpu -> load state dict -> model to gpu\r\n2. initialize model on cpu -> model to gpu -> load state dict\r\n\r\n**However, using just one embedding layer in my model will solve this.**\r\n\r\n## To Reproduce\r\n\r\nRun the code below, and it does these things:\r\n1. fix random seeds\r\n2. generate fake embedding weights and samples\r\n3. initialize one model, train it, save it on disk\r\n4. three different ways of doing inference, and the first  two asserts will fail\r\n\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nThe assert in above code should all pass. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 410.104\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] pytorch-pretrained-bert==0.6.2\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.0.2            py36h7b6447c_0\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.3.0           py36_cu10.0.130_1    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",high priority|module: serialization|triaged|module: random,rgommers,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI have a model which has two embedding layers. And I initialize the model, train it on GPU, and save its state dict to disk using `torch.save(model.state_dict, 'tmp.pt')`. \r\n\r\nAnd now I want to use previous model to do inference jobs, I find two ways below can cause different results:\r\n1. initialize model on cpu -> load state dict -> model to gpu\r\n2. initialize model on cpu -> model to gpu -> load state dict\r\n\r\n**However, using just one embedding layer in my model will solve this.**\r\n\r\n## To Reproduce\r\n\r\nRun the code below, and it does these things:\r\n1. fix random seeds\r\n2. generate fake embedding weights and samples\r\n3. initialize one model, train it, save it on disk\r\n4. three different ways of doing inference, and the first  two asserts will fail\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\ndef seed_torch(seed):\r\n    import random\r\n    random.seed(seed)\r\n    \r\n    import os\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    \r\n    import numpy as np\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\r\n    torch.backends.cudnn.benchmark = False\r\n    torch.backends.cudnn.deterministic = True\r\n    \r\nseed_torch(2019)\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, vectors):\r\n        super(Model, self).__init__()\r\n        self.emb = nn.Embedding.from_pretrained(\r\n            vectors, freeze=False,\r\n            padding_idx=0\r\n        )\r\n\r\n        # dummy embedding layers, just initialized and not used\r\n        self.emb2 = nn.Embedding.from_pretrained(\r\n            vectors, freeze=False,\r\n            padding_idx=0\r\n        )\r\n        self.linear = nn.Linear(128, 2)\r\n        \r\n    def forward(self, s):\r\n        s = self.emb(s)\r\n        s = self.linear(s)\r\n        s = s.sum(dim=1)\r\n        return s\r\n    \r\nvectors = torch.randn(1000, 128)\r\n# fake samples\r\ntrain_s = torch.randint(1, 1000, size=(100, 30)).to('cuda')\r\ntrain_y = torch.ones(100).to(dtype=torch.long).to('cuda')\r\n\r\n# train\r\nmodel = Model(vectors).to('cuda')\r\ncriterion = nn.CrossEntropyLoss(reduction='sum')\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n\r\nmodel.train()\r\nmodel.zero_grad()\r\nout = model(train_s)\r\nloss = criterion(out, train_y)\r\nloss.backward()\r\noptimizer.step()\r\n\r\ntorch.save(model.state_dict(), 'tmp.pt')\r\n\r\n# inference, model init -> to cuda -> load state dict\r\nm1 = Model(vectors).to('cuda')\r\nm1.load_state_dict(torch.load('tmp.pt'))\r\nm1.eval()\r\nwith torch.no_grad():\r\n    o1 = m1(train_s.to('cuda'))\r\n    \r\n# inference, model init -> load state dit -> to cuda\r\nm2 = Model(vectors)\r\nm2.load_state_dict(torch.load('tmp.pt'))\r\nm2 = m2.to('cuda')\r\nm2.eval()\r\nwith torch.no_grad():\r\n    o2 = m2(train_s.to('cuda'))\r\n    \r\n# inference on cpu\r\nm3 = Model(vectors)\r\nm3.load_state_dict(torch.load('tmp.pt'))\r\nm3.eval()\r\nwith torch.no_grad():\r\n    o3 = m3(train_s.to('cpu'))\r\n    \r\nassert torch.allclose(o1, o2)\r\nassert torch.allclose(o1.cpu(), o3)\r\nassert torch.allclose(o2.cpu(), o3)\r\n```\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nThe assert in above code should all pass. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 410.104\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] pytorch-pretrained-bert==0.6.2\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.0.2            py36h7b6447c_0\r\n[conda] mkl_fft                   1.0.12           py36ha843d7b_0\r\n[conda] mkl_random                1.0.2            py36hd81dba3_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision               0.3.0           py36_cu10.0.130_1    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\ndef seed_torch(seed):\r\n    import random\r\n    random.seed(seed)\r\n    \r\n    import os\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n    \r\n    import numpy as np\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\r\n    torch.backends.cudnn.benchmark = False\r\n    torch.backends.cudnn.deterministic = True\r\n    \r\nseed_torch(2019)\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, vectors):\r\n        super(Model, self).__init__()\r\n        self.emb = nn.Embedding.from_pretrained(\r\n            vectors, freeze=False,\r\n            padding_idx=0\r\n        )\r\n\r\n        # dummy embedding layers, just initialized and not used\r\n        self.emb2 = nn.Embedding.from_pretrained(\r\n            vectors, freeze=False,\r\n            padding_idx=0\r\n        )\r\n        self.linear = nn.Linear(128, 2)\r\n        \r\n    def forward(self, s):\r\n        s = self.emb(s)\r\n        s = self.linear(s)\r\n        s = s.sum(dim=1)\r\n        return s\r\n    \r\nvectors = torch.randn(1000, 128)\r\n# fake samples\r\ntrain_s = torch.randint(1, 1000, size=(100, 30)).to('cuda')\r\ntrain_y = torch.ones(100).to(dtype=torch.long).to('cuda')\r\n\r\n# train\r\nmodel = Model(vectors).to('cuda')\r\ncriterion = nn.CrossEntropyLoss(reduction='sum')\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n\r\nmodel.train()\r\nmodel.zero_grad()\r\nout = model(train_s)\r\nloss = criterion(out, train_y)\r\nloss.backward()\r\noptimizer.step()\r\n\r\ntorch.save(model.state_dict(), 'tmp.pt')\r\n\r\n# inference, model init -> to cuda -> load state dict\r\nm1 = Model(vectors).to('cuda')\r\nm1.load_state_dict(torch.load('tmp.pt'))\r\nm1.eval()\r\nwith torch.no_grad():\r\n    o1 = m1(train_s.to('cuda'))\r\n    \r\n# inference, model init -> load state dit -> to cuda\r\nm2 = Model(vectors)\r\nm2.load_state_dict(torch.load('tmp.pt'))\r\nm2 = m2.to('cuda')\r\nm2.eval()\r\nwith torch.no_grad():\r\n    o2 = m2(train_s.to('cuda'))\r\n    \r\n# inference on cpu\r\nm3 = Model(vectors)\r\nm3.load_state_dict(torch.load('tmp.pt'))\r\nm3.eval()\r\nwith torch.no_grad():\r\n    o3 = m3(train_s.to('cpu'))\r\n    \r\nassert torch.allclose(o1, o2)\r\nassert torch.allclose(o1.cpu(), o3)\r\nassert torch.allclose(o2.cpu(), o3)\r\n"
23711,"MyScriptModule.code can not get Typehint for first input## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n\r\n2. \r\n\r\n3.\r\nin script_src i suppose to get type hint for each input, however, the source is \r\n\r\nthe type hind for first output is missing\r\n\r\n\r\n## Expected behavior\r\n\r\n\r\n## Environment\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): Conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: jit|triaged,eellison,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n```python\r\nclass MyScriptModule(torch.jit.ScriptModule):\r\n        def __init__(self):\r\n            super(MyScriptModule, self).__init__()\r\n        @torch.jit.script_method\r\n        def forward(self, input_fs1:List[str], input_fs2:str, input_fs3:bool)->Tuple[List[str],str]:\r\n            if input_fs3:\r\n                res_list = [""""] * len(input_fs1)\r\n                for i in range(len(input_fs1)):\r\n                    res_list[i] = (input_fs1[i] + input_fs2)\r\n                return (res_list, ""input is True"")\r\n            else:\r\n                res_list = [""""] * len(input_fs1)\r\n                for i in range(len(input_fs1)):\r\n                    res_list[i] = (input_fs1[i] + input_fs2)\r\n                return (res_list, ""input is False"")\r\nm = MyScriptModule()\r\nscript_src = m.code\r\n```\r\n2. \r\n```python\r\nprint(script_src)\r\n```\r\n3.\r\nin script_src i suppose to get type hint for each input, however, the source is \r\n```python\r\ndef forward(input_fs1,\r\n    input_fs2: str,\r\n    input_fs3: bool) -> Tuple[List[str], str]:\r\n  if input_fs3:\r\n    res_list = torch.mul([""""], torch.len(input_fs1))\r\n    for i in range(torch.len(input_fs1)):\r\n      _1 = torch.add(torch.select(input_fs1, i), input_fs2)\r\n      _2 = torch._set_item(res_list, i, _1)\r\n    _0 = (res_list, ""input is True"")\r\n  else:\r\n    res_list0 = torch.mul([""""], torch.len(input_fs1))\r\n    for i0 in range(torch.len(input_fs1)):\r\n      _3 = torch.add(torch.select(input_fs1, i0), input_fs2)\r\n      _4 = torch._set_item(res_list0, i0, _3)\r\n    _0 = (res_list0, ""input is False"")\r\n  return _0\r\n```\r\nthe type hind for first output is missing\r\n\r\n\r\n## Expected behavior\r\n\r\n\r\n## Environment\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): Conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: None\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nclass MyScriptModule(torch.jit.ScriptModule):\r\n        def __init__(self):\r\n            super(MyScriptModule, self).__init__()\r\n        @torch.jit.script_method\r\n        def forward(self, input_fs1:List[str], input_fs2:str, input_fs3:bool)->Tuple[List[str],str]:\r\n            if input_fs3:\r\n                res_list = [""""] * len(input_fs1)\r\n                for i in range(len(input_fs1)):\r\n                    res_list[i] = (input_fs1[i] + input_fs2)\r\n                return (res_list, ""input is True"")\r\n            else:\r\n                res_list = [""""] * len(input_fs1)\r\n                for i in range(len(input_fs1)):\r\n                    res_list[i] = (input_fs1[i] + input_fs2)\r\n                return (res_list, ""input is False"")\r\nm = MyScriptModule()\r\nscript_src = m.code\r\n"
23651,"Add sparse softmax/log_softmax functionality (ignore zero entries)## \U0001f680 Feature\r\nIt would be nice if `torch.sparse` will be extended to include `torch.sparse.min(...)` and `torch.sparse.max(...)` with same interface as existing `torch.sparse.sum(...)`.\r\n\r\nFor example\r\n\r\n\r\n## Motivation\r\nTaking min/max (argmin/argmax if possible) is quite rudimentary. \r\nFor instance suppose I have a sparse matrix `s` with sparse logits and I wish to *robustly* compute the output probabilities using `softmax`. If one wants to avoid overflow/underflow in the exp-sum process, one needs to subtract the `max` across the relevant dimension.\r\n \r\n## Pitch\r\nMin/max over a selected dimension is quite basic and exists for dense tensor. It would be nice to have similar functionality for sparse tensors as well.\r\n\r\n## Alternatives\r\nVery painfully iterate over the entries of the relevant dimension. **Painfully**.\r\n\r\n## Additional context\r\nThinking this over, there is an issue about the ""sparse"" entries: How should the ""sparse"" zeros be taken into account? For instance, if all the non-zeros entries in a row are strictly positive, does `min` over this row return 0 (value of ""sparse"" elements) or the minimal value of the greater than zero elements of the row?\r\n\r\nI think, what I am actually looking for is to *ignore* the sparse zeros of the matrix and only consider the values stored in `s._values()` and do `min/max` on these values based on their row/col arrangement in `s`.  \r\nIn that case, it is very possible to get `torch.sparse.min(s, dim=0) != torch.min(s.to_dense(), dim=0)`.\n\ncc @ezyang @gchanan @zou3519 @vincentqb",high priority|module: sparse|feature|triaged,pearu,"## \U0001f680 Feature\r\nIt would be nice if `torch.sparse` will be extended to include `torch.sparse.min(...)` and `torch.sparse.max(...)` with same interface as existing `torch.sparse.sum(...)`.\r\n\r\nFor example\r\n```python\r\ns = torch.sparse.FloatTensor(i, v, (height, width))  # make a sparse float tensor\r\ntorch.sparse.min(s, dim=1)\r\n# returns minimum along dim=1\r\ntorch.sparse.max(s, dim=0)\r\n# returns maximum along dim=0\r\n```\r\n\r\n## Motivation\r\nTaking min/max (argmin/argmax if possible) is quite rudimentary. \r\nFor instance suppose I have a sparse matrix `s` with sparse logits and I wish to *robustly* compute the output probabilities using `softmax`. If one wants to avoid overflow/underflow in the exp-sum process, one needs to subtract the `max` across the relevant dimension.\r\n \r\n## Pitch\r\nMin/max over a selected dimension is quite basic and exists for dense tensor. It would be nice to have similar functionality for sparse tensors as well.\r\n\r\n## Alternatives\r\nVery painfully iterate over the entries of the relevant dimension. **Painfully**.\r\n\r\n## Additional context\r\nThinking this over, there is an issue about the ""sparse"" entries: How should the ""sparse"" zeros be taken into account? For instance, if all the non-zeros entries in a row are strictly positive, does `min` over this row return 0 (value of ""sparse"" elements) or the minimal value of the greater than zero elements of the row?\r\n\r\nI think, what I am actually looking for is to *ignore* the sparse zeros of the matrix and only consider the values stored in `s._values()` and do `min/max` on these values based on their row/col arrangement in `s`.  \r\nIn that case, it is very possible to get `torch.sparse.min(s, dim=0) != torch.min(s.to_dense(), dim=0)`.\n\ncc @ezyang @gchanan @zou3519 @vincentqb","python\r\ns = torch.sparse.FloatTensor(i, v, (height, width))  # make a sparse float tensor\r\ntorch.sparse.min(s, dim=1)\r\n# returns minimum along dim=1\r\ntorch.sparse.max(s, dim=0)\r\n# returns maximum along dim=0\r\n"
23616,"[jit] string.split only splits on newlinesIt should split on any whitespace, right now it just splits on `' '`\r\n\r\n\r\noutputs\r\n\r\n```\r\n['a', 'b', 'c', 'd']\r\n['a', 'b\\nc\\td']\r\n```\n\ncc @suo",oncall: jit|triaged|jit-backlog,bwasti,"It should split on any whitespace, right now it just splits on `' '`\r\n\r\n```python\r\ndef fn(x):\r\n    # type: (str)\r\n    return x.split()\r\n\r\ns = ""a b\\nc\\td""\r\nprint(fn(s))\r\nprint(torch.jit.script(fn)(s))\r\n```\r\noutputs\r\n\r\n```\r\n['a', 'b', 'c', 'd']\r\n['a', 'b\\nc\\td']\r\n```\n\ncc @suo","python\r\ndef fn(x):\r\n    # type: (str)\r\n    return x.split()\r\n\r\ns = ""a b\\nc\\td""\r\nprint(fn(s))\r\nprint(torch.jit.script(fn)(s))\r\n"
23403,"Memory Format support for Resnet modelsWe define 4D tensor as stored in channels last memory format, when dimensions order is NCHW and  _(If size of any dimension is equal to 1, this dimension strides value is not taken into account)_.\r\n\r\nChannels last contiguous tensor is channel last tensor which occupies contiguous memory block. So ```x.is_contiguous(memory_format=torch.channels_last)``` checks if tensor is channels last contiguous.\r\n\r\nThe goal of the experiment is to use channels last memory format in all Resnet (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) model's operators and to measure performance gains on the Volta devices with CudNN library available.  \r\n\r\nThis experiment requires:\r\n1) Update operators kernels to follow the next rule: if one of the operator's inputs is channel last tensor, all outputs should also be in the channel last memory format.\r\n2) For better performance gain, update DataLoader to output channel last tensors.\r\n\r\nTo avoid changing the model itself and more importantly, introduce this optimization to the existing saved models. We need to introduce the next changes:\r\n- [ ] `to` operator should preserve memory format. `copy_device_to_device` should be memory format aware. (#23899)\r\n- [ ] `empty_like` operator should preserve memory format by default. (#23899)\r\n- [ ] `resize_as_` operator should be memory format aware. (#23899)\r\n- [x] `clone` operator should preserve memory format. (#23899)\r\n- [ ] `scatter` and `gather` functions should be memory format aware. (#24121)\r\n- [ ] `TensorIterator` based point-wise operators should preserve memory format (#24038).\r\n- [ ] `adaptive_avg_pool2d_cuda` and `adaptive_avg_pool2d_backward_cuda` should have channel last optimized kernels. (#24396)\r\n- [x] `max_pool2d_with_indices_cuda` and `max_pool2d_with_indices_backward_cuda` should have channel last optimized kernels (#24872).\r\n- [ ] `cudnn_batch_norm` and `cudnn_batch_norm_backward` should support channels last memory format. (#23861)\r\n- [ ] `cudnn_convolution_forward` and `cudnn_convolution_backward` should support channels last memory format. (#23861)\r\n\r\nWriting memory format aware operators require special functions introduced in #23391\r\n\r\n\r\n\r\n## Notes\r\n\r\n- Resnet calls ```x = x.reshape(x.size(0), -1)``` before linear layers, we are going to update `reshape` and `view` code and convert tensor's memory format to `torch.contiguous_format` at this step.\r\n- Making old models/libraries to work faster also requires **BC sacrifices**, such as \r\n`empty_like` ( and all _like operators ) will return channels last tensor if input is channels last, similar will apply to `to`, `clone`, `resize_as`. We are thinking about the ability to control suggest_memory_format behaviors by the global variable.",module: cudnn|triaged|module: memory format,VitalyFedyunin,"We define 4D tensor as stored in channels last memory format, when dimensions order is NCHW and ```C-strides < W-strides < H-strides < N-strides``` _(If size of any dimension is equal to 1, this dimension strides value is not taken into account)_.\r\n\r\nChannels last contiguous tensor is channel last tensor which occupies contiguous memory block. So ```x.is_contiguous(memory_format=torch.channels_last)``` checks if tensor is channels last contiguous.\r\n\r\nThe goal of the experiment is to use channels last memory format in all Resnet (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) model's operators and to measure performance gains on the Volta devices with CudNN library available.  \r\n\r\nThis experiment requires:\r\n1) Update operators kernels to follow the next rule: if one of the operator's inputs is channel last tensor, all outputs should also be in the channel last memory format.\r\n2) For better performance gain, update DataLoader to output channel last tensors.\r\n\r\nTo avoid changing the model itself and more importantly, introduce this optimization to the existing saved models. We need to introduce the next changes:\r\n- [ ] `to` operator should preserve memory format. `copy_device_to_device` should be memory format aware. (#23899)\r\n- [ ] `empty_like` operator should preserve memory format by default. (#23899)\r\n- [ ] `resize_as_` operator should be memory format aware. (#23899)\r\n- [x] `clone` operator should preserve memory format. (#23899)\r\n- [ ] `scatter` and `gather` functions should be memory format aware. (#24121)\r\n- [ ] `TensorIterator` based point-wise operators should preserve memory format (#24038).\r\n- [ ] `adaptive_avg_pool2d_cuda` and `adaptive_avg_pool2d_backward_cuda` should have channel last optimized kernels. (#24396)\r\n- [x] `max_pool2d_with_indices_cuda` and `max_pool2d_with_indices_backward_cuda` should have channel last optimized kernels (#24872).\r\n- [ ] `cudnn_batch_norm` and `cudnn_batch_norm_backward` should support channels last memory format. (#23861)\r\n- [ ] `cudnn_convolution_forward` and `cudnn_convolution_backward` should support channels last memory format. (#23861)\r\n\r\nWriting memory format aware operators require special functions introduced in #23391\r\n\r\n```cpp\r\nauto memory_format = input_tensor.suggest_memory_format();\r\nauto output_tensor = at::empty(output_shape, memory_format);\r\n\r\nswitch (memory_format) {\r\n  case MemoryFormat::ChannelsLast: {\r\n    input_cl_contiguous = input_tensor.contiguous(\r\n        MemoryFormat::ChannelsLast); // if kernel requires memory contiguous\r\n                                     // tensor\r\n    // .... kernel code\r\n    break;\r\n  }\r\n  case MemoryFormat::Contiguous: {\r\n    // .... standard kernel\r\n    break;\r\n  }\r\n  default:\r\n    TORCH_CHECK(\r\n        false,\r\n        ""Unsupported memory format. Supports only ChannelsLast, Contiguous"");\r\n}\r\n```\r\n\r\n## Notes\r\n\r\n- Resnet calls ```x = x.reshape(x.size(0), -1)``` before linear layers, we are going to update `reshape` and `view` code and convert tensor's memory format to `torch.contiguous_format` at this step.\r\n- Making old models/libraries to work faster also requires **BC sacrifices**, such as \r\n`empty_like` ( and all _like operators ) will return channels last tensor if input is channels last, similar will apply to `to`, `clone`, `resize_as`. We are thinking about the ability to control suggest_memory_format behaviors by the global variable.",C-strides < W-strides < H-strides < N-strides
23401,"RuntimeError: cuda runtime error (3) : initialization error AFTER daemonization## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nCalling `torch.cuda.is_available()` or `torch.cuda.device_count()` BEFORE `with daemon.DaemonContext(...):` and then call any torch API that internally calls ` th._C._cuda_getDevice()` AFTER daemonization throws the following runtime error:\r\n\r\n> RuntimeError: cuda runtime error (3) : initialization error at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp:52\r\n\r\nHowever, the use case is to set all the seeds and visible GPUs according to the availability of GPUs and user config as early as possible as part of the application framework without explicit initialization code by user  and let the user decide whether to daemonize or not later.\r\n\r\n## To Reproduce\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nSteps to reproduce the behavior:\r\n- Install `python-daemon` which is the implementation of PEP 3143.\r\n- Run the following script to reproduce error\r\n\r\n\r\n\r\nOuput:\r\n```sh\r\n$ python test-daemon.py \r\nBEFORE: 2\r\n$ \r\n$ THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp line=52 error=3 : initialization error\r\nTraceback (most recent call last):\r\n  File ""test-daemon.py"", line 28, in <module>\r\n    print('prev_idx:', th._C._cuda_getDevice())\r\nRuntimeError: cuda runtime error (3) : initialization error at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp:52\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nShould produce no error as called in the following order:\r\n\r\n\r\n\r\nOutput:\r\n```\r\nAFTER: 2\r\nprev_idx: 0\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Ubuntu 16.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6.8 from conda\r\n - CUDA/cuDNN version: 10.0.130/7.1.4\r\n - GPU models and configuration: 2 x TITAN RTX\r\n - Any other relevant information: driver 418.59\r\n",high priority|module: cuda|triaged,peterbell10,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nCalling `torch.cuda.is_available()` or `torch.cuda.device_count()` BEFORE `with daemon.DaemonContext(...):` and then call any torch API that internally calls ` th._C._cuda_getDevice()` AFTER daemonization throws the following runtime error:\r\n\r\n> RuntimeError: cuda runtime error (3) : initialization error at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp:52\r\n\r\nHowever, the use case is to set all the seeds and visible GPUs according to the availability of GPUs and user config as early as possible as part of the application framework without explicit initialization code by user  and let the user decide whether to daemonize or not later.\r\n\r\n## To Reproduce\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\nSteps to reproduce the behavior:\r\n- Install `python-daemon` which is the implementation of PEP 3143.\r\n- Run the following script to reproduce error\r\n\r\n```python\r\nimport sys, daemon, torch as th\r\nprint('BEFORE:', th.cuda.device_count())\r\nwith daemon.DaemonContext(stdout=sys.stdout, stderr=sys.stderr, chroot_directory=None, working_directory='.'):\r\n    print('prev_idx:', th._C._cuda_getDevice())\r\n```\r\n\r\nOuput:\r\n```sh\r\n$ python test-daemon.py \r\nBEFORE: 2\r\n$ \r\n$ THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp line=52 error=3 : initialization error\r\nTraceback (most recent call last):\r\n  File ""test-daemon.py"", line 28, in <module>\r\n    print('prev_idx:', th._C._cuda_getDevice())\r\nRuntimeError: cuda runtime error (3) : initialization error at /opt/conda/conda-bld/pytorch_1556653099582/work/torch/csrc/cuda/Module.cpp:52\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\nShould produce no error as called in the following order:\r\n\r\n```python\r\nimport sys, daemon, torch as th\r\nwith daemon.DaemonContext(stdout=sys.stdout, stderr=sys.stderr, chroot_directory=None, working_directory='.'):\r\n    print('AFTER:', th.cuda.device_count())\r\n    print('prev_idx:', th._C._cuda_getDevice())\r\n```\r\n\r\nOutput:\r\n```\r\nAFTER: 2\r\nprev_idx: 0\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1.0\r\n - OS (e.g., Linux): Ubuntu 16.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6.8 from conda\r\n - CUDA/cuDNN version: 10.0.130/7.1.4\r\n - GPU models and configuration: 2 x TITAN RTX\r\n - Any other relevant information: driver 418.59\r\n","python\r\nimport sys, daemon, torch as th\r\nprint('BEFORE:', th.cuda.device_count())\r\nwith daemon.DaemonContext(stdout=sys.stdout, stderr=sys.stderr, chroot_directory=None, working_directory='.'):\r\n    print('prev_idx:', th._C._cuda_getDevice())\r\n"
23352,"[Feature Request] inferred module dimensions## Motivation\r\n\r\nThere\u2019s a lot of cases in framework design where we want to be able to treat components like lego blocks when building various model architectures. The following case studies in PyText are the 90% common cases, not the exceptions, ie. most people build models from these architecture families.\r\n\r\nDocument classifiers in PyText follow a general pattern of Embedding \u2192 Representation \u2192 Decoder.\r\n\r\n* Embedding\r\n    * Word embedding\r\n    * BiTransformer\r\n    * Character embedding\r\n* Representation\r\n    * Encoder\r\n        * DocNN\r\n        * LSTM (possibly bidirectional)\r\n    * Pooling layer\r\n        * Max pool\r\n        * Attention\r\n        * etc\r\n* Decoder\r\n    * Almost always an MLP, needs to have an output dimension of #classes\r\n\r\nWithin the 4 categories of Embedding/Encoder/Pooling/Decoder though, each boundary depends on the dimensionality of the previous component \u2014 the encoder depends on the embedding dimension, the pooling depends on the encoder dimension (including eg. whether the LSTM is bidirectional), and the decoder depends on the pooling layer output dimension (and #classes).\r\n\r\nHowever, from a user standpoint, it is extremely desirable to not have to make sure that these dimensions match up exactly. For instance, if a product team wants to experiment with DocNN vs BiLSTM with self attention and two different sizes of word embeddings, they would need to know how to compute the correct input dimensions for their encoders, pooling layers, and MLPs for each of the 4 cases. If they have dense features, those might also factor into the computation for the MLP layer.\r\n\r\nPyText solves this problem by having modules of each of these categories compute and track the appropriate dimensionality internally to their constructor, and then constructing the model and wiring the various blocks together before training. See [DocNN](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/representations/docnn.py#L30) representation_dim computation for an example of this, and [Model.from_config](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/model.py#L167) for where we use this to construct the model.\r\n\r\nThis solution works for us, but only in the case where we take the responsibility of constructing the model, along with all of its subcomponents, away from the user. If we wanted to eg. allow the user to pass in their own representation layer, we wouldn\u2019t be able to ensure the dimensionality match, the user is therefore responsible for also constructing _every other model component_. As a direct result of this design, PyText is abysmally hard to use in a notebook, and we built a Config system for managing hyperparameters that essentially boils down to allowing users to partially construct modules, then let us take over and finish building the whole model. See one of our [BiLSTM Config](https://github.com/facebookresearch/pytext/blob/master/pytext/models/representations/bilstm_doc_attention.py#L35) classes for an example.\r\n\r\nThis issue is not specific to PyText. Consider this snippet from the demo example from the AllenNLP website:\r\n\r\n\r\n(source - https://allennlp.org/tutorials)\r\n\r\nNote that similar to PyText, we need to match up the EMBEDDING_DIM between token_embedding and the LSTM module; we therefore couldn\u2019t construct an LstmTagger object with a default argument for just one of word_embeddings or lstm, because there would be no way to create them with the appropriate dimensions.\r\n\r\n## Proposal - Lazy Modules\r\n\r\nWe propose adding lazily inferred dimensions to some core Torch components, such as Linear, ConvNd, and RNN. This syntax should allow constructing these components (and any derived or more complex user components containing them) to instantiate the models without fully defining their dimensions, and have them inferred from the tensors passed in the first time their forward function is called. We propose using -1 to represent these dimensions similar to how -1 is used in reshape now (None has also been suggested, covered in alternatives below). For example\r\n\r\n\r\n\r\nThis would allow building more complex components that could also infer their input dimensions; consider a simple MLP implementation, much like what PyText uses for its decoders:\r\n\r\n\r\n\r\nNow this MLP can be used in any PyText model, regardless of what the output shape of representation/pooling is:\r\n\r\n\r\n\r\nAnd in the cases of PyText and AllenNLP these could allow interfaces that are much simpler and more modular:\r\n\r\n\r\n\r\n## Sample implementation\r\n\r\nThis will be supplied shortly following this proposal as a pull request in torch.nn.lazy, but for now here is the proposed implementation.\r\n\r\n\r\n\r\n## Drawbacks\r\n\r\nLazy modules don\u2019t work perfectly with everything in torch. In particular, if they\u2019re not yet finalized (ie. -1 was passed to a dimension to be inferred, but forward hasn\u2019t been called yet), the following behaviors might be unintuitive:\r\n\r\n* torch.save/torch.jit.script won\u2019t work properly\r\n    * both confirmed to work fine with the sample implementation once initialization finishes\r\n* module.apply()\r\n* any parameter initialization functions\r\n* module.parameters() won\u2019t return all of the model parameters, so\r\n    * creating an optimizer/scheduler normally, ie torch.optim.Adam(module.parameters()) will potentially have unintended side-effects, as not all of the module\u2019s eventual parameters will optimized\r\n    * calling module.to() will move the parameters that exist, but once forward is called and new parameters are created, they will be on the default device\r\n        * this can potentially be solved with some tighter init behavior and unit tests, as there\u2019s a finite number of components that need to be made lazy, and they can usually look at other parameters on the module and assume that users want them to be on the same device\r\n        * alternatively it may make sense to have a concept of module-level device residency\r\n\r\n## Alternatives\r\n\r\n### None instead of -1\r\n\r\nUse the above proposal, but pass None to indicate an inferred dimension rather than -1. -1 was chosen for the proposal because it mirrors the semantics of reshape and a few other operators, but None may be more intuitive.\r\n\r\n### Support -1 (or None) for Tensor dimensions\r\n\r\nInstead of implementing this at the Module level with lazy modules, implement this at the Tensor level. Have Tensors be able to be lazy- or partially-initialized, and then be able to be finalized with their final dimensions. This would solve most of the above drawbacks above, as for instance we\u2019d always be making and setting all parameters, and it would also likely simplify its integration as all known useful cases of lazy dimensions eventually boil down to passing them to Tensors. However, it\u2019s less clear what this implementation would look like, for instance how the Tensors would generically be able to infer the parameters.\r\n\r\n### Lazy Parameters\r\n\r\nIf this were implemented at the Parameter level, it would require pretty radically increasing the complexity of how Parameters work, but could solve most of the drawbacks of this proposal while also leaving Tensor untouched. We can flesh out this design more if there is significant feedback in that direction.\r\n\r\n### Support via metaclass vs non-metaclass\r\n\r\nThe current proposal implementation uses a metaclass for Lazy modules to introduce a novel initialization pattern for lazy components (this same pattern could eg. also be used to solve an analogous problems for Optimizer/Scheduler in the future). However, using metaclasses always comes at a cost in Python, especially for libraries that are being built on-top-of, because multiple inheritance in the case of multiple metaclasses is a problem with no generic solution. In other words, if any libraries built on top of pytorch want to use metaclasses in their own Module subclasses, they\u2019ll need to do non-trivial work to make sure those metaclasses are compatible with any metaclass used in pytorch.\r\nThe proposal could likely be modified to, for example, use a decorator on the __init__ function instead of a metaclass.",feature|module: nn|triaged,gchanan|dzhulgakov,"## Motivation\r\n\r\nThere\u2019s a lot of cases in framework design where we want to be able to treat components like lego blocks when building various model architectures. The following case studies in PyText are the 90% common cases, not the exceptions, ie. most people build models from these architecture families.\r\n\r\nDocument classifiers in PyText follow a general pattern of Embedding \u2192 Representation \u2192 Decoder.\r\n\r\n* Embedding\r\n    * Word embedding\r\n    * BiTransformer\r\n    * Character embedding\r\n* Representation\r\n    * Encoder\r\n        * DocNN\r\n        * LSTM (possibly bidirectional)\r\n    * Pooling layer\r\n        * Max pool\r\n        * Attention\r\n        * etc\r\n* Decoder\r\n    * Almost always an MLP, needs to have an output dimension of #classes\r\n\r\nWithin the 4 categories of Embedding/Encoder/Pooling/Decoder though, each boundary depends on the dimensionality of the previous component \u2014 the encoder depends on the embedding dimension, the pooling depends on the encoder dimension (including eg. whether the LSTM is bidirectional), and the decoder depends on the pooling layer output dimension (and #classes).\r\n\r\nHowever, from a user standpoint, it is extremely desirable to not have to make sure that these dimensions match up exactly. For instance, if a product team wants to experiment with DocNN vs BiLSTM with self attention and two different sizes of word embeddings, they would need to know how to compute the correct input dimensions for their encoders, pooling layers, and MLPs for each of the 4 cases. If they have dense features, those might also factor into the computation for the MLP layer.\r\n\r\nPyText solves this problem by having modules of each of these categories compute and track the appropriate dimensionality internally to their constructor, and then constructing the model and wiring the various blocks together before training. See [DocNN](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/representations/docnn.py#L30) representation_dim computation for an example of this, and [Model.from_config](https://github.com/facebookresearch/pytext/blob/a64370c7f1aac1c0491ccf5b2b0987909c9e0d94/pytext/models/model.py#L167) for where we use this to construct the model.\r\n\r\nThis solution works for us, but only in the case where we take the responsibility of constructing the model, along with all of its subcomponents, away from the user. If we wanted to eg. allow the user to pass in their own representation layer, we wouldn\u2019t be able to ensure the dimensionality match, the user is therefore responsible for also constructing _every other model component_. As a direct result of this design, PyText is abysmally hard to use in a notebook, and we built a Config system for managing hyperparameters that essentially boils down to allowing users to partially construct modules, then let us take over and finish building the whole model. See one of our [BiLSTM Config](https://github.com/facebookresearch/pytext/blob/master/pytext/models/representations/bilstm_doc_attention.py#L35) classes for an example.\r\n\r\nThis issue is not specific to PyText. Consider this snippet from the demo example from the AllenNLP website:\r\n\r\n```python\r\nEMBEDDING_DIM = 6\r\nHIDDEN_DIM = 6\r\n\r\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\r\nembedding_dim=EMBEDDING_DIM)\r\nword_embeddings = BasicTextFieldEmbedder({""tokens"": token_embedding})\r\nlstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\r\nmodel = LstmTagger(word_embeddings, lstm, vocab)\r\n```\r\n(source - https://allennlp.org/tutorials)\r\n\r\nNote that similar to PyText, we need to match up the EMBEDDING_DIM between token_embedding and the LSTM module; we therefore couldn\u2019t construct an LstmTagger object with a default argument for just one of word_embeddings or lstm, because there would be no way to create them with the appropriate dimensions.\r\n\r\n## Proposal - Lazy Modules\r\n\r\nWe propose adding lazily inferred dimensions to some core Torch components, such as Linear, ConvNd, and RNN. This syntax should allow constructing these components (and any derived or more complex user components containing them) to instantiate the models without fully defining their dimensions, and have them inferred from the tensors passed in the first time their forward function is called. We propose using -1 to represent these dimensions similar to how -1 is used in reshape now (None has also been suggested, covered in alternatives below). For example\r\n\r\n```python\r\n>>> l = nn.Linear(-1, 3)\r\n>>> l(torch.rand(2, 4)).size()\r\n[2, 3]\r\n>>> l(torch.rand(2, 5))\r\nTypeError(...)\r\n```\r\n\r\nThis would allow building more complex components that could also infer their input dimensions; consider a simple MLP implementation, much like what PyText uses for its decoders:\r\n\r\n```python\r\nclass MLP(nn.Module):\r\n    def __init__(self, dims: List[int], dropout: float = 0.):\r\n        super().__init__()\r\n        layers = []\r\n        for in_dim, dim in zip([-1] + dims, dims):\r\n            layers.extend([\r\n                nn.Linear(in_dim, dim),\r\n                ReLU(),\r\n                nn.LayerNorm(dim),\r\n                nn.Dropout(dropout),\r\n            ])\r\n        self.layers = nn.Sequential(*layers)\r\n\r\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(input)\r\n```\r\n\r\nNow this MLP can be used in any PyText model, regardless of what the output shape of representation/pooling is:\r\n\r\n```python\r\nmlp1 = MLP([4, 5, 6])\r\nmlp2 = MLP([4, 5, 6])\r\n\r\nmlp1(torch.rand(2, 4))  # finalizes mlp1 as expecting input dimension 4\r\nmlp2(torch.rand(2, 5))  # finalizes mlp2 as expecting input dimension 5\r\n```\r\n\r\nAnd in the cases of PyText and AllenNLP these could allow interfaces that are much simpler and more modular:\r\n\r\n```python\r\nclass Model(nn.Module):\r\n    def __init__(\r\n        self,\r\n        embedding = WordEmbedding(),\r\n        representation = BiLSTM(),\r\n        decoder = MLP([4, 5, 6]),\r\n    ):\r\n        self.embedding = embedding\r\n        self.representation = representation\r\n        self.decoder = decoder\r\n    \r\n    def forward(self, tokens):\r\n        embedded = self.embedding(tokens)\r\n        representation = self.representation(embedded)\r\n        return self.decoder(representation)\r\n        \r\nmodel1 = Model()\r\nmodel2 = Model(representation=DocNN())\r\nmodel3 = Model(\r\n    embedding=WordEmbedding(embedding_dim=200, vocab_size=10000),\r\n    decoder=MLP([2, 50, 10]),\r\n)\r\n```\r\n\r\n## Sample implementation\r\n\r\nThis will be supplied shortly following this proposal as a pull request in torch.nn.lazy, but for now here is the proposed implementation.\r\n\r\n```python\r\nfrom torch import nn\r\n\r\n# LazyModuleMeta re-implements the type construction semantics for objects to allow\r\n# a slight variant on syntax. Essentially anything with this metaclass can optionally\r\n# execute a single yield statement during its constructor (normal constructors also work fine).\r\n# If it does yield during construction, then a __lazy_init__ function is populated;\r\n# any code occurring before yield in the constuctor will be called as normal during object creation,\r\n# and any code after yield will instead be deferred to the first call of __lazy_init__.\r\n\r\nclass LazyInitMeta(type):\r\n    def __call__(cls, *args, **kwargs):\r\n        if hasattr(cls, '__new__'):\r\n            obj = cls.__new__(cls, *args, **kwargs)\r\n        else:\r\n            obj = object.__new__(cls)\r\n        \r\n        def initialize(obj):\r\n            res = obj.__init__(*args, **kwargs)\r\n            if isinstance(res, types.GeneratorType):\r\n                next(res, None)\r\n                def lazy_init(call_args):\r\n                    try:\r\n                        res.send(call_args)\r\n                    except StopIteration:\r\n                        pass\r\n                    finally:\r\n                        obj.__lazy_init__ = None\r\n                obj.__lazy_init__ = lazy_init\r\n            else:\r\n                obj.__lazy_init__ = None\r\n            \r\n        if isinstance(obj, cls):\r\n            initialize(obj)\r\n        return obj\r\n        \r\n# Here's a Lazy nn.Module implementation using LazyInitMeta, calling __lazy_init__ before the first\r\n# forward pass.\r\n    \r\nclass LazyModule(nn.Module, metaclass=LazyInitMeta):\r\n    def __init__(self):\r\n        nn.Module.__init__(self)\r\n    \r\n    def __call__(self, *args, **kwargs):\r\n        if self.__lazy_init__:\r\n            self.__lazy_init__(call_args=(args, kwargs))\r\n        return nn.Module.__call__(self, *args, **kwargs)\r\n        \r\n# Optionally lazy Linear module, based on the current torch implementation of nn.Linear.\r\n\r\nclass Linear(nn.Linear, LazyModule):\r\n    def __init__(self, in_features, out_features, bias=True):\r\n        LazyModule.__init__(self)\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        if bias:\r\n            self.bias = nn.Parameter(torch.Tensor(out_features))\r\n        else:\r\n            self.register_parameter('bias', None)\r\n            \r\n        if in_features == -1:\r\n            self.register_parameter('weight', None)\r\n            ([input], _) = yield  # lazy init remainder\r\n            in_features = self.in_features = input.size()[-1]\r\n            \r\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\r\n        self.reset_parameters()\r\n```\r\n\r\n## Drawbacks\r\n\r\nLazy modules don\u2019t work perfectly with everything in torch. In particular, if they\u2019re not yet finalized (ie. -1 was passed to a dimension to be inferred, but forward hasn\u2019t been called yet), the following behaviors might be unintuitive:\r\n\r\n* torch.save/torch.jit.script won\u2019t work properly\r\n    * both confirmed to work fine with the sample implementation once initialization finishes\r\n* module.apply()\r\n* any parameter initialization functions\r\n* module.parameters() won\u2019t return all of the model parameters, so\r\n    * creating an optimizer/scheduler normally, ie torch.optim.Adam(module.parameters()) will potentially have unintended side-effects, as not all of the module\u2019s eventual parameters will optimized\r\n    * calling module.to() will move the parameters that exist, but once forward is called and new parameters are created, they will be on the default device\r\n        * this can potentially be solved with some tighter init behavior and unit tests, as there\u2019s a finite number of components that need to be made lazy, and they can usually look at other parameters on the module and assume that users want them to be on the same device\r\n        * alternatively it may make sense to have a concept of module-level device residency\r\n\r\n## Alternatives\r\n\r\n### None instead of -1\r\n\r\nUse the above proposal, but pass None to indicate an inferred dimension rather than -1. -1 was chosen for the proposal because it mirrors the semantics of reshape and a few other operators, but None may be more intuitive.\r\n\r\n### Support -1 (or None) for Tensor dimensions\r\n\r\nInstead of implementing this at the Module level with lazy modules, implement this at the Tensor level. Have Tensors be able to be lazy- or partially-initialized, and then be able to be finalized with their final dimensions. This would solve most of the above drawbacks above, as for instance we\u2019d always be making and setting all parameters, and it would also likely simplify its integration as all known useful cases of lazy dimensions eventually boil down to passing them to Tensors. However, it\u2019s less clear what this implementation would look like, for instance how the Tensors would generically be able to infer the parameters.\r\n\r\n### Lazy Parameters\r\n\r\nIf this were implemented at the Parameter level, it would require pretty radically increasing the complexity of how Parameters work, but could solve most of the drawbacks of this proposal while also leaving Tensor untouched. We can flesh out this design more if there is significant feedback in that direction.\r\n\r\n### Support via metaclass vs non-metaclass\r\n\r\nThe current proposal implementation uses a metaclass for Lazy modules to introduce a novel initialization pattern for lazy components (this same pattern could eg. also be used to solve an analogous problems for Optimizer/Scheduler in the future). However, using metaclasses always comes at a cost in Python, especially for libraries that are being built on-top-of, because multiple inheritance in the case of multiple metaclasses is a problem with no generic solution. In other words, if any libraries built on top of pytorch want to use metaclasses in their own Module subclasses, they\u2019ll need to do non-trivial work to make sure those metaclasses are compatible with any metaclass used in pytorch.\r\nThe proposal could likely be modified to, for example, use a decorator on the __init__ function instead of a metaclass.","python\r\nEMBEDDING_DIM = 6\r\nHIDDEN_DIM = 6\r\n\r\ntoken_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\r\nembedding_dim=EMBEDDING_DIM)\r\nword_embeddings = BasicTextFieldEmbedder({""tokens"": token_embedding})\r\nlstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\r\nmodel = LstmTagger(word_embeddings, lstm, vocab)\r\n"
23122,"torch.jit.trace_module creates only one method## \U0001f41b Bug\r\n\r\n`torch.jit.trace_module` creates only one method as [`return` is in the `for` loop](https://github.com/pytorch/pytorch/blob/77353636de32a207cf0a332395f91011bc2f07fb/torch/jit/__init__.py#L890-L906):\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\nThe `module` has methods `forward`, `mul_100` and `add_100`.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.2.0.dev20190719\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\n## Additional context\r\n",high priority|oncall: jit|triaged,Krovatkin,"## \U0001f41b Bug\r\n\r\n`torch.jit.trace_module` creates only one method as [`return` is in the `for` loop](https://github.com/pytorch/pytorch/blob/77353636de32a207cf0a332395f91011bc2f07fb/torch/jit/__init__.py#L890-L906):\r\n\r\n```python\r\nfor method_name, example_inputs in inputs.items():\r\n    # ...\r\n    \r\n    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)\r\n    \r\n    # ...\r\n    \r\n    return module\r\n```\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\nclass Module(nn.Module):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        return x\r\n\r\n    def mul_100(self, x):\r\n        return x * 100\r\n\r\n    def add_100(self, x):\r\n        return x + 100\r\n\r\n\r\nx = torch.ones(1)\r\nmodule = torch.jit.trace_module(Module(),\r\n                                {'forward': x, 'mul_100': x, 'add_100': x})\r\n# print False\r\nprint(all(hasattr(module, method_name)\r\n          for method_name in ['forward', 'mul_100', 'add_100']))\r\n```\r\n\r\n## Expected behavior\r\nThe `module` has methods `forward`, `mul_100` and `add_100`.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.2.0.dev20190719\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\n## Additional context\r\n","python\r\nfor method_name, example_inputs in inputs.items():\r\n    # ...\r\n    \r\n    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, _force_outplace)\r\n    \r\n    # ...\r\n    \r\n    return module\r\n"
23074,"Hang at initializing DistributedDataParallel Distributed processes hang at the initilization of DistributedDataParallel line.\r\n\r\n### System env\r\nTwo nodes, both have same environment and hardware\r\nUbuntu 16.04.5 LTS (GNU/Linux 4.4.0-131-generic x86_64)\r\npytorch 1.1.0\r\n2*RTX TITAN 24GB\r\nCuda compilation tools, release 10.0, V10.0.130\r\nPython 3.6.5\r\n[GCC 5.4.0 20160609] on linux\r\n\r\n### Part of codes\r\n\r\n\r\n### Command lines\r\nNode 0:\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr=""172.16.10.23"" --master_port=2333 distributed_train.py -fm -cn cluster-3036 -pid ditrb700 -rlid ditrb3036_4 -ptc 25 -epo 500 -relearn -batch 4 -gpu 0,1 -world_size 4\r\n```\r\n\r\nNode 1:\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=1 --master_addr=""172.16.10.23"" --master_port=2333 distributed_train.py -fm -cn cluster-3036 -pid ditrb700 -rlid ditrb3036_4 -ptc 25 -epo 500 -relearn -batch 4 -gpu 0,1 -world_size 4\r\n```\r\n\r\n### Terminal output:\r\nNode 0:\r\n```\r\nathena02:51627:51627 [1] NCCL INFO NET/Socket : Using [0]enp96s0f0:172.16.10.23<0>\r\nathena02:51627:51627 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena02:51627:51627 [1] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nathena02:51627:51819 [1] NCCL INFO Setting affinity for GPU 1 to ff,c00ffc00\r\nathena02:51627:51819 [1] NCCL INFO comm 0x7ff8c80019a0 rank 1 nranks 4 cudaDev 1 nvmlDev 1\r\nDistribute generator:0...\r\nathena02:51626:51626 [0] NCCL INFO NET/Socket : Using [0]enp96s0f0:172.16.10.23<0>\r\nathena02:51626:51626 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena02:51626:51626 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.2+cuda10.0\r\nathena02:51626:51822 [0] NCCL INFO Setting affinity for GPU 0 to 3ff003ff\r\nathena02:51626:51822 [0] NCCL INFO comm 0x7f38d00019a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0\r\n```\r\n\r\nNode 1:\r\n```\r\nathena03:80214:80214 [0] NCCL INFO NET/Socket : Using [0]eno1:172.16.10.24<0>\r\nathena03:80214:80214 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena03:80214:80214 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.2+cuda10.0\r\nathena03:80214:80445 [0] NCCL INFO Setting affinity for GPU 0 to 5555,55555555\r\nathena03:80214:80445 [0] NCCL INFO comm 0x7ff2dc0019a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0\r\nathena03:80215:80215 [1] NCCL INFO NET/Socket : Using [0]eno1:172.16.10.24<0>\r\nathena03:80215:80215 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena03:80215:80215 [1] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nathena03:80215:80446 [1] NCCL INFO Setting affinity for GPU 1 to aaaa,aaaaaaaa\r\nathena03:80215:80446 [1] NCCL INFO comm 0x7fb9840019a0 rank 1 nranks 4 cudaDev 1 nvmlDev 1\r\n**Weriod output here:**\r\nathena03:80214:80444 [0] bootstrap.cu:102 NCCL WARN Bootstrap Root : rank 1 of 4 ranks has already checked in\r\n```\r\n\r\n### Node-0 rank-0 Subprocess tracing\r\ngdb -p 51626\r\ngdb bt\r\n```\r\n#10 _PyCFunction_FastCallKeywords () at Objects/methodobject.c:294\r\n#11 0x000000000054de49 in call_function () at Python/ceval.c:4824\r\n#12 0x000000000054644a in _PyEval_EvalFrameDefault () at Python/ceval.c:3322\r\n#13 0x000000000054eeed in PyEval_EvalFrameEx (throwflag=0, f=0x59ce138) at Python/ceval.c:753\r\n#14 _PyFunction_FastCall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>)at Python/ceval.c:4906\r\n```\r\n\r\n",oncall: distributed|triaged,mrshenli,"Distributed processes hang at the initilization of DistributedDataParallel line.\r\n\r\n### System env\r\nTwo nodes, both have same environment and hardware\r\nUbuntu 16.04.5 LTS (GNU/Linux 4.4.0-131-generic x86_64)\r\npytorch 1.1.0\r\n2*RTX TITAN 24GB\r\nCuda compilation tools, release 10.0, V10.0.130\r\nPython 3.6.5\r\n[GCC 5.4.0 20160609] on linux\r\n\r\n### Part of codes\r\n```python\r\nimport apex.amp as amp\r\n...\r\n...\r\ntorch.cuda.set_device(args.local_rank)\r\ntorch.distributed.init_process_group(\r\n        'nccl',\r\n        init_method='env://',\r\n        world_size=args.world_size,\r\n        rank=args.local_rank,\r\n    )     \r\n...\r\n...\r\nclass testNet(nn.Module):\r\n    def __init__(self):\r\n        super(testNet, self).__init__()\r\n        self.layer_down = torch.nn.Sequential(\r\n            nn.Conv2d(32, 32, kernel_size=1, stride=2)\r\n        )\r\n    def forward(self, x):\r\n        x = self.layer_down(x)\r\n        return x\r\ngenerator = testNet().type(torch.FloatTensor).to(torch.device('cuda:{}'.format(args.local_rank)))\r\n\r\n**Hanging line here:** \r\ngenerator = nn.parallel.DistributedDataParallel(\r\n                self.generator,\r\n                device_ids=[self.local_rank],\r\n                output_device=self.local_rank,\r\n            )\r\n```\r\n\r\n### Command lines\r\nNode 0:\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr=""172.16.10.23"" --master_port=2333 distributed_train.py -fm -cn cluster-3036 -pid ditrb700 -rlid ditrb3036_4 -ptc 25 -epo 500 -relearn -batch 4 -gpu 0,1 -world_size 4\r\n```\r\n\r\nNode 1:\r\n```\r\nNCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=COLL python3 -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=1 --master_addr=""172.16.10.23"" --master_port=2333 distributed_train.py -fm -cn cluster-3036 -pid ditrb700 -rlid ditrb3036_4 -ptc 25 -epo 500 -relearn -batch 4 -gpu 0,1 -world_size 4\r\n```\r\n\r\n### Terminal output:\r\nNode 0:\r\n```\r\nathena02:51627:51627 [1] NCCL INFO NET/Socket : Using [0]enp96s0f0:172.16.10.23<0>\r\nathena02:51627:51627 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena02:51627:51627 [1] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nathena02:51627:51819 [1] NCCL INFO Setting affinity for GPU 1 to ff,c00ffc00\r\nathena02:51627:51819 [1] NCCL INFO comm 0x7ff8c80019a0 rank 1 nranks 4 cudaDev 1 nvmlDev 1\r\nDistribute generator:0...\r\nathena02:51626:51626 [0] NCCL INFO NET/Socket : Using [0]enp96s0f0:172.16.10.23<0>\r\nathena02:51626:51626 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena02:51626:51626 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.2+cuda10.0\r\nathena02:51626:51822 [0] NCCL INFO Setting affinity for GPU 0 to 3ff003ff\r\nathena02:51626:51822 [0] NCCL INFO comm 0x7f38d00019a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0\r\n```\r\n\r\nNode 1:\r\n```\r\nathena03:80214:80214 [0] NCCL INFO NET/Socket : Using [0]eno1:172.16.10.24<0>\r\nathena03:80214:80214 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena03:80214:80214 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nNCCL version 2.4.2+cuda10.0\r\nathena03:80214:80445 [0] NCCL INFO Setting affinity for GPU 0 to 5555,55555555\r\nathena03:80214:80445 [0] NCCL INFO comm 0x7ff2dc0019a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0\r\nathena03:80215:80215 [1] NCCL INFO NET/Socket : Using [0]eno1:172.16.10.24<0>\r\nathena03:80215:80215 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\nathena03:80215:80215 [1] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nathena03:80215:80446 [1] NCCL INFO Setting affinity for GPU 1 to aaaa,aaaaaaaa\r\nathena03:80215:80446 [1] NCCL INFO comm 0x7fb9840019a0 rank 1 nranks 4 cudaDev 1 nvmlDev 1\r\n**Weriod output here:**\r\nathena03:80214:80444 [0] bootstrap.cu:102 NCCL WARN Bootstrap Root : rank 1 of 4 ranks has already checked in\r\n```\r\n\r\n### Node-0 rank-0 Subprocess tracing\r\ngdb -p 51626\r\ngdb bt\r\n```\r\n#10 _PyCFunction_FastCallKeywords () at Objects/methodobject.c:294\r\n#11 0x000000000054de49 in call_function () at Python/ceval.c:4824\r\n#12 0x000000000054644a in _PyEval_EvalFrameDefault () at Python/ceval.c:3322\r\n#13 0x000000000054eeed in PyEval_EvalFrameEx (throwflag=0, f=0x59ce138) at Python/ceval.c:753\r\n#14 _PyFunction_FastCall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>)at Python/ceval.c:4906\r\n```\r\n\r\n","python\r\nimport apex.amp as amp\r\n...\r\n...\r\ntorch.cuda.set_device(args.local_rank)\r\ntorch.distributed.init_process_group(\r\n        'nccl',\r\n        init_method='env://',\r\n        world_size=args.world_size,\r\n        rank=args.local_rank,\r\n    )     \r\n...\r\n...\r\nclass testNet(nn.Module):\r\n    def __init__(self):\r\n        super(testNet, self).__init__()\r\n        self.layer_down = torch.nn.Sequential(\r\n            nn.Conv2d(32, 32, kernel_size=1, stride=2)\r\n        )\r\n    def forward(self, x):\r\n        x = self.layer_down(x)\r\n        return x\r\ngenerator = testNet().type(torch.FloatTensor).to(torch.device('cuda:{}'.format(args.local_rank)))\r\n\r\n**Hanging line here:** \r\ngenerator = nn.parallel.DistributedDataParallel(\r\n                self.generator,\r\n                device_ids=[self.local_rank],\r\n                output_device=self.local_rank,\r\n            )\r\n"
22976,"Conv2d ONNX export via TorchScript## \U0001f41b Bug\r\n\r\nA model using `torch.nn.Conv2D` can be exported by tracing, but not if it is a ScriptModule.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Check that `Conv2d` exports to ONNX\r\n\r\n```\r\ngraph(%0 : Float(16, 3, 100, 250),\r\n      %1 : Float(64, 3, 3, 3),\r\n      %2 : Float(64)):\r\n  %3 : Float(16, 64, 100, 250) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1, %2), scope: Model2/Conv2d[block_1] # /home/jonny/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\r\n  return (%3)\r\n```\r\n2. Try it as a TorchScript module\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-46-c12a5924cc60> in <module>\r\n     17                                 verbose=True,\r\n     18                                 operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n---> 19                                 example_outputs=model(x),\r\n     20                                )\r\n     21 model_onnx\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py in _export(*args, **kwargs)\r\n     24 def _export(*args, **kwargs):\r\n     25     from torch.onnx import utils\r\n---> 26     result = utils._export(*args, **kwargs)\r\n     27     return result\r\n     28 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes)\r\n    427                                                         output_names, operator_export_type,\r\n    428                                                         example_outputs, propagate,\r\n--> 429                                                         _retain_param_name, do_constant_folding)\r\n    430 \r\n    431         # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop)\r\n    328 \r\n    329     graph = _optimize_graph(graph, operator_export_type,\r\n--> 330                             _disable_torch_constant_prop=_disable_torch_constant_prop)\r\n    331 \r\n    332     if isinstance(model, torch.jit.ScriptModule) or isinstance(model, torch.jit.Function):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop)\r\n    230         torch._C._jit_pass_erase_number_types(graph)\r\n    231 \r\n--> 232         graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n    233         torch._C._jit_pass_lint(graph)\r\n    234         from torch.onnx.symbolic_helper import _export_onnx_opset_version\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)\r\n     55 def _run_symbolic_function(*args, **kwargs):\r\n     56     from torch.onnx import utils\r\n---> 57     return utils._run_symbolic_function(*args, **kwargs)\r\n     58 \r\n     59 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    661                                   ""torch.onnx.symbolic_opset{}.{} does not exist""\r\n    662                                   .format(op_name, opset_version, op_name))\r\n--> 663                 op_fn = sym_registry.get_registered_op(op_name, '', opset_version)\r\n    664                 return op_fn(g, *inputs, **attrs)\r\n    665 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_registry.py in get_registered_op(opname, domain, version)\r\n     89         warnings.warn(""ONNX export failed. The ONNX domain and/or version are None."")\r\n     90     global _registry\r\n---> 91     return _registry[(domain, version)][opname]\r\n\r\nKeyError: 'conv2d'\r\n```\r\n\r\n## Expected behavior\r\n\r\nI had hoped it would behave identically :innocent:\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0.dev20190717\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 410.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.2.0.dev20190717\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.1.0               py3.7_cpu_0    pytorch\r\n[conda] pytorch-nightly           1.2.0.dev20190717 py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py37_cuNone_1    pytorch",oncall: jit|module: onnx|triaged,houseroad,"## \U0001f41b Bug\r\n\r\nA model using `torch.nn.Conv2D` can be exported by tracing, but not if it is a ScriptModule.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Check that `Conv2d` exports to ONNX\r\n```python\r\nimport torch\r\nclass Model1(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        \r\n        self.block_1 = torch.nn.Conv2d(3, 64, 3, padding=1)\r\n\r\n    def forward(self, x):\r\n        x = self.block_1(x)\r\n        return x\r\n\r\nx = torch.randn(16, 3, 100, 250)\r\nmodel = Model1()\r\nmodel.eval()\r\nmodel_onnx = torch.onnx._export(model, x, ""conv2d.onnx"",\r\n                                verbose=True,\r\n                                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n                                example_outputs=model(x),\r\n                               )\r\nmodel_onnx\r\n```\r\n```\r\ngraph(%0 : Float(16, 3, 100, 250),\r\n      %1 : Float(64, 3, 3, 3),\r\n      %2 : Float(64)):\r\n  %3 : Float(16, 64, 100, 250) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1, %2), scope: Model2/Conv2d[block_1] # /home/jonny/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py:340:0\r\n  return (%3)\r\n```\r\n2. Try it as a TorchScript module\r\n```python\r\nimport torch\r\nclass Model2(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        \r\n        self.block_1 = torch.nn.Conv2d(3, 64, 3, padding=1)\r\n        \r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        x = self.block_1(x)\r\n        return x\r\n\r\nx = torch.randn(16, 3, 100, 250)\r\nmodel = Model2()\r\nmodel.eval()\r\nmodel_onnx = torch.onnx._export(model, x, ""conv2d.onnx"",\r\n                                verbose=True,\r\n                                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n                                example_outputs=model(x),\r\n                               )\r\nmodel_onnx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-46-c12a5924cc60> in <module>\r\n     17                                 verbose=True,\r\n     18                                 operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n---> 19                                 example_outputs=model(x),\r\n     20                                )\r\n     21 model_onnx\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py in _export(*args, **kwargs)\r\n     24 def _export(*args, **kwargs):\r\n     25     from torch.onnx import utils\r\n---> 26     result = utils._export(*args, **kwargs)\r\n     27     return result\r\n     28 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes)\r\n    427                                                         output_names, operator_export_type,\r\n    428                                                         example_outputs, propagate,\r\n--> 429                                                         _retain_param_name, do_constant_folding)\r\n    430 \r\n    431         # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop)\r\n    328 \r\n    329     graph = _optimize_graph(graph, operator_export_type,\r\n--> 330                             _disable_torch_constant_prop=_disable_torch_constant_prop)\r\n    331 \r\n    332     if isinstance(model, torch.jit.ScriptModule) or isinstance(model, torch.jit.Function):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop)\r\n    230         torch._C._jit_pass_erase_number_types(graph)\r\n    231 \r\n--> 232         graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n    233         torch._C._jit_pass_lint(graph)\r\n    234         from torch.onnx.symbolic_helper import _export_onnx_opset_version\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)\r\n     55 def _run_symbolic_function(*args, **kwargs):\r\n     56     from torch.onnx import utils\r\n---> 57     return utils._run_symbolic_function(*args, **kwargs)\r\n     58 \r\n     59 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    661                                   ""torch.onnx.symbolic_opset{}.{} does not exist""\r\n    662                                   .format(op_name, opset_version, op_name))\r\n--> 663                 op_fn = sym_registry.get_registered_op(op_name, '', opset_version)\r\n    664                 return op_fn(g, *inputs, **attrs)\r\n    665 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_registry.py in get_registered_op(opname, domain, version)\r\n     89         warnings.warn(""ONNX export failed. The ONNX domain and/or version are None."")\r\n     90     global _registry\r\n---> 91     return _registry[(domain, version)][opname]\r\n\r\nKeyError: 'conv2d'\r\n```\r\n\r\n## Expected behavior\r\n\r\nI had hoped it would behave identically :innocent:\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0.dev20190717\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 410.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.2.0.dev20190717\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.1.0               py3.7_cpu_0    pytorch\r\n[conda] pytorch-nightly           1.2.0.dev20190717 py3.7_cuda10.0.130_cudnn7.5.1_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py37_cuNone_1    pytorch","python\r\nimport torch\r\nclass Model1(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()        \r\n        \r\n        self.block_1 = torch.nn.Conv2d(3, 64, 3, padding=1)\r\n\r\n    def forward(self, x):\r\n        x = self.block_1(x)\r\n        return x\r\n\r\nx = torch.randn(16, 3, 100, 250)\r\nmodel = Model1()\r\nmodel.eval()\r\nmodel_onnx = torch.onnx._export(model, x, ""conv2d.onnx"",\r\n                                verbose=True,\r\n                                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n                                example_outputs=model(x),\r\n                               )\r\nmodel_onnx\r\n"
22969,"TorchScript/ONNX unsupported constant kind s## \U0001f41b Bug\r\n\r\nWhen I try to run this MRE I get the following error:\r\n```\r\n..\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    600                 else:\r\n    601                     raise RuntimeError(""Unsupported prim::Constant kind: `{}`. Send a bug report."".format(\r\n--> 602                         n.kindOf(""value"")))\r\n    603             elif n.mustBeNone() or op_name == ""ListConstruct"" or op_name == ""ListUnpack"":\r\n    604                 # None is not an ONNX operator; keep it as None\r\n\r\nRuntimeError: Unsupported prim::Constant kind: `s`. Send a bug report.\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet\r\n\r\n## Expected behavior\r\n\r\nI would expect no error.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 410.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.1.0               py3.7_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py37_cuNone_1    pytorch",oncall: jit|triaged,houseroad,"## \U0001f41b Bug\r\n\r\nWhen I try to run this MRE I get the following error:\r\n```\r\n..\r\n~/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    600                 else:\r\n    601                     raise RuntimeError(""Unsupported prim::Constant kind: `{}`. Send a bug report."".format(\r\n--> 602                         n.kindOf(""value"")))\r\n    603             elif n.mustBeNone() or op_name == ""ListConstruct"" or op_name == ""ListUnpack"":\r\n    604                 # None is not an ONNX operator; keep it as None\r\n\r\nRuntimeError: Unsupported prim::Constant kind: `s`. Send a bug report.\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet\r\n```python\r\nimport torch\r\nclass Model1(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        x = torch.nn.functional.pad(x, (0, 99))\r\n        return x\r\n\r\nx = torch.randn(32, 1, 500)\r\nmodel_onnx = torch.onnx._export(Model1(), x, ""tuple_value.onnx"",\r\n                                verbose=True,\r\n                                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n                                example_outputs=Model1()(x))\r\n```\r\n## Expected behavior\r\n\r\nI would expect no error.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 410.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.3.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.1.0               py3.7_cpu_0    pytorch\r\n[conda] torchvision-cpu           0.3.0             py37_cuNone_1    pytorch","python\r\nimport torch\r\nclass Model1(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        x = torch.nn.functional.pad(x, (0, 99))\r\n        return x\r\n\r\nx = torch.randn(32, 1, 500)\r\nmodel_onnx = torch.onnx._export(Model1(), x, ""tuple_value.onnx"",\r\n                                verbose=True,\r\n                                operator_export_type=torch.onnx.OperatorExportTypes.ONNX,\r\n                                example_outputs=Model1()(x))\r\n"
22809,"cat() call in ScriptModule w/constant arguments causes loading of saved module to fail## \U0001f41b Bug\r\n\r\nSee #20335 for context, this seems related.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nRun the following script:\r\n\r\n\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 26, in <module>\r\n    cm = th.jit.load(""mod.ptj"") # <---this will fail with the same error as in the original repro above.\r\n  File ""/mnt/home/gbschwartz/anaconda/envs/py3_newpytorch_cuda10/lib/python3.7/site-packages/torch/jit/__init__.py"", line 151, in load\r\n    torch._C.import_ir_module(module_lookup, f, map_location, _extra_files)\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::cat(Tensor[] tensors, int dim=<default>) -> Tensor:\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type.\r\n  \r\n  aten::cat(Tensor[] tensors, int dim=<default>, *, Tensor out) -> Tensor:\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type.\r\n\r\nThe original call is:\r\nat code/mod.py:3:8\r\nop_version_set = 1\r\ndef forward(self) -> Tensor:\r\n  _0 = torch.cat([CONSTANTS.c0, CONSTANTS.c0], 0)\r\n       ~~~~~~~~~ <--- HERE\r\n  return torch.sum(_0, dtype=None)\r\nCompiled from code test.py(9): forward\r\n```",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\nSee #20335 for context, this seems related.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nRun the following script:\r\n\r\n```python\r\nimport torch as th\r\n\r\nclass Mod(th.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        return th.cat(2*[x], dim=0)\r\n        #return th.cat((x, x), dim=0) <-- Unlike before, this will still cause the load below to fail.\r\n\r\nclass ScriptMod(th.jit.ScriptModule):\r\n    def __init__(self, mod):\r\n        super().__init__()\r\n        x = th.zeros(1, 3)\r\n        mod_fn = lambda : mod(x)\r\n        self.mod = th.jit.trace(mod_fn, tuple())\r\n\r\n    @th.jit.script_method\r\n    def forward(self):\r\n        return self.mod()\r\n\r\nif __name__ == ""__main__"":\r\n    with th.no_grad():\r\n        cm = ScriptMod(Mod())\r\n        cm.save(""mod.ptj"")\r\n        cm = th.jit.load(""mod.ptj"") # <-- This will fail with the same error as in the original repro.\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 26, in <module>\r\n    cm = th.jit.load(""mod.ptj"") # <---this will fail with the same error as in the original repro above.\r\n  File ""/mnt/home/gbschwartz/anaconda/envs/py3_newpytorch_cuda10/lib/python3.7/site-packages/torch/jit/__init__.py"", line 151, in load\r\n    torch._C.import_ir_module(module_lookup, f, map_location, _extra_files)\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following operator variants are available:\r\n  \r\n  aten::cat(Tensor[] tensors, int dim=<default>) -> Tensor:\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type.\r\n  \r\n  aten::cat(Tensor[] tensors, int dim=<default>, *, Tensor out) -> Tensor:\r\n  Expected a value of type 'List[Tensor]' for argument 'tensors' but instead found type 'List[Tensor]'.\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type.\r\n\r\nThe original call is:\r\nat code/mod.py:3:8\r\nop_version_set = 1\r\ndef forward(self) -> Tensor:\r\n  _0 = torch.cat([CONSTANTS.c0, CONSTANTS.c0], 0)\r\n       ~~~~~~~~~ <--- HERE\r\n  return torch.sum(_0, dtype=None)\r\nCompiled from code test.py(9): forward\r\n```","python\r\nimport torch as th\r\n\r\nclass Mod(th.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        return th.cat(2*[x], dim=0)\r\n        #return th.cat((x, x), dim=0) <-- Unlike before, this will still cause the load below to fail.\r\n\r\nclass ScriptMod(th.jit.ScriptModule):\r\n    def __init__(self, mod):\r\n        super().__init__()\r\n        x = th.zeros(1, 3)\r\n        mod_fn = lambda : mod(x)\r\n        self.mod = th.jit.trace(mod_fn, tuple())\r\n\r\n    @th.jit.script_method\r\n    def forward(self):\r\n        return self.mod()\r\n\r\nif __name__ == ""__main__"":\r\n    with th.no_grad():\r\n        cm = ScriptMod(Mod())\r\n        cm.save(""mod.ptj"")\r\n        cm = th.jit.load(""mod.ptj"") # <-- This will fail with the same error as in the original repro.\r\n"
22615,"[Bug] torch.from_numpy fails to convert np.int32 when np.dtype.num=7## \U0001f41b Bug\r\n\r\nI ran into the following error when trying to convert an integer numpy ndarray to a torch tensor:\r\n`>>> torch.from_numpy(b)`\r\n\r\n> Traceback (most recent call last):\r\n>   File ""<input>"", line 1, in <module>\r\n> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.\r\n> \r\n\r\n## To Reproduce\r\nHere's some example code that triggers it\r\n\r\n\r\n\r\n> tensor([1, 2, 3], dtype=torch.int32)\r\n\r\n\r\n\r\n> array([1, 0, 1], dtype=int32)\r\n\r\n\r\n> Traceback (most recent call last):\r\n>   File ""<input>"", line 1, in <module>\r\n> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.\r\n\r\n\r\n---\r\n\r\nI was looking through the members/properties of the dtype in a debugger, and I notice\r\n`np.dtype.num` changed from 7 to 5 after the modulus:\r\n\r\n\r\n> (dtype('int32'), 5)\r\n\r\n> (dtype('int32'), 7)\r\n\r\n---\r\n\r\nI can work around it by using `b.astype(int)` and this works fine, but it's just confusing that the error message in torch complains it's not an int32 when it is.\r\n\r\n## Environment\r\nwindows10\r\nAnaconda python 3.6 env \r\ntorch                1.1.0 (installed via conda)\r\nnumpy                1.16.2+mkl\r\n",module: error checking|triaged|module: numpy|small,pbelevich,"## \U0001f41b Bug\r\n\r\nI ran into the following error when trying to convert an integer numpy ndarray to a torch tensor:\r\n`>>> torch.from_numpy(b)`\r\n\r\n> Traceback (most recent call last):\r\n>   File ""<input>"", line 1, in <module>\r\n> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.\r\n> \r\n\r\n## To Reproduce\r\nHere's some example code that triggers it\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\na = np.array([1,2,3])\r\ntorch.from_numpy(a) # This works fine\r\n```\r\n\r\n> tensor([1, 2, 3], dtype=torch.int32)\r\n\r\n```python\r\nb = a % 2\r\nb\r\n```\r\n\r\n> array([1, 0, 1], dtype=int32)\r\n```python\r\ntorch.from_numpy(b)\r\n```\r\n\r\n> Traceback (most recent call last):\r\n>   File ""<input>"", line 1, in <module>\r\n> TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.\r\n\r\n\r\n---\r\n\r\nI was looking through the members/properties of the dtype in a debugger, and I notice\r\n`np.dtype.num` changed from 7 to 5 after the modulus:\r\n\r\n```python\r\nb.dtype , b.dtype.num\r\n```\r\n> (dtype('int32'), 5)\r\n```python\r\na.dtype, a.dtype.num\r\n```\r\n> (dtype('int32'), 7)\r\n\r\n---\r\n\r\nI can work around it by using `b.astype(int)` and this works fine, but it's just confusing that the error message in torch complains it's not an int32 when it is.\r\n\r\n## Environment\r\nwindows10\r\nAnaconda python 3.6 env \r\ntorch                1.1.0 (installed via conda)\r\nnumpy                1.16.2+mkl\r\n","python\r\nimport numpy as np\r\nimport torch\r\na = np.array([1,2,3])\r\ntorch.from_numpy(a) # This works fine\r\n"
22603,"[jit] Can't use kwargs in python methods\r\n\r\nThis gives me the error\r\n```\r\n(Tensor 0, Tensor 1) -> None:\r\nArgument 1 not provided.\r\n```",oncall: jit|triaged,driazati,"```python\r\nclass Foo(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def bar(self, x, dim=0):\r\n        print(x.size(dim))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        self.bar(x, dim=0)\r\n\r\nfoo = Foo()\r\n```\r\n\r\nThis gives me the error\r\n```\r\n(Tensor 0, Tensor 1) -> None:\r\nArgument 1 not provided.\r\n```","python\r\nclass Foo(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def bar(self, x, dim=0):\r\n        print(x.size(dim))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        self.bar(x, dim=0)\r\n\r\nfoo = Foo()\r\n"
22581,"Batched Triu And Tril Incorrect for Some Inputs## \U0001f41b Bug\r\n\r\nWhen passing a specific 4D tensor to `triu` or `tril`, the GPU implementation produces non-deterministic results. The CPU implementation produces `nan` values.\r\n\r\n## To Reproduce\r\n\r\nThis is the minimum reproducible example I could come up with:\r\n\r\n\r\n\r\nNote that the issue does not seem to occur when:\r\n- the transpose is omitted\r\n- if `x.size(0) > 1`\r\n- if the tensor has more or fewer dimensions\r\n\r\n## Expected behavior\r\n\r\nValues should be deterministic and not produces `NaN`\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla V100-SXM2-16GB\r\nNvidia driver version: 418.67\r\ncuDNN version: 7501\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.1\r\n[pip] torch==1.1.0\r\n[pip] torchfile==0.1.0\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>",high priority|triaged,vishwakftw,"## \U0001f41b Bug\r\n\r\nWhen passing a specific 4D tensor to `triu` or `tril`, the GPU implementation produces non-deterministic results. The CPU implementation produces `nan` values.\r\n\r\n## To Reproduce\r\n\r\nThis is the minimum reproducible example I could come up with:\r\n\r\n```python\r\nx = torch.randn(1, 4, 4, 4)\r\nx = x.transpose(0, 1)\r\nfor i in range(10):\r\n    # note: results are often different on each run\r\n    # or on CPU, outputs `nan`\r\n    print(x.triu().sum())\r\n```\r\n\r\nNote that the issue does not seem to occur when:\r\n- the transpose is omitted\r\n- if `x.size(0) > 1`\r\n- if the tensor has more or fewer dimensions\r\n\r\n## Expected behavior\r\n\r\nValues should be deterministic and not produces `NaN`\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla V100-SXM2-16GB\r\nNvidia driver version: 418.67\r\ncuDNN version: 7501\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.1\r\n[pip] torch==1.1.0\r\n[pip] torchfile==0.1.0\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>","python\r\nx = torch.randn(1, 4, 4, 4)\r\nx = x.transpose(0, 1)\r\nfor i in range(10):\r\n    # note: results are often different on each run\r\n    # or on CPU, outputs `nan`\r\n    print(x.triu().sum())\r\n"
22483,[jit] Support for `range(n)` as a valueThis is of particular use in list comprehensions:\r\nThis should work:\r\n,oncall: jit|triaged,eellison,This is of particular use in list comprehensions:\r\nThis should work:\r\n```python\r\n@torch.jit.script\r\ndef f():\r\n    x = [i for i in range(5)]\r\n    print(x)\r\n```,python\r\n@torch.jit.script\r\ndef f():\r\n    x = [i for i in range(5)]\r\n    print(x)\r\n
22242,"lu: When not using pivoting, return the identity permutation instead of zerosSome of my qpth users have told me that updating to the latest version of PyTorch and replacing the btrifact/btrisolve calls with the LU ones wasn't working and I didn't believe them until I tried it myself :)\r\n\r\nThese updates have broken unpivoted LU factorizations/solves on CUDA. The LU factorization code used to return the identity permutation when pivoting wasn't used but now returns all zeros as the pivots. This PR reverts it back to return the identity permutation. I've not yet tested this code as I'm having some trouble compiling PyTorch with this and am hitting #21700 and am not sure how to disable that option.\r\n\r\nHere's a MWE to reproduce the broken behavior, and my fix.\r\n\r\n\r\n\r\nOutput:\r\n\r\n```\r\nA_lu_cuda_nopivot\r\n (tensor([[[ 2.8465, -0.7560,  0.8716, -1.7337],\r\n         [-0.2656,  5.5724, -1.1316,  0.6678],\r\n         [ 0.3062, -0.2031,  1.4206, -0.5438],\r\n         [-0.6091,  0.1198, -0.3828,  1.5103]]], device='cuda:0'), tensor([[0, 0, 0, 0]], device='cuda:0', dtype=torch.int32))\r\n\r\n\r\n-----\r\n\r\n\r\nA_lu_cuda_pivot\r\n (tensor([[[ 2.8465, -0.7560,  0.8716, -1.7337],\r\n         [-0.2656,  5.5724, -1.1316,  0.6678],\r\n         [ 0.3062, -0.2031,  1.4206, -0.5438],\r\n         [-0.6091,  0.1198, -0.3828,  1.5103]]], device='cuda:0'), tensor([[0, 0, 0, 0]], device='cuda:0', dtype=torch.int32))\r\n\r\n\r\n(tensor([[-0.3121, -0.1673, -0.4450, -0.2483]]),\r\n tensor([[-0.1661, -0.1875, -0.5694, -0.4772]], device='cuda:0'),\r\n tensor([[-0.3121, -0.1673, -0.4450, -0.2483]], device='cuda:0'),\r\n tensor([[-0.3121, -0.1673, -0.4450, -0.2483]], device='cuda:0'))\r\n```",module: cuda|merge-this-please|Merged,vishwakftw,"Some of my qpth users have told me that updating to the latest version of PyTorch and replacing the btrifact/btrisolve calls with the LU ones wasn't working and I didn't believe them until I tried it myself :)\r\n\r\nThese updates have broken unpivoted LU factorizations/solves on CUDA. The LU factorization code used to return the identity permutation when pivoting wasn't used but now returns all zeros as the pivots. This PR reverts it back to return the identity permutation. I've not yet tested this code as I'm having some trouble compiling PyTorch with this and am hitting #21700 and am not sure how to disable that option.\r\n\r\nHere's a MWE to reproduce the broken behavior, and my fix.\r\n\r\n```python\r\ntorch.manual_seed(0)\r\n\r\nn = 4\r\nL = torch.randn(n,n)\r\nA = L.mm(L.t()).unsqueeze(0)\r\nb = torch.randn(1, n)\r\n\r\nA_lu_cpu = torch.lu(A)\r\nA_lu_cuda_nopivot = torch.lu(A.cuda(), pivot=False)\r\nA_lu_cuda_pivot = torch.lu(A.cuda(), pivot=True)\r\nprint('A_lu_cuda_nopivot\\n', A_lu_cuda_nopivot)\r\nprint('-----\\nA_lu_cuda_pivot\\n', A_lu_cuda_nopivot)\r\n\r\nx_cpu = b.lu_solve(*A_lu_cpu)\r\nx_cuda_nopivot = b.cuda().lu_solve(*A_lu_cuda_nopivot)\r\nx_cuda_nopivot_fixed = b.cuda().lu_solve(\r\n    A_lu_cuda_nopivot[0], torch.arange(1, n+1, device='cuda:0').int())\r\nx_cuda_pivot = b.cuda().lu_solve(*A_lu_cuda_pivot)\r\n\r\nprint(x_cpu, x_cuda_nopivot, x_cuda_nopivot_fixed, x_cuda_pivot)\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nA_lu_cuda_nopivot\r\n (tensor([[[ 2.8465, -0.7560,  0.8716, -1.7337],\r\n         [-0.2656,  5.5724, -1.1316,  0.6678],\r\n         [ 0.3062, -0.2031,  1.4206, -0.5438],\r\n         [-0.6091,  0.1198, -0.3828,  1.5103]]], device='cuda:0'), tensor([[0, 0, 0, 0]], device='cuda:0', dtype=torch.int32))\r\n\r\n\r\n-----\r\n\r\n\r\nA_lu_cuda_pivot\r\n (tensor([[[ 2.8465, -0.7560,  0.8716, -1.7337],\r\n         [-0.2656,  5.5724, -1.1316,  0.6678],\r\n         [ 0.3062, -0.2031,  1.4206, -0.5438],\r\n         [-0.6091,  0.1198, -0.3828,  1.5103]]], device='cuda:0'), tensor([[0, 0, 0, 0]], device='cuda:0', dtype=torch.int32))\r\n\r\n\r\n(tensor([[-0.3121, -0.1673, -0.4450, -0.2483]]),\r\n tensor([[-0.1661, -0.1875, -0.5694, -0.4772]], device='cuda:0'),\r\n tensor([[-0.3121, -0.1673, -0.4450, -0.2483]], device='cuda:0'),\r\n tensor([[-0.3121, -0.1673, -0.4450, -0.2483]], device='cuda:0'))\r\n```","python\r\ntorch.manual_seed(0)\r\n\r\nn = 4\r\nL = torch.randn(n,n)\r\nA = L.mm(L.t()).unsqueeze(0)\r\nb = torch.randn(1, n)\r\n\r\nA_lu_cpu = torch.lu(A)\r\nA_lu_cuda_nopivot = torch.lu(A.cuda(), pivot=False)\r\nA_lu_cuda_pivot = torch.lu(A.cuda(), pivot=True)\r\nprint('A_lu_cuda_nopivot\\n', A_lu_cuda_nopivot)\r\nprint('-----\\nA_lu_cuda_pivot\\n', A_lu_cuda_nopivot)\r\n\r\nx_cpu = b.lu_solve(*A_lu_cpu)\r\nx_cuda_nopivot = b.cuda().lu_solve(*A_lu_cuda_nopivot)\r\nx_cuda_nopivot_fixed = b.cuda().lu_solve(\r\n    A_lu_cuda_nopivot[0], torch.arange(1, n+1, device='cuda:0').int())\r\nx_cuda_pivot = b.cuda().lu_solve(*A_lu_cuda_pivot)\r\n\r\nprint(x_cpu, x_cuda_nopivot, x_cuda_nopivot_fixed, x_cuda_pivot)\r\n"
22131,"[dataloader] Add a context= argument for multiprocessingPyTorch version: `1.2.0.dev20190607`, librosa version: `0.6.3`\r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/11727, https://github.com/librosa/librosa/issues/747\r\n\r\n\r\n\r\nAnother strange DataLoader behavior. Does not break if `set_start_method` is removed. Breaks this way if class definition is inside if. \r\n\r\n\r\n\r\nDoes not break, but it's very impractical to put only some imports inside the if:\r\n",module: dataloader|triaged|enhancement,ssnl,"PyTorch version: `1.2.0.dev20190607`, librosa version: `0.6.3`\r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/11727, https://github.com/librosa/librosa/issues/747\r\n\r\n```python\r\n# bug1.py\r\n# python3 bug1.py\r\n\r\n# Does not break if `import librosa` is removed or set_start_method is removed\r\n\r\nimport torch\r\nimport torch.utils.data\r\nimport librosa\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n    def __getitem__(self, index):\r\n        return torch.zeros(2, 4)\r\n\r\n    def __len__(self):\r\n        return 1\r\n\r\nif __name__ == '__main__':\r\n    torch.multiprocessing.set_start_method('spawn', force = True)\r\n    loader = torch.utils.data.DataLoader(Dataset(), num_workers = 1)\r\n    next(iter(loader))\r\n\r\n#/miniconda/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\r\n#  len(cache))\r\n```\r\n\r\nAnother strange DataLoader behavior. Does not break if `set_start_method` is removed. Breaks this way if class definition is inside if. \r\n\r\n```python\r\n# bug2.py\r\n# python3 bug2.py\r\n\r\nif __name__ == '__main__':\r\n    import torch\r\n    import torch.utils.data\r\n    import librosa\r\n\r\n    class Dataset(torch.utils.data.Dataset):\r\n        def __getitem__(self, index):\r\n            return torch.zeros(2, 4)\r\n\r\n        def __len__(self):\r\n            return 1\r\n\r\n    torch.multiprocessing.set_start_method('spawn', force = True)\r\n    loader = torch.utils.data.DataLoader(Dataset(), num_workers = 1)\r\n    next(iter(loader))\r\n\r\n#Traceback (most recent call last):\r\n#  File ""<string>"", line 1, in <module>\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/spawn.py"", line 105, in spawn_main\r\n#    exitcode = _main(fd)\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/spawn.py"", line 115, in _main\r\n#    self = reduction.pickle.load(from_parent)\r\n#AttributeError: Can't get attribute 'Dataset' on <module '__mp_main__' from #'/deepspeech.pytorch/convasr/bug_.py'>\r\n#Traceback (most recent call last):\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 512, in _try_get_batch\r\n#    data = self.data_queue.get(timeout=timeout)\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/queues.py"", line 104, in get\r\n#    if not self._poll(timeout):\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/connection.py"", line 257, in poll\r\n#    return self._poll(timeout)\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/connection.py"", line 414, in _poll\r\n#    r = wait([self], timeout)\r\n#  File ""/miniconda/lib/python3.7/multiprocessing/connection.py"", line 920, in wait\r\n#    ready = selector.select(timeout)\r\n#  File ""/miniconda/lib/python3.7/selectors.py"", line 415, in select\r\n#    fd_event_list = self._selector.poll(timeout)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py"", line 63, in handler\r\n#    _error_if_any_worker_fails()\r\n#RuntimeError: DataLoader worker (pid 6082) exited unexpectedly with exit code 1. Details are #lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\r\n\r\n#During handling of the above exception, another exception occurred:\r\n\r\n#Traceback (most recent call last):\r\n#  File ""bug_.py"", line 15, in <module>\r\n#    next(iter(loader))\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 577, in #__next__\r\n#    idx, batch = self._get_batch()\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 554, in #_get_batch\r\n#    success, data = self._try_get_batch()\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 520, in #_try_get_batch\r\n#    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str))\r\n# RuntimeError: DataLoader worker (pid(s) 6082) exited unexpectedly\r\n```\r\n\r\nDoes not break, but it's very impractical to put only some imports inside the if:\r\n```python\r\nimport torch\r\nimport torch.utils.data\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n    def __getitem__(self, index):\r\n        return torch.zeros(2, 4)\r\n\r\n    def __len__(self):\r\n        return 1\r\n\r\n\r\nif __name__ == '__main__':\r\n    import librosa\r\n    torch.multiprocessing.set_start_method('spawn', force = True)\r\n    loader = torch.utils.data.DataLoader(Dataset(), num_workers = 1)\r\n    next(iter(loader))\r\n```","python\r\n# bug1.py\r\n# python3 bug1.py\r\n\r\n# Does not break if `import librosa` is removed or set_start_method is removed\r\n\r\nimport torch\r\nimport torch.utils.data\r\nimport librosa\r\n\r\nclass Dataset(torch.utils.data.Dataset):\r\n    def __getitem__(self, index):\r\n        return torch.zeros(2, 4)\r\n\r\n    def __len__(self):\r\n        return 1\r\n\r\nif __name__ == '__main__':\r\n    torch.multiprocessing.set_start_method('spawn', force = True)\r\n    loader = torch.utils.data.DataLoader(Dataset(), num_workers = 1)\r\n    next(iter(loader))\r\n\r\n#/miniconda/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown\r\n#  len(cache))\r\n"
22124,"jit to support bitwise inplace ops: |=, &=, <<=, >>=, ^=If `~mask` is replaced by `1 - mask`, everything works. Also the error message is quite cryptic :)\r\n\r\n",high priority|oncall: jit,Chillee,"If `~mask` is replaced by `1 - mask`, everything works. Also the error message is quite cryptic :)\r\n\r\n```python\r\nimport torch\r\n\r\nclass ReLUDropoutInplace(torch.nn.Module):\r\n    def __init__(self, p : float):\r\n        super(ReLUDropoutInplace, self).__init__()\r\n        self.p = p\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        if self.training:\r\n            p1m = 1. - self.p\r\n            mask = torch.rand_like(input) < p1m\r\n            mask *= (input > 0)\r\n            return input.masked_fill_(~mask, 0).mul_(1. / p1m)\r\n        else:\r\n            return input.clamp_(min = 0)\r\n\r\nm = ReLUDropoutInplace(0.5) # fails with:\r\n\r\n#  File ""foo.py"", line 3, in <module>\r\n#    class ReLUDropoutInplace(torch.nn.Module):\r\n#  File ""foo.py"", line 8, in ReLUDropoutInplace\r\n#    @torch.jit.script_method\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1106, in script_method\r\n#    ast = get_jit_def(fn, self_name=""ScriptModule"")\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 169, in get_jit_def\r\n#    return build_def(ctx, py_ast.body[0], type_line, self_name)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 209, in build_def\r\n#    build_stmts(ctx, body))\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in build_stmts\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in <listcomp>\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 343, in build_If\r\n#    build_stmts(ctx, stmt.body),\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in build_stmts\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in <listcomp>\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 288, in build_Return\r\n#    return Return(r, None if stmt.value is None else build_expr(ctx, stmt.value))\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 415, in build_Call\r\n#    func = build_expr(ctx, expr.func)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 401, in build_Attribute\r\n#    value = build_expr(ctx, expr.value)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 416, in build_Call\r\n#    args = [build_expr(ctx, py_arg) for py_arg in expr.args]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 416, in <listcomp>\r\n#    args = [build_expr(ctx, py_arg) for py_arg in expr.args]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 480, in build_UnaryOp\r\n#    r = ctx.make_range(expr.lineno, expr.col_offset, expr.col_offset + len(op_token))\r\n#TypeError: object of type 'NoneType' has no len()\r\n```","python\r\nimport torch\r\n\r\nclass ReLUDropoutInplace(torch.nn.Module):\r\n    def __init__(self, p : float):\r\n        super(ReLUDropoutInplace, self).__init__()\r\n        self.p = p\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        if self.training:\r\n            p1m = 1. - self.p\r\n            mask = torch.rand_like(input) < p1m\r\n            mask *= (input > 0)\r\n            return input.masked_fill_(~mask, 0).mul_(1. / p1m)\r\n        else:\r\n            return input.clamp_(min = 0)\r\n\r\nm = ReLUDropoutInplace(0.5) # fails with:\r\n\r\n#  File ""foo.py"", line 3, in <module>\r\n#    class ReLUDropoutInplace(torch.nn.Module):\r\n#  File ""foo.py"", line 8, in ReLUDropoutInplace\r\n#    @torch.jit.script_method\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1106, in script_method\r\n#    ast = get_jit_def(fn, self_name=""ScriptModule"")\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 169, in get_jit_def\r\n#    return build_def(ctx, py_ast.body[0], type_line, self_name)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 209, in build_def\r\n#    build_stmts(ctx, body))\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in build_stmts\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in <listcomp>\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 343, in build_If\r\n#    build_stmts(ctx, stmt.body),\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in build_stmts\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 125, in <listcomp>\r\n#    stmts = [build_stmt(ctx, s) for s in stmts]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 288, in build_Return\r\n#    return Return(r, None if stmt.value is None else build_expr(ctx, stmt.value))\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 415, in build_Call\r\n#    func = build_expr(ctx, expr.func)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 401, in build_Attribute\r\n#    value = build_expr(ctx, expr.value)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 416, in build_Call\r\n#    args = [build_expr(ctx, py_arg) for py_arg in expr.args]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 416, in <listcomp>\r\n#    args = [build_expr(ctx, py_arg) for py_arg in expr.args]\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 185, in __call__\r\n#    return method(ctx, node)\r\n#  File ""/miniconda/lib/python3.7/site-packages/torch/jit/frontend.py"", line 480, in build_UnaryOp\r\n#    r = ctx.make_range(expr.lineno, expr.col_offset, expr.col_offset + len(op_token))\r\n#TypeError: object of type 'NoneType' has no len()\r\n"
22109,"Multiple (redundant?) return statements in test_jit.pyIn https://github.com/pytorch/pytorch/blob/master/test/test_jit.py#L5900-L5945 there are several methods with multiple return statements like below:\r\n\r\n\r\n\r\nIt seems they are redundant, as subsequent return statements are unreachable. If they are there for a debugging purpose, a comment would be nice.",oncall: jit|triaged,Chillee,"In https://github.com/pytorch/pytorch/blob/master/test/test_jit.py#L5900-L5945 there are several methods with multiple return statements like below:\r\n\r\n```python\r\n@torch.jit.script_method\r\ndef forward(self, x):\r\n    x = self.conv(x)\r\n    return x\r\n    x = self.bn(x)\r\n    return F.relu(x, inplace=True)\r\n```\r\n\r\nIt seems they are redundant, as subsequent return statements are unreachable. If they are there for a debugging purpose, a comment would be nice.","python\r\n@torch.jit.script_method\r\ndef forward(self, x):\r\n    x = self.conv(x)\r\n    return x\r\n    x = self.bn(x)\r\n    return F.relu(x, inplace=True)\r\n"
22066,torch.onnx._export does not support tensor sum with multiple dims## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nError message\r\n\r\n\r\n\r\n## Environment\r\n - PyTorch Version : 1.0.1.post2\r\n - OS: Ubuntu 18.04.2 LTS\r\n - How you installed PyTorch : pip install torch\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: 10.1/7.5\r\n - GPU models and configuration: NVIDIA 1080Ti\r\n\r\n,module: onnx|triaged,spandantiwari|houseroad,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nclass Test(nn.Module):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n\r\n    def forward(self, x):\r\n        return x.sum(dim=(2, 3), keepdim=True)\r\n\r\nmodel = Test()\r\n\r\nx = torch.zeros((16, 3, 256, 256))\r\n\r\ntorch.onnx._export(model, x, ""test.onnx"", verbose=True)\r\n\r\n```\r\nError message\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-4158e5bbe3ab> in <module>\r\n     13 x = torch.zeros((16, 3, 256, 256))\r\n     14 \r\n---> 15 torch.onnx._export(model, x, ""test.onnx"", verbose=True)\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/__init__.py in _export(*args, **kwargs)\r\n     20 def _export(*args, **kwargs):\r\n     21     from torch.onnx import utils\r\n---> 22     return utils._export(*args, **kwargs)\r\n     23 \r\n     24 \r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)\r\n    279                                                training, input_names,\r\n    280                                                output_names, operator_export_type,\r\n--> 281                                                example_outputs, propagate)\r\n    282 \r\n    283     # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, f, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate)\r\n    225         params = list(_unique_state_dict(model).values())\r\n    226 \r\n--> 227     graph = _optimize_graph(graph, operator_export_type)\r\n    228 \r\n    229     # NB: ONNX requires complete information about output types, which might be\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type)\r\n    153 \r\n    154     if operator_export_type != OperatorExportTypes.RAW:\r\n--> 155         graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n    156         torch._C._jit_pass_lint(graph)\r\n    157         torch._C._jit_pass_onnx_peephole(graph)\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)\r\n     50 def _run_symbolic_function(*args, **kwargs):\r\n     51     from torch.onnx import utils\r\n---> 52     return utils._run_symbolic_function(*args, **kwargs)\r\n     53 \r\n     54 \r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)\r\n    502                     return None\r\n    503                 fn = getattr(torch.onnx.symbolic, op_name)\r\n--> 504                 return fn(g, *inputs, **attrs)\r\n    505 \r\n    506         elif ns == ""prim"":\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/symbolic.py in symbolic(g, self, dim, keepdim)\r\n    329         else:\r\n    330             # dim-reduce path\r\n--> 331             dim, keepdim = _get_const(dim, 'i', 'dim'), _get_const(keepdim, 'i', 'keepdim')\r\n    332             return g.op(onnx_op_name, self, axes_i=[dim], keepdims_i=keepdim)\r\n    333     return symbolic\r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/symbolic.py in _get_const(value, desc, arg_name)\r\n     73     if _is_value(value) and value.node().kind() != 'onnx::Constant':\r\n     74         raise RuntimeError(""ONNX symbolic expected a constant value of the {} argument"".format(arg_name))\r\n---> 75     return _parse_arg(value, desc)\r\n     76 \r\n     77 \r\n\r\n~/env/py3.6/lib/python3.6/site-packages/torch/onnx/symbolic.py in _parse_arg(value, desc)\r\n     46     tval = value.node()['value']\r\n     47     if desc == 'i':\r\n---> 48         return int(tval)\r\n     49     elif desc == 'f':\r\n     50         return float(tval)\r\n\r\nValueError: only one element tensors can be converted to Python scalars\r\n\r\n```\r\n\r\n\r\n## Environment\r\n - PyTorch Version : 1.0.1.post2\r\n - OS: Ubuntu 18.04.2 LTS\r\n - How you installed PyTorch : pip install torch\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: 10.1/7.5\r\n - GPU models and configuration: NVIDIA 1080Ti\r\n\r\n","python\r\nimport torch\r\nfrom torch import nn\r\n\r\nclass Test(nn.Module):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n\r\n    def forward(self, x):\r\n        return x.sum(dim=(2, 3), keepdim=True)\r\n\r\nmodel = Test()\r\n\r\nx = torch.zeros((16, 3, 256, 256))\r\n\r\ntorch.onnx._export(model, x, ""test.onnx"", verbose=True)\r\n\r\n"
21965,"base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set## \U0001f41b Bug\r\n\r\nOne of the arguments of `torch.optim.lr_scheduler.CyclicLR` is `base_lr`. This is described in the documentation as:\r\n\r\n> Initial learning rate which is the lower boundary in the cycle for each parameter group.\r\n\r\nWhen this class actually computes the learning rate on each step, it uses the attribute `self.base_lrs`. This is set in the parent class, [`torch.optim.lr_scheduler. _LRScheduler `](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L26)\r\n\r\nHowever, due to the implementation of this (it uses the value 'initial_lr' from each parameter group), this actually produces the wrong behaviour when that 'initial_lr' key is already set. In effect, the CyclicLR scheduler now cycles between whatever value was the `initial_lr` value for each parameter group, and the `max_lr`.\r\n\r\nThis is a problem, as 'initial_lr' gets set by most optimizers, making it impossible to chain multiple optimizers.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nThis produces the following plot:\r\n![image](https://user-images.githubusercontent.com/4092425/59760026-3b06ff80-9289-11e9-942b-4ac14669739e.png)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe `base_lr` parameter to CyclicLR should override the parent class implementation of the learning rate. CyclicLR should **always** use the base_lr parameter specified, and not rely on what's present in parameter groups. I would expect the following graph instead:\r\n\r\n![image](https://user-images.githubusercontent.com/4092425/59760204-a8b32b80-9289-11e9-8e95-6ec8a0b8aca1.png)\r\n\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\n$ python collect_env.py\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] gpytorch==0.2.1\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-revgrad==0.0.1\r\n[pip3] pytorch-swag==0.0.1\r\n[pip3] torch==1.1.0\r\n[pip3] torch-nightly==1.0.0.dev20190319\r\n[pip3] torch-vision==0.1.6.dev0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchgeometry==0.1.2\r\n[pip3] torchvision==0.2.3a0+a2e6b70\r\n[pip3] torchvision-nightly==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] gpytorch                  0.2.1                     <pip>\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0\r\n[conda] mkl_random                1.0.2            py37h27c97d8_0\r\n[conda] pytorch                   1.0.1                   py3.7_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20190319         py3.7_0    pytorch\r\n[conda] pytorch-revgrad           0.0.1                     <pip>\r\n[conda] pytorch-swag              0.0.1                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torch                     1.0.1.post2               <pip>\r\n[conda] torch-nightly             1.0.0.dev20190319           <pip>\r\n[conda] torch-vision              0.1.6.dev0                <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchgeometry             0.1.2                     <pip>\r\n[conda] torchvision               0.2.3a0+a2e6b70           <pip>\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n[conda] torchvision-nightly       0.2.1                     <pip>\r\n```",module: optimizer|triaged,vincentqb,"## \U0001f41b Bug\r\n\r\nOne of the arguments of `torch.optim.lr_scheduler.CyclicLR` is `base_lr`. This is described in the documentation as:\r\n\r\n> Initial learning rate which is the lower boundary in the cycle for each parameter group.\r\n\r\nWhen this class actually computes the learning rate on each step, it uses the attribute `self.base_lrs`. This is set in the parent class, [`torch.optim.lr_scheduler. _LRScheduler `](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L26)\r\n\r\nHowever, due to the implementation of this (it uses the value 'initial_lr' from each parameter group), this actually produces the wrong behaviour when that 'initial_lr' key is already set. In effect, the CyclicLR scheduler now cycles between whatever value was the `initial_lr` value for each parameter group, and the `max_lr`.\r\n\r\nThis is a problem, as 'initial_lr' gets set by most optimizers, making it impossible to chain multiple optimizers.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\n\r\nmodel = torch.nn.Linear(2, 1)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\r\nlr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.1)\r\nlr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.1, max_lr=0.3, step_size_up=1, step_size_down=3)\r\n\r\nlrs = []\r\n\r\nfor i in range(40):\r\n    if i <= lr_scheduler_1.T_max:\r\n        lr_scheduler_1.step()\r\n    else:\r\n        lr_scheduler_2.step()\r\n    lrs.append(\r\n        optimizer.param_groups[0][""lr""]\r\n    )\r\n\r\nplt.plot(lrs)\r\n```\r\n\r\nThis produces the following plot:\r\n![image](https://user-images.githubusercontent.com/4092425/59760026-3b06ff80-9289-11e9-942b-4ac14669739e.png)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe `base_lr` parameter to CyclicLR should override the parent class implementation of the learning rate. CyclicLR should **always** use the base_lr parameter specified, and not rely on what's present in parameter groups. I would expect the following graph instead:\r\n\r\n![image](https://user-images.githubusercontent.com/4092425/59760204-a8b32b80-9289-11e9-8e95-6ec8a0b8aca1.png)\r\n\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\n$ python collect_env.py\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.5\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] gpytorch==0.2.1\r\n[pip3] msgpack-numpy==0.4.3.2\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-revgrad==0.0.1\r\n[pip3] pytorch-swag==0.0.1\r\n[pip3] torch==1.1.0\r\n[pip3] torch-nightly==1.0.0.dev20190319\r\n[pip3] torch-vision==0.1.6.dev0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchgeometry==0.1.2\r\n[pip3] torchvision==0.2.3a0+a2e6b70\r\n[pip3] torchvision-nightly==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] gpytorch                  0.2.1                     <pip>\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0\r\n[conda] mkl_random                1.0.2            py37h27c97d8_0\r\n[conda] pytorch                   1.0.1                   py3.7_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20190319         py3.7_0    pytorch\r\n[conda] pytorch-revgrad           0.0.1                     <pip>\r\n[conda] pytorch-swag              0.0.1                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torch                     1.0.1.post2               <pip>\r\n[conda] torch-nightly             1.0.0.dev20190319           <pip>\r\n[conda] torch-vision              0.1.6.dev0                <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchgeometry             0.1.2                     <pip>\r\n[conda] torchvision               0.2.3a0+a2e6b70           <pip>\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n[conda] torchvision-nightly       0.2.1                     <pip>\r\n```","python\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\n\r\nmodel = torch.nn.Linear(2, 1)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\r\nlr_scheduler_1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.1)\r\nlr_scheduler_2 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.1, max_lr=0.3, step_size_up=1, step_size_down=3)\r\n\r\nlrs = []\r\n\r\nfor i in range(40):\r\n    if i <= lr_scheduler_1.T_max:\r\n        lr_scheduler_1.step()\r\n    else:\r\n        lr_scheduler_2.step()\r\n    lrs.append(\r\n        optimizer.param_groups[0][""lr""]\r\n    )\r\n\r\nplt.plot(lrs)\r\n"
21875,In-place operation on differentiable view leaks memory\r\n\r\nThis is reproducible on master.\r\n\r\nInitial investigation shows that the leak is not because of saving Variables for backward (the SavedVariable constructor was not called).\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @ngimel,module: autograd|module: cuda|module: memory usage|triaged|actionable,VitalyFedyunin,"```python\r\nimport torch\r\nimport gc\r\n\r\ngc.collect()\r\nprint(torch.cuda.memory_allocated(0))  # prints 0\r\n\r\nm = torch.nn.Linear(2,3)\r\nmw = m.weight[:]\r\nm.to('cuda')\r\n# with torch.no_grad():  # Without using `torch.no_grad()`, the next line will leak CUDA memory\r\nmw[0][0] = 5\r\n\r\ndel mw\r\ndel m\r\ngc.collect()\r\nprint(torch.cuda.memory_allocated(0))  # prints 512\r\n```\r\n\r\nThis is reproducible on master.\r\n\r\nInitial investigation shows that the leak is not because of saving Variables for backward (the SavedVariable constructor was not called).\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @ngimel","python\r\nimport torch\r\nimport gc\r\n\r\ngc.collect()\r\nprint(torch.cuda.memory_allocated(0))  # prints 0\r\n\r\nm = torch.nn.Linear(2,3)\r\nmw = m.weight[:]\r\nm.to('cuda')\r\n# with torch.no_grad():  # Without using `torch.no_grad()`, the next line will leak CUDA memory\r\nmw[0][0] = 5\r\n\r\ndel mw\r\ndel m\r\ngc.collect()\r\nprint(torch.cuda.memory_allocated(0))  # prints 512\r\n"
21579,"`torch.arange` output shape ## \U0001f4da Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nAs described [here](https://pytorch.org/docs/stable/torch.html#torch.arange) the output size of `torch.arange` is `int((end-start)/step)` (i.e. the floor). But it's the ceiling:\r\n\r\n",module: docs|triaged,umanwizard,"## \U0001f4da Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nAs described [here](https://pytorch.org/docs/stable/torch.html#torch.arange) the output size of `torch.arange` is `int((end-start)/step)` (i.e. the floor). But it's the ceiling:\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\ndef verify_out_shape(start, end, steps): \r\n    for s in steps: \r\n        dif = end-start \r\n        assert torch.arange(start, end, step=s).size(0) == np.ceil(dif/s)\r\n\r\nverify_out_shape(1, 400, [0.5, 0.7, 0.3, 1.3])  \r\nverify_out_shape(23, 731, [0.5, 0.7, 0.3, 1.3])  \r\n```","python\r\nimport torch\r\nimport numpy as np\r\ndef verify_out_shape(start, end, steps): \r\n    for s in steps: \r\n        dif = end-start \r\n        assert torch.arange(start, end, step=s).size(0) == np.ceil(dif/s)\r\n\r\nverify_out_shape(1, 400, [0.5, 0.7, 0.3, 1.3])  \r\nverify_out_shape(23, 731, [0.5, 0.7, 0.3, 1.3])  \r\n"
21526,"Segmentation fault on large concat operation## \U0001f41b Bug\r\n\r\nWhen concatenating two large tensors a 'Segmentation fault (core dumped)' is thrown. Tested on an older version of pytorch(1.0.1) and bug was not present.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n**Output:**\r\n`Segmentation fault (core dumped)`\r\n\r\n## Expected behavior\r\n\r\nExpected: a concatenated tensor with lots of ones\r\n\r\n## Environment\r\nTried it on 2 computers:\r\n**Build 1:**\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: Quadro P4000\r\nGPU 1: TITAN RTX\r\n\r\nNvidia driver version: 418.43\r\ncuDNN version: /usr/lib64/libcudnn.so.7.2.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\n**Build 2**\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.0\r\n[pip] torch==1.0.0\r\n[pip] torchvision==0.1.9\r\n[conda] Could not collect\r\n\r\n## Additional context\r\nBoth machines have 64GB of RAM, so I expect the whole tensor should easily fit\r\n\r\n",triaged,umanwizard,"## \U0001f41b Bug\r\n\r\nWhen concatenating two large tensors a 'Segmentation fault (core dumped)' is thrown. Tested on an older version of pytorch(1.0.1) and bug was not present.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nconcat_list = []\r\n\r\nconcat_list.append(torch.ones((6500,1024*512), dtype=torch.uint8))\r\nconcat_list.append(torch.ones((4500,1024*512), dtype=torch.uint8))\r\n\r\nccat = torch.cat(concat_list)\r\n```\r\n\r\n**Output:**\r\n`Segmentation fault (core dumped)`\r\n\r\n## Expected behavior\r\n\r\nExpected: a concatenated tensor with lots of ones\r\n\r\n## Environment\r\nTried it on 2 computers:\r\n**Build 1:**\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: Quadro P4000\r\nGPU 1: TITAN RTX\r\n\r\nNvidia driver version: 418.43\r\ncuDNN version: /usr/lib64/libcudnn.so.7.2.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n\r\n**Build 2**\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.0\r\n[pip] torch==1.0.0\r\n[pip] torchvision==0.1.9\r\n[conda] Could not collect\r\n\r\n## Additional context\r\nBoth machines have 64GB of RAM, so I expect the whole tensor should easily fit\r\n\r\n","python\r\nimport torch\r\nconcat_list = []\r\n\r\nconcat_list.append(torch.ones((6500,1024*512), dtype=torch.uint8))\r\nconcat_list.append(torch.ones((4500,1024*512), dtype=torch.uint8))\r\n\r\nccat = torch.cat(concat_list)\r\n"
21412,"Support sublist arguments for torch.einsum## \U0001f680 Feature\r\nSupport the argument format `torch.einsum(op0, sublist0, op1, sublist1, ..., [sublistout])`. It would allow the users to use more than 26 tensors (i.e. the number of lower case letters) in Einstein Summation.\r\n\r\n## Motivation\r\n\r\nThis feature is already available in [Numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\r\n\r\n## Pitch\r\n\r\nPerform the operation below in PyTorch:\r\n\r\n\r\n\r\n## Additional context\r\n\r\nI actually don't know many contexts where one really needs to call Einstein Summation on more than 26 tensors. However, for some use cases one could gain some readability if you call Einstein Summation with, say, 5 or 6 tensors. \n\ncc @mruberry @rgommers @heitorschueroff @vincentqb @vishwakftw @jianyuh @nikitaved @pearu",triaged|module: numpy|module: linear algebra|function request,heitorschueroff,"## \U0001f680 Feature\r\nSupport the argument format `torch.einsum(op0, sublist0, op1, sublist1, ..., [sublistout])`. It would allow the users to use more than 26 tensors (i.e. the number of lower case letters) in Einstein Summation.\r\n\r\n## Motivation\r\n\r\nThis feature is already available in [Numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\r\n\r\n## Pitch\r\n\r\nPerform the operation below in PyTorch:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nx = np.random.rand(6).reshape(2, 3)\r\ny = np.random.rand(6).reshape(2, 3)\r\nz = np.random.rand(6).reshape(2, 3)\r\nw = np.random.rand(6).reshape(2, 3)\r\nr1 = np.einsum(\r\n            x, [0, 1], \r\n            y, [0, 2], \r\n            z, [0, 3], \r\n            w, [0, 4], \r\n            [1, 2, 3, 4]\r\n     ) # Not currently supported by PyTorch\r\nr2 = np.einsum('ni, nj, nk, nl -> ijkl', x, y, z, w) # Supported by PyTorch\r\nprint((r1 == r2).all())  # Prints True\r\n```\r\n\r\n## Additional context\r\n\r\nI actually don't know many contexts where one really needs to call Einstein Summation on more than 26 tensors. However, for some use cases one could gain some readability if you call Einstein Summation with, say, 5 or 6 tensors. \n\ncc @mruberry @rgommers @heitorschueroff @vincentqb @vishwakftw @jianyuh @nikitaved @pearu","python\r\nimport numpy as np\r\n\r\nx = np.random.rand(6).reshape(2, 3)\r\ny = np.random.rand(6).reshape(2, 3)\r\nz = np.random.rand(6).reshape(2, 3)\r\nw = np.random.rand(6).reshape(2, 3)\r\nr1 = np.einsum(\r\n            x, [0, 1], \r\n            y, [0, 2], \r\n            z, [0, 3], \r\n            w, [0, 4], \r\n            [1, 2, 3, 4]\r\n     ) # Not currently supported by PyTorch\r\nr2 = np.einsum('ni, nj, nk, nl -> ijkl', x, y, z, w) # Supported by PyTorch\r\nprint((r1 == r2).all())  # Prints True\r\n"
21408,"Type mismatch of aten::mul backward## \U0001f41b Bug\r\n\r\n``aten::mul`` accepts ``Double`` as the second argument, but always returns ``Float`` when backwarding.\r\n\r\n## To Reproduce\r\n\r\nThe following reproduces the problem.\r\n\r\n\r\n\r\nThis gives you an error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""mul_type.py"", line 11, in <module>\r\n    z.backward(torch.randn_like(x))\r\n  File ""/home01/mtnk/miniconda3/envs/PTv1.2.0.dev20190605/lib/python3.7/site-packages/torch/tensor.py"", line 118, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home01/mtnk/miniconda3/envs/PTv1.2.0.dev20190605/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: Function MulBackward0 returned an invalid gradient at index 1 - expected type torch.DoubleTensor but got torch.FloatTensor\r\n```\r\n\r\nThe graph of the traced module was: \r\n```\r\ngraph(%x : Float(3, 3),\r\n      %y : Double(3, 3)):\r\n  %d : Double() = aten::sum(%y)\r\n  %3 : Float(3, 3) = aten::mul(%x, %d)\r\n  return (%3)\r\n```\r\n\r\n## Expected behavior\r\n\r\nI think aten::mul should return a Double value while running backward if the second argument of forward is Double.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0.dev20190605\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.5.1804 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0.dev20190605\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.2.0.dev20190605 py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n\r\n",high priority|oncall: jit|triaged,ailzhang,"## \U0001f41b Bug\r\n\r\n``aten::mul`` accepts ``Double`` as the second argument, but always returns ``Float`` when backwarding.\r\n\r\n## To Reproduce\r\n\r\nThe following reproduces the problem.\r\n\r\n```python\r\nimport torch\r\n\r\ndef f(x, y):\r\n    d = y.sum()\r\n    return x * d\r\n\r\nx = torch.randn(3,3, requires_grad=True)\r\ny = torch.randn(3,3, requires_grad=True, dtype=torch.float64)\r\ntf = torch.jit.trace(f, (x, y))\r\nz = tf(x, y)\r\nz.backward(torch.randn_like(x))\r\n```\r\n\r\nThis gives you an error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""mul_type.py"", line 11, in <module>\r\n    z.backward(torch.randn_like(x))\r\n  File ""/home01/mtnk/miniconda3/envs/PTv1.2.0.dev20190605/lib/python3.7/site-packages/torch/tensor.py"", line 118, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home01/mtnk/miniconda3/envs/PTv1.2.0.dev20190605/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: Function MulBackward0 returned an invalid gradient at index 1 - expected type torch.DoubleTensor but got torch.FloatTensor\r\n```\r\n\r\nThe graph of the traced module was: \r\n```\r\ngraph(%x : Float(3, 3),\r\n      %y : Double(3, 3)):\r\n  %d : Double() = aten::sum(%y)\r\n  %3 : Float(3, 3) = aten::mul(%x, %d)\r\n  return (%3)\r\n```\r\n\r\n## Expected behavior\r\n\r\nI think aten::mul should return a Double value while running backward if the second argument of forward is Double.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0.dev20190605\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.5.1804 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0.dev20190605\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.2.0.dev20190605 py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n\r\n","python\r\nimport torch\r\n\r\ndef f(x, y):\r\n    d = y.sum()\r\n    return x * d\r\n\r\nx = torch.randn(3,3, requires_grad=True)\r\ny = torch.randn(3,3, requires_grad=True, dtype=torch.float64)\r\ntf = torch.jit.trace(f, (x, y))\r\nz = tf(x, y)\r\nz.backward(torch.randn_like(x))\r\n"
21406,"matmul in a ScriptModule requires much memory## \U0001f41b Bug\r\n\r\nmatmul in a ScriptModule requires much more memory than in a regular (non-JIT) module.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior: Run the following script.\r\n\r\n\r\nI checked the GPU memory usage at the end of the script (by inserting input()).\r\nnvidia-smi said\r\n```\r\n|    0    155985      C   python                                      8109MiB |\r\n```\r\n\r\nThe non-JIT version\r\n\r\nrequires much less.\r\nnvidia-smi showed\r\n```\r\n|    0    156120      C   python                                      2739MiB |\r\n```\r\n\r\nThe memory usage increases during backward.\r\nMore precisely,  ``matmul`` in ``LinearAlgebra.cpp`` is called in ``DifferentiableGraphOp`` and uses much memory.\r\nThe memory usage even increases by the second call of ``backward``.\r\n\r\nI thought this could be related to #18862, but the latest nightly build ``1.2.0.dev20190605`` didn't solve the problem.\r\n\r\n## Expected behavior\r\n\r\nI expect that the JIT version requires the almost same amount of memory as the non-JIT version.\r\n\r\n## Environment\r\n\r\nI created a new conda environment and installed the latest nightly build (``1.2.0.dev20190605``).\r\n\r\nPyTorch version: 1.2.0.dev20190605\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.5.1804 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0.dev20190605\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.2.0.dev20190605 py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",high priority|oncall: jit|triaged,ailzhang,"## \U0001f41b Bug\r\n\r\nmatmul in a ScriptModule requires much more memory than in a regular (non-JIT) module.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior: Run the following script.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc = nn.Linear(768, 100000)\r\n\r\n    def forward(self, x):\r\n        x = x * 2\r\n        x = x * 2\r\n        x = x * 2\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# trace on cpu\r\nm = Net()\r\nx = torch.randn(10, 128, 768)\r\njm = torch.jit.trace(m, x)\r\njm = jm.to(""cuda"")\r\n\r\nfor i in range(0,2):\r\n    x = torch.randn(10, 128, 768, requires_grad=True).to(""cuda"")\r\n    y = jm(x)\r\n    tgt = torch.randn_like(y)\r\n    y.backward(tgt)\r\n```\r\nI checked the GPU memory usage at the end of the script (by inserting input()).\r\nnvidia-smi said\r\n```\r\n|    0    155985      C   python                                      8109MiB |\r\n```\r\n\r\nThe non-JIT version\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc = nn.Linear(768, 100000)\r\n\r\n    def forward(self, x):\r\n        x = x * 2\r\n        x = x * 2\r\n        x = x * 2\r\n        x = self.fc(x)\r\n        return x\r\n\r\n\r\nm = Net().to(""cuda"")\r\n\r\nfor i in range(0,2):\r\n    x = torch.randn(10, 128, 768, requires_grad=True).to(""cuda"")\r\n    y = m(x)\r\n    tgt = torch.randn_like(y)\r\n    y.backward(tgt)\r\n```\r\nrequires much less.\r\nnvidia-smi showed\r\n```\r\n|    0    156120      C   python                                      2739MiB |\r\n```\r\n\r\nThe memory usage increases during backward.\r\nMore precisely,  ``matmul`` in ``LinearAlgebra.cpp`` is called in ``DifferentiableGraphOp`` and uses much memory.\r\nThe memory usage even increases by the second call of ``backward``.\r\n\r\nI thought this could be related to #18862, but the latest nightly build ``1.2.0.dev20190605`` didn't solve the problem.\r\n\r\n## Expected behavior\r\n\r\nI expect that the JIT version requires the almost same amount of memory as the non-JIT version.\r\n\r\n## Environment\r\n\r\nI created a new conda environment and installed the latest nightly build (``1.2.0.dev20190605``).\r\n\r\nPyTorch version: 1.2.0.dev20190605\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.5.1804 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.14.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.2.0.dev20190605\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.2.0.dev20190605 py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc = nn.Linear(768, 100000)\r\n\r\n    def forward(self, x):\r\n        x = x * 2\r\n        x = x * 2\r\n        x = x * 2\r\n        x = self.fc(x)\r\n        return x\r\n\r\n# trace on cpu\r\nm = Net()\r\nx = torch.randn(10, 128, 768)\r\njm = torch.jit.trace(m, x)\r\njm = jm.to(""cuda"")\r\n\r\nfor i in range(0,2):\r\n    x = torch.randn(10, 128, 768, requires_grad=True).to(""cuda"")\r\n    y = jm(x)\r\n    tgt = torch.randn_like(y)\r\n    y.backward(tgt)\r\n"
21344,"Make DDP failure recoverable@kuttas @pietern and I had a discussion on how to make DDP failure recoverable. The process involves the following steps:\r\n\r\n\r\n\r\nThis does not work in today's DDP. Currently, to get better performance, DDP assigns the [original module](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/nn/parallel/distributed.py#L316) to the first module replica instead of creating a new one. Then, it creates a new `Reducer` to [add post hooks](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/distributed/c10d/reducer.cpp#L108-L112) to sync params. However, because every reconstructed DDP instance wraps the same original module, all their reducers will add hooks to the same set of variables. Hence, after 10 recoveries, each param (variable) in the original module will have 11 hooks introduced by 11 different reducers, where only the last one is still alive. We thought about several potential solutions:\r\n\r\n#### Solution 1: Force Module Replication\r\n\r\nForce DDP to create new module replicas instead of using the original module. In this way, those variables in the replicas will die together with the DDP instance. But it will make the DDP slower. Maybe make it an option?\r\n\r\n#### Solution 2: Delete Hooks in Destructor\r\n\r\nI feel the best way would be deleting those hooks from model variables when destructing a `Reducer`, but I didn't find a clean way to do that. The [`add_post_hook`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L250) function takes unique parameters, and we can get those hooks through [`post_hooks`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L254). Directly looping through the the hooks vector and find the target to delete seems to be too hackish.\r\n\r\n#### Solution 3: Create New Variables (?)\r\n\r\nNot sure if this can work. Instead of creating replica (as in Solution 1), let DDP create a new variable for every parameter in the original module. All DDP forward and backward pass will use those new variables. I think this won't work if the application only wraps part of the model using DDP, because there will be two disjoint autograd graphs (?)\r\n\r\n\r\n@soumith @gchanan @ezyang  thoughts?\r\n",oncall: distributed|module: autograd|triaged,mrshenli,"@kuttas @pietern and I had a discussion on how to make DDP failure recoverable. The process involves the following steps:\r\n\r\n```python\r\nm = SomeModel()\r\ndist.init_process_group()\r\nddp = DistributedDataParallel(m)\r\n# got error\r\ndist.destroy_process_group()\r\ndist.init_process_group()\r\ndel ddp\r\nddp = DistributedDataParallel(m)\r\n```\r\n\r\nThis does not work in today's DDP. Currently, to get better performance, DDP assigns the [original module](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/nn/parallel/distributed.py#L316) to the first module replica instead of creating a new one. Then, it creates a new `Reducer` to [add post hooks](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/distributed/c10d/reducer.cpp#L108-L112) to sync params. However, because every reconstructed DDP instance wraps the same original module, all their reducers will add hooks to the same set of variables. Hence, after 10 recoveries, each param (variable) in the original module will have 11 hooks introduced by 11 different reducers, where only the last one is still alive. We thought about several potential solutions:\r\n\r\n#### Solution 1: Force Module Replication\r\n\r\nForce DDP to create new module replicas instead of using the original module. In this way, those variables in the replicas will die together with the DDP instance. But it will make the DDP slower. Maybe make it an option?\r\n\r\n#### Solution 2: Delete Hooks in Destructor\r\n\r\nI feel the best way would be deleting those hooks from model variables when destructing a `Reducer`, but I didn't find a clean way to do that. The [`add_post_hook`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L250) function takes unique parameters, and we can get those hooks through [`post_hooks`](https://github.com/pytorch/pytorch/blob/fa4ca4e70e98fbe944d1435929ea4fd17c7bed7d/torch/csrc/autograd/function.h#L254). Directly looping through the the hooks vector and find the target to delete seems to be too hackish.\r\n\r\n#### Solution 3: Create New Variables (?)\r\n\r\nNot sure if this can work. Instead of creating replica (as in Solution 1), let DDP create a new variable for every parameter in the original module. All DDP forward and backward pass will use those new variables. I think this won't work if the application only wraps part of the model using DDP, because there will be two disjoint autograd graphs (?)\r\n\r\n\r\n@soumith @gchanan @ezyang  thoughts?\r\n",python\r\nm = SomeModel()\r\ndist.init_process_group()\r\nddp = DistributedDataParallel(m)\r\n# got error\r\ndist.destroy_process_group()\r\ndist.init_process_group()\r\ndel ddp\r\nddp = DistributedDataParallel(m)\r\n
21282,"TorchScript support for torch.nn.utils.rnn.pack_padded_sequence() and torch.nn.utils.rnn.pad_packed_sequence()## \U0001f680 Feature\r\nAdd TorchScript support for `torch.nn.utils.rnn.pack_padded_sequence()` and `torch.nn.utils.rnn.pad_packed_sequence()`.\r\n\r\n## Motivation\r\n\r\nMotivated by https://discuss.pytorch.org/t/torch-jit-script-for-torch-nn-utils-rnn-pack-padded-sequence/30668, this appears to be a desired feature given that tracing is usually not an option when these functions are used (for variable length sequences)\r\n\r\n## Pitch\r\n\r\nRight now, the following code:\r\n\r\n\r\nFails with: \r\n\r\n```\r\nfor operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\r\nexpected a value of type Tensor for argument '2' but found bool\r\n@torch.jit.script_method\r\ndef forward(self, x_in, x_lengths):\r\n    max_length = x_lengths[0]\r\n    x_in_shortened = x_in[:,:max_length]\r\n\r\n    x_packed = nn.utils.rnn.pack_padded_sequence(x_in_shortened, \r\n                                                 x_lengths,\r\n                                                 True, # batch_first\r\n                                                 ~~~~ <--- HERE\r\n                                                 True # enforce_sorted\r\n                                                )\r\n```\r\n\r\nSo it seems that it expects Tensors for the arguments `batch_first` and `enforce_sorted` instead of Python bools. Okay, if we change them to:\r\n\r\n\r\nInsantiation works, but it fails when the forward method is applied:\r\n\r\n```\r\nmy_lstm(torch.Tensor([[3,1,2,3,4]]), torch.Tensor([5]).long())\r\n\r\n# Fails with:\r\nTypeError: _pack_padded_sequence(): argument 'batch_first' (position 3) must be bool, not Tensor\r\n```\r\n\r\nSo currently, either instantiation or forward pass is always going to fail. I believe that this is just argument parsing, and that the ""hard"" part of this issue is to actually make the functions compatible with TorchScript...\r\n\r\nBTW Thank you for PyTorch overall. \xa1It's awesome!",oncall: jit|triaged,driazati,"## \U0001f680 Feature\r\nAdd TorchScript support for `torch.nn.utils.rnn.pack_padded_sequence()` and `torch.nn.utils.rnn.pad_packed_sequence()`.\r\n\r\n## Motivation\r\n\r\nMotivated by https://discuss.pytorch.org/t/torch-jit-script-for-torch-nn-utils-rnn-pack-padded-sequence/30668, this appears to be a desired feature given that tracing is usually not an option when these functions are used (for variable length sequences)\r\n\r\n## Pitch\r\n\r\nRight now, the following code:\r\n\r\n```python\r\nfrom torch import nn\r\nimport torch.jit\r\n\r\nclass MyLSTM(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n\r\n        super(MyLSTM, self).__init__()\r\n        \r\n        self.rnn = nn.LSTM(input_size=300,\r\n                           hidden_size=32,\r\n                           batch_first=True,\r\n                           num_layers=1,\r\n                           bidirectional=False)\r\n    \r\n    @torch.jit.script_method\r\n    def forward(self, x_in, x_lengths):\r\n        max_length = x_lengths[0]\r\n        x_in_shortened = x_in[:,:max_length]\r\n        \r\n        x_packed = nn.utils.rnn.pack_padded_sequence(x_in_shortened, \r\n                                                     x_lengths,\r\n                                                     True, # batch_first\r\n                                                     True # enforce_sorted\r\n                                                    )\r\n        \r\n        lstm_outputs = self.rnn(x_packed)\r\n        sequence_outputs = nn.utils.rnn.pad_packed_sequence(lstm_outputs[0], \r\n                                                            batch_first=True, \r\n                                                            padding_value=0.0, \r\n                                                            total_length=max_length)\r\n        \r\n        lstm_out = sequence_outputs[0]\r\n        out_seq_length = sequence_outputs[1]\r\n\r\n        return lstm_out\r\n\r\n# Instantiation:\r\nmy_lstm = MyLSTM()\r\n```\r\nFails with: \r\n\r\n```\r\nfor operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\r\nexpected a value of type Tensor for argument '2' but found bool\r\n@torch.jit.script_method\r\ndef forward(self, x_in, x_lengths):\r\n    max_length = x_lengths[0]\r\n    x_in_shortened = x_in[:,:max_length]\r\n\r\n    x_packed = nn.utils.rnn.pack_padded_sequence(x_in_shortened, \r\n                                                 x_lengths,\r\n                                                 True, # batch_first\r\n                                                 ~~~~ <--- HERE\r\n                                                 True # enforce_sorted\r\n                                                )\r\n```\r\n\r\nSo it seems that it expects Tensors for the arguments `batch_first` and `enforce_sorted` instead of Python bools. Okay, if we change them to:\r\n\r\n```python\r\nx_packed = nn.utils.rnn.pack_padded_sequence(x_in_shortened, \r\n                                             x_lengths,\r\n                                             torch.ones(1).byte().squeeze(), # batch_first\r\n                                             torch.ones(1).byte().squeeze() # enforce_sorted\r\n                                            )\r\n```\r\nInsantiation works, but it fails when the forward method is applied:\r\n\r\n```\r\nmy_lstm(torch.Tensor([[3,1,2,3,4]]), torch.Tensor([5]).long())\r\n\r\n# Fails with:\r\nTypeError: _pack_padded_sequence(): argument 'batch_first' (position 3) must be bool, not Tensor\r\n```\r\n\r\nSo currently, either instantiation or forward pass is always going to fail. I believe that this is just argument parsing, and that the ""hard"" part of this issue is to actually make the functions compatible with TorchScript...\r\n\r\nBTW Thank you for PyTorch overall. \xa1It's awesome!","python\r\nfrom torch import nn\r\nimport torch.jit\r\n\r\nclass MyLSTM(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n\r\n        super(MyLSTM, self).__init__()\r\n        \r\n        self.rnn = nn.LSTM(input_size=300,\r\n                           hidden_size=32,\r\n                           batch_first=True,\r\n                           num_layers=1,\r\n                           bidirectional=False)\r\n    \r\n    @torch.jit.script_method\r\n    def forward(self, x_in, x_lengths):\r\n        max_length = x_lengths[0]\r\n        x_in_shortened = x_in[:,:max_length]\r\n        \r\n        x_packed = nn.utils.rnn.pack_padded_sequence(x_in_shortened, \r\n                                                     x_lengths,\r\n                                                     True, # batch_first\r\n                                                     True # enforce_sorted\r\n                                                    )\r\n        \r\n        lstm_outputs = self.rnn(x_packed)\r\n        sequence_outputs = nn.utils.rnn.pad_packed_sequence(lstm_outputs[0], \r\n                                                            batch_first=True, \r\n                                                            padding_value=0.0, \r\n                                                            total_length=max_length)\r\n        \r\n        lstm_out = sequence_outputs[0]\r\n        out_seq_length = sequence_outputs[1]\r\n\r\n        return lstm_out\r\n\r\n# Instantiation:\r\nmy_lstm = MyLSTM()\r\n"
21257,"Weird sampling from multinomial_alias_draw## \U0001f41b Bug\r\n\r\nWhich distribution is `torch._multinomial_alias_draw` sampling from?\r\nRelated to #4115 and #18906.\r\n\r\n## To Reproduce\r\n\r\nSampling 1 element at a time, hence, here we ignore whether it is sampling with or without replacement. In this case, the distribution to draw samples from equals the normalized weights. \r\n\r\n\r\nResults found: \r\n\r\n- [ALIAS] p-values < 0.001 and kl_div > 0.3\r\n- [NO ALIAS] p-values > 0.1 and kl_div < 0.01\r\n\r\n## Expected behavior\r\n\r\nSample from a multinomial distribution.\r\n\r\n## Environment\r\n\r\nToday's nightly version of PyTorch.",high priority|triaged|module: random,vishwakftw,"## \U0001f41b Bug\r\n\r\nWhich distribution is `torch._multinomial_alias_draw` sampling from?\r\nRelated to #4115 and #18906.\r\n\r\n## To Reproduce\r\n\r\nSampling 1 element at a time, hence, here we ignore whether it is sampling with or without replacement. In this case, the distribution to draw samples from equals the normalized weights. \r\n\r\n```python\r\nimport torch\r\nimport scipy.stats\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\n\r\nn = 10000\r\nreplace = True\r\ndevice = 'cpu'\r\n\r\nmultinomial_alias_samples = []\r\nmultinomial_samples = []\r\n\r\nweights = torch.Tensor([0.1, 0.6, 0.2, 0.1], device=device)\r\nJ, q = torch._multinomial_alias_setup(weights)\r\n\r\nfor _ in range(n):\r\n\tmultinomial_alias_samples += torch._multinomial_alias_draw(\r\n\t\t\t\t\t\t\t      q, \r\n\t\t\t\t\t\t\t      J, \r\n\t\t\t\t\t\t\t      1\r\n\t\t\t\t\t\t\t    ).cpu().numpy().tolist()\r\n\tmultinomial_samples += torch.multinomial(\r\n\t\t\t\t\t\t  weights,\r\n\t\t\t\t\t\t  1,\r\n\t\t\t\t\t\t  replace\r\n\t\t\t\t\t\t).cpu().numpy().tolist()\r\n\r\ncorrect_dist =  weights / weights.sum()\r\ncorrect_dist = correct_dist.to('cpu')\r\n_, multinomial_alias_dist = np.unique(multinomial_alias_samples, return_counts=True)\r\n\r\n_, p = scipy.stats.chisquare(multinomial_alias_dist, correct_dist.numpy() * n)\r\nprint(""[ALIAS] Chi-Squared Test p-value {:.3f}"".format(p))\r\nmultinomial_alias_dist = torch.Tensor(multinomial_alias_dist) / n\r\nprint(""[ALIAS] KL Divergence {:.3f}"".format(\r\n\t\t\t\t\t    F.kl_div(\r\n\t\t\t\t\t\tmultinomial_alias_dist.log(), \r\n\t\t\t\t\t\tcorrect_dist, \r\n\t\t\t\t\t\treduction='sum')\r\n\t\t\t\t\t    )\r\n      )\r\n\r\n_, multinomial_dist = np.unique(multinomial_samples, return_counts=True)\r\n_, p = scipy.stats.chisquare(multinomial_dist, correct_dist.numpy() * n)\r\nprint(""[NO ALIAS] Chi-Squared Test p-value {:.3f}"".format(p))\r\nmultinomial_dist = torch.Tensor(multinomial_dist) / n\r\nprint(""[NO ALIAS] KL Divergence {:.3f}"".format(\r\n\t\t\t\t\t    F.kl_div(\r\n\t\t\t\t\t\tmultinomial_dist.log(), \r\n\t\t\t\t\t\tcorrect_dist, \r\n\t\t\t\t\t\treduction='sum')\r\n\t\t\t\t\t    )\r\n      )\r\n\r\n```\r\nResults found: \r\n\r\n- [ALIAS] p-values < 0.001 and kl_div > 0.3\r\n- [NO ALIAS] p-values > 0.1 and kl_div < 0.01\r\n\r\n## Expected behavior\r\n\r\nSample from a multinomial distribution.\r\n\r\n## Environment\r\n\r\nToday's nightly version of PyTorch.","python\r\nimport torch\r\nimport scipy.stats\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\n\r\nn = 10000\r\nreplace = True\r\ndevice = 'cpu'\r\n\r\nmultinomial_alias_samples = []\r\nmultinomial_samples = []\r\n\r\nweights = torch.Tensor([0.1, 0.6, 0.2, 0.1], device=device)\r\nJ, q = torch._multinomial_alias_setup(weights)\r\n\r\nfor _ in range(n):\r\n\tmultinomial_alias_samples += torch._multinomial_alias_draw(\r\n\t\t\t\t\t\t\t      q, \r\n\t\t\t\t\t\t\t      J, \r\n\t\t\t\t\t\t\t      1\r\n\t\t\t\t\t\t\t    ).cpu().numpy().tolist()\r\n\tmultinomial_samples += torch.multinomial(\r\n\t\t\t\t\t\t  weights,\r\n\t\t\t\t\t\t  1,\r\n\t\t\t\t\t\t  replace\r\n\t\t\t\t\t\t).cpu().numpy().tolist()\r\n\r\ncorrect_dist =  weights / weights.sum()\r\ncorrect_dist = correct_dist.to('cpu')\r\n_, multinomial_alias_dist = np.unique(multinomial_alias_samples, return_counts=True)\r\n\r\n_, p = scipy.stats.chisquare(multinomial_alias_dist, correct_dist.numpy() * n)\r\nprint(""[ALIAS] Chi-Squared Test p-value {:.3f}"".format(p))\r\nmultinomial_alias_dist = torch.Tensor(multinomial_alias_dist) / n\r\nprint(""[ALIAS] KL Divergence {:.3f}"".format(\r\n\t\t\t\t\t    F.kl_div(\r\n\t\t\t\t\t\tmultinomial_alias_dist.log(), \r\n\t\t\t\t\t\tcorrect_dist, \r\n\t\t\t\t\t\treduction='sum')\r\n\t\t\t\t\t    )\r\n      )\r\n\r\n_, multinomial_dist = np.unique(multinomial_samples, return_counts=True)\r\n_, p = scipy.stats.chisquare(multinomial_dist, correct_dist.numpy() * n)\r\nprint(""[NO ALIAS] Chi-Squared Test p-value {:.3f}"".format(p))\r\nmultinomial_dist = torch.Tensor(multinomial_dist) / n\r\nprint(""[NO ALIAS] KL Divergence {:.3f}"".format(\r\n\t\t\t\t\t    F.kl_div(\r\n\t\t\t\t\t\tmultinomial_dist.log(), \r\n\t\t\t\t\t\tcorrect_dist, \r\n\t\t\t\t\t\treduction='sum')\r\n\t\t\t\t\t    )\r\n      )\r\n\r\n"
21193,"ScriptModule using pack_padded_sequence prototype seems falseTrying to use pack_padded_sequence through a torch.jit.ScriptModule reports the following prototype which seems false ? pack_padded_sequence's prototype is (Tensor, Tensor, bool, bool).\r\n\r\n\r\n\r\nWhen trying to use the following module:\r\n\r\n",oncall: jit|triaged,driazati,"Trying to use pack_padded_sequence through a torch.jit.ScriptModule reports the following prototype which seems false ? pack_padded_sequence's prototype is (Tensor, Tensor, bool, bool).\r\n\r\n```python\r\nRuntimeError: \r\nfor operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\r\nargument 0 not provided.\r\n```\r\n\r\nWhen trying to use the following module:\r\n\r\n```python\r\nfrom typing import List\r\n\r\nimport torch\r\nfrom torch.jit import ScriptModule, script_method, annotate\r\nfrom torch.nn import Dropout, ModuleList, ParameterList, Parameter, GRU\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n\r\n__author__ = 'Morgan Funtowicz'\r\n\r\n\r\nclass StackedBiRNN(ScriptModule):\r\n    """"""\r\n    Stacked Bi-directional RNNs.\r\n    Differs from standard PyTorch library in that it has the option to save\r\n    and concat the hidden states between layers. (i.e. the output hidden size\r\n    for each sequence input is num_layers * hidden_size).\r\n    """"""\r\n\r\n    __constants__ = ['padding', 'dropout_rate', 'num_layers', 'concat_layers', '_rnns']\r\n\r\n    def __init__(self, input_size, hidden_size, num_layers,\r\n                 dropout_p=0., rnn_type=GRU,\r\n                 concat=False, padding=False):\r\n        super(StackedBiRNN, self).__init__()\r\n\r\n        self.padding = padding\r\n        self.dropout_rate = dropout_p\r\n        self.num_layers = num_layers\r\n        self.concat_layers = concat\r\n\r\n        rnns = []\r\n        for i in range(num_layers):\r\n            input_size = input_size if i == 0 else 2 * hidden_size\r\n\r\n            rnns.append(rnn_type(input_size, hidden_size, batch_first=True, bidirectional=True))\r\n\r\n        self._rnns = ModuleList(rnns)\r\n        self._dropout = Dropout(dropout_p)\r\n\r\n    @property\r\n    def hidden_size(self):\r\n        return self._rnns[0].hidden_size\r\n\r\n    @script_method\r\n    def forward(self, x: torch.Tensor, lengths: torch.Tensor):\r\n\r\n        # Buffer storing RNN layers output\r\n        outputs = annotate(List[torch.Tensor], [x])\r\n\r\n        # Iterate through each RNN layer\r\n        for rnn in self._rnns:\r\n\r\n            # Retrieve the latest output (output from previous layer)\r\n            rnn_input = outputs[-1]\r\n\r\n            # Apply dropout to input\r\n            if self.dropout_rate > 0:\r\n                rnn_input = self._dropout(rnn_input)\r\n\r\n            rnn_input = pack_padded_sequence(input=rnn_input, lengths=lengths, batch_first=True, enforce_sorted=False)\r\n\r\n            # Go through the RNN and get output\r\n            rnn.flatten_parameters()\r\n            out = rnn(rnn_input)[0]\r\n            outputs += [pad_packed_sequence(sequence=out, batch_first=True)[0]]\r\n\r\n        # Concat hidden layers or take final\r\n        if self.concat_layers:\r\n            return torch.cat(outputs[1:], dim=-1)\r\n        else:\r\n            return outputs[-1]\r\n```","python\r\nRuntimeError: \r\nfor operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\r\nargument 0 not provided.\r\n"
21183,"[jit] Save source maps for imported code error reportingWe should copy JavaScript and optionally save source maps along with the serialized model so that errors on imported code don't show their highlight on the Python-printed code\r\n\r\n\r\n\r\nprints\r\n\r\n```\r\nRuntimeError: \r\nExpected 4-dimensional input for 4-dimensional weight 5 5, but got 2-dimensional input of size [20, 20] instead:\r\noperation failed in interpreter:\r\n            ret10 = x\r\n          ret5 = ret10\r\n        ret0 = ret5\r\n      ret = ret0\r\n    _122 = _0.weight\r\n    _123 = _0.bias\r\n    _124 = [0, 0]\r\n    _1 = torch.conv2d(ret, _122, _123, [1, 1], _124, [1, 1], 1)\r\n  else:\r\n    _1 = torch.conv2d(x, _0.weight, _0.bias, [1, 1], [0, 0], [1, 1], 1)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n  return _1\r\n```\r\n\r\nbut should print\r\n\r\n```\r\nExpected 4-dimensional input for 4-dimensional weight 5 5, but got 2-dimensional input of size [20, 20] instead:\r\noperation failed in interpreter:\r\n@weak_script_method\r\ndef forward(self, input):\r\n    if self.padding_mode == 'circular':\r\n        expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\r\n                            (self.padding[0] + 1) // 2, self.padding[0] // 2)\r\n        return F.conv2d(F.pad(input, expanded_padding, mode='circular'),\r\n                        self.weight, self.bias, self.stride,\r\n                        _pair(0), self.dilation, self.groups)\r\n    return F.conv2d(input, self.weight, self.bias, self.stride,\r\n           ~~~~~~~~ <--- HERE\r\n                    self.padding, self.dilation, self.groups)\r\n\r\n```",oncall: jit|triaged,jamesr66a,"We should copy JavaScript and optionally save source maps along with the serialized model so that errors on imported code don't show their highlight on the Python-printed code\r\n\r\n```python\r\nclass M(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.conv = nn.Conv2d(5, 5, 2)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\nM().save(""m.pt"")\r\nloaded = torch.jit.load(""m.pt"")\r\nloaded(torch.ones(20, 20))\r\n```\r\n\r\nprints\r\n\r\n```\r\nRuntimeError: \r\nExpected 4-dimensional input for 4-dimensional weight 5 5, but got 2-dimensional input of size [20, 20] instead:\r\noperation failed in interpreter:\r\n            ret10 = x\r\n          ret5 = ret10\r\n        ret0 = ret5\r\n      ret = ret0\r\n    _122 = _0.weight\r\n    _123 = _0.bias\r\n    _124 = [0, 0]\r\n    _1 = torch.conv2d(ret, _122, _123, [1, 1], _124, [1, 1], 1)\r\n  else:\r\n    _1 = torch.conv2d(x, _0.weight, _0.bias, [1, 1], [0, 0], [1, 1], 1)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n  return _1\r\n```\r\n\r\nbut should print\r\n\r\n```\r\nExpected 4-dimensional input for 4-dimensional weight 5 5, but got 2-dimensional input of size [20, 20] instead:\r\noperation failed in interpreter:\r\n@weak_script_method\r\ndef forward(self, input):\r\n    if self.padding_mode == 'circular':\r\n        expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\r\n                            (self.padding[0] + 1) // 2, self.padding[0] // 2)\r\n        return F.conv2d(F.pad(input, expanded_padding, mode='circular'),\r\n                        self.weight, self.bias, self.stride,\r\n                        _pair(0), self.dilation, self.groups)\r\n    return F.conv2d(input, self.weight, self.bias, self.stride,\r\n           ~~~~~~~~ <--- HERE\r\n                    self.padding, self.dilation, self.groups)\r\n\r\n```","python\r\nclass M(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.conv = nn.Conv2d(5, 5, 2)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\nM().save(""m.pt"")\r\nloaded = torch.jit.load(""m.pt"")\r\nloaded(torch.ones(20, 20))\r\n"
21173,"[jit] Modifying a list while iterating doesn't work\r\n\r\nthrows\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""../test.py"", line 32, in <module>\r\n    print(fn([1, 2, 3, 4]))\r\nRuntimeError: \r\nlist index out of range:\r\noperation failed in interpreter:\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> List[int]\r\n    new_list = [0]\r\n    for i in x:\r\n    ~~~~~~~~~~~...  <--- HERE\r\n        if i == 2:\r\n            x.remove(i)\r\n        new_list.append(i)\r\n    return new_list\r\n\r\n```\r\n",oncall: jit,jamesr66a,"```python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> List[int]\r\n    new_list = [0]\r\n    for i in x:\r\n        if i == 2:\r\n            x.remove(i)\r\n        new_list.append(i)\r\n    return new_list\r\n```\r\n\r\nthrows\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""../test.py"", line 32, in <module>\r\n    print(fn([1, 2, 3, 4]))\r\nRuntimeError: \r\nlist index out of range:\r\noperation failed in interpreter:\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> List[int]\r\n    new_list = [0]\r\n    for i in x:\r\n    ~~~~~~~~~~~...  <--- HERE\r\n        if i == 2:\r\n            x.remove(i)\r\n        new_list.append(i)\r\n    return new_list\r\n\r\n```\r\n",python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (List[int]) -> List[int]\r\n    new_list = [0]\r\n    for i in x:\r\n        if i == 2:\r\n            x.remove(i)\r\n        new_list.append(i)\r\n    return new_list\r\n
21108,[DataParallel] flatten_parameters doesn't work under torch.no_grad## \U0001f41b Bug\r\n\r\nWhen the model is using `DataParallel` and we call `flatten_parameters` inside the model under `torch.no_grad` it throws this error:\r\n\r\n> RuntimeError: set_storage is not allowed on Tensor created from .data or .detach()\r\n\r\nworks fine otherwise. This behavior only happens on 1.1.0 and was working fine on 1.0.1.post2\r\n\r\n## To Reproduce\r\n\r\nRun the code below on 1.1.0 to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\n`flatten_parameters` should work as it does without DataParallel\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.1\r\n[pip] numpy==1.16.4\r\n[pip] numpydoc==0.7.0\r\n[pip] pytorch-nlp==0.3.5\r\n[pip] pytorch-pretrained-bert==0.3.0\r\n[pip] torch==1.1.0\r\n[pip] torchfile==0.1.0\r\n[pip] torchtext==0.2.3\r\n[pip] torchvision==0.2.0\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] faiss-cpu                 1.2.1           py36_cuda9.0.176_1    pytorch\r\n[conda] faiss-gpu                 1.2.1           py36_cuda9.0.176_1    pytorch\r\n[conda] magma-cuda90              2.3.0                         1    pytorch\r\n[conda] mkl                       2018.0.1             h19d6760_4    anaconda\r\n[conda] mkl-fft                   1.0.0                     <pip>\r\n[conda] mkl-include               2018.0.3                      1\r\n[conda] mkl-random                1.0.1                     <pip>\r\n[conda] mkl-service               1.1.2            py36h17a0993_4\r\n[conda] mkl_fft                   1.0.2           np114py36_intel_0  [intel]  intel\r\n[conda] mkl_random                1.0.1           np114py36_intel_0  [intel]  intel\r\n[conda] mkldnn                    0.14.0                        0    mingfeima\r\n[conda] nccl2                     1.0                           0    pytorch\r\n[conda] pytorch-nlp               0.3.5                     <pip>\r\n[conda] pytorch-pretrained-bert   0.3.0                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>\r\n\r\n,oncall: distributed|triaged,mrshenli,"## \U0001f41b Bug\r\n\r\nWhen the model is using `DataParallel` and we call `flatten_parameters` inside the model under `torch.no_grad` it throws this error:\r\n\r\n> RuntimeError: set_storage is not allowed on Tensor created from .data or .detach()\r\n\r\nworks fine otherwise. This behavior only happens on 1.1.0 and was working fine on 1.0.1.post2\r\n\r\n## To Reproduce\r\n\r\nRun the code below on 1.1.0 to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.rnn = torch.nn.LSTM(300, 1024, 1, batch_first=True, bidirectional=True)\r\n    def forward(self, x):\r\n        self.rnn.flatten_parameters()\r\n        return self.rnn(x)  # N * T * hidden_dim\r\n\r\n\r\nmodel = torch.nn.DataParallel(Model().to('cuda'))\r\n\r\nwith torch.no_grad():\r\n    x = model(torch.rand(2, 4, 300))\r\n```\r\n\r\n## Expected behavior\r\n\r\n`flatten_parameters` should work as it does without DataParallel\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.1\r\n[pip] numpy==1.16.4\r\n[pip] numpydoc==0.7.0\r\n[pip] pytorch-nlp==0.3.5\r\n[pip] pytorch-pretrained-bert==0.3.0\r\n[pip] torch==1.1.0\r\n[pip] torchfile==0.1.0\r\n[pip] torchtext==0.2.3\r\n[pip] torchvision==0.2.0\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] faiss-cpu                 1.2.1           py36_cuda9.0.176_1    pytorch\r\n[conda] faiss-gpu                 1.2.1           py36_cuda9.0.176_1    pytorch\r\n[conda] magma-cuda90              2.3.0                         1    pytorch\r\n[conda] mkl                       2018.0.1             h19d6760_4    anaconda\r\n[conda] mkl-fft                   1.0.0                     <pip>\r\n[conda] mkl-include               2018.0.3                      1\r\n[conda] mkl-random                1.0.1                     <pip>\r\n[conda] mkl-service               1.1.2            py36h17a0993_4\r\n[conda] mkl_fft                   1.0.2           np114py36_intel_0  [intel]  intel\r\n[conda] mkl_random                1.0.1           np114py36_intel_0  [intel]  intel\r\n[conda] mkldnn                    0.14.0                        0    mingfeima\r\n[conda] nccl2                     1.0                           0    pytorch\r\n[conda] pytorch-nlp               0.3.5                     <pip>\r\n[conda] pytorch-pretrained-bert   0.3.0                     <pip>\r\n[conda] torch                     1.1.0                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>\r\n\r\n","python\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.rnn = torch.nn.LSTM(300, 1024, 1, batch_first=True, bidirectional=True)\r\n    def forward(self, x):\r\n        self.rnn.flatten_parameters()\r\n        return self.rnn(x)  # N * T * hidden_dim\r\n\r\n\r\nmodel = torch.nn.DataParallel(Model().to('cuda'))\r\n\r\nwith torch.no_grad():\r\n    x = model(torch.rand(2, 4, 300))\r\n"
20978,"Cannot print LayerNorm in ScriptModule## \U0001f41b Bug\r\nVersion: master (1.2.0.dev20190526)\r\nGot KeyError: 'elementwise_affine' when printing a ScriptModule with a LayerNorm in it.\r\n\r\n## To Reproduce\r\n\r\n\r\nThe result is:\r\n```\r\nraceback (most recent call last):\r\n  File ""/home/chenyufei/Development/nn-parser/local_scripts/pytorch_1_2_master_bug.py"", line 14, in <module>\r\n    print(m)\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1044, in __repr__\r\n    mod_str = repr(module)\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1038, in __repr__\r\n    extra_repr = self.extra_repr()\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/normalization.py"", line 161, in extra_repr\r\n    'elementwise_affine={elementwise_affine}'.format(**self.__dict__)\r\nKeyError: 'elementwise_affine'\r\n```\r\n\r\n## Expected behavior\r\nIn stable version, it is:\r\n```\r\nTest(\r\n  (layer_norm): WeakScriptModuleProxy()\r\n)\r\n```\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0.dev20190526\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 980 Ti\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.1\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-pretrained-bert==0.6.1\r\n[pip3] torch==1.0.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchtext==0.4.0\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\nVersion: master (1.2.0.dev20190526)\r\nGot KeyError: 'elementwise_affine' when printing a ScriptModule with a LayerNorm in it.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nfrom torch.nn import LayerNorm\r\nfrom torch.jit import ScriptModule\r\n\r\n\r\nclass Test(ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.layer_norm = LayerNorm(dim)\r\n\r\n\r\nif __name__ == '__main__':\r\n    m = Test(100)\r\n    print(m)\r\n```\r\n\r\nThe result is:\r\n```\r\nraceback (most recent call last):\r\n  File ""/home/chenyufei/Development/nn-parser/local_scripts/pytorch_1_2_master_bug.py"", line 14, in <module>\r\n    print(m)\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1044, in __repr__\r\n    mod_str = repr(module)\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1038, in __repr__\r\n    extra_repr = self.extra_repr()\r\n  File ""/home/chenyufei/.local/anaconda3.7/lib/python3.7/site-packages/torch/nn/modules/normalization.py"", line 161, in extra_repr\r\n    'elementwise_affine={elementwise_affine}'.format(**self.__dict__)\r\nKeyError: 'elementwise_affine'\r\n```\r\n\r\n## Expected behavior\r\nIn stable version, it is:\r\n```\r\nTest(\r\n  (layer_norm): WeakScriptModuleProxy()\r\n)\r\n```\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.2.0.dev20190526\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 980 Ti\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.1\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] pytorch-pretrained-bert==0.6.1\r\n[pip3] torch==1.0.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchtext==0.4.0\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n","python\r\nimport torch\r\nfrom torch.nn import LayerNorm\r\nfrom torch.jit import ScriptModule\r\n\r\n\r\nclass Test(ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.layer_norm = LayerNorm(dim)\r\n\r\n\r\nif __name__ == '__main__':\r\n    m = Test(100)\r\n    print(m)\r\n"
20972,"logsigmoid fails on -inf## \U0001f41b Bug\r\n\r\ntorch.nn.functional.logsigmoid returns nan when its input is -inf, the correct return value should be -inf. Algorithm should be adjusted to return the correct value (and correct derivative), or the special case should be tested for and replaced.\r\n\r\nThe impact of the issue is that if a -inf is present in an input tensor the output is corrupted and the gradient will fail to compute for the full tensor, even though -inf should not be problematic.\r\n\r\nIn fixing the error the gradient should also be checked and should be 1.0 (currently gradient calculation fails due to nan.)\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet below and get a nan result.\r\n\r\n\r\n## Expected behavior\r\n\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): 1.1 (cpu only)\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): n/a\r\n - Python version:3.7.1\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n\r\n## Additional context\r\n\r\nProblem should occur generally as it is a flaw in the algorithm, I have not tested on a gpu, only on a cpu.\r\n",module: nn|triaged,peterbell10,"## \U0001f41b Bug\r\n\r\ntorch.nn.functional.logsigmoid returns nan when its input is -inf, the correct return value should be -inf. Algorithm should be adjusted to return the correct value (and correct derivative), or the special case should be tested for and replaced.\r\n\r\nThe impact of the issue is that if a -inf is present in an input tensor the output is corrupted and the gradient will fail to compute for the full tensor, even though -inf should not be problematic.\r\n\r\nIn fixing the error the gradient should also be checked and should be 1.0 (currently gradient calculation fails due to nan.)\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet below and get a nan result.\r\n```python\r\n>>> x = torch.tensor(-float('Inf'),requires_grad=True)\r\n>>> y = torch.nn.functional.logsigmoid(x)\r\n>>> y.backward()\r\n>>> print('y = '+str(y.item()))\r\ny = nan\r\n>>> print('dy/dx = '+str(x.grad.item()))\r\ndy/dx = nan\r\n```\r\n\r\n## Expected behavior\r\n```python\r\n>>> x = torch.tensor(-float('Inf'),requires_grad=True)\r\n>>> y = torch.nn.functional.logsigmoid(x)\r\n>>> y.backward()\r\n>>> print('y = '+str(y.item()))\r\ny = -inf\r\n>>> print('dy/dx = '+str(x.grad.item()))\r\ndy/dx = 1.0\r\n```\r\n\r\n## Environment\r\n - PyTorch Version (e.g., 1.0): 1.1 (cpu only)\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source): n/a\r\n - Python version:3.7.1\r\n - CUDA/cuDNN version: n/a\r\n - GPU models and configuration: n/a\r\n - Any other relevant information: n/a\r\n\r\n## Additional context\r\n\r\nProblem should occur generally as it is a flaw in the algorithm, I have not tested on a gpu, only on a cpu.\r\n","python\r\n>>> x = torch.tensor(-float('Inf'),requires_grad=True)\r\n>>> y = torch.nn.functional.logsigmoid(x)\r\n>>> y.backward()\r\n>>> print('y = '+str(y.item()))\r\ny = nan\r\n>>> print('dy/dx = '+str(x.grad.item()))\r\ndy/dx = nan\r\n"
20823,torch.Size is not pickleable in Python 2## \U0001f41b Bug\r\n\r\ntorch.Size objects (returned by Tensor.shape) are not pickleable in Python 2.7. They can be pickled in Python 3.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Environment\r\n\r\nPyTorch 1.0.1.post2 (but I believe it also is present in master as of 5/22/19)\r\nPython 2.7.15 on Mac OS X,module: pickle|module: serialization|triaged,ailzhang,## \U0001f41b Bug\r\n\r\ntorch.Size objects (returned by Tensor.shape) are not pickleable in Python 2.7. They can be pickled in Python 3.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport pickle\r\n\r\npickle.dumps(torch.randn(10).shape)  # TypeError: can't pickle Size objects\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch 1.0.1.post2 (but I believe it also is present in master as of 5/22/19)\r\nPython 2.7.15 on Mac OS X,python\r\nimport torch\r\nimport pickle\r\n\r\npickle.dumps(torch.randn(10).shape)  # TypeError: can't pickle Size objects\r\n
20798,"Version 1.1.0 get ""No grad accumulator for a saved leaf!"" in ScriptModule## \U0001f41b Bug\r\n\r\nWhen using LayerNorm in ScriptModule in version 1.1.0, I get ""RuntimeError: No grad accumulator for a saved leaf!"". But it's OK in 1.0.1.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nGet ""RuntimeError: No grad accumulator for a saved leaf!"" in 1.1.0. But it's OK in 1.0.1,or with torch.jit removed in 1.1.0.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 980 Ti\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.1\r\n[pip3] numpy==1.16.3\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch",oncall: jit,wanchaol,"## \U0001f41b Bug\r\n\r\nWhen using LayerNorm in ScriptModule in version 1.1.0, I get ""RuntimeError: No grad accumulator for a saved leaf!"". But it's OK in 1.0.1.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nfrom torch.nn import LayerNorm, Linear\r\nfrom torch.jit import ScriptModule, script_method\r\n\r\nclass Test(ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.layer_norm = LayerNorm(dim)\r\n        self.projection = Linear(dim, dim)\r\n    \r\n    @script_method\r\n    def forward(self, inputs):\r\n        return self.layer_norm(inputs + self.projection(inputs))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    m = Test(512)\r\n    input_tensor = torch.randn((10, 11, 512))\r\n    output_tensor = m(input_tensor)\r\n    output_tensor.sum().backward()\r\n```\r\n\r\n## Expected behavior\r\n\r\nGet ""RuntimeError: No grad accumulator for a saved leaf!"" in 1.1.0. But it's OK in 1.0.1,or with torch.jit removed in 1.1.0.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 6.5.0-2ubuntu1~18.04) 6.5.0 20181026\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 980 Ti\r\nGPU 1: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 390.116\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy==0.4.3.1\r\n[pip3] numpy==1.16.3\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37he904b0f_5  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.1.0           py3.7_cuda9.0.176_cudnn7.5.1_0    pytorch\r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch","python\r\nimport torch\r\nfrom torch.nn import LayerNorm, Linear\r\nfrom torch.jit import ScriptModule, script_method\r\n\r\nclass Test(ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n        self.layer_norm = LayerNorm(dim)\r\n        self.projection = Linear(dim, dim)\r\n    \r\n    @script_method\r\n    def forward(self, inputs):\r\n        return self.layer_norm(inputs + self.projection(inputs))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    m = Test(512)\r\n    input_tensor = torch.randn((10, 11, 512))\r\n    output_tensor = m(input_tensor)\r\n    output_tensor.sum().backward()\r\n"
20755,Type conversion from float64 to float32 (cpu) sometimes crashes\r\n\r\n[bug.zip](https://github.com/pytorch/pytorch/files/3202664/bug.zip)\r\n,high priority|triaged,colesbury,"```python\r\nimport torch\r\n\r\na = torch.rand(3, 3, dtype = torch.float64)\r\nprint(a.dtype, a.device) # torch.float64 cpu\r\nc = a.to(torch.float32)\r\n#works\r\n\r\nb = torch.load('bug.pt')\r\nprint(b.dtype, b.device) # torch.float64 cpu\r\nc = b.to(torch.float32)\r\n# RuntimeError: expected scalar type Float but found Double\r\n\r\nd = b.clone().to(torch.float32)\r\n# works\r\n```\r\n\r\n[bug.zip](https://github.com/pytorch/pytorch/files/3202664/bug.zip)\r\n","python\r\nimport torch\r\n\r\na = torch.rand(3, 3, dtype = torch.float64)\r\nprint(a.dtype, a.device) # torch.float64 cpu\r\nc = a.to(torch.float32)\r\n#works\r\n\r\nb = torch.load('bug.pt')\r\nprint(b.dtype, b.device) # torch.float64 cpu\r\nc = b.to(torch.float32)\r\n# RuntimeError: expected scalar type Float but found Double\r\n\r\nd = b.clone().to(torch.float32)\r\n# works\r\n"
20500,"[jit] RuntimeError: v->type()->isSubtypeOf(elem_type)## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nThis code:\r\n\r\n\r\nyields the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""C:/Development/_PytorchDev/pytorch-box2pix/jit.py"", line 75, in <module>\r\n    test = MyMod2()\r\n  File ""C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py"", line 1047, in init_then_register\r\n    _create_methods_from_stubs(self, methods)\r\n  File ""C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py"", line 1012, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError: v->type()->isSubtypeOf(elem_type) ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1272, please report a bug to PyTorch. (createList at ..\\torch\\csrc\\jit\\ir.cpp:1272)\r\n(no backtrace available)\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould work without a problem.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n\n\ncc @ezyang @gchanan @zou3519 @suo",high priority|oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nThis code:\r\n```python\r\nclass MyMod2(torch.jit.ScriptModule):\r\n    __constants__ = ['mean', 'std']\r\n    def __init__(self):\r\n        super(MyMod2, self).__init__()\r\n\r\n        self.mean = [0.485, 0.456, 0.406]\r\n        self.std = [0.229, 0.224, 0.225]\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        mean = torch.tensor(self.mean)\r\n        std = torch.tensor(self.std)\r\n        return input.sub(mean[:, None, None]).div_(std[:, None, None])\r\n```\r\n\r\nyields the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File ""C:/Development/_PytorchDev/pytorch-box2pix/jit.py"", line 75, in <module>\r\n    test = MyMod2()\r\n  File ""C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py"", line 1047, in init_then_register\r\n    _create_methods_from_stubs(self, methods)\r\n  File ""C:\\Python36\\lib\\site-packages\\torch\\jit\\__init__.py"", line 1012, in _create_methods_from_stubs\r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError: v->type()->isSubtypeOf(elem_type) ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1272, please report a bug to PyTorch. (createList at ..\\torch\\csrc\\jit\\ir.cpp:1272)\r\n(no backtrace available)\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould work without a problem.\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n\n\ncc @ezyang @gchanan @zou3519 @suo","python\r\nclass MyMod2(torch.jit.ScriptModule):\r\n    __constants__ = ['mean', 'std']\r\n    def __init__(self):\r\n        super(MyMod2, self).__init__()\r\n\r\n        self.mean = [0.485, 0.456, 0.406]\r\n        self.std = [0.229, 0.224, 0.225]\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        mean = torch.tensor(self.mean)\r\n        std = torch.tensor(self.std)\r\n        return input.sub(mean[:, None, None]).div_(std[:, None, None])\r\n"
20433,"Dataloader's memory usage keeps increasing during one single epoch.(All codes were tested on Pytorch 1.0.0 and Pytorch 1.0.1. Memory capacity of my machine is `256Gb`. )\r\n\r\n## Description and Reproduction\r\n\r\nHi,\r\n\r\nI create a dataloader to load features from local files by their file paths but find this results in OOM problem even though the code is simple.\r\n\r\nThe dataloader can be simplfied as:\r\n\r\n\r\n\r\nDuring each epoch, the memory usage is about `13GB` at the very beginning and keeps inscreasing and finally up to about `46Gb`, like this:\r\n\r\n![Annotation 2019-05-13 191331](https://user-images.githubusercontent.com/21999660/57630625-89f9af00-75d0-11e9-830b-c8e6ede5b322.png)\r\n\r\n\r\nAlthough it will decrease to `13GB` at the beginning of next epoch, this problem is serious to me because in my real project the `infoset` is about `40Gb` due to the large number of samples and finally leads to `Out of Memory` (OOM) at the end of the first epoch.\r\n\r\n## Expected behavior\r\n\r\nI have found that the problem is caused by the first line of `MyDataset.__getitem__()` : `info = self.infoset[index]`, in the following code, if I remove this line, then memory usage is normal, which is also my expected behavior.\r\n\r\n\r\n\r\nAnd the corresponding mem usage is stable at `13GB`:\r\n\r\n![Annotation 2019-05-13 191509](https://user-images.githubusercontent.com/21999660/57631324-dd203180-75d1-11e9-8386-ea3be4609543.png)\r\n\r\n## More test\r\n\r\nIn the following code, I don't even load features in `__getitem__()` but just read a string of `infoset`, but get the same problem:\r\n\r\n\r\n\r\nMem usage:\r\n\r\n\r\n![Annotation 2019-05-13 191418](https://user-images.githubusercontent.com/21999660/57631896-f37abd00-75d2-11e9-90f5-9139eb4215b8.png)\r\n\r\nAny suggestions or reasons about this problem?\r\n\r\nThanks.",module: dataloader|triaged,VitalyFedyunin,"(All codes were tested on Pytorch 1.0.0 and Pytorch 1.0.1. Memory capacity of my machine is `256Gb`. )\r\n\r\n## Description and Reproduction\r\n\r\nHi,\r\n\r\nI create a dataloader to load features from local files by their file paths but find this results in OOM problem even though the code is simple.\r\n\r\nThe dataloader can be simplfied as:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data as data\r\nimport time\r\n\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset,\r\n    # such as a list of file names or paths.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset is 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n    info = self.infoset[index]  # problem is here\r\n    items = {}\r\n    items['features'] = self.load_feature(info)\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n    return feature\r\n\r\n  def __len__(self):\r\n    return 8000000\r\n\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n  for i, sample in enumerate(dataloader):\r\n    print(i, len(dataloader))\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n```\r\n\r\nDuring each epoch, the memory usage is about `13GB` at the very beginning and keeps inscreasing and finally up to about `46Gb`, like this:\r\n\r\n![Annotation 2019-05-13 191331](https://user-images.githubusercontent.com/21999660/57630625-89f9af00-75d0-11e9-830b-c8e6ede5b322.png)\r\n\r\n\r\nAlthough it will decrease to `13GB` at the beginning of next epoch, this problem is serious to me because in my real project the `infoset` is about `40Gb` due to the large number of samples and finally leads to `Out of Memory` (OOM) at the end of the first epoch.\r\n\r\n## Expected behavior\r\n\r\nI have found that the problem is caused by the first line of `MyDataset.__getitem__()` : `info = self.infoset[index]`, in the following code, if I remove this line, then memory usage is normal, which is also my expected behavior.\r\n\r\n```python\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset may be 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n\r\n    # info = self.infoset[index]  # problem is here\r\n    info = 'fake info'\r\n\r\n    items = {}\r\n    items['features'] = self.load_feature(info)\r\n\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n\r\n    return feature\r\n\r\n  def __len__(self):\r\n\r\n    return 8000000\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n\r\n  for i, sample in enumerate(dataloader):\r\n\r\n    print(i, len(dataloader))\r\n\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n```\r\n\r\nAnd the corresponding mem usage is stable at `13GB`:\r\n\r\n![Annotation 2019-05-13 191509](https://user-images.githubusercontent.com/21999660/57631324-dd203180-75d1-11e9-8386-ea3be4609543.png)\r\n\r\n## More test\r\n\r\nIn the following code, I don't even load features in `__getitem__()` but just read a string of `infoset`, but get the same problem:\r\n\r\n```python\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset may be 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n\r\n    info = self.infoset[index]  # problem is here\r\n\r\n    items = {}\r\n    # items['features'] = self.load_feature(info)\r\n\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n\r\n    return feature\r\n\r\n  def __len__(self):\r\n\r\n    return 8000000\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n\r\n  for i, sample in enumerate(dataloader):\r\n\r\n    print(i, len(dataloader))\r\n\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n\r\n```\r\n\r\nMem usage:\r\n\r\n\r\n![Annotation 2019-05-13 191418](https://user-images.githubusercontent.com/21999660/57631896-f37abd00-75d2-11e9-90f5-9139eb4215b8.png)\r\n\r\nAny suggestions or reasons about this problem?\r\n\r\nThanks.","python\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data as data\r\nimport time\r\n\r\nclass MyDataSet(data.Dataset):\r\n\r\n  def __init__(self):\r\n    super(MyDataSet, self).__init__()\r\n\r\n    # Assume that the self.infoset here contains the description information about the dataset,\r\n    # such as a list of file names or paths.\r\n    # Here it is a list of strings. I set it aoubt 8Gb in memory.\r\n    # In my real project, this infoset is 40Gb in memory.\r\n    self.infoset = [str(i).zfill(1024) for i in range(len(self))]\r\n\r\n\r\n  def __getitem__(self, index):\r\n    info = self.infoset[index]  # problem is here\r\n    items = {}\r\n    items['features'] = self.load_feature(info)\r\n    return items\r\n\r\n  def load_feature(self, info):\r\n    '''\r\n    Load feature from files\r\n    '''\r\n    feature = torch.Tensor(np.ones([8, 4, 2], dtype=np.float32))\r\n    return feature\r\n\r\n  def __len__(self):\r\n    return 8000000\r\n\r\n\r\ndataset = MyDataSet()\r\n\r\ndataloader = data.DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=16, pin_memory=True)\r\n\r\nwhile True:\r\n  for i, sample in enumerate(dataloader):\r\n    print(i, len(dataloader))\r\n    time.sleep(0.05) # slow down the process to see the mem-usage increasing during one epoch\r\n"
20421,"The result of  gloo all_gather errorI use gloo for Model Parallelism, when I use all_gather, the result is error.\r\n\r\nThere are two process, I expect the all_gather result is [Tensor1, Tensor2], but the result is actually [Tensor1, Tensor1].\r\nThe Tensor2 is like this\r\n![image](https://user-images.githubusercontent.com/50440190/57607871-949b5080-759e-11e9-9f9e-d39b4faaf1a0.png)\r\nThe result  is like this, The senod Tensor in the result should be equal as the Tensor2\r\n![image](https://user-images.githubusercontent.com/50440190/57608339-581c2480-759f-11e9-9c08-75d657551d9c.png)\r\nBut before all_gather, I use torch.reshape(torch.tensor(range(xxxx), dtype=torch.float32), [16, 16, 16]) to create two Tensor and all_gather. The result is correctly.\r\nThe code is:\r\n\r\n\r\n\r\nEnvironment:\r\n     macos\r\n     pytorch                   1.0.1\r\n     pytorch-cpu               1.1.0\r\n     numpy                     1.16.2\r\n\r\n\r\nPS: We use torch.chunk to split the Tensor and the dim is 0, and all_gather the chunked tensor by gloo, the all_gather  result is error.I think  although I use contiguous() to make memory contiguous, but the it is not effective after I chunk the tensor at dim 0.\r\n\r\n\r\n",oncall: distributed|triaged,mrshenli,"I use gloo for Model Parallelism, when I use all_gather, the result is error.\r\n\r\nThere are two process, I expect the all_gather result is [Tensor1, Tensor2], but the result is actually [Tensor1, Tensor1].\r\nThe Tensor2 is like this\r\n![image](https://user-images.githubusercontent.com/50440190/57607871-949b5080-759e-11e9-9f9e-d39b4faaf1a0.png)\r\nThe result  is like this, The senod Tensor in the result should be equal as the Tensor2\r\n![image](https://user-images.githubusercontent.com/50440190/57608339-581c2480-759f-11e9-9c08-75d657551d9c.png)\r\nBut before all_gather, I use torch.reshape(torch.tensor(range(xxxx), dtype=torch.float32), [16, 16, 16]) to create two Tensor and all_gather. The result is correctly.\r\nThe code is:\r\n\r\n```python\r\nfor _ in range(stage.get_devices_num()):\r\n        gather_tensor.append(torch.zeros_like(in_slice))\r\ndist.all_gather(gather_tensor, in_slice.contiguous(), group=group)\r\n```\r\n\r\nEnvironment:\r\n     macos\r\n     pytorch                   1.0.1\r\n     pytorch-cpu               1.1.0\r\n     numpy                     1.16.2\r\n\r\n\r\nPS: We use torch.chunk to split the Tensor and the dim is 0, and all_gather the chunked tensor by gloo, the all_gather  result is error.I think  although I use contiguous() to make memory contiguous, but the it is not effective after I chunk the tensor at dim 0.\r\n\r\n\r\n","python\r\nfor _ in range(stage.get_devices_num()):\r\n        gather_tensor.append(torch.zeros_like(in_slice))\r\ndist.all_gather(gather_tensor, in_slice.contiguous(), group=group)\r\n"
20335,"cat()/stack() calls in JIT do not accept lists of tensors## \U0001f41b Bug\r\n\r\nWhen including a call to `th.cat(tensors)` in a ScriptModule, if `tensors` is a tuple of tensors the ScriptModule can be constructed, but if `tensors` is a list of tensors, the construction fails.\r\n\r\n## To Reproduce\r\n\r\nRun this short repro example:\r\n\r\n\r\nThe module will fail to construct unless the first cat() call is replaced w/the second.\r\n\r\n## Expected behavior\r\n\r\ncat() / stack() accepts lists or tuples of  #tensors\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7\r\n\r\n## Additional Info\r\n\r\nStack trace of failed construction:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 17, in <module>\r\n    mod = Mod()\r\n  File "".../lib/python3.7/site-packages/torch/jit/__init__.py"", line 1049, in init_then_register                                    \r\n    _create_methods_from_stubs(self, methods)\r\n  File "".../lib/python3.7/site-packages/torch/jit/__init__.py"", line 1014, in _create_methods_from_stubs                            \r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError:\r\narguments for call are not valid:\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\nfor call at:\r\n@th.jit.script_method\r\ndef forward(self):\r\n    x1, x2 = self.fn()\r\n    return th.cat([x1, x2], dim=1) # <---- Fails\r\n           ~~~~~~ <--- HERE\r\n```",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\nWhen including a call to `th.cat(tensors)` in a ScriptModule, if `tensors` is a tuple of tensors the ScriptModule can be constructed, but if `tensors` is a list of tensors, the construction fails.\r\n\r\n## To Reproduce\r\n\r\nRun this short repro example:\r\n```python\r\nimport torch as th\r\n\r\ndef f():\r\n    return th.ones(3), th.ones(3)\r\n\r\nclass Mod(th.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fn = th.jit.trace(f, tuple())\r\n\r\n    @th.jit.script_method\r\n    def forward(self):\r\n        x1, x2 = self.fn()\r\n        return th.cat([x1, x2], dim=1) # <---- Fails\r\n        #return th.cat((x1, x2), dim=1) # <---- Succeeds\r\n\r\nmod = Mod()\r\n```\r\n\r\nThe module will fail to construct unless the first cat() call is replaced w/the second.\r\n\r\n## Expected behavior\r\n\r\ncat() / stack() accepts lists or tuples of  #tensors\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.1\r\n - OS (e.g., Linux): linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7\r\n\r\n## Additional Info\r\n\r\nStack trace of failed construction:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 17, in <module>\r\n    mod = Mod()\r\n  File "".../lib/python3.7/site-packages/torch/jit/__init__.py"", line 1049, in init_then_register                                    \r\n    _create_methods_from_stubs(self, methods)\r\n  File "".../lib/python3.7/site-packages/torch/jit/__init__.py"", line 1014, in _create_methods_from_stubs                            \r\n    self._c._create_methods(self, defs, rcbs, defaults)\r\nRuntimeError:\r\narguments for call are not valid:\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\n\r\n  for operator aten::cat(Tensor[] tensors, int dim=<default>, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor[] for argument 'tensors' but found Tensor[]\r\n  Empty lists default to List[Tensor]. Use torch.jit.annotate(List[my_type], []) to create an empty list of another type                                                                 \r\n  @th.jit.script_method\r\n  def forward(self):\r\n      x1, x2 = self.fn()\r\n      return th.cat([x1, x2], dim=1) # <---- Fails\r\n                    ~~~~~~~ <--- HERE\r\nfor call at:\r\n@th.jit.script_method\r\ndef forward(self):\r\n    x1, x2 = self.fn()\r\n    return th.cat([x1, x2], dim=1) # <---- Fails\r\n           ~~~~~~ <--- HERE\r\n```","python\r\nimport torch as th\r\n\r\ndef f():\r\n    return th.ones(3), th.ones(3)\r\n\r\nclass Mod(th.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fn = th.jit.trace(f, tuple())\r\n\r\n    @th.jit.script_method\r\n    def forward(self):\r\n        x1, x2 = self.fn()\r\n        return th.cat([x1, x2], dim=1) # <---- Fails\r\n        #return th.cat((x1, x2), dim=1) # <---- Succeeds\r\n\r\nmod = Mod()\r\n"
20322,"Segmentation fault occur when using index_copy_## \U0001f41b Bug\r\n\r\nWhen using the index_copy_ function, the programme will throw a segmentation fault.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nRun the code below:\r\n\r\n\r\nI know the code following will work, but it seems that the issue above is an unexpected performent.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl                       2019.1                      144    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] pytorch-cpu               1.0.1               py3.7_cpu_2    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n```\r\n",high priority|module: crash|triaged,izdeby,"## \U0001f41b Bug\r\n\r\nWhen using the index_copy_ function, the programme will throw a segmentation fault.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nRun the code below:\r\n```python\r\nimport torch\r\na = torch.randn(3, 5)\r\nc = torch.zeros(3)\r\na.index_copy_(dim=1, index=torch.tensor([3]), source=c)\r\n```\r\n```bash\r\nMrFive@mrfive-home:~$ python\r\nPython 3.7.2 (default, Dec 29 2018, 06:19:36) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import torch\r\n>>> a = torch.randn(3, 5)\r\n>>> b = torch.zeros(3, 4)\r\n>>> c = torch.zeros(3)\r\n>>> a.index_copy_(dim=1, index=torch.tensor([3]), source=c)\r\nSegmentation fault (core dumped)\r\n```\r\nI know the code following will work, but it seems that the issue above is an unexpected performent.\r\n```python\r\na.index_copy_(dim=1, index=torch.tensor([3]), source=c.unsqueeze(1))\r\n```\r\n## Environment\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl                       2019.1                      144    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\n[conda] pytorch-cpu               1.0.1               py3.7_cpu_2    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n```\r\n","python\r\nimport torch\r\na = torch.randn(3, 5)\r\nc = torch.zeros(3)\r\na.index_copy_(dim=1, index=torch.tensor([3]), source=c)\r\n"
20273,"Linker errors when linking statically (avx perfkernels) [Caffe2]## \U0001f41b Bug\r\n\r\nI've been trying to build the caffe2 v1.1.0 release and link it statically with our application, but I'm getting a lot of undefined symbols related to avx functions.\r\n\r\n## To Reproduce\r\n\r\nI've built caffe2 142c973f4179e768164cd578951489e89021b29c using the following flags:\r\n\r\nMinimal application code:\r\n\r\nBuild flags:\r\n\r\nOutput:\r\n```\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_false(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2caa): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_false__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_false(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2e3a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_false__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_false(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2fca): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_false(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x315a): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x32ec): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x347c): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_true(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x360a): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_true__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_true(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x379a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_true__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_true(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x392a): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_true(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3aba): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3c4c): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3ddc): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x858): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x958): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<c10::Half, float>(int, float, c10::Half const*, float*)':\r\ntyped_axpy.cc:(.text+0x10d): undefined reference to `caffe2::TypedAxpyHalffloat__avx2_fma(int, float, c10::Half const*, float*)'\r\ntyped_axpy.cc:(.text+0x131): undefined reference to `caffe2::TypedAxpyHalffloat__avx_f16c(int, float, c10::Half const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<unsigned char, float>(int, float, unsigned char const*, float*)':\r\ntyped_axpy.cc:(.text+0x221): undefined reference to `caffe2::TypedAxpy_uint8_float__avx2_fma(int, float, unsigned char const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update(int, float const*, float const*, float const*, float*, float*, float, float, float)':\r\nadagrad.cc:(.text+0x621): undefined reference to `caffe2::adagrad_update__avx_f16c(int, float const*, float const*, float const*, float*, float*, float, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update_prefetch(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x789): undefined reference to `caffe2::adagrad_update_prefetch__avx_f16c(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_fp16_update_prefetch(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)':\r\nadagrad.cc:(.text+0xac9): undefined reference to `caffe2::adagrad_fp16_update_prefetch__avx_f16c(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::rowwise_adagrad_update(int, float*, float*, float const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xd36): undefined reference to `caffe2::rowwise_adagrad_update__avx_f16c(int, float*, float*, float const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<int>(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xfd6): undefined reference to `caffe2::sparse_adagrad_int32_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<long>(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x1276): undefined reference to `caffe2::sparse_adagrad_int64_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::quantize_and_compress(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)':\r\nmath_cpu_base.cc:(.text+0x31d): undefined reference to `caffe2::math::quantize_and_compress__avx2(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::decompress_and_dequantize(unsigned char const*, float*, unsigned long)':\r\nmath_cpu_base.cc:(.text+0x455): undefined reference to `caffe2::math::decompress_and_dequantize__avx2(unsigned char const*, float*, unsigned long)'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n## Environment\r\n\r\n - Caffe2 version: 142c973f4179e768164cd578951489e89021b29c (v1.1.0)\r\n - OS: Ubuntu 18.04 x86_64\r\n - Build command you used (if compiling from source): cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n\r\n",caffe2,ezyang,"## \U0001f41b Bug\r\n\r\nI've been trying to build the caffe2 v1.1.0 release and link it statically with our application, but I'm getting a lot of undefined symbols related to avx functions.\r\n\r\n## To Reproduce\r\n\r\nI've built caffe2 142c973f4179e768164cd578951489e89021b29c using the following flags:\r\n```bash\r\n$ cd pytorch\r\n$ mkdir build\r\n$ cd build\r\n$ cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n```\r\nMinimal application code:\r\n```c++\r\n#include <caffe2/core/init.h>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    caffe2::GlobalInit();\r\n    return 0;\r\n}\r\n```\r\nBuild flags:\r\n```bash\r\n$ g++ main.cpp -I /home/giachero/runtimes/caffe2-v1.1.0-avx/include/ -L /home/giachero/runtimes/caffe2-v1.1.0-avx/lib/ -Wl,--whole-archive -lcaffe2 -Wl,--no-whole-archive -lcaffe2_protos -lcpuinfo -lc10 -lclog -lonnx -lprotobuf -lonnx_proto -lonnxifi_loader -lm -lpthread -ldl -o test\r\n```\r\nOutput:\r\n```\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_false(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2caa): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_false__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_false(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2e3a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_false__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_false(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x2fca): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_false(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x315a): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_false__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x32ec): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x347c): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_float_float_true(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x360a): undefined reference to `caffe2::EmbeddingLookup_int32_t_float_float_true__avx2_fma(long, long, long, long, float const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_float_float_true(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x379a): undefined reference to `caffe2::EmbeddingLookup_int64_t_float_float_true__avx2_fma(long, long, long, long, float const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_half_float_true(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x392a): undefined reference to `caffe2::EmbeddingLookup_int32_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_half_float_true(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3aba): undefined reference to `caffe2::EmbeddingLookup_int64_t_half_float_true__avx2_fma(long, long, long, long, c10::Half const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3c4c): undefined reference to `caffe2::EmbeddingLookup_int32_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(embedding_lookup.cc.o): In function `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)':\r\nembedding_lookup.cc:(.text+0x3ddc): undefined reference to `caffe2::EmbeddingLookup_int64_t_uint8_t_float_true__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x858): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int64_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, long const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(fused_8bit_rowwise_embedding_lookup.cc.o): In function `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)':\r\nfused_8bit_rowwise_embedding_lookup.cc:(.text+0x958): undefined reference to `caffe2::Fused8BitRowwiseEmbeddingLookup_int32_t_uint8_t_float_false__avx2_fma(long, long, long, long, unsigned char const*, int const*, int const*, float const*, bool, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<c10::Half, float>(int, float, c10::Half const*, float*)':\r\ntyped_axpy.cc:(.text+0x10d): undefined reference to `caffe2::TypedAxpyHalffloat__avx2_fma(int, float, c10::Half const*, float*)'\r\ntyped_axpy.cc:(.text+0x131): undefined reference to `caffe2::TypedAxpyHalffloat__avx_f16c(int, float, c10::Half const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(typed_axpy.cc.o): In function `void caffe2::TypedAxpy<unsigned char, float>(int, float, unsigned char const*, float*)':\r\ntyped_axpy.cc:(.text+0x221): undefined reference to `caffe2::TypedAxpy_uint8_float__avx2_fma(int, float, unsigned char const*, float*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update(int, float const*, float const*, float const*, float*, float*, float, float, float)':\r\nadagrad.cc:(.text+0x621): undefined reference to `caffe2::adagrad_update__avx_f16c(int, float const*, float const*, float const*, float*, float*, float, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_update_prefetch(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x789): undefined reference to `caffe2::adagrad_update_prefetch__avx_f16c(int, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::adagrad_fp16_update_prefetch(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)':\r\nadagrad.cc:(.text+0xac9): undefined reference to `caffe2::adagrad_fp16_update_prefetch__avx_f16c(int, c10::Half const*, c10::Half const*, float const*, c10::Half const*, c10::Half const*, c10::Half*, c10::Half*, c10::Half*, c10::Half*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `caffe2::rowwise_adagrad_update(int, float*, float*, float const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xd36): undefined reference to `caffe2::rowwise_adagrad_update__avx_f16c(int, float*, float*, float const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<int>(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0xfd6): undefined reference to `caffe2::sparse_adagrad_int32_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, int const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(adagrad.cc.o): In function `int caffe2::sparse_adagrad<long>(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)':\r\nadagrad.cc:(.text+0x1276): undefined reference to `caffe2::sparse_adagrad_int64_t__avx_f16c(int, int, unsigned long, float const*, float const*, float const*, long const*, float*, float*, float, float)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::quantize_and_compress(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)':\r\nmath_cpu_base.cc:(.text+0x31d): undefined reference to `caffe2::math::quantize_and_compress__avx2(float const*, unsigned char*, unsigned long, unsigned long, bool, float const*)'\r\n/home/giachero/runtimes/caffe2-v1.1.0-avx/lib//libcaffe2.a(math_cpu_base.cc.o): In function `caffe2::math::decompress_and_dequantize(unsigned char const*, float*, unsigned long)':\r\nmath_cpu_base.cc:(.text+0x455): undefined reference to `caffe2::math::decompress_and_dequantize__avx2(unsigned char const*, float*, unsigned long)'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n## Environment\r\n\r\n - Caffe2 version: 142c973f4179e768164cd578951489e89021b29c (v1.1.0)\r\n - OS: Ubuntu 18.04 x86_64\r\n - Build command you used (if compiling from source): cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n\r\n",bash\r\n$ cd pytorch\r\n$ mkdir build\r\n$ cd build\r\n$ cmake .. -DCMAKE_INSTALL_PREFIX=/home/giachero/runtimes/caffe2-v1.1.0-avx -DBUILD_PYTHON=OFF -DUSE_CUDA=OFF -DUSE_NATIVE_ARCH=ON -DUSE_GFLAGS=OFF -DUSE_GLOG=OFF -DUSE_GLOO=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TEST=OFF -DBUILD_BINARY=OFF -DUSE_LMDB=OFF -DUSE_LEVELDB=OFF -DUSE_MPI=OFF -DUSE_OPENMP=OFF -DUSE_OPENCV=OFF -DBUILD_ATEN_MOBILE=ON -DUSE_NNPACK=OFF -DCAFFE2_DISABLE_NUMA=1 -DUSE_QNNPACK=OFF\r\n
20247,"Problem running custom operator test, pytorch 1.1.0## \U0001f41b Bug\r\n\r\nI cannot save and then load a custom operator, using the test code available in pytorch/test/custom_operator in Pytorch 1.1.0.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n## Expected behavior\r\n\r\npass AT_ASSERT(module != nullptr); in load_serialized_module_with_custom_op_and_execute\r\n\r\n## output\r\n\r\nI get the following error:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7ff8121cd441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7ff8121ccd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x9728f2 (0x7ff855c088f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ff855c05f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0xa6f91b (0x7ff855d0591b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ff855d06f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ff855d07088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in ./test_custom_ops)\r\nframe #8: main + 0x85 (0x445787 in ./test_custom_ops)\r\nframe #9: __libc_start_main + 0xf0 (0x7ff811862830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x442969 in ./test_custom_ops)\r\n```\r\nAborted (core dumped)\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n - OS: Linux 16.04\r\n - both with torch installation via pip and using pytorch dockerfile\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 10.0, 7.5\r\n - GPU models and configuration: GV100 && 2080\r\n",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\nI cannot save and then load a custom operator, using the test code available in pytorch/test/custom_operator in Pytorch 1.1.0.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```bash\r\ncd pytorch/test/custom_operator\r\n# Change save path of the model (like model.save in test_custom_ops.py) to current folder\r\npython3 test_custom_ops.py # generates a something.pt file\r\nmkdir build && cd build && cmake -DCMAKE_PRIFIX_PATH=/path/to/libtorch/ .. && make \r\n./test_custom_ops something.pt\r\n```\r\n## Expected behavior\r\n\r\npass AT_ASSERT(module != nullptr); in load_serialized_module_with_custom_op_and_execute\r\n\r\n## output\r\n\r\nI get the following error:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7ff8121cd441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7ff8121ccd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x9728f2 (0x7ff855c088f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ff855c05f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0xa6f91b (0x7ff855d0591b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ff855d06f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ff855d07088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\r\nframe #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in ./test_custom_ops)\r\nframe #8: main + 0x85 (0x445787 in ./test_custom_ops)\r\nframe #9: __libc_start_main + 0xf0 (0x7ff811862830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x442969 in ./test_custom_ops)\r\n```\r\nAborted (core dumped)\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version: 1.1.0\r\n - OS: Linux 16.04\r\n - both with torch installation via pip and using pytorch dockerfile\r\n - Python version: 3.5\r\n - CUDA/cuDNN version: 10.0, 7.5\r\n - GPU models and configuration: GV100 && 2080\r\n",bash\r\ncd pytorch/test/custom_operator\r\n# Change save path of the model (like model.save in test_custom_ops.py) to current folder\r\npython3 test_custom_ops.py # generates a something.pt file\r\nmkdir build && cd build && cmake -DCMAKE_PRIFIX_PATH=/path/to/libtorch/ .. && make \r\n./test_custom_ops something.pt\r\n
20239,[jit] add support for tuple unpacking in for loops## \U0001f680 Feature\r\nAdd support for tuple unpacking in for loops\r\n\r\n## Motivation\r\n\r\nMakes working with tuples a better experience and the code shorter. \r\n\r\n## Pitch\r\n\r\nThis should be possible:\r\n\r\n\r\n\r\n## Alternatives\r\n\r\nA current workaround is to use:\r\n\r\n\r\n,oncall: jit|triaged,Krovatkin,"## \U0001f680 Feature\r\nAdd support for tuple unpacking in for loops\r\n\r\n## Motivation\r\n\r\nMakes working with tuples a better experience and the code shorter. \r\n\r\n## Pitch\r\n\r\nThis should be possible:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  fm = [(64, 32), (32, 16), (16, 8)]\r\n  \r\n  for width, height in fm:\r\n    pass\r\n```\r\n\r\n## Alternatives\r\n\r\nA current workaround is to use:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  fm = [(64, 32), (32, 16), (16, 8)]\r\n  \r\n  for f in fm:\r\n\twidth, height = f\r\n    pass\r\n```\r\n","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  fm = [(64, 32), (32, 16), (16, 8)]\r\n  \r\n  for width, height in fm:\r\n    pass\r\n"
20230,"[jit] torch.tensor doesn't support list of tuples## \U0001f41b Bug\r\n\r\nCannot create a tensor using `torch.tensor` from a list of tuples.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\ngives the following error:\r\n```\r\nRuntimeError: \r\nInput list to torch.tensor must be of ints, floats, or bools, got (int, int, int, int):\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould not give an error. Works in pure python.\r\n\r\n## Environment\r\nPytorch 1.1.0\r\n\r\n## Additional context\r\n\r\nA workaround is to use a list of lists instead.\r\n",oncall: jit|low priority|triaged,eellison,"## \U0001f41b Bug\r\n\r\nCannot create a tensor using `torch.tensor` from a list of tuples.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n```\r\n\r\ngives the following error:\r\n```\r\nRuntimeError: \r\nInput list to torch.tensor must be of ints, floats, or bools, got (int, int, int, int):\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n         ~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould not give an error. Works in pure python.\r\n\r\n## Environment\r\nPytorch 1.1.0\r\n\r\n## Additional context\r\n\r\nA workaround is to use a list of lists instead.\r\n","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test():\r\n  li = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)]\r\n  return torch.tensor(li)\r\n"
20169,"[extension-cpp] extra_compile_args inplace operations## \U0001f41b Bug\r\n\r\nProblem occurs when building multiple extension, and giving the same dictionnary for `extra_compile_args`\r\n\r\n## To Reproduce\r\n\r\nVery easy to reproduce with https://github.com/pytorch/extension-cpp/tree/master/cuda\r\n\r\nchange the setup.py to have a second fake extension and add extra_compile_args.\r\nthis version of `setup.py` won't work, but the second proposed version will:\r\n\r\n\r\nThis outputs (although it compiles)\r\n```\r\n<command-line>:0:0: warning: ""TORCH_EXTENSION_NAME"" redefined\r\n<command-line>:0:0: note: this is the location of the previous definition\r\n```\r\nand importing first module gets the error\r\n```\r\nIn [1]: import lltm_cuda                                                                                                                                                                 \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-15b747285754> in <module>\r\n----> 1 import lltm_cuda\r\n\r\nImportError: dynamic module does not define module export function (PyInit_lltm_cuda)\r\n```\r\n-------\r\n\r\nThis works fine and both modules can be imported.\r\n\r\n## Expected behavior\r\n\r\nSame behaviour between the two versions of the install script.\r\n\r\n## Additional context\r\n\r\nThe problem comes from the fact that a simple shallow copy of dictionnary is not enough, since compile args are nested into two lists, one for C++ and one for cuda.\r\nRecommended actions should be either change this line \r\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L373\r\nto make a `copy.deepcopy` instead of a simple `copy.copy`, or change this line https://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L376 to make a second `copy.copy` .\r\n\r\nFirst solution is less verbose but might be less secure, since we can't control the depth of the copy. I'm personally more inclined to first solution as I don't see a way to unintentionally copy too much with a simple `extra_compile_args` dictionnary.",module: cpp-extensions|triaged,ClementPinard,"## \U0001f41b Bug\r\n\r\nProblem occurs when building multiple extension, and giving the same dictionnary for `extra_compile_args`\r\n\r\n## To Reproduce\r\n\r\nVery easy to reproduce with https://github.com/pytorch/extension-cpp/tree/master/cuda\r\n\r\nchange the setup.py to have a second fake extension and add extra_compile_args.\r\nthis version of `setup.py` won't work, but the second proposed version will:\r\n\r\n```python\r\nargs = {'nvcc':[], 'cxx':[]}\r\n\r\nsetup(\r\n    name='lltm_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('lltm_cuda', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n        CUDAExtension('lltm_cuda2', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n```\r\nThis outputs (although it compiles)\r\n```\r\n<command-line>:0:0: warning: ""TORCH_EXTENSION_NAME"" redefined\r\n<command-line>:0:0: note: this is the location of the previous definition\r\n```\r\nand importing first module gets the error\r\n```\r\nIn [1]: import lltm_cuda                                                                                                                                                                 \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-15b747285754> in <module>\r\n----> 1 import lltm_cuda\r\n\r\nImportError: dynamic module does not define module export function (PyInit_lltm_cuda)\r\n```\r\n-------\r\n```python\r\nargs = {'nvcc':[], 'cxx':[]}\r\n\r\nsetup(\r\n    name='lltm_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('lltm_cuda', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n        CUDAExtension('lltm_cuda2', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=copy.deepcopy(args)),\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n```\r\nThis works fine and both modules can be imported.\r\n\r\n## Expected behavior\r\n\r\nSame behaviour between the two versions of the install script.\r\n\r\n## Additional context\r\n\r\nThe problem comes from the fact that a simple shallow copy of dictionnary is not enough, since compile args are nested into two lists, one for C++ and one for cuda.\r\nRecommended actions should be either change this line \r\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L373\r\nto make a `copy.deepcopy` instead of a simple `copy.copy`, or change this line https://github.com/pytorch/pytorch/blob/master/torch/utils/cpp_extension.py#L376 to make a second `copy.copy` .\r\n\r\nFirst solution is less verbose but might be less secure, since we can't control the depth of the copy. I'm personally more inclined to first solution as I don't see a way to unintentionally copy too much with a simple `extra_compile_args` dictionnary.","python\r\nargs = {'nvcc':[], 'cxx':[]}\r\n\r\nsetup(\r\n    name='lltm_cuda',\r\n    ext_modules=[\r\n        CUDAExtension('lltm_cuda', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n        CUDAExtension('lltm_cuda2', [\r\n            'lltm_cuda.cpp',\r\n            'lltm_cuda_kernel.cu',\r\n        ], extra_compile_args=args),\r\n    ],\r\n    cmdclass={\r\n        'build_ext': BuildExtension\r\n    })\r\n"
20146,"CPU Memory leak when using weight_decay in libtorch## Bug\r\nWhen using weight_decay in libtorch, CPU Memory usage is slowly increasing.\r\n\r\n## To Reproduce\r\nI used docker container ""nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04"", stable libtorch(1.1) for cuda9.0. This phenomenon can be reproduced using the mnist example in the pytorch/example repository.\r\n\r\nrewrite examples/cpp/mnist/mnist.cpp l.148\r\n\r\n\r\n\r\nBecause the speed of increase is very slow, it may be better to increase the number of epochs. This happens with both CPU learning and GPU learning.\r\n\r\n## Environment\r\n- OS: Ubuntu 18.04.2 LTS\r\n- GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CMake version: version 3.10.2\r\n- CUDA runtime version: 9.2.148\r\n- GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\n- Nvidia driver version: 410.104\r\n- cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\n## Additional context\r\nThis phenomenon happens also in cuda10.0 and libtorch nightly build for cuda10.0.",high priority|module: cpp|triaged,yf225,"## Bug\r\nWhen using weight_decay in libtorch, CPU Memory usage is slowly increasing.\r\n\r\n## To Reproduce\r\nI used docker container ""nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04"", stable libtorch(1.1) for cuda9.0. This phenomenon can be reproduced using the mnist example in the pytorch/example repository.\r\n\r\nrewrite examples/cpp/mnist/mnist.cpp l.148\r\n\r\n```cpp\r\n- model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5));\r\n+ model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5).weight_decay(1e-4));\r\n```\r\n\r\nBecause the speed of increase is very slow, it may be better to increase the number of epochs. This happens with both CPU learning and GPU learning.\r\n\r\n## Environment\r\n- OS: Ubuntu 18.04.2 LTS\r\n- GCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CMake version: version 3.10.2\r\n- CUDA runtime version: 9.2.148\r\n- GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\r\n- Nvidia driver version: 410.104\r\n- cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\n## Additional context\r\nThis phenomenon happens also in cuda10.0 and libtorch nightly build for cuda10.0.","cpp\r\n- model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5));\r\n+ model.parameters(), torch::optim::SGDOptions(0.01).momentum(0.5).weight_decay(1e-4));\r\n"
20113,"[JIT][script] JIT for torch.norm ignores argument defaults## \U0001f41b Bug\r\n\r\ntorch.norm `p` argument default is ignored by JIT\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n\r\n\r\nfails with:\r\n\r\n> RuntimeError:\r\n> arguments for call are not valid:\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar p=<default>) -> Tensor:\r\n>   keyword argument dim unknown\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar? p, *, int dtype) -> Tensor:\r\n>   argument p not provided.\r\n>   @torch.jit.script\r\n>   def norm_test():\r\n>     t = torch.ones(10, 5)\r\n>     return torch.norm(t, dim=1, keepdim=True)\r\n>            ~~~~~~~~~~ <--- HERE\r\n\r\n2.\r\n\r\nWorks fine\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: version 3.9.0-rc5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 419.67\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.3\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-ignite==0.1.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda91                    1.0                           0    pytorch\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda90_cudnn7_1    pytorch\r\n[conda] pytorch-ignite            0.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    pypi_0    pypi\r\n\n\ncc @suo",oncall: jit|triaged|jit-backlog,eellison,"## \U0001f41b Bug\r\n\r\ntorch.norm `p` argument default is ignored by JIT\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef norm_test():\r\n  t = torch.ones(10, 5)\r\n  return torch.norm(t, dim=1, keepdim=True)\r\n```\r\n\r\nfails with:\r\n\r\n> RuntimeError:\r\n> arguments for call are not valid:\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar p=<default>) -> Tensor:\r\n>   keyword argument dim unknown\r\n>   \r\n>   for operator aten::norm(Tensor self, Scalar? p, *, int dtype) -> Tensor:\r\n>   argument p not provided.\r\n>   @torch.jit.script\r\n>   def norm_test():\r\n>     t = torch.ones(10, 5)\r\n>     return torch.norm(t, dim=1, keepdim=True)\r\n>            ~~~~~~~~~~ <--- HERE\r\n\r\n2.\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef norm_test():\r\n  t = torch.ones(10, 5)\r\n  return torch.norm(t, p=2, dim=1)\r\n```\r\nWorks fine\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: Could not collect\r\nCMake version: version 3.9.0-rc5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 419.67\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.14.3\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-ignite==0.1.0\r\n[pip] torch==1.1.0\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda91                    1.0                           0    pytorch\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n[conda] pytorch                   1.1.0           py3.6_cuda90_cudnn7_1    pytorch\r\n[conda] pytorch-ignite            0.1.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    pypi_0    pypi\r\n\n\ncc @suo","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef norm_test():\r\n  t = torch.ones(10, 5)\r\n  return torch.norm(t, dim=1, keepdim=True)\r\n"
20101,jit tracing error for nn.Sequential with nn.Conv2d in torch 1.1.0 ## \U0001f41b Bug\r\n\r\n \r\nwhen tracing nn.Sequential with nn.Conv2d in torch 1.1.0 \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nRaises the following error \r\n\r\n\r\n## Expected behavior\r\n\r\nExpected to convert without issues\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\ncuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpy-image-widget==2019.1.6\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n,high priority|oncall: jit|triaged,Krovatkin,"## \U0001f41b Bug\r\n\r\n```python\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\n``` \r\nwhen tracing nn.Sequential with nn.Conv2d in torch 1.1.0 \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nfrom torch import nn\r\nimport torch.jit\r\n\r\nmodel = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n\r\ntorch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n```\r\n\r\nRaises the following error \r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-5e9a2f5de8a5> in <module>\r\n      4 model = nn.Sequential(nn.Conv2d(2, 2, 1, 1, 1))\r\n      5 \r\n----> 6 torch.jit.trace(model.forward, torch.randn(1, 1, 2, 2))\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class)\r\n    693         traced = torch._C._create_function_from_trace(name, func, example_inputs,\r\n    694                                                       var_lookup_fn,\r\n--> 695                                                       _force_outplace)\r\n    696 \r\n    697     # Check the trace against new traces created from user-specified inputs\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/container.py in forward(self, input)\r\n     90     def forward(self, input):\r\n     91         for module in self._modules.values():\r\n---> 92             input = module(input)\r\n     93         return input\r\n     94 \r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    489             hook(self, input)\r\n    490         if torch._C._get_tracing_state():\r\n--> 491             result = self._slow_forward(*input, **kwargs)\r\n    492         else:\r\n    493             result = self.forward(*input, **kwargs)\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    479         tracing_state._traced_module_stack.append(self)\r\n    480         try:\r\n--> 481             result = self.forward(*input, **kwargs)\r\n    482         finally:\r\n    483             tracing_state.pop_scope()\r\n\r\n~/.virtualenvs/test/lib/python3.6/site-packages/torch/nn/modules/conv.py in forward(self, input)\r\n    336                             _pair(0), self.dilation, self.groups)\r\n    337         return F.conv2d(input, self.weight, self.bias, self.stride,\r\n--> 338                         self.padding, self.dilation, self.groups)\r\n    339 \r\n    340 \r\n\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\nTensor:\r\n(1,1,.,.) = \r\n  0.3550\r\n\r\n(2,1,.,.) = \r\n 0.01 *\r\n  9.7722\r\n\r\n(1,2,.,.) = \r\n -0.5052\r\n\r\n(2,2,.,.) = \r\n  0.5900\r\n[ Variable[CPUType]{2,2,1,1} ]\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpected to convert without issues\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\ncuDNN version: /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpy-image-widget==2019.1.6\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n","python\r\nRuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient\r\n"
20090,"How to add dynamically allocated strings to Pickler?The following code prints `111` and `111`, instead of `222` and `111`, because `222` is skipped [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L68). Is this by design as Pickler only works for statically allocated strings? Or is there a way to correctly add dynamically allocated strings? (and all other types listed [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L104-L114))? \r\n\r\n\r\n\r\ncc @zdevito ",oncall: jit|triaged,driazati,"The following code prints `111` and `111`, instead of `222` and `111`, because `222` is skipped [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L68). Is this by design as Pickler only works for statically allocated strings? Or is there a way to correctly add dynamically allocated strings? (and all other types listed [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/pickler.cpp#L104-L114))? \r\n\r\n```c++\r\n  std::string str1 = ""111"";\r\n  std::string str2 = ""222"";\r\n\r\n  std::vector<at::Tensor> tensor_table;\r\n  torch::jit::Pickler pickler(&tensor_table);\r\n  pickler.start();\r\n  pickler.addIValue(str1);\r\n  pickler.addIValue(str2);\r\n  pickler.finish();\r\n\r\n  auto buffer = new char[pickler.stack().size()];\r\n  memcpy(buffer, pickler.stack().data(), pickler.stack().size());\r\n\r\n  torch::jit::Unpickler unpickler(buffer, pickler.stack().size(), &tensor_table);\r\n  auto values = unpickler.parse_ivalue_list();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n```\r\n\r\ncc @zdevito ","c++\r\n  std::string str1 = ""111"";\r\n  std::string str2 = ""222"";\r\n\r\n  std::vector<at::Tensor> tensor_table;\r\n  torch::jit::Pickler pickler(&tensor_table);\r\n  pickler.start();\r\n  pickler.addIValue(str1);\r\n  pickler.addIValue(str2);\r\n  pickler.finish();\r\n\r\n  auto buffer = new char[pickler.stack().size()];\r\n  memcpy(buffer, pickler.stack().data(), pickler.stack().size());\r\n\r\n  torch::jit::Unpickler unpickler(buffer, pickler.stack().size(), &tensor_table);\r\n  auto values = unpickler.parse_ivalue_list();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n  std::cout << values.back().toStringRef() << std::endl;\r\n  values.pop_back();\r\n"
20060,"[JIT] LSTMCell still not speedup with 1.1.0I've tried the [template provided in fastrnn](https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py#L94-L120) with latest PyTorch, however, it seems the speedup still not there yet. Here is the benchmark code\r\n\r\n\r\n\r\n```\r\n - PyTorch Version: 1.1.0\r\n - OS: Ubuntu 18.04\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7\r\n```",oncall: jit|triaged,wanchaol,"I've tried the [template provided in fastrnn](https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py#L94-L120) with latest PyTorch, however, it seems the speedup still not there yet. Here is the benchmark code\r\n\r\n```python\r\nfrom torch import jit\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ntorch.manual_seed(0)\r\n\r\nclass LSTMCell(jit.ScriptModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(LSTMCell, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\r\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\r\n        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\r\n        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\r\n\r\n    @jit.script_method\r\n    def forward(self, input, state):\r\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\r\n        hx, cx = state\r\n        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +\r\n                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\r\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\r\n\r\n        ingate = torch.sigmoid(ingate)\r\n        forgetgate = torch.sigmoid(forgetgate)\r\n        cellgate = torch.tanh(cellgate)\r\n        outgate = torch.sigmoid(outgate)\r\n\r\n        cy = (forgetgate * cx) + (ingate * cellgate)\r\n        hy = outgate * torch.tanh(cy)\r\n\r\n        return hy, (hy, cy)\r\n    \r\nimport time\r\n\r\nx = torch.randn(64, 512)\r\nh = torch.zeros(64, 512)\r\nc = torch.zeros_like(h)\r\n\r\nrnn1 = LSTMCell(512, 512)\r\nrnn2 = nn.LSTMCell(512, 512)\r\n\r\nt = time.perf_counter()\r\nrnn1(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\nt = time.perf_counter()\r\nrnn2(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\n>>> 0.06589111499488354\r\n>>> 0.006260981783270836\r\n```\r\n\r\n```\r\n - PyTorch Version: 1.1.0\r\n - OS: Ubuntu 18.04\r\n - How you installed PyTorch: pip\r\n - Python version: 3.7\r\n```","python\r\nfrom torch import jit\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\ntorch.manual_seed(0)\r\n\r\nclass LSTMCell(jit.ScriptModule):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(LSTMCell, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\r\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\r\n        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\r\n        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\r\n\r\n    @jit.script_method\r\n    def forward(self, input, state):\r\n        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\r\n        hx, cx = state\r\n        gates = (torch.mm(input, self.weight_ih.t()) + self.bias_ih +\r\n                 torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\r\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\r\n\r\n        ingate = torch.sigmoid(ingate)\r\n        forgetgate = torch.sigmoid(forgetgate)\r\n        cellgate = torch.tanh(cellgate)\r\n        outgate = torch.sigmoid(outgate)\r\n\r\n        cy = (forgetgate * cx) + (ingate * cellgate)\r\n        hy = outgate * torch.tanh(cy)\r\n\r\n        return hy, (hy, cy)\r\n    \r\nimport time\r\n\r\nx = torch.randn(64, 512)\r\nh = torch.zeros(64, 512)\r\nc = torch.zeros_like(h)\r\n\r\nrnn1 = LSTMCell(512, 512)\r\nrnn2 = nn.LSTMCell(512, 512)\r\n\r\nt = time.perf_counter()\r\nrnn1(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\nt = time.perf_counter()\r\nrnn2(x, (h, c))\r\nprint(time.perf_counter() - t)\r\n\r\n>>> 0.06589111499488354\r\n>>> 0.006260981783270836\r\n"
20052,"SegFault and other errors on instantiating subclass of torch.FloatTensor and torch.Tensor## \U0001f41b Bug\r\n\r\nInstantiating a subclass of `torch.FloatTensor`, `torch.ByteTensor`, or `torch.BoolTensor` causes a **segmentation fault**, a `c10::Error: Unrecognized Scalartype UNKNOWN_SCALAR`, and/or other odd errors.\r\n\r\nInstantiating a subclass of `torch.Tensor` with parameters and overriding `__init__` fails with a `TypeError`.\r\n\r\n## To Reproduce\r\n\r\nRun the following 4 lines on a fresh python REPL (or some variant):\r\n\r\n\r\nWith python2.7 and pytorch 1.1.0, I get\r\n`Segmentation fault (core dumped)`\r\n\r\n\r\nWith python3.5.2 and pytorch 1.1.0, it's a bit less predictable.\r\n\r\nSometimes I get a `RuntimeError`:\r\n\r\n\r\nSometimes, I get a segmentation fault:\r\n```Segmentation fault (core dumped)```\r\n\r\nSometimes I get a `c10::Error`:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Unrecognized Scalartype UNKNOWN_SCALAR (please report this error) (scalarTypeToTypeMeta at /pytorch/c10/core/ScalarType.h:136)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fb12efc9441 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fb12efc8d7a in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x2fb757 (0x7fb16e493757 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x3caee6 (0x7fb16e562ee6 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: torch::utils::legacy_tensor_ctor(at::Type const&, c10::ScalarType, _object*, _object*) + 0x210 (0x7fb16e6c0000 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x510d8b (0x7fb16e6a8d8b in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: /usr/people/myusername/python3env/bin/python3.5() [0x57efe5]\r\nframe #7: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #8: PyEval_EvalFrameEx + 0x4ec6 (0x53bba6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #9: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #10: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #11: /usr/people/myusername/python3env/bin/python3.5() [0x54a7c5]\r\nframe #12: PyCFunction_Call + 0x4f (0x4e9b7f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #13: PyEval_EvalFrameEx + 0x614 (0x5372f4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #14: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #15: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #16: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #17: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #18: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #19: PyEval_EvalFrameEx + 0x4ce6 (0x53b9c6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #20: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #21: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #22: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #23: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #24: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #25: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #26: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #27: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #28: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #29: PyEval_EvalCodeEx + 0x13b (0x540f9b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #30: /usr/people/myusername/python3env/bin/python3.5() [0x4ebe37]\r\nframe #31: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #32: PyEval_EvalFrameEx + 0x252b (0x53920b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #33: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #34: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #35: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #36: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #37: /usr/people/myusername/python3env/bin/python3.5() [0x60c272]\r\nframe #38: PyRun_FileExFlags + 0x9a (0x60e71a in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #39: PyRun_SimpleFileExFlags + 0x1bc (0x60ef0c in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #40: Py_Main + 0x456 (0x63fb26 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #41: main + 0xe1 (0x4cfeb1 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #42: __libc_start_main + 0xf0 (0x7fb1e860b830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #43: _start + 0x29 (0x5d6049 in /usr/people/myusername/python3env/bin/python3.5)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\nAnd occasionally, it just runs without crashing and outputs\r\n```\r\ntensor([], dtype=torch.uint8)\r\n```\r\nwhich is almost what I would expect except that this is a `ByteTensor` and not a `FloatTensor`.\r\n\r\nEach time I run it, I get a different one of these errors, with no discernible pattern. Sometimes running it two times in a row gets the same resulting error, sometimes not.\r\n\r\nThe same behavior occurs with other variants including\r\n- replacing `torch.FloatTensor` with another tensor datatype such as `torch.ByteTensor` and `torch.BoolTensor`\r\n- passing parameters to the instantiation.\r\n- adding a body to the class definition\r\n\r\nWhile I encountered/produced all these errors while running on a single machine (environment specification below), I have tried this on other machines as well and I get similarly erratic behavior, including on Google Colaboratory.\r\n\r\n-----------------\r\n\r\nA different issue occurs when instantiating a subclass of `torch.Tensor`.\r\n\r\nRunning the following code:\r\n\r\nproduces a `TypeError`:\r\n\r\nThis error occurs for any nonzero number of parameters.\r\nNote that just `MyTensor()` works fine, as it does when I do not override `__init__`. But this is overriding `__init__` with its superclass's `__init__`, which should do nothing, and yet it causes it to fail. I understand that the `torch.Tensor` class is defined in the C backend, and that it only uses the `__new__` method to initialize. However, the `__init__` method of `torch.Tensor` should at least swallow its arguments before passing them on to `object` so that overriding `__init__` doesn't have to avoid calling its superclass (which is messy to deal with).\r\n\r\n## Expected behavior\r\n\r\nRunning\r\n\r\nshould instantiate a `MyFloatTensor` as subclass of a `torch.FloatTensor` and should output the same as calling\r\n\r\n\r\nRunning\r\n\r\nshould instantiate a 2-element `MyTensor` as subclass of a `torch.Tensor` and should output the same as calling\r\n\r\n\r\nIt's possible that pytorch tensor types weren't intended to be extended, which would be quite unfortunate. I would argue that it should be possible to subclass them, and doing so would be quite useful for me in my research. But in any case, attempting to subclass them should never cause a Segmentation Fault, and that usually seems to be indicative of deeper problem.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.77\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\r\n## Related GitHub Issues\r\nhttps://github.com/pytorch/pytorch/issues/17249 and https://github.com/pytorch/pytorch/issues/17716 are both related to subclassing PyTorch Tensors but don't mention these errors.",high priority|triaged,nairbv,"## \U0001f41b Bug\r\n\r\nInstantiating a subclass of `torch.FloatTensor`, `torch.ByteTensor`, or `torch.BoolTensor` causes a **segmentation fault**, a `c10::Error: Unrecognized Scalartype UNKNOWN_SCALAR`, and/or other odd errors.\r\n\r\nInstantiating a subclass of `torch.Tensor` with parameters and overriding `__init__` fails with a `TypeError`.\r\n\r\n## To Reproduce\r\n\r\nRun the following 4 lines on a fresh python REPL (or some variant):\r\n```python\r\nimport torch\r\nclass MyFloatTensor(torch.FloatTensor):\r\n    pass\r\nMyFloatTensor()\r\n```\r\n\r\nWith python2.7 and pytorch 1.1.0, I get\r\n`Segmentation fault (core dumped)`\r\n\r\n\r\nWith python3.5.2 and pytorch 1.1.0, it's a bit less predictable.\r\n\r\nSometimes I get a `RuntimeError`:\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-6-57316e947464> in <module>\r\n----> 1 MyFloatTensor()\r\n\r\nRuntimeError: Unknown backend\r\n```\r\n\r\nSometimes, I get a segmentation fault:\r\n```Segmentation fault (core dumped)```\r\n\r\nSometimes I get a `c10::Error`:\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Unrecognized Scalartype UNKNOWN_SCALAR (please report this error) (scalarTypeToTypeMeta at /pytorch/c10/core/ScalarType.h:136)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fb12efc9441 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fb12efc8d7a in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libc10.so)\r\nframe #2: <unknown function> + 0x2fb757 (0x7fb16e493757 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x3caee6 (0x7fb16e562ee6 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: torch::utils::legacy_tensor_ctor(at::Type const&, c10::ScalarType, _object*, _object*) + 0x210 (0x7fb16e6c0000 in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x510d8b (0x7fb16e6a8d8b in /usr/people/myusername/python3env/lib/python3.5/site-packages/torch/lib/libtorch_python.so)\r\nframe #6: /usr/people/myusername/python3env/bin/python3.5() [0x57efe5]\r\nframe #7: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #8: PyEval_EvalFrameEx + 0x4ec6 (0x53bba6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #9: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #10: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #11: /usr/people/myusername/python3env/bin/python3.5() [0x54a7c5]\r\nframe #12: PyCFunction_Call + 0x4f (0x4e9b7f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #13: PyEval_EvalFrameEx + 0x614 (0x5372f4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #14: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #15: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #16: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #17: PyEval_EvalFrameEx + 0x5ce5 (0x53c9c5 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #18: _PyGen_Send + 0x133 (0x4ed7d3 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #19: PyEval_EvalFrameEx + 0x4ce6 (0x53b9c6 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #20: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #21: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #22: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #23: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #24: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #25: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #26: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #27: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #28: PyEval_EvalFrameEx + 0x4b04 (0x53b7e4 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #29: PyEval_EvalCodeEx + 0x13b (0x540f9b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #30: /usr/people/myusername/python3env/bin/python3.5() [0x4ebe37]\r\nframe #31: PyObject_Call + 0x47 (0x5c1797 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #32: PyEval_EvalFrameEx + 0x252b (0x53920b in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #33: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #34: PyEval_EvalFrameEx + 0x50b2 (0x53bd92 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #35: /usr/people/myusername/python3env/bin/python3.5() [0x540199]\r\nframe #36: PyEval_EvalCode + 0x1f (0x540e4f in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #37: /usr/people/myusername/python3env/bin/python3.5() [0x60c272]\r\nframe #38: PyRun_FileExFlags + 0x9a (0x60e71a in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #39: PyRun_SimpleFileExFlags + 0x1bc (0x60ef0c in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #40: Py_Main + 0x456 (0x63fb26 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #41: main + 0xe1 (0x4cfeb1 in /usr/people/myusername/python3env/bin/python3.5)\r\nframe #42: __libc_start_main + 0xf0 (0x7fb1e860b830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #43: _start + 0x29 (0x5d6049 in /usr/people/myusername/python3env/bin/python3.5)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\nAnd occasionally, it just runs without crashing and outputs\r\n```\r\ntensor([], dtype=torch.uint8)\r\n```\r\nwhich is almost what I would expect except that this is a `ByteTensor` and not a `FloatTensor`.\r\n\r\nEach time I run it, I get a different one of these errors, with no discernible pattern. Sometimes running it two times in a row gets the same resulting error, sometimes not.\r\n\r\nThe same behavior occurs with other variants including\r\n- replacing `torch.FloatTensor` with another tensor datatype such as `torch.ByteTensor` and `torch.BoolTensor`\r\n- passing parameters to the instantiation.\r\n- adding a body to the class definition\r\n\r\nWhile I encountered/produced all these errors while running on a single machine (environment specification below), I have tried this on other machines as well and I get similarly erratic behavior, including on Google Colaboratory.\r\n\r\n-----------------\r\n\r\nA different issue occurs when instantiating a subclass of `torch.Tensor`.\r\n\r\nRunning the following code:\r\n```python\r\nimport torch\r\nclass MyTensor(torch.Tensor):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\nMyTensor(2)\r\n```\r\nproduces a `TypeError`:\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-c115513591af> in <module>\r\n----> 1 MyTensor(2)\r\n\r\n<ipython-input-2-0187aa9fddd0> in __init__(self, *args, **kwargs)\r\n      1 class MyTensor(torch.Tensor):\r\n      2     def __init__(self, *args, **kwargs):\r\n----> 3         super().__init__(*args, **kwargs)\r\n\r\nTypeError: object.__init__() takes no parameters\r\n```\r\nThis error occurs for any nonzero number of parameters.\r\nNote that just `MyTensor()` works fine, as it does when I do not override `__init__`. But this is overriding `__init__` with its superclass's `__init__`, which should do nothing, and yet it causes it to fail. I understand that the `torch.Tensor` class is defined in the C backend, and that it only uses the `__new__` method to initialize. However, the `__init__` method of `torch.Tensor` should at least swallow its arguments before passing them on to `object` so that overriding `__init__` doesn't have to avoid calling its superclass (which is messy to deal with).\r\n\r\n## Expected behavior\r\n\r\nRunning\r\n```python\r\nimport torch\r\nclass MyFloatTensor(torch.FloatTensor):\r\n    pass\r\nMyFloatTensor()\r\n```\r\nshould instantiate a `MyFloatTensor` as subclass of a `torch.FloatTensor` and should output the same as calling\r\n```python\r\n>>> torch.FloatTensor()\r\ntensor([])\r\n```\r\n\r\nRunning\r\n```python\r\nimport torch\r\nclass MyTensor(torch.Tensor):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\nMyTensor(2)\r\n```\r\nshould instantiate a 2-element `MyTensor` as subclass of a `torch.Tensor` and should output the same as calling\r\n```python\r\n>>> torch.Tensor(2)\r\ntensor([6.5783e-31, 4.5766e-41])\r\n```\r\n\r\nIt's possible that pytorch tensor types weren't intended to be extended, which would be quite unfortunate. I would argue that it should be possible to subclass them, and doing so would be quite useful for me in my research. But in any case, attempting to subclass them should never cause a Segmentation Fault, and that usually seems to be indicative of deeper problem.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.77\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.1\r\n[pip3] torch==1.1.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\r\n## Related GitHub Issues\r\nhttps://github.com/pytorch/pytorch/issues/17249 and https://github.com/pytorch/pytorch/issues/17716 are both related to subclassing PyTorch Tensors but don't mention these errors.",python\r\nimport torch\r\nclass MyFloatTensor(torch.FloatTensor):\r\n    pass\r\nMyFloatTensor()\r\n
20004,"[JIT] cannot access to initialized attribute in script-annotated class __getitem__## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n```\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 4, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 819, in script\r\n    _jit_script_class_compile(ast, _rcb)\r\nRuntimeError: \r\nTried to access to nonexistent attribute first. Did you forget to initialize it in __init__()?:\r\ndef __getitem__(self, k):\r\n    return self.first\r\n           ~~~~~~~~~~ <--- HERE\r\n```\r\n## Expected behavior\r\n\r\nIt should compile the class.\r\n\r\n## Environment\r\n\r\n$ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Mac OSX 10.14.3\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: 1.1.0\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/lib/libcudnn.7.dylib\r\n/usr/local/cuda/lib/libcudnn.dylib\r\n/usr/local/cuda/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.1.0                   py3.7_0    pytorch\r\n[conda] torch                     1.1.0a0+6732358           dev_0    <develop>\r\n[conda] torchvision               0.2.2                      py_3    pytorch",high priority|oncall: jit|triaged,suo,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n```python\r\n@torch.jit.script\r\nclass Pair:\r\n    def __init__(self, first, second):\r\n        self.first = first\r\n        self.second = second\r\n\r\n    def sum(self):\r\n        return self.first + self.second\r\n\r\n    def __getitem__(self, k):\r\n        return self.first\r\n```\r\n```\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 4, in <module>\r\n    @torch.jit.script\r\n  File ""/Users/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 819, in script\r\n    _jit_script_class_compile(ast, _rcb)\r\nRuntimeError: \r\nTried to access to nonexistent attribute first. Did you forget to initialize it in __init__()?:\r\ndef __getitem__(self, k):\r\n    return self.first\r\n           ~~~~~~~~~~ <--- HERE\r\n```\r\n## Expected behavior\r\n\r\nIt should compile the class.\r\n\r\n## Environment\r\n\r\n$ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.1.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect\r\n\r\nOS: Mac OSX 10.14.3\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: 1.1.0\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/lib/libcudnn.7.dylib\r\n/usr/local/cuda/lib/libcudnn.dylib\r\n/usr/local/cuda/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0\r\n[pip3] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.3                      199  \r\n[conda] mkl-include               2019.3                      199  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.1.0                   py3.7_0    pytorch\r\n[conda] torch                     1.1.0a0+6732358           dev_0    <develop>\r\n[conda] torchvision               0.2.2                      py_3    pytorch","python\r\n@torch.jit.script\r\nclass Pair:\r\n    def __init__(self, first, second):\r\n        self.first = first\r\n        self.second = second\r\n\r\n    def sum(self):\r\n        return self.first + self.second\r\n\r\n    def __getitem__(self, k):\r\n        return self.first\r\n"
19925,"ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each## \U0001f41b Bug\r\n\r\nI managed to run distributed training using 2 nodes and 2 GPUs each, but failed with NCCL error when using 4 GPUs on the 2 nodes. The error occurs when fist calling dist.broadcast.\r\n\r\n## To Reproduce\r\n\r\nHere is the codebase:\r\nhttps://github.com/mlperf/training/tree/master/rnn_translator/pytorch\r\n\r\nI changed a few places for my script on multi-machine multi-gpu settings work. Here is the init_process_group:\r\n\r\nThis is `dist_multiproc.py` which I use as a distributed python module\r\n\r\nThis is the content of `dist_config.txt`:\r\n```\r\ngpus:4\r\nworld_size:8\r\nmaster_addr:eco-11\r\nmaster_port:23451\r\nnccl_ifname:enp3s0f0\r\n```\r\nHere is the arguments of process with rank 0, there are 8 processes in total, one for each GPU\r\n```\r\nNamespace(batch_size=64, beam_size=5, bucketing=True, cov_penalty_factor=0.1, cuda=True, cudnn=True, cupti=False, dataset_dir='/mnt/dataset/wmt_ende/', disable_eval=False, dist_url='env://', epochs=8, eval_batch_size=32, gpu_rank=0, grad_clip=5.0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, math='fp32', max_length_train=50, max_length_val=150, max_size=None, min_length_train=0, min_length_val=0, model_config=""{'hidden_size': 1024,'num_layers': 4,                         'dropout': 0.2, 'share_embedding': True}"", num_minibatches=20, optimization_config=""{'optimizer': 'Adam', 'lr': 5e-4}"", print_freq=10, profile=False, profile_dir='./profile', rank=0, results_dir='../results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, seed=1, smoothing=0.1, start_epoch=0, target_bleu=21.8, workers=0, world_size=8)\r\n```\r\nThe training is then triggered by\r\n```\r\npython -m dist_multiproc train.py ...\r\n```\r\n\r\nHere are the outputs from node 1 (master) and node 2:\r\nnode 1:\r\n```\r\neco-11:19785:20063 [0] NCCL INFO Ring 00 : 4 -> 0 [receive] via NET/Socket/0\r\neco-11:19785:20063 [0] NCCL INFO Ring 00 : 0 -> 4 [send] via NET/Socket/0\r\neco-11:19785:20063 [0] NCCL INFO Trees [0] -1->0->1/4/-1\r\neco-11:19785:20063 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size 149999\r\neco-11:19785:20063 [0] NCCL INFO comm 0x7f4aac001ac0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\neco-11:19785:19785 [0] NCCL INFO Launch mode Parallel\r\n```\r\nnode 2:\r\n```\r\neco-12:3817:3817 [0] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.70.40.12<0>\r\neco-12:3817:3817 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\neco-12:3817:3817 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\neco-12:3817:4096 [0] NCCL INFO Setting affinity for GPU 0 to ff00,0000ff00\r\neco-12:3817:4096 [0] NCCL INFO comm 0x7f7b9c001ac0 rank 4 nranks 8 cudaDev 0 nvmlDev 0\r\neco-12:3817:4096 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  SOC\r\neco-12:3817:4096 [0] NCCL INFO NCCL_BUFFSIZE set by environment to 16777216.\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 3 -> 4 [receive] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 4[0] -> 5[1] via direct shared memory\r\n7: Initializing fp32 optimizer\r\n7: Number of parameters: 169336518\r\n7: Starting epoch 0\r\n6: Initializing fp32 optimizer\r\n6: Number of parameters: 169336518\r\n6: Starting epoch 0\r\nTraceback (most recent call last):\r\n  File ""train.py"", line 465, in <module>\r\n    main()\r\n  File ""train.py"", line 281, in main\r\n    trainer = trainers.Seq2SeqTrainer(**trainer_options)\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/trainer.py"", line 70, in __init__\r\n    self.model = DDP(self.model, log_dir=self.log_dir)\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/distributed.py"", line 104, in __init__\r\n    flat_dist_call([param.data for param in self.module.parameters()], dist.broadcast, log_dir=self.log_dir, iteration=0, msg=""broadcast"", extra_args=(0,) )\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/distributed.py"", line 37, in flat_dist_call\r\n    call(coalesced, *extra_args)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/distributed/distributed_c10d.py"", line 737, in broadcast\r\n    work = _default_pg.broadcast([tensor], opts)\r\nRuntimeError: NCCL error in: /root/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled cuda error\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 4 -> 0 [send] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 0 -> 4 [receive] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Trees [0] 0->4->5/-1/-1\r\neco-12:3817:4096 [0] NCCL INFO comm 0x7f7b9c001ac0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n```\r\n## Expected behavior\r\n\r\nIt should proceed normally without NCCL error just like in 2-GPU + 2-node case\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```",oncall: distributed|triaged,mrshenli,"## \U0001f41b Bug\r\n\r\nI managed to run distributed training using 2 nodes and 2 GPUs each, but failed with NCCL error when using 4 GPUs on the 2 nodes. The error occurs when fist calling dist.broadcast.\r\n\r\n## To Reproduce\r\n\r\nHere is the codebase:\r\nhttps://github.com/mlperf/training/tree/master/rnn_translator/pytorch\r\n\r\nI changed a few places for my script on multi-machine multi-gpu settings work. Here is the init_process_group:\r\n```python\r\n    if args.cuda:\r\n        torch.cuda.set_device(args.gpu_rank)\r\n\r\n    # initialize distributed backend\r\n    distributed = args.world_size > 1\r\n    if distributed:\r\n        backend = 'nccl' if args.cuda else 'gloo'\r\n        dist.init_process_group(backend=backend, rank=args.rank,\r\n                                init_method=args.dist_url,\r\n                                world_size=args.world_size)\r\n```\r\nThis is `dist_multiproc.py` which I use as a distributed python module\r\n```python\r\nimport sys\r\nimport subprocess\r\nimport json\r\nimport os\r\n\r\ndef host_to_rank(hostname):\r\n    if hostname == ""eco-11"":\r\n        return 0\r\n    if hostname == ""eco-12"":\r\n        return 1\r\n    if hostname == ""eco-13"":\r\n        return 2\r\n    if hostname == ""eco-14"":\r\n        return 3\r\n\r\ndef main():\r\n    argslist = list(sys.argv)[1:]\r\n    configs = {}\r\n    with open(""dist_config.txt"") as configfile:\r\n        for line in configfile.readlines():\r\n            key = line.split("":"")[0]\r\n            value = line.split("":"")[1].strip()\r\n            configs[key] = value\r\n    world_size = int(configs[""world_size""])\r\n    gpus = int(configs[""gpus""])\r\n    master_addr = configs[""master_addr""]\r\n    master_port = configs[""master_port""]\r\n    nccl_ifname = configs[""nccl_ifname""]\r\n\r\n    os.environ[""MASTER_ADDR""] = master_addr\r\n    os.environ[""MASTER_PORT""] = master_port\r\n    os.environ[""NCCL_SOCKET_IFNAME""] = nccl_ifname\r\n    os.environ[""NCCL_DEBUG""] = ""INFO""\r\n    os.environ[""NCCL_BUFFSIZE""] = str(16 * 1024 * 1024)\r\n\r\n    node_rank = host_to_rank(os.environ[""HOSTNAME""])\r\n    workers = []\r\n\r\n    for i in range(gpus):\r\n        if '--rank' in argslist:\r\n            argslist[argslist.index('--rank') + 1] = str(node_rank * gpus + i)\r\n        else:\r\n            argslist.append('--rank')\r\n            argslist.append(str(node_rank * gpus + i))\r\n        if '--gpu-rank' in argslist:\r\n            argslist[argslist.index('--gpu-rank') + 1] = str(i)\r\n        else:\r\n            argslist.append('--gpu-rank')\r\n            argslist.append(str(i))\r\n        if '--world-size' in argslist:\r\n            argslist[argslist.index('--world-size') + 1] = str(world_size)\r\n        else:\r\n            argslist.append('--world-size')\r\n            argslist.append(str(world_size))\r\n\r\n        stdout = None if i == 0 else open(""GPU_"" + str(i) + "".log"", ""w"")\r\n        worker = subprocess.Popen([str(sys.executable)] + argslist, stdout=stdout)\r\n        workers.append(worker)\r\n\r\n    returncode = 0\r\n    try:\r\n        for worker in workers:\r\n            worker_returncode = worker.wait()\r\n            if worker_returncode != 0:\r\n                returncode = 1\r\n    except KeyboardInterrupt:\r\n        print('Pressed CTRL-C, TERMINATING')\r\n        for worker in workers:\r\n            worker.terminate()\r\n        for worker in workers:\r\n            worker.wait()\r\n        raise\r\n\r\n    sys.exit(returncode)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\nThis is the content of `dist_config.txt`:\r\n```\r\ngpus:4\r\nworld_size:8\r\nmaster_addr:eco-11\r\nmaster_port:23451\r\nnccl_ifname:enp3s0f0\r\n```\r\nHere is the arguments of process with rank 0, there are 8 processes in total, one for each GPU\r\n```\r\nNamespace(batch_size=64, beam_size=5, bucketing=True, cov_penalty_factor=0.1, cuda=True, cudnn=True, cupti=False, dataset_dir='/mnt/dataset/wmt_ende/', disable_eval=False, dist_url='env://', epochs=8, eval_batch_size=32, gpu_rank=0, grad_clip=5.0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, math='fp32', max_length_train=50, max_length_val=150, max_size=None, min_length_train=0, min_length_val=0, model_config=""{'hidden_size': 1024,'num_layers': 4,                         'dropout': 0.2, 'share_embedding': True}"", num_minibatches=20, optimization_config=""{'optimizer': 'Adam', 'lr': 5e-4}"", print_freq=10, profile=False, profile_dir='./profile', rank=0, results_dir='../results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, seed=1, smoothing=0.1, start_epoch=0, target_bleu=21.8, workers=0, world_size=8)\r\n```\r\nThe training is then triggered by\r\n```\r\npython -m dist_multiproc train.py ...\r\n```\r\n\r\nHere are the outputs from node 1 (master) and node 2:\r\nnode 1:\r\n```\r\neco-11:19785:20063 [0] NCCL INFO Ring 00 : 4 -> 0 [receive] via NET/Socket/0\r\neco-11:19785:20063 [0] NCCL INFO Ring 00 : 0 -> 4 [send] via NET/Socket/0\r\neco-11:19785:20063 [0] NCCL INFO Trees [0] -1->0->1/4/-1\r\neco-11:19785:20063 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size 149999\r\neco-11:19785:20063 [0] NCCL INFO comm 0x7f4aac001ac0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\neco-11:19785:19785 [0] NCCL INFO Launch mode Parallel\r\n```\r\nnode 2:\r\n```\r\neco-12:3817:3817 [0] NCCL INFO NET/Socket : Using [0]enp3s0f0:10.70.40.12<0>\r\neco-12:3817:3817 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\neco-12:3817:3817 [0] misc/ibvwrap.cu:63 NCCL WARN Failed to open libibverbs.so[.1]\r\neco-12:3817:4096 [0] NCCL INFO Setting affinity for GPU 0 to ff00,0000ff00\r\neco-12:3817:4096 [0] NCCL INFO comm 0x7f7b9c001ac0 rank 4 nranks 8 cudaDev 0 nvmlDev 0\r\neco-12:3817:4096 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  SOC\r\neco-12:3817:4096 [0] NCCL INFO NCCL_BUFFSIZE set by environment to 16777216.\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 3 -> 4 [receive] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 4[0] -> 5[1] via direct shared memory\r\n7: Initializing fp32 optimizer\r\n7: Number of parameters: 169336518\r\n7: Starting epoch 0\r\n6: Initializing fp32 optimizer\r\n6: Number of parameters: 169336518\r\n6: Starting epoch 0\r\nTraceback (most recent call last):\r\n  File ""train.py"", line 465, in <module>\r\n    main()\r\n  File ""train.py"", line 281, in main\r\n    trainer = trainers.Seq2SeqTrainer(**trainer_options)\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/trainer.py"", line 70, in __init__\r\n    self.model = DDP(self.model, log_dir=self.log_dir)\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/distributed.py"", line 104, in __init__\r\n    flat_dist_call([param.data for param in self.module.parameters()], dist.broadcast, log_dir=self.log_dir, iteration=0, msg=""broadcast"", extra_args=(0,) )\r\n  File ""/root/dl_parallelization/baselines/pytorch/seq2seq/pytorch/seq2seq/train/distributed.py"", line 37, in flat_dist_call\r\n    call(coalesced, *extra_args)\r\n  File ""/usr/local/lib/python2.7/dist-packages/torch/distributed/distributed_c10d.py"", line 737, in broadcast\r\n    work = _default_pg.broadcast([tensor], opts)\r\nRuntimeError: NCCL error in: /root/pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled cuda error\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 4 -> 0 [send] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Ring 00 : 0 -> 4 [receive] via NET/Socket/0\r\neco-12:3817:4096 [0] NCCL INFO Trees [0] 0->4->5/-1/-1\r\neco-12:3817:4096 [0] NCCL INFO comm 0x7f7b9c001ac0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n```\r\n## Expected behavior\r\n\r\nIt should proceed normally without NCCL error just like in 2-GPU + 2-node case\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 410.78\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```","python\r\n    if args.cuda:\r\n        torch.cuda.set_device(args.gpu_rank)\r\n\r\n    # initialize distributed backend\r\n    distributed = args.world_size > 1\r\n    if distributed:\r\n        backend = 'nccl' if args.cuda else 'gloo'\r\n        dist.init_process_group(backend=backend, rank=args.rank,\r\n                                init_method=args.dist_url,\r\n                                world_size=args.world_size)\r\n"
19840,"Using DistributedDataParallel through NCCL throws RuntimeErrorHello :),\r\n\r\nI have a setup with DDP and a training using BPTT over a sequence of images, but trying to call backward over the critic (Adversarial setup, with detached Generator output) it leads to the following:  \r\n\r\n\r\n\r\nPython 3.7, CUDA 10.0, CuDNN 7.5.0, PyTorch master (3803d1c901b6e2632682b02589b2d5aa888b0a88)\r\n\r\nDistributed Data Parallel, Multi-Process Single-GPU over 4xV100\r\n\r\nI was working fine with the same code before updating this afternoon.",oncall: distributed|triaged,pietern,"Hello :),\r\n\r\nI have a setup with DDP and a training using BPTT over a sequence of images, but trying to call backward over the critic (Adversarial setup, with detached Generator output) it leads to the following:  \r\n\r\n```python\r\nself.reducer.prepare_for_backward(list(_find_tensors(output)))self.reducer.prepare_for_backward(list(_find_tensors(output)))\r\n\r\nRuntimeErrorRuntimeError: : Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing its output (the return value of `forward`). You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`. If you already have this argument set, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /home/mfuntowi/Workspace/pytorch/torch/csrc/distributed/c10d/reducer.cpp:401)\r\n```\r\n\r\nPython 3.7, CUDA 10.0, CuDNN 7.5.0, PyTorch master (3803d1c901b6e2632682b02589b2d5aa888b0a88)\r\n\r\nDistributed Data Parallel, Multi-Process Single-GPU over 4xV100\r\n\r\nI was working fine with the same code before updating this afternoon.","python\r\nself.reducer.prepare_for_backward(list(_find_tensors(output)))self.reducer.prepare_for_backward(list(_find_tensors(output)))\r\n\r\nRuntimeErrorRuntimeError: : Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing its output (the return value of `forward`). You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`. If you already have this argument set, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /home/mfuntowi/Workspace/pytorch/torch/csrc/distributed/c10d/reducer.cpp:401)\r\n"
19637,"[jit] Traced {zeros,empty}()/{zeros,empty}_like() calls do not respect device args.## \U0001f41b Bug\r\n\r\n\r\nTracing a call to `torch.{zeros,empty}_like(x)` produces a script module where the resulting allocation is always made on the same device as `x` was in the trace, not at runtime.\r\n\r\nIf the call is made in a function/method annotated with `@th.jit.script{,_method}` then everything works as expected.\r\n\r\nIn the case of `zeros(sizes, device=device)`, this is understandable since I can see that the passed non-tensor args (the device) would be considered constants for tracing, but in the case of zeros_like it is not obvious.\r\n\r\n(sorry, not 100% sure this is a bug but the behavior isn't entirely expected. Perhaps a warning is warranted for traced `..._like(x)`?)\r\n\r\n## To Reproduce\r\n\r\nSimple test:\r\n\r\n\r\n1. Run the script above on a machine with 2 GPUs (comment out the last 2 lines for a single-GPU machine).\r\n\r\n## Expected behavior\r\nCode prints\r\n```\r\ncpu\r\ncuda:0\r\ncuda:1\r\n```\r\n\r\nActual behavior:\r\nCode prints\r\n```\r\ncpu\r\ncpu\r\ncpu\r\n```\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Nightly 20190423\r\n - OS (e.g., Linux): Linux (Fedora 29)\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0.130 / 7.4.2\r\n - GPU models and configuration: 2x Titan RTX",oncall: jit|triaged,suo,"## \U0001f41b Bug\r\n\r\n\r\nTracing a call to `torch.{zeros,empty}_like(x)` produces a script module where the resulting allocation is always made on the same device as `x` was in the trace, not at runtime.\r\n\r\nIf the call is made in a function/method annotated with `@th.jit.script{,_method}` then everything works as expected.\r\n\r\nIn the case of `zeros(sizes, device=device)`, this is understandable since I can see that the passed non-tensor args (the device) would be considered constants for tracing, but in the case of zeros_like it is not obvious.\r\n\r\n(sorry, not 100% sure this is a bug but the behavior isn't entirely expected. Perhaps a warning is warranted for traced `..._like(x)`?)\r\n\r\n## To Reproduce\r\n\r\nSimple test:\r\n```python\r\nimport torch as th\r\n\r\ndef make_zeros(x):\r\n    return th.zeros_like(x)\r\n\r\nx = th.zeros(3)\r\nfunc = th.jit.trace(make_zeros, (x,))\r\n\r\nprint(func(x).device)\r\nx = th.zeros(3).cuda(0)\r\nprint(func(x).device)\r\nx = th.zeros(3).cuda(1)\r\nprint(func(x).device)\r\n```\r\n\r\n1. Run the script above on a machine with 2 GPUs (comment out the last 2 lines for a single-GPU machine).\r\n\r\n## Expected behavior\r\nCode prints\r\n```\r\ncpu\r\ncuda:0\r\ncuda:1\r\n```\r\n\r\nActual behavior:\r\nCode prints\r\n```\r\ncpu\r\ncpu\r\ncpu\r\n```\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): Nightly 20190423\r\n - OS (e.g., Linux): Linux (Fedora 29)\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: 10.0.130 / 7.4.2\r\n - GPU models and configuration: 2x Titan RTX","python\r\nimport torch as th\r\n\r\ndef make_zeros(x):\r\n    return th.zeros_like(x)\r\n\r\nx = th.zeros(3)\r\nfunc = th.jit.trace(make_zeros, (x,))\r\n\r\nprint(func(x).device)\r\nx = th.zeros(3).cuda(0)\r\nprint(func(x).device)\r\nx = th.zeros(3).cuda(1)\r\nprint(func(x).device)\r\n"
19453,"How to load PyTorch model with LSTM using C++ api## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Establish a PyTorch model with LSTM module using python, and store the script module after using torch.jit.trace. Python code like this:\r\n\r\n\r\n\r\n\r\n\r\n2. Load the model and move the model to GPU, then when the script exit, there is a core dump. However, If we don't move the model to gpu, the cpp script exits normally.My cpp script like this:\r\n\r\n\r\n\r\n> Note: There is a bug to move init hidden state tensor of lstm to gpu [link](https://github.com/pytorch/pytorch/issues/15272) I use two methods to solve this problem, one is to specify the device in python model using hard code, another is to pass init hidden state as input parameter of forward in cpp script, which may cause a warning [link](https://discuss.pytorch.org/t/rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memory/6011/14)\r\n\r\nthe gdb trace info like this:\r\n\r\n\r\n\r\n3. When I remove the LSTM in python model, then the cpp script exits normally.\r\n\r\n4. I guess the hidden state of LSTM cause the core dump, maybe relate to the release the init hidden state memory?\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nSo, When I want to load a model with LSTM using c++, how to deal with the hidden state, and how to avoid core dump?\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.0\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",module: cpp|triaged,yf225,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Establish a PyTorch model with LSTM module using python, and store the script module after using torch.jit.trace. Python code like this:\r\n\r\n```python\r\nclass MyModule(nn.Module):\r\n    def __init__(self, N, M):\r\n        super(MyModule, self).__init__()\r\n        self.lstm = nn.LSTM(M, M, batch_first=True)\r\n        self.linear = nn.Linear(M, 1)\r\n\r\n    def forward(self, inputs, h0, c0):\r\n\r\n        output, (_, _) = self.lstm(inputs, h0, c0)\r\n        output, _ = torch.max(output, dim=1)\r\n        # output, _ = torch.max(inputs, dim=1)\r\n        output = self.linear(output)\r\n        return output\r\n\r\nbatch_size = 8\r\nh = 33\r\nw = 45\r\nmodel =  MyModule(h, w)\r\ndata = np.random.normal(1, 1, size=(batch_size, h, w))\r\ndata = torch.Tensor(data)\r\nh0, c0 = torch.zeros(1, batch_size, w), torch.zeros(1, batch_size, w)\r\n\r\ntraced_script_module = torch.jit.trace(model, (data, h0,c0))\r\ntraced_script_module.save('model.pt')\r\n```\r\n\r\n\r\n\r\n2. Load the model and move the model to GPU, then when the script exit, there is a core dump. However, If we don't move the model to gpu, the cpp script exits normally.My cpp script like this:\r\n\r\n```c++\r\nint main(int argc, const char* argv[]) {\r\n  if (argc != 2) {\r\n    std::cerr << ""usage: example-app <path-to-exported-script-module>\\n"";\r\n    return -1;\r\n  }\r\n\r\n  // Deserialize the ScriptModule from a file using torch::jit::load().\r\n  std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n\r\n  assert(module != nullptr);\r\n  std::cout << ""ok\\n"";\r\n  this->module->to(at::Device(""cuda:0""))\r\n\r\n  vector<torch::jit::IValue> inputs;\r\n  int b = 2, h = 33, w = 45;\r\n  vector<float> data(b*h*w, 1.0);\r\n  torch::Tensor data_tensor = torch::from_blob(data.data(), {b, h, w}.to(at::Device(""cuda:0""));\r\n  torch::Tensor h0 = torch::from_blob(vector<float>(1*b*w, 0.0), {b, h, w}).to(at::Device(""cuda:0""));\r\n  torch::Tensor c0 = torch::from_blob(vector<float>(1*b*w, 0.0), {b, h, w}).to(at::Device(""cuda:0""));\r\n  inputs.push_back(data_tensor);\r\n  inputs.push_back(h0);\r\n  inputs.push(c0);\r\n  torch::Tensor output = module->forward(inputs).toTensor().cpu();\r\n  auto accessor = output.accessor<float, 2>();\r\n  vector<float> answer(b);\r\n  for (int i=0; i<accessor.size(0); ++i){\r\n        answer[i] = accessor[i][0];\r\n  }\r\n  cout << ""predict ok"" << endl;\r\n}\r\n```\r\n\r\n> Note: There is a bug to move init hidden state tensor of lstm to gpu [link](https://github.com/pytorch/pytorch/issues/15272) I use two methods to solve this problem, one is to specify the device in python model using hard code, another is to pass init hidden state as input parameter of forward in cpp script, which may cause a warning [link](https://discuss.pytorch.org/t/rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memory/6011/14)\r\n\r\nthe gdb trace info like this:\r\n\r\n```shell\r\n(gdb) where\r\n#0  0x00007ffff61ca9fe in ?? () from /usr/local/cuda/lib64/libcudart.so.10.0\r\n#1  0x00007ffff61cf96b in ?? () from /usr/local/cuda/lib64/libcudart.so.10.0\r\n#2  0x00007ffff61e4be2 in cudaDeviceSynchronize () from /usr/local/cuda/lib64/libcudart.so.10.0\r\n#3  0x00007fffb945dcf4 in cudnnDestroy () from repo/pytorch_cpp/libtorch/lib/libcaffe2_gpu.so\r\n#4  0x00007fffb4fca17d in std::unordered_map<int, std::vector<at::native::(anonymous namespace)::Handle, std::allocator<at::native::(anonymous namespace)::Handle> >, std::hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, std::vector<at::native::(anonymous namespace)::Handle, std::allocator<at::native::(anonymous namespace)::Handle> > > > >::~unordered_map() () from repo/pytorch_cpp/libtorch/lib/libcaffe2_gpu.so\r\n#5  0x00007fffb31fe615 in __cxa_finalize (d=0x7fffe8519680) at cxa_finalize.c:83\r\n#6  0x00007fffb4dd3ac3 in __do_global_dtors_aux () from repo/pytorch_cpp/libtorch/lib/libcaffe2_gpu.so\r\n#7  0x00007fffffffe010 in ?? ()\r\n#8  0x00007ffff7de5b73 in _dl_fini () at dl-fini.c:138\r\nBacktrace stopped: frame did not save the PC\r\n\r\n```\r\n\r\n3. When I remove the LSTM in python model, then the cpp script exits normally.\r\n\r\n4. I guess the hidden state of LSTM cause the core dump, maybe relate to the release the init hidden state memory?\r\n\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nSo, When I want to load a model with LSTM using c++, how to deal with the hidden state, and how to avoid core dump?\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.0\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nclass MyModule(nn.Module):\r\n    def __init__(self, N, M):\r\n        super(MyModule, self).__init__()\r\n        self.lstm = nn.LSTM(M, M, batch_first=True)\r\n        self.linear = nn.Linear(M, 1)\r\n\r\n    def forward(self, inputs, h0, c0):\r\n\r\n        output, (_, _) = self.lstm(inputs, h0, c0)\r\n        output, _ = torch.max(output, dim=1)\r\n        # output, _ = torch.max(inputs, dim=1)\r\n        output = self.linear(output)\r\n        return output\r\n\r\nbatch_size = 8\r\nh = 33\r\nw = 45\r\nmodel =  MyModule(h, w)\r\ndata = np.random.normal(1, 1, size=(batch_size, h, w))\r\ndata = torch.Tensor(data)\r\nh0, c0 = torch.zeros(1, batch_size, w), torch.zeros(1, batch_size, w)\r\n\r\ntraced_script_module = torch.jit.trace(model, (data, h0,c0))\r\ntraced_script_module.save('model.pt')\r\n"
19422,"Document that autograd::Profiler::RecordFunction is available in Python## \U0001f680 Feature\r\n\r\n`torch.autograd.profiler.profile().export_chrome_trace` is great. Adding information to it from Python would make it even better.\r\n\r\n## Motivation\r\n\r\nIn multi-threaded PyTorch code, it can be useful to further annotate the tracing output by adding additional information to it.\r\n\r\n## Pitch\r\n\r\nOne easy / cheap way of doing this:\r\n\r\n\r\n\r\nAnd then in Python\r\n\r\n\r\n",high priority|module: docs|module: autograd|triaged|small,prasunanand,"## \U0001f680 Feature\r\n\r\n`torch.autograd.profiler.profile().export_chrome_trace` is great. Adding information to it from Python would make it even better.\r\n\r\n## Motivation\r\n\r\nIn multi-threaded PyTorch code, it can be useful to further annotate the tracing output by adding additional information to it.\r\n\r\n## Pitch\r\n\r\nOne easy / cheap way of doing this:\r\n\r\n```cc\r\nPYBIND11_MODULE(torch, m) {\r\n  py::class_<torch::autograd::profiler::RecordFunction>(m, ""Record"")\r\n      .def(py::init<std::string>(), py::arg(""name""));\r\n}\r\n```\r\n\r\nAnd then in Python\r\n\r\n```python\r\nrecord = torch.Record(""model"")\r\noutputs = model(frame)\r\ndel record  # Alternatively, turn it into a context manager.\r\n```\r\n","cc\r\nPYBIND11_MODULE(torch, m) {\r\n  py::class_<torch::autograd::profiler::RecordFunction>(m, ""Record"")\r\n      .def(py::init<std::string>(), py::arg(""name""));\r\n}\r\n"
19413,Make Node::cast `const`Address this TODO in `ir.h` (`Node::cast()`)\r\n\r\n,oncall: jit|low priority,Krovatkin,"Address this TODO in `ir.h` (`Node::cast()`)\r\n\r\n```cpp\r\n  // Dynamically cast this node to the subclass indicated by the\r\n  // template variable, returning nullptr if the cast is invalid..\r\n  //\r\n  // Example usage: if(auto s = n.cast<Select>()) { ... }\r\n  //\r\n  // TODO: Make this const correct\r\n```","cpp\r\n  // Dynamically cast this node to the subclass indicated by the\r\n  // template variable, returning nullptr if the cast is invalid..\r\n  //\r\n  // Example usage: if(auto s = n.cast<Select>()) { ... }\r\n  //\r\n  // TODO: Make this const correct\r\n"
19363,"[JIT] Cannot access nn.Linear.in_features in ScriptModule## \U0001f41b Bug\r\n\r\n\r\n\r\nThe code above throws AttributeError\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nLooking at the code (https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L1226), I think WeakScriptModuleProxy should have copied in_features and other fields to self here.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.1 144\r\n[conda] mkl-service 1.1.2 py37he904b0f_5\r\n[conda] mkl_fft 1.0.6 py37hd81dba3_0\r\n[conda] mkl_random 1.0.2 py37hd81dba3_0\r\n[conda] pytorch 1.0.1 py3.7_cuda10.0.130_cudnn7.4.2_2 pytorch\r\n\r\nhttps://discuss.pytorch.org/t/cannot-access-nn-linear-in-features-in-scriptmodule/42833",oncall: jit|triaged,driazati,"## \U0001f41b Bug\r\n\r\n```python\r\nimport torch.nn as nn\r\nimport torch.jit as jit\r\n\r\nclass TestModule(jit.ScriptModule):\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.linear = nn.Linear(16, 16)\r\n\r\n\r\nm = TestModule()\r\nprint(m.linear.in_features)\r\n```\r\n\r\nThe code above throws AttributeError\r\n\r\n```python\r\n>>> m.linear.in_features\r\nTraceback (most recent call last):\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1197, in __getattr__\r\n    return ScriptModule.__getattr__(self, attr)\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1102, in __getattr__\r\n    return Module.__getattr__(self, attr)\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 535, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'WeakScriptModuleProxy' object has no attribute 'in_features'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 1200, in __getattr__\r\n    return getattr(self.__dict__[""_original""](), attr)\r\nAttributeError: 'NoneType' object has no attribute 'in_features'\r\n```\r\n\r\n## Expected behavior\r\n\r\nLooking at the code (https://github.com/pytorch/pytorch/blob/master/torch/jit/__init__.py#L1226), I think WeakScriptModuleProxy should have copied in_features and other fields to self here.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.1 144\r\n[conda] mkl-service 1.1.2 py37he904b0f_5\r\n[conda] mkl_fft 1.0.6 py37hd81dba3_0\r\n[conda] mkl_random 1.0.2 py37hd81dba3_0\r\n[conda] pytorch 1.0.1 py3.7_cuda10.0.130_cudnn7.4.2_2 pytorch\r\n\r\nhttps://discuss.pytorch.org/t/cannot-access-nn-linear-in-features-in-scriptmodule/42833","python\r\nimport torch.nn as nn\r\nimport torch.jit as jit\r\n\r\nclass TestModule(jit.ScriptModule):\r\n  def __init__(self):\r\n    super().__init__()\r\n    self.linear = nn.Linear(16, 16)\r\n\r\n\r\nm = TestModule()\r\nprint(m.linear.in_features)\r\n"
19351,"Printing gc.get_objects() makes Runtime Error when importing torch## \U0001f41b Bug\r\n\r\nI'd like to print the return of the `gc.get_objects()` function (I have memory issues) but I can't. Whenever I do it, I get the following error :\r\n\r\nBut what's more strange is that using the length are affecting it to a variable (not printing it) doesn't produce an error.\r\n\r\n## To Reproduce\r\n\r\nI'm using python3.7.0 and latest version of pytorch (1.0.1.post2). Just run the following code in a file :\r\n\r\n\r\n## Possible understanding\r\nI think there's a use of the `__file__` variable somewhere and according to [this](https://stackoverflow.com/a/41546830) answer of SO, if the application is frozen it just fails using the variable.\r\n\r\nHope you can help\r\n\r\nEdit : added more working examples\r\nEdit 2 : I conducted some experiments, and it seems it's only due to the printing part. I navigated in the files but don't understand where to look when I encounter torch._C",oncall: jit,suo|ailzhang,"## \U0001f41b Bug\r\n\r\nI'd like to print the return of the `gc.get_objects()` function (I have memory issues) but I can't. Whenever I do it, I get the following error :\r\n```python\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 5, in <module>\r\n    print(gc.get_objects())\r\n  File ""<frozen importlib._bootstrap>"", line 302, in _module_repr\r\n  File ""<frozen importlib._bootstrap>"", line 295, in _module_repr\r\n  File "".../site-packages/torch/_ops.py"", line 60, in __getattr__\r\n    op = torch._C._jit_get_operation(qualified_op_name)\r\nRuntimeError: No such operator __file__::__file__\r\n```\r\nBut what's more strange is that using the length are affecting it to a variable (not printing it) doesn't produce an error.\r\n\r\n## To Reproduce\r\n\r\nI'm using python3.7.0 and latest version of pytorch (1.0.1.post2). Just run the following code in a file :\r\n```python\r\nimport gc\r\nimport torch\r\nprint(len(gc.get_objects))  # Works\r\na = gc.get_objects()  # works\r\nprint(gc.get_objects())  # Fails\r\nfor obj in gc.get_objects():\r\n    print(obj)  # Works until encountering torch\r\n                    # Though it works on some files\r\nprint(sum(map(sys.getsizeof, gc.get_objects())))  # Works\r\nprint(collections.Counter(map(type, gc.get_objects())))  # Works\r\n```\r\n\r\n## Possible understanding\r\nI think there's a use of the `__file__` variable somewhere and according to [this](https://stackoverflow.com/a/41546830) answer of SO, if the application is frozen it just fails using the variable.\r\n\r\nHope you can help\r\n\r\nEdit : added more working examples\r\nEdit 2 : I conducted some experiments, and it seems it's only due to the printing part. I navigated in the files but don't understand where to look when I encounter torch._C","python\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 5, in <module>\r\n    print(gc.get_objects())\r\n  File ""<frozen importlib._bootstrap>"", line 302, in _module_repr\r\n  File ""<frozen importlib._bootstrap>"", line 295, in _module_repr\r\n  File "".../site-packages/torch/_ops.py"", line 60, in __getattr__\r\n    op = torch._C._jit_get_operation(qualified_op_name)\r\nRuntimeError: No such operator __file__::__file__\r\n"
19219,"Repeated torch.distributed.broadcast calls lead to OOM (with NCCL)## \U0001f41b Bug\r\n\r\nRepeated calls to `torch.distributed.broadcast` using the NCCL backend lead to out-of-memory errors, even when the ""live"" set of Tensors is small.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 11.93 GiB total capacity; 845.00 KiB already allocated; 56.00 MiB free; 11.52 GiB cached)\r\n```\r\n\r\n(On PyTorch master 4/12/2019 1c836e7b)\r\n\r\n## Suggested fix part 1\r\n\r\nIn addition to freeing the available cached blocks, the CUDA caching allocator should free all blocks that are held because they're used in multiple streams. One way to achieve this is to loop over all the events in `cuda_event`, synchronize on the event and `cudaFree` the block once its `event_count` reaches zero. This is effectively a modified version of `process_events`.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/1c836e7bb9eafdb743376d09ccbd88ceaad76b05/c10/cuda/CUDACachingAllocator.cpp#L463-L477\r\n\r\nhttps://github.com/pytorch/pytorch/blob/1c836e7bb9eafdb743376d09ccbd88ceaad76b05/c10/cuda/CUDACachingAllocator.cpp#L543\r\n\r\n## Suggested fix part 2 (needs discussion)\r\n\r\nThe above fix will avoid the out of memory error, but the tight loop around `dist.broadcast` will still use all available memory before freeing all cached blocks. This is not desirable. To avoid this, I think `torch.distributed.broadcast` should operate solely on the caller's current stream by default, and only use a background stream if specified.",high priority|oncall: distributed|module: cuda|triaged,colesbury,"## \U0001f41b Bug\r\n\r\nRepeated calls to `torch.distributed.broadcast` using the NCCL backend lead to out-of-memory errors, even when the ""live"" set of Tensors is small.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\ndef worker(rank):\r\n    for itr in range(1000):\r\n        x = torch.randn(int(25 * 1024 * 1024), device='cuda')  # 25 MiB\r\n        dist.broadcast(x, src=1, async_op=False)\r\n        del x\r\n\r\ndef main(rank, init_method, world_size):\r\n    torch.cuda.set_device(rank)\r\n    dist.init_process_group(""nccl"", init_method, rank=rank, world_size=world_size)\r\n    worker(rank)\r\n\r\nif __name__ == '__main__':\r\n    init_method = 'tcp://127.0.0.1:23123'\r\n    world_size = 2\r\n    torch.multiprocessing.spawn(main, (init_method, world_size), nprocs=world_size)\r\n```\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 11.93 GiB total capacity; 845.00 KiB already allocated; 56.00 MiB free; 11.52 GiB cached)\r\n```\r\n\r\n(On PyTorch master 4/12/2019 1c836e7b)\r\n\r\n## Suggested fix part 1\r\n\r\nIn addition to freeing the available cached blocks, the CUDA caching allocator should free all blocks that are held because they're used in multiple streams. One way to achieve this is to loop over all the events in `cuda_event`, synchronize on the event and `cudaFree` the block once its `event_count` reaches zero. This is effectively a modified version of `process_events`.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/1c836e7bb9eafdb743376d09ccbd88ceaad76b05/c10/cuda/CUDACachingAllocator.cpp#L463-L477\r\n\r\nhttps://github.com/pytorch/pytorch/blob/1c836e7bb9eafdb743376d09ccbd88ceaad76b05/c10/cuda/CUDACachingAllocator.cpp#L543\r\n\r\n## Suggested fix part 2 (needs discussion)\r\n\r\nThe above fix will avoid the out of memory error, but the tight loop around `dist.broadcast` will still use all available memory before freeing all cached blocks. This is not desirable. To avoid this, I think `torch.distributed.broadcast` should operate solely on the caller's current stream by default, and only use a background stream if specified.","python\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\ndef worker(rank):\r\n    for itr in range(1000):\r\n        x = torch.randn(int(25 * 1024 * 1024), device='cuda')  # 25 MiB\r\n        dist.broadcast(x, src=1, async_op=False)\r\n        del x\r\n\r\ndef main(rank, init_method, world_size):\r\n    torch.cuda.set_device(rank)\r\n    dist.init_process_group(""nccl"", init_method, rank=rank, world_size=world_size)\r\n    worker(rank)\r\n\r\nif __name__ == '__main__':\r\n    init_method = 'tcp://127.0.0.1:23123'\r\n    world_size = 2\r\n    torch.multiprocessing.spawn(main, (init_method, world_size), nprocs=world_size)\r\n"
19122,"[feature request] torch.trapz or some other method for numerical integration similar to numpynumpy and matlab both have a trapz (and a simps) function, which makes it convenient to integrate y values over some x values. It seems pytorch is lacking this convenient function (even though it has other functions like cumsum). It would be great if we could have an implementation of trapz or simps similar to [numpy's](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.trapz.html). \r\n\r\nOne simple motivation is that pytorch is desirable as a significantly faster alternative to numpy, and trapz is a standard and necessary feature in that library.\r\n\r\nOne main concrete example of its use is for [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) when using a non-standard distribution. \r\n\r\ntrapz (and simps) should also have a very simple implementation as well. For the 1D case, it's as simple as:\r\n",feature|triaged|module: numpy,umanwizard,"numpy and matlab both have a trapz (and a simps) function, which makes it convenient to integrate y values over some x values. It seems pytorch is lacking this convenient function (even though it has other functions like cumsum). It would be great if we could have an implementation of trapz or simps similar to [numpy's](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.trapz.html). \r\n\r\nOne simple motivation is that pytorch is desirable as a significantly faster alternative to numpy, and trapz is a standard and necessary feature in that library.\r\n\r\nOne main concrete example of its use is for [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) when using a non-standard distribution. \r\n\r\ntrapz (and simps) should also have a very simple implementation as well. For the 1D case, it's as simple as:\r\n```python\r\ndef trapz(y, x=None, dx=1.0):\r\n  y_avg = (y[1:] + y[:-1])/2\r\n  if not (x is None):\r\n    dx = x[1:] - x[:-1]\r\n  return sum(y_avg*dx)\r\n```","python\r\ndef trapz(y, x=None, dx=1.0):\r\n  y_avg = (y[1:] + y[:-1])/2\r\n  if not (x is None):\r\n    dx = x[1:] - x[:-1]\r\n  return sum(y_avg*dx)\r\n"
19064,"Compiling from master yields std::runtime_errorI recompiled PyTorch to match some changes in our internal infrastructure and since, every attempt to import torch yields to the following error message:\r\n\r\n\r\n\r\nCompilation done on commit : 72e171d\r\nOS : CentOS Linux release 7.6.1810 (Core)\r\nCC: GCC 4.8.5\r\nCuda: 10.1\r\nCuDNN: 7.5.0 ",high priority|oncall: distributed|module: pybind,pietern|mrshenli,"I recompiled PyTorch to match some changes in our internal infrastructure and since, every attempt to import torch yields to the following error message:\r\n\r\n```bash\r\npython main.py --debug --fp16 --cuda --cudnn\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  arg(): could not convert default argument 'timeout: std::chrono::duration<long, std::ratio<1l, 1000l> >' in method '<class 'torch.distributed.ProcessGroupGloo'>.__init__' into a Python object (type not registered yet?)\r\nAborted\r\n```\r\n\r\nCompilation done on commit : 72e171d\r\nOS : CentOS Linux release 7.6.1810 (Core)\r\nCC: GCC 4.8.5\r\nCuda: 10.1\r\nCuDNN: 7.5.0 ","bash\r\npython main.py --debug --fp16 --cuda --cudnn\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  arg(): could not convert default argument 'timeout: std::chrono::duration<long, std::ratio<1l, 1000l> >' in method '<class 'torch.distributed.ProcessGroupGloo'>.__init__' into a Python object (type not registered yet?)\r\nAborted\r\n"
19037,"Differentiable 1-D, 2-D covariance (numpy.cov) clone## \U0001f680 Feature\r\nA differentiable way to calculate covariance for a tensor of random variables similar to `numpy.cov`.\r\n\r\n## Motivation\r\nSometimes the covariance of the data can be used as a norm (as opposed to the implicit identity matrix in the standard x'y norm we have x'Sy where S is the covariance matrix, for example between x and y). It would be good to have a differentiable way of calculating these to allow backpropagation.\r\n\r\n## Pitch\r\nHere is my partial clone of `numpy.cov` that's tested with `pytest` against what `numpy.cov` finds.\r\n\r\n\r\nTesting\r\n\r\n\r\n## Alternatives\r\nI don't think there's an existing method to compute the covariance in pytorch - are there alternatives? And is this an OK format for computation (instead of a forward and backward method)?\r\n\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @mruberry @rgommers @heitorschueroff",high priority|triaged|enhancement|module: numpy,heitorschueroff,"## \U0001f680 Feature\r\nA differentiable way to calculate covariance for a tensor of random variables similar to `numpy.cov`.\r\n\r\n## Motivation\r\nSometimes the covariance of the data can be used as a norm (as opposed to the implicit identity matrix in the standard x'y norm we have x'Sy where S is the covariance matrix, for example between x and y). It would be good to have a differentiable way of calculating these to allow backpropagation.\r\n\r\n## Pitch\r\nHere is my partial clone of `numpy.cov` that's tested with `pytest` against what `numpy.cov` finds.\r\n```python\r\ndef cov(x, rowvar=False, bias=False, ddof=None, aweights=None):\r\n    """"""Estimates covariance matrix like numpy.cov""""""\r\n    # ensure at least 2D\r\n    if x.dim() == 1:\r\n        x = x.view(-1, 1)\r\n\r\n    # treat each column as a data point, each row as a variable\r\n    if rowvar and x.shape[0] != 1:\r\n        x = x.t()\r\n\r\n    if ddof is None:\r\n        if bias == 0:\r\n            ddof = 1\r\n        else:\r\n            ddof = 0\r\n\r\n    w = aweights\r\n    if w is not None:\r\n        if not torch.is_tensor(w):\r\n            w = torch.tensor(w, dtype=torch.float)\r\n        w_sum = torch.sum(w)\r\n        avg = torch.sum(x * (w/w_sum)[:,None], 0)\r\n    else:\r\n        avg = torch.mean(x, 0)\r\n\r\n    # Determine the normalization\r\n    if w is None:\r\n        fact = x.shape[0] - ddof\r\n    elif ddof == 0:\r\n        fact = w_sum\r\n    elif aweights is None:\r\n        fact = w_sum - ddof\r\n    else:\r\n        fact = w_sum - ddof * torch.sum(w * w) / w_sum\r\n\r\n    xm = x.sub(avg.expand_as(x))\r\n\r\n    if w is None:\r\n        X_T = xm.t()\r\n    else:\r\n        X_T = torch.mm(torch.diag(w), xm).t()\r\n\r\n    c = torch.mm(X_T, xm)\r\n    c = c / fact\r\n\r\n    return c.squeeze()\r\n```\r\n\r\nTesting\r\n\r\n```python\r\ndef assert_same_cov(A, w=None):\r\n    c1 = np.cov(A, rowvar=False, aweights=w)\r\n    c2 = cov(torch.tensor(A, dtype=torch.float), aweights=w)\r\n    assert np.linalg.norm(c2.numpy() - c1) < 1e-6\r\n\r\n\r\ndef test_cov():\r\n    a = [1, 2, 3, 4]\r\n    assert_same_cov(a)\r\n    A = [[1, 2], [3, 4]]\r\n    assert_same_cov(A)\r\n\r\n    assert_same_cov(a, w=[1, 1, 1, 1])\r\n    assert_same_cov(a, w=[2, 0.5, 3, 1])\r\n\r\n    assert_same_cov(A, w=[1, 1])\r\n    assert_same_cov(A, w=[2, 0.5])\r\n```\r\n## Alternatives\r\nI don't think there's an existing method to compute the covariance in pytorch - are there alternatives? And is this an OK format for computation (instead of a forward and backward method)?\r\n\n\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @mruberry @rgommers @heitorschueroff","python\r\ndef cov(x, rowvar=False, bias=False, ddof=None, aweights=None):\r\n    """"""Estimates covariance matrix like numpy.cov""""""\r\n    # ensure at least 2D\r\n    if x.dim() == 1:\r\n        x = x.view(-1, 1)\r\n\r\n    # treat each column as a data point, each row as a variable\r\n    if rowvar and x.shape[0] != 1:\r\n        x = x.t()\r\n\r\n    if ddof is None:\r\n        if bias == 0:\r\n            ddof = 1\r\n        else:\r\n            ddof = 0\r\n\r\n    w = aweights\r\n    if w is not None:\r\n        if not torch.is_tensor(w):\r\n            w = torch.tensor(w, dtype=torch.float)\r\n        w_sum = torch.sum(w)\r\n        avg = torch.sum(x * (w/w_sum)[:,None], 0)\r\n    else:\r\n        avg = torch.mean(x, 0)\r\n\r\n    # Determine the normalization\r\n    if w is None:\r\n        fact = x.shape[0] - ddof\r\n    elif ddof == 0:\r\n        fact = w_sum\r\n    elif aweights is None:\r\n        fact = w_sum - ddof\r\n    else:\r\n        fact = w_sum - ddof * torch.sum(w * w) / w_sum\r\n\r\n    xm = x.sub(avg.expand_as(x))\r\n\r\n    if w is None:\r\n        X_T = xm.t()\r\n    else:\r\n        X_T = torch.mm(torch.diag(w), xm).t()\r\n\r\n    c = torch.mm(X_T, xm)\r\n    c = c / fact\r\n\r\n    return c.squeeze()\r\n"
19030,"Support __get_state__/__set_state__ for JIT serialization## \U0001f680 Feature\r\nFor custom C classes or for complex modules in JIT, there needs to be ability to supply customer serialization code.\r\n\r\n## Motivation\r\n\r\nWe want to support custom opaque representations with the ability to serialize them. This is pretty important for opaque optimized layouts in inference (e.g. when using fbgemm), currently we use a hack with pack/unpack: https://github.com/pytorch/pytorch/blob/master/torch/jit/quantized.py#L27\r\n\r\nThis functionality will also be useful for custom C class bindings which would need to be serialized as well.\r\n\r\n## Pitch\r\n\r\nProposal is to implement get_state/set_state similarly how they are handled in pickle: one can return any value (`IValue`) that will be serialized and later passed to `__set_state__` for deserialization.\r\n\r\n\r\n\r\n## Alternatives\r\n\r\nInstead of following Pickle style we can define some other interface for it",high priority|oncall: jit|feature,driazati,"## \U0001f680 Feature\r\nFor custom C classes or for complex modules in JIT, there needs to be ability to supply customer serialization code.\r\n\r\n## Motivation\r\n\r\nWe want to support custom opaque representations with the ability to serialize them. This is pretty important for opaque optimized layouts in inference (e.g. when using fbgemm), currently we use a hack with pack/unpack: https://github.com/pytorch/pytorch/blob/master/torch/jit/quantized.py#L27\r\n\r\nThis functionality will also be useful for custom C class bindings which would need to be serialized as well.\r\n\r\n## Pitch\r\n\r\nProposal is to implement get_state/set_state similarly how they are handled in pickle: one can return any value (`IValue`) that will be serialized and later passed to `__set_state__` for deserialization.\r\n\r\n```python\r\nclass MyLayerPacked(torch.nn.Module):\r\n    # same as before\r\n    def __init__(self, original: MyLayer):\r\n        self.packed_weight = self.__pack__(original.weight) # ByteTensor with pointer\r\n    def forward(self, x):\r\n        return torch.my_mm_fast(self.packed_weight, x)\r\n    # it has to be script method to work in C++\r\n    @torch.jit.script_method\r\n    def __getstate__(self):\r\n        return {'weight': torch.unpack_from_my_mm(self.packed_weight)}\r\n    def __setstate__(self, state):\r\n        # attribute mutability is required here\r\n        self.packed_weight = torch.pack_for_my_mm(state['weight'])\r\n```\r\n\r\n## Alternatives\r\n\r\nInstead of following Pickle style we can define some other interface for it","python\r\nclass MyLayerPacked(torch.nn.Module):\r\n    # same as before\r\n    def __init__(self, original: MyLayer):\r\n        self.packed_weight = self.__pack__(original.weight) # ByteTensor with pointer\r\n    def forward(self, x):\r\n        return torch.my_mm_fast(self.packed_weight, x)\r\n    # it has to be script method to work in C++\r\n    @torch.jit.script_method\r\n    def __getstate__(self):\r\n        return {'weight': torch.unpack_from_my_mm(self.packed_weight)}\r\n    def __setstate__(self, state):\r\n        # attribute mutability is required here\r\n        self.packed_weight = torch.pack_for_my_mm(state['weight'])\r\n"
18987,"Performance issue of unique on CPURelated: https://github.com/pytorch/pytorch/issues/15804 https://github.com/pytorch/pytorch/issues/18405 https://github.com/pytorch/pytorch/pull/16145 https://github.com/pytorch/pytorch/pull/18648\r\n\r\nThe performance on GPU has been fully resolved.\r\n\r\nNewest benchmark on CPU on a tensor of shape `torch.Size([15320, 2])`:\r\n\r\n\r\n\r\n```\r\n1.1.0a0+13f03a4\r\n60.5 ms \xb1 115 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.5 ms \xb1 150 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.7 ms \xb1 170 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.7 ms \xb1 217 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n\r\n\r\n```\r\n1.1.0a0+13f03a4\r\n308 \xb5s \xb1 690 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n906 \xb5s \xb1 769 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n60 ms \xb1 96.9 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.2 ms \xb1 254 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\n\ncc @VitalyFedyunin @ngimel",module: performance|module: cpu|triaged|module: sorting and selection,zasdfgbnm,"Related: https://github.com/pytorch/pytorch/issues/15804 https://github.com/pytorch/pytorch/issues/18405 https://github.com/pytorch/pytorch/pull/16145 https://github.com/pytorch/pytorch/pull/18648\r\n\r\nThe performance on GPU has been fully resolved.\r\n\r\nNewest benchmark on CPU on a tensor of shape `torch.Size([15320, 2])`:\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True)\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n\r\n```\r\n1.1.0a0+13f03a4\r\n60.5 ms \xb1 115 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.5 ms \xb1 150 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.7 ms \xb1 170 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.7 ms \xb1 217 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(sorted=True, return_inverse=False)\r\n%timeit a.unique(sorted=True, return_inverse=True)\r\n%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n\r\n```\r\n1.1.0a0+13f03a4\r\n308 \xb5s \xb1 690 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n906 \xb5s \xb1 769 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n60 ms \xb1 96.9 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n60.2 ms \xb1 254 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n```\n\ncc @VitalyFedyunin @ngimel","python\r\nprint(torch.__version__)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True)\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=True, return_counts=True)\r\n"
18945,"binary_cross_entropy does not implement double-backwards## \U0001f680 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nImplement the double backwards for `binary_cross_entropy`.\r\n\r\n## Motivation\r\n\r\nMost of the losses support double-backwards, but `binary_cross_entropy` seems not to support it.\r\n\r\n## Pitch\r\n\r\n\r\nraises\r\n\r\n\r\n## Alternatives\r\n\r\nNote that `binary_cross_entropy` can be very easily implemented in Python as follows:\r\n\r\nwhich enables double-backprop.\r\n\r\nA simple way of implementing it would be to just add an entry for the double-backwards in https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/auto_double_backwards.py.\r\n\r\nAnother (potentially desired) way would involve moving the loss to ATen, and writing the double-backwards as well.\r\n\r\nOriginally reported by @f-t-s",module: double backwards|module: nn|triaged|enhancement,gchanan|anjali411,"## \U0001f680 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nImplement the double backwards for `binary_cross_entropy`.\r\n\r\n## Motivation\r\n\r\nMost of the losses support double-backwards, but `binary_cross_entropy` seems not to support it.\r\n\r\n## Pitch\r\n\r\n```python\r\nx = torch.rand(10, 4, requires_grad=True)\r\ny = torch.rand((10, 4))\r\nloss = F.binary_cross_entropy(x, y)\r\nloss2 = torch.autograd.grad(loss, x, create_graph=True)\r\n\r\nloss2[0].sum().backward()\r\n```\r\nraises\r\n```python\r\nRuntimeError                              Traceback (most recent call last)\r\n~/tst7.py in <module>()\r\n----> 1 a[0].sum().backward()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n    105                 products. Defaults to ``False``.\r\n    106         """"""\r\n--> 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n    108\r\n    109     def register_hook(self, hook):\r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     91     Variable._execution_engine.run_backward(\r\n     92         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 93         allow_unreachable=True)  # allow_unreachable flag\r\n     94\r\n     95\r\n\r\nRuntimeError: derivative for _thnn_binary_cross_entropy_backward is not implemented\r\n```\r\n\r\n## Alternatives\r\n\r\nNote that `binary_cross_entropy` can be very easily implemented in Python as follows:\r\n```python\r\ndef binary_cross_entropy(x, y):\r\n    loss = -(x.log() * y + (1 - x).log() * (1 - y))\r\n    return loss.mean()\r\n```\r\nwhich enables double-backprop.\r\n\r\nA simple way of implementing it would be to just add an entry for the double-backwards in https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/auto_double_backwards.py.\r\n\r\nAnother (potentially desired) way would involve moving the loss to ATen, and writing the double-backwards as well.\r\n\r\nOriginally reported by @f-t-s","python\r\nx = torch.rand(10, 4, requires_grad=True)\r\ny = torch.rand((10, 4))\r\nloss = F.binary_cross_entropy(x, y)\r\nloss2 = torch.autograd.grad(loss, x, create_graph=True)\r\n\r\nloss2[0].sum().backward()\r\n"
18858,[jit] Iteration over dictionariesThis should work\r\n\r\n\r\n\r\n\r\n\r\nWe also should start out with a conservative approach to [iterator invalidation](https://en.cppreference.com/w/cpp/container/unordered_map) and disallow editing the map during iteration.,oncall: jit|feature,driazati,"This should work\r\n\r\n```python\r\n@torch.jit.script\r\ndef fn():\r\n    a_dict = {'a': 1, 'b': 2, 'c': 3}\r\n    sum = 0\r\n    for key in a_dict:\r\n      sum += a_dict[key]\r\n    return sum\r\n```\r\n\r\n\r\n\r\nWe also should start out with a conservative approach to [iterator invalidation](https://en.cppreference.com/w/cpp/container/unordered_map) and disallow editing the map during iteration.","python\r\n@torch.jit.script\r\ndef fn():\r\n    a_dict = {'a': 1, 'b': 2, 'c': 3}\r\n    sum = 0\r\n    for key in a_dict:\r\n      sum += a_dict[key]\r\n    return sum\r\n"
18850,"Allow tracing of models which output `None`## \U0001f680 Feature\r\nAllow tracing of models which output tuples with some attributes equal to `None`\r\n\r\n## Motivation\r\nIn big complex models, the forward pass should be able to output different information, depending on whether the model is being trained or runs in inference (test) mode. In test mode (the one which usually gets traced) some training outputs can be possibly missing. For example, in RL actor-critic methods, during training I want to visualize/plot the output of the critic. However, at test time, I want to be able to perform inference as fast as possible, which requires running the forward pass only of the actor but not of the critic, and return only the actor output.\r\n\r\n## Pitch\r\n\r\n\r\nAt the moment, `torch.jit.trace(module, (torch.zeros(), True))` will fail, because `critic_out` will be `None` and tracing will fail.\r\n\r\n## Alternatives\r\nReturn tuples of different size all the time. It's too scary to image the ugliness of the resulting code, especially with large models\r\n\r\n## Additional context\r\nApplicable to any deeplearning scenario.\n\ncc @suo",oncall: jit|feature|triaged,zdevito,"## \U0001f680 Feature\r\nAllow tracing of models which output tuples with some attributes equal to `None`\r\n\r\n## Motivation\r\nIn big complex models, the forward pass should be able to output different information, depending on whether the model is being trained or runs in inference (test) mode. In test mode (the one which usually gets traced) some training outputs can be possibly missing. For example, in RL actor-critic methods, during training I want to visualize/plot the output of the critic. However, at test time, I want to be able to perform inference as fast as possible, which requires running the forward pass only of the actor but not of the critic, and return only the actor output.\r\n\r\n## Pitch\r\n```python\r\nclass ActorCriticModule(torch.nn.Module):\r\n  def __init__(self):\r\n    self.actor = torch.nn.Sequential(...)\r\n    self.critic = torch.nn.Sequential(...)\r\n\r\n  def forward(self, x, test_mode: bool) -> tuple:\r\n    if test_mode:\r\n      critic_out  = None\r\n    else:\r\n      critic_out = self.critic(x)\r\n    actor_out = self.actor(x)\r\n    return actor_out, critic_out\r\n\r\nmodule = ActorCriticModule()\r\ntorch.jit.trace(module, (torch.zeros(), True))\r\n```\r\n\r\nAt the moment, `torch.jit.trace(module, (torch.zeros(), True))` will fail, because `critic_out` will be `None` and tracing will fail.\r\n\r\n## Alternatives\r\nReturn tuples of different size all the time. It's too scary to image the ugliness of the resulting code, especially with large models\r\n\r\n## Additional context\r\nApplicable to any deeplearning scenario.\n\ncc @suo","python\r\nclass ActorCriticModule(torch.nn.Module):\r\n  def __init__(self):\r\n    self.actor = torch.nn.Sequential(...)\r\n    self.critic = torch.nn.Sequential(...)\r\n\r\n  def forward(self, x, test_mode: bool) -> tuple:\r\n    if test_mode:\r\n      critic_out  = None\r\n    else:\r\n      critic_out = self.critic(x)\r\n    actor_out = self.actor(x)\r\n    return actor_out, critic_out\r\n\r\nmodule = ActorCriticModule()\r\ntorch.jit.trace(module, (torch.zeros(), True))\r\n"
18825,"Wrong derivative backpropagating through Cholesky factorization?## \U0001f41b Bug\r\nI am getting different derivatives when I compute `g(A)` via `A -> g(A)` versus `A -> LL' -> g(LL')`. The derivatives are off in a predictable way, and it's easy to correct (~2 lines of code), leading me to believe this is a bug.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\nCalling `inv_mat2.trace().backward()` should produce `corrected_deriv` in `mat.grad`, not what is currently going there.",high priority|triaged|module: derivatives,vishwakftw,"## \U0001f41b Bug\r\nI am getting different derivatives when I compute `g(A)` via `A -> g(A)` versus `A -> LL' -> g(LL')`. The derivatives are off in a predictable way, and it's easy to correct (~2 lines of code), leading me to believe this is a bug.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\n\r\nmat = torch.randn(4, 4, dtype=torch.float64)\r\nmat = (mat @ mat.transpose(-1, -2)).div_(5).add_(torch.eye(4, dtype=torch.float64))\r\nmat = mat.detach().clone().requires_grad_(True)\r\nmat_clone = mat.detach().clone().requires_grad_(True)\r\n\r\n# Way 1\r\ninv_mat1 = mat_clone.inverse()  # A^{-1} = A^{-1}\r\n\r\n# Way 2\r\nchol_mat = mat.cholesky()\r\nchol_inv_mat = chol_mat.inverse().transpose(-2, -1)\r\ninv_mat2 = chol_inv_mat @ chol_inv_mat.transpose(-2, -1)  # A^{-1} = L^{-T}L^{-1}\r\n\r\n# True\r\nprint('Are these both A^{-1}?', bool(torch.norm(inv_mat1 - inv_mat2) < 1e-8))\r\n\r\ninv_mat1.trace().backward()\r\ninv_mat2.trace().backward()\r\n\r\nprint('Way 1\\n', mat_clone.grad)\r\nprint('Way 2\\n', mat.grad)  # :-(\r\n\r\ncorrected_deriv = mat.grad.clone() / 2\r\ncorrected_deriv = corrected_deriv.tril() + corrected_deriv.tril().t()\r\nprint('Corrected derivative\\n', corrected_deriv)  # Simple correction to derivative works.\r\n\r\n# True\r\nprint('Is the corrected derivative correct?', bool(torch.norm(corrected_deriv - mat_clone.grad) < 1e-8))\r\n```\r\n\r\n## Expected behavior\r\nCalling `inv_mat2.trace().backward()` should produce `corrected_deriv` in `mat.grad`, not what is currently going there.","python\r\nimport torch\r\n\r\nmat = torch.randn(4, 4, dtype=torch.float64)\r\nmat = (mat @ mat.transpose(-1, -2)).div_(5).add_(torch.eye(4, dtype=torch.float64))\r\nmat = mat.detach().clone().requires_grad_(True)\r\nmat_clone = mat.detach().clone().requires_grad_(True)\r\n\r\n# Way 1\r\ninv_mat1 = mat_clone.inverse()  # A^{-1} = A^{-1}\r\n\r\n# Way 2\r\nchol_mat = mat.cholesky()\r\nchol_inv_mat = chol_mat.inverse().transpose(-2, -1)\r\ninv_mat2 = chol_inv_mat @ chol_inv_mat.transpose(-2, -1)  # A^{-1} = L^{-T}L^{-1}\r\n\r\n# True\r\nprint('Are these both A^{-1}?', bool(torch.norm(inv_mat1 - inv_mat2) < 1e-8))\r\n\r\ninv_mat1.trace().backward()\r\ninv_mat2.trace().backward()\r\n\r\nprint('Way 1\\n', mat_clone.grad)\r\nprint('Way 2\\n', mat.grad)  # :-(\r\n\r\ncorrected_deriv = mat.grad.clone() / 2\r\ncorrected_deriv = corrected_deriv.tril() + corrected_deriv.tril().t()\r\nprint('Corrected derivative\\n', corrected_deriv)  # Simple correction to derivative works.\r\n\r\n# True\r\nprint('Is the corrected derivative correct?', bool(torch.norm(corrected_deriv - mat_clone.grad) < 1e-8))\r\n"
18796,"[Feature Request] Flattened indices option for max pooling## \U0001f680 Feature\r\nOption to return flattened indices for max pooling layers.\r\n\r\n## Motivation\r\nSome operations require to have access to the flattened indices that ONNX returns initially. For instance, let's consider this vision example: I am working on a layer that counts how many times the element (pixel) at position `i` was the winner (selected as maximum) in a max pooling 2d operation.\r\n\r\nAs things are right now, this is the solution:\r\n\r\n\r\nThis implementation is very slow. The overhead introduced by running this for-loop could be avoided if flattened indices were returned. This is how the code would look like then:\r\n\r\n\r\nThe indices for max pooling 2d are currently referencing local frames, non-flattened. See [this issue](https://github.com/pytorch/pytorch/pull/16455#issuecomment-460776407) for a clearer picture of what this means.\r\n\r\n## Pitch\r\nThis feature would allow to return flattened indices, in the same way as [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax) does.\r\n\r\n## Alternatives\r\n1. To implement `apply_along_axis`. This is a draft showing how it would solve my problem:\r\n\r\n2. To add a frequency counter that takes the indices returned by max pooling 2d (for instance) and sets in the location of the tensor corresponding to each index, the amount of times it was encountered in the returned indices.\r\n\r\n## Additional context\r\nIn short, what I am trying to implement is the following:\r\n\r\n![CodeCogsEqn (2)](https://user-images.githubusercontent.com/7946422/55476426-d1c30900-5616-11e9-85ba-4ae3f2a91677.gif)\r\nwhere C contains the unfolded set of kernel windows.\r\n\r\nIn words: for each kernel window `c`, it adds `1` in the output at the location of the winner, `1[argmax c]`.\r\n\n\ncc @albanD @mruberry @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof",module: nn|triaged|enhancement|module: pooling,fmassa,"## \U0001f680 Feature\r\nOption to return flattened indices for max pooling layers.\r\n\r\n## Motivation\r\nSome operations require to have access to the flattened indices that ONNX returns initially. For instance, let's consider this vision example: I am working on a layer that counts how many times the element (pixel) at position `i` was the winner (selected as maximum) in a max pooling 2d operation.\r\n\r\nAs things are right now, this is the solution:\r\n```python\r\ndef forward(self, input):\r\n    _, indices = self.maxpool2d(input)\r\n    N, C, H, W = input.size()\r\n    minlength = H * W\r\n    Nout, Cout, Hout, Wout = indices.size()\r\n    indices_count = [t.bincount(minlength=minlength) for t in indices.view(-1, Hout * Wout).unbind()]\r\n    return torch.stack(indices_count).view(input.size())\r\n```\r\n\r\nThis implementation is very slow. The overhead introduced by running this for-loop could be avoided if flattened indices were returned. This is how the code would look like then:\r\n```python\r\ndef forward(self, input):\r\n    input_flattened_length, input_shape = input.nelement(), input.size()\r\n    _, indices = self.maxpool2d(input)\r\n    return indices.view(-1).bincount(minlength=input_flattened_length).view(input_shape)\r\n```\r\n\r\nThe indices for max pooling 2d are currently referencing local frames, non-flattened. See [this issue](https://github.com/pytorch/pytorch/pull/16455#issuecomment-460776407) for a clearer picture of what this means.\r\n\r\n## Pitch\r\nThis feature would allow to return flattened indices, in the same way as [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax) does.\r\n\r\n## Alternatives\r\n1. To implement `apply_along_axis`. This is a draft showing how it would solve my problem:\r\n```python\r\ndef forward(self, input):\r\n    _, indices = self.maccpool2d(-input)\r\n    N, C, H, W = input.size()\r\n    minlength = H * W\r\n    Nout, Cout, Hout, Wout = indices.size()\r\n    r = np.apply_along_axis(lambda x: np.bincount(x, minlength=minlength), axis=-1, arr=indices.view(N, C, -1).numpy())\r\n    return torch.from_numpy(r).view(input.size())\r\n```\r\n2. To add a frequency counter that takes the indices returned by max pooling 2d (for instance) and sets in the location of the tensor corresponding to each index, the amount of times it was encountered in the returned indices.\r\n\r\n## Additional context\r\nIn short, what I am trying to implement is the following:\r\n\r\n![CodeCogsEqn (2)](https://user-images.githubusercontent.com/7946422/55476426-d1c30900-5616-11e9-85ba-4ae3f2a91677.gif)\r\nwhere C contains the unfolded set of kernel windows.\r\n\r\nIn words: for each kernel window `c`, it adds `1` in the output at the location of the winner, `1[argmax c]`.\r\n\n\ncc @albanD @mruberry @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof","python\r\ndef forward(self, input):\r\n    _, indices = self.maxpool2d(input)\r\n    N, C, H, W = input.size()\r\n    minlength = H * W\r\n    Nout, Cout, Hout, Wout = indices.size()\r\n    indices_count = [t.bincount(minlength=minlength) for t in indices.view(-1, Hout * Wout).unbind()]\r\n    return torch.stack(indices_count).view(input.size())\r\n"
18776,"Value of torch.backends.cudnn.benchmark Baked into JIT-Traced Modules ( 150x slowdown on ConvTranspose2d() ) [jit] [libtorch] [cudnn] ## \U0001f41b Bug\r\n\r\nIf you trace a module with `torch.jit.trace(...)` and load that script module in C++ via LibTorch, the resulting behavior in C++ depends on whether or not the `torch.backends.cudnn.benchmark` flag was set. Calls to `at::globalContext().setBenchmarkCuDNN(true/false)` from the C++ API at runtime appear to have no effect.\r\n\r\n## To Reproduce\r\n\r\n**NOTE**: I was not able to verify this issue still exists on the latest nightly (20190402) because it appears the latest nightly (at least on Windows) cannot run JIT-traced models. Even the simplest model gives the following error:\r\n\r\n```\r\nINVALID_ARGUMENT:: Cannot find field. (deserialize at ..\\torch\\csrc\\jit\\import.cpp:108)\r\n(no backtrace available)\r\n```\r\n\r\n1) Run the python script below: `python test.py 0` or `python test.py 1`\r\n2) Compile + run the C++ code below.\r\n3) Observe:\r\n  a) Average time per call. I see ~0.8ms in the python script and either ~0.8 or ~120ms in C++ depending on the flag used in python. In either case, C++ sets benchmarking ON. (GTX 1080)\r\n  b) Kernel run by CuDNN. w/either setting of the flag, the python code runs `cudnn::detail::dgrad_engine<...>`. With the flag ON, it runs `cudnn::detail::dgrad2d_alg1_1<...>` once (taking ~120ms) and then chooses the faster `dgrad_engine`. If the flag was ON in python, C++ also chooses `dgrad_engine` but if the flag was OFF in python, it always chooses `dgrad2d_alg1_1` regardless of the flag setting in C++.\r\n\r\nI observed the choice of kernel using `nvprof python test.py 0/1`.\r\n\r\nPython Script (`test.py`):\r\n\r\n\r\nC++ Code:\r\n\r\n\r\n## Expected Behavior\r\n\r\nI would expect that either: 1) the C++ setting of `at::globalContext().setBenchmarkCuDNN(true)` should be respected (choosing the correct algorithm) or 2) at least print a warning that it is being overridden by the value of the flag at trace time.\r\n\r\n## Additional Info\r\nI printed the JIT graphs generated with benchmarking ON/OFF and got the following with the flag OFF:\r\n```\r\ngraph(%input : Float(1, 8, 512, 512),\r\n      %1 : Float(8, 3, 4, 4),\r\n      %2 : Float(3)):\r\n  %3 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %4 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %5 : int[] = prim::ListConstruct(%3, %4), scope: ConvTranspose2d\r\n  %6 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %7 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %8 : int[] = prim::ListConstruct(%6, %7), scope: ConvTranspose2d\r\n  %9 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %10 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %11 : int[] = prim::ListConstruct(%9, %10), scope: ConvTranspose2d\r\n  %12 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %13 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %14 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %15 : int[] = prim::ListConstruct(%13, %14), scope: ConvTranspose2d\r\n  %16 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %17 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %18 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %19 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %20 : Float(1, 3, 1024, 1024) = aten::_convolution(%input, %1, %2, %5, %8, %11, %12, %15, %16, %17, %18, %19), scope: ConvTranspose2d\r\n  return (%20)\r\n```\r\n\r\nThe only change when the flag is ON is that register %17 is 1 instead of 0. I suppose this is where the ""hardcoding"" of the flag might be happening?\r\n\r\n## Environment\r\nPython code was run on Linux, C++ code was run on Windows\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20190311 on linux, 2336f0ba0 on Windows\r\n - OS (e.g., Linux): Fedora 29, Windows 10 1809\r\n - How you installed PyTorch (`conda`, `pip`, source): conda (pytorch-nightly)\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: CUDA 10, cuDNN 7.4.2\r\n - GPU models and configuration: Titan RTX (linux), GTX 1080 (windows)\n\ncc @suo",oncall: jit|triaged,bzinodev,"## \U0001f41b Bug\r\n\r\nIf you trace a module with `torch.jit.trace(...)` and load that script module in C++ via LibTorch, the resulting behavior in C++ depends on whether or not the `torch.backends.cudnn.benchmark` flag was set. Calls to `at::globalContext().setBenchmarkCuDNN(true/false)` from the C++ API at runtime appear to have no effect.\r\n\r\n## To Reproduce\r\n\r\n**NOTE**: I was not able to verify this issue still exists on the latest nightly (20190402) because it appears the latest nightly (at least on Windows) cannot run JIT-traced models. Even the simplest model gives the following error:\r\n\r\n```\r\nINVALID_ARGUMENT:: Cannot find field. (deserialize at ..\\torch\\csrc\\jit\\import.cpp:108)\r\n(no backtrace available)\r\n```\r\n\r\n1) Run the python script below: `python test.py 0` or `python test.py 1`\r\n2) Compile + run the C++ code below.\r\n3) Observe:\r\n  a) Average time per call. I see ~0.8ms in the python script and either ~0.8 or ~120ms in C++ depending on the flag used in python. In either case, C++ sets benchmarking ON. (GTX 1080)\r\n  b) Kernel run by CuDNN. w/either setting of the flag, the python code runs `cudnn::detail::dgrad_engine<...>`. With the flag ON, it runs `cudnn::detail::dgrad2d_alg1_1<...>` once (taking ~120ms) and then chooses the faster `dgrad_engine`. If the flag was ON in python, C++ also chooses `dgrad_engine` but if the flag was OFF in python, it always chooses `dgrad2d_alg1_1` regardless of the flag setting in C++.\r\n\r\nI observed the choice of kernel using `nvprof python test.py 0/1`.\r\n\r\nPython Script (`test.py`):\r\n```python\r\nimport sys\r\nimport time\r\n\r\nimport torch as th\r\nth.backends.cudnn.benchmark = bool(int(sys.argv[1]))\r\n\r\nmod = th.nn.ConvTranspose2d(8, 3, 4, 2, 1).cuda()\r\ninp = th.zeros(1, 8, 512, 512).cuda()\r\n\r\nmod(inp); mod(inp); mod(inp)\r\n\r\nsmod = th.jit.trace(mod, (inp,), check_trace=False)\r\nsmod.save(""smod.ptj"")\r\n\r\nN = 1000\r\nth.cuda.synchronize()\r\nstart = time.time()\r\nfor _ in range(N):\r\n    mod(inp)\r\n    th.cuda.synchronize()\r\nend = time.time()\r\nprint(""Time (ms):"", 1000*(end-start)/N)\r\n```\r\n\r\nC++ Code:\r\n```c++\r\n#include <chrono>\r\n#include <iostream>\r\n\r\n#include <c10/cuda/CUDAGuard.h>\r\n#include <torch/script.h>\r\n#include <torch/torch.h>\r\n\r\n#include <cuda_runtime_api.h>\r\n\r\nint main() {\r\n  at::globalContext().setBenchmarkCuDNN(true);\r\n  auto nograd = torch::NoGradGuard();\r\n\r\n  try {\r\n    auto mod = torch::jit::load(""smod.ptj"");\r\n    mod->to(torch::kCUDA);\r\n    torch::jit::Stack input_stack = {torch::zeros({1, 8, 512, 512}, torch::kCUDA)};\r\n\r\n    mod->forward(input_stack);\r\n    mod->forward(input_stack);\r\n    mod->forward(input_stack);\r\n\r\n    const int N = 100;\r\n    cudaDeviceSynchronize();\r\n    const auto start = std::chrono::high_resolution_clock::now();\r\n    for (int i = 0; i < N; ++i) {\r\n      mod->forward(input_stack);\r\n      cudaDeviceSynchronize();\r\n    }\r\n    const auto end = std::chrono::high_resolution_clock::now();\r\n    const float elapsed = std::chrono::duration<float, std::milli>(end - start).count() / N;\r\n    std::cout << ""Time (ms): "" << elapsed << std::endl;\r\n  } catch (c10::Error e) {\r\n    std::cerr << e.what() << std::endl;\r\n    return 1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\n## Expected Behavior\r\n\r\nI would expect that either: 1) the C++ setting of `at::globalContext().setBenchmarkCuDNN(true)` should be respected (choosing the correct algorithm) or 2) at least print a warning that it is being overridden by the value of the flag at trace time.\r\n\r\n## Additional Info\r\nI printed the JIT graphs generated with benchmarking ON/OFF and got the following with the flag OFF:\r\n```\r\ngraph(%input : Float(1, 8, 512, 512),\r\n      %1 : Float(8, 3, 4, 4),\r\n      %2 : Float(3)):\r\n  %3 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %4 : int = prim::Constant[value=2](), scope: ConvTranspose2d\r\n  %5 : int[] = prim::ListConstruct(%3, %4), scope: ConvTranspose2d\r\n  %6 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %7 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %8 : int[] = prim::ListConstruct(%6, %7), scope: ConvTranspose2d\r\n  %9 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %10 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %11 : int[] = prim::ListConstruct(%9, %10), scope: ConvTranspose2d\r\n  %12 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %13 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %14 : int = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %15 : int[] = prim::ListConstruct(%13, %14), scope: ConvTranspose2d\r\n  %16 : int = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %17 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %18 : bool = prim::Constant[value=0](), scope: ConvTranspose2d\r\n  %19 : bool = prim::Constant[value=1](), scope: ConvTranspose2d\r\n  %20 : Float(1, 3, 1024, 1024) = aten::_convolution(%input, %1, %2, %5, %8, %11, %12, %15, %16, %17, %18, %19), scope: ConvTranspose2d\r\n  return (%20)\r\n```\r\n\r\nThe only change when the flag is ON is that register %17 is 1 instead of 0. I suppose this is where the ""hardcoding"" of the flag might be happening?\r\n\r\n## Environment\r\nPython code was run on Linux, C++ code was run on Windows\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20190311 on linux, 2336f0ba0 on Windows\r\n - OS (e.g., Linux): Fedora 29, Windows 10 1809\r\n - How you installed PyTorch (`conda`, `pip`, source): conda (pytorch-nightly)\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: CUDA 10, cuDNN 7.4.2\r\n - GPU models and configuration: Titan RTX (linux), GTX 1080 (windows)\n\ncc @suo","python\r\nimport sys\r\nimport time\r\n\r\nimport torch as th\r\nth.backends.cudnn.benchmark = bool(int(sys.argv[1]))\r\n\r\nmod = th.nn.ConvTranspose2d(8, 3, 4, 2, 1).cuda()\r\ninp = th.zeros(1, 8, 512, 512).cuda()\r\n\r\nmod(inp); mod(inp); mod(inp)\r\n\r\nsmod = th.jit.trace(mod, (inp,), check_trace=False)\r\nsmod.save(""smod.ptj"")\r\n\r\nN = 1000\r\nth.cuda.synchronize()\r\nstart = time.time()\r\nfor _ in range(N):\r\n    mod(inp)\r\n    th.cuda.synchronize()\r\nend = time.time()\r\nprint(""Time (ms):"", 1000*(end-start)/N)\r\n"
18720,"Magic value not found when loading traced model## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nI use `traced_script_module = torch.jit.trace(alphaposemodel, x)` transform alphapose model\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\nthen, use pyotrch c++ load the ""duc_se.pt""\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  memcmp(""PYTORCH1"", buf, kMagicValueLength) != 0 ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:75, please report a bug to PyTorch. File is an unsupported archive format from the preview release. (PyTorchStreamReader at /pytorch/caffe2/serialize/inline_container.cc:75)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fe38f617021 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fe38f6168ea in /data4/zjf/code/Pose-Estimation/libtorch/lib/libc10.so)\r\nframe #2: torch::jit::PyTorchStreamReader::PyTorchStreamReader(std::string, std::istream*) + 0x8c6 (0x7fe3c019c316 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libcaffe2.so)\r\nframe #3: <unknown function> + 0x6663b3 (0x7fe3caae23b3 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #4: torch::jit::load(std::istream&, c10::optional<c10::Device>) + 0x7f (0x7fe3caae573f in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #5: torch::jit::load(std::string const&, c10::optional<c10::Device>) + 0x6c (0x7fe3caae58cc in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #6: main + 0x94 (0x4013d7 in ./example-app)\r\nframe #7: __libc_start_main + 0xf0 (0x7fe38feca830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #8: _start + 0x29 (0x4011d9 in ./example-app)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n\r\n\r\n - PyTorch Version :1.0 latest\r\n - OS: ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source):docker\r\n - Python version:3.6.5\r\n - CUDA/cuDNN version:cuda 9.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: jit,driazati,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nI use `traced_script_module = torch.jit.trace(alphaposemodel, x)` transform alphapose model\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\nfrom dataloader import Mscoco\r\n\r\nfrom SPPE.src.main_fast_inference import *\r\nfrom torch.autograd import Variable\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'\r\n\r\ndef LoadModel():\r\n    pose_dataset = Mscoco()\r\n    model = InferenNet_fast(4 * 1 + 1, pose_dataset)\r\n\r\n    model = model.cuda()\r\n\r\n    return model\r\n\r\nif __name__ == '__main__':\r\n\r\n    \r\n    model = LoadModel()\r\n    # print(""model"", model)\r\n\r\n    x = Variable(torch.randn(1, 3, 320, 256).cuda(), requires_grad=True)\r\n    result = model(x)\r\n    print(""torch result:"", result)\r\n    traced_script_module = torch.jit.trace(model, x)\r\n    output = traced_script_module(x)\r\n    print(""--------------------------------"")\r\n    print(""trave result:"", output)\r\n    traced_script_module.save(""/data4/zjf/code/Pose-Estimation/AlphaPose-pytorch-v2/models/duc_se.pt"")\r\n```\r\n\r\nthen, use pyotrch c++ load the ""duc_se.pt""\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  memcmp(""PYTORCH1"", buf, kMagicValueLength) != 0 ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:75, please report a bug to PyTorch. File is an unsupported archive format from the preview release. (PyTorchStreamReader at /pytorch/caffe2/serialize/inline_container.cc:75)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fe38f617021 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fe38f6168ea in /data4/zjf/code/Pose-Estimation/libtorch/lib/libc10.so)\r\nframe #2: torch::jit::PyTorchStreamReader::PyTorchStreamReader(std::string, std::istream*) + 0x8c6 (0x7fe3c019c316 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libcaffe2.so)\r\nframe #3: <unknown function> + 0x6663b3 (0x7fe3caae23b3 in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #4: torch::jit::load(std::istream&, c10::optional<c10::Device>) + 0x7f (0x7fe3caae573f in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #5: torch::jit::load(std::string const&, c10::optional<c10::Device>) + 0x6c (0x7fe3caae58cc in /data4/zjf/code/Pose-Estimation/libtorch/lib/libtorch.so.1)\r\nframe #6: main + 0x94 (0x4013d7 in ./example-app)\r\nframe #7: __libc_start_main + 0xf0 (0x7fe38feca830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #8: _start + 0x29 (0x4011d9 in ./example-app)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version :1.0 latest\r\n - OS: ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source):docker\r\n - Python version:3.6.5\r\n - CUDA/cuDNN version:cuda 9.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nimport torchvision\r\n\r\nfrom dataloader import Mscoco\r\n\r\nfrom SPPE.src.main_fast_inference import *\r\nfrom torch.autograd import Variable\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '2'\r\n\r\ndef LoadModel():\r\n    pose_dataset = Mscoco()\r\n    model = InferenNet_fast(4 * 1 + 1, pose_dataset)\r\n\r\n    model = model.cuda()\r\n\r\n    return model\r\n\r\nif __name__ == '__main__':\r\n\r\n    \r\n    model = LoadModel()\r\n    # print(""model"", model)\r\n\r\n    x = Variable(torch.randn(1, 3, 320, 256).cuda(), requires_grad=True)\r\n    result = model(x)\r\n    print(""torch result:"", result)\r\n    traced_script_module = torch.jit.trace(model, x)\r\n    output = traced_script_module(x)\r\n    print(""--------------------------------"")\r\n    print(""trave result:"", output)\r\n    traced_script_module.save(""/data4/zjf/code/Pose-Estimation/AlphaPose-pytorch-v2/models/duc_se.pt"")\r\n"
18689,"distributed.all_gather function stuck when using NCCL backend ## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am trying to use distributed.all_gather to gather gradients in multi nodes. but I found the all_gather function stuck and no error throw\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. set up env variable `MASTER_ADDR` `MASTER_PORT`, save the following code as `main.py`\r\n\r\n2. run the code with `python main.py --rank 0` and `python main.py --rank 1`\r\n\r\nbtw, when I execute this code manually in `ipython`, I found the all_gather quickly go through, but it stuck when trying to print tensor\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nprocess will print a list of tensor\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\nGPU 4: Tesla K80\r\nGPU 5: Tesla K80\r\nGPU 6: Tesla K80\r\nGPU 7: Tesla K80\r\n\r\nNvidia driver version: 384.145\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.3\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: distributed|triaged|module: nccl,mrshenli,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI am trying to use distributed.all_gather to gather gradients in multi nodes. but I found the all_gather function stuck and no error throw\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. set up env variable `MASTER_ADDR` `MASTER_PORT`, save the following code as `main.py`\r\n```python\r\nimport torch.distributed as dist\r\nimport torch as t\r\n\r\nimport argparse\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n            ""--rank"",\r\n            type=int,\r\n            help=""the rank of proc""\r\n            )\r\n    args = parser.parse_args()\r\n    dist.init_process_group(""nccl"", rank=args.rank, world_size=2)\r\n    tmp = [t.randn(5).cuda()] * 2\r\n    tensor = t.ones(5).cuda()\r\n    dist.all_gather(tmp, tensor)\r\n    print(tmp)\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n2. run the code with `python main.py --rank 0` and `python main.py --rank 1`\r\n\r\nbtw, when I execute this code manually in `ipython`, I found the all_gather quickly go through, but it stuck when trying to print tensor\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nprocess will print a list of tensor\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\nGPU 1: Tesla K80\r\nGPU 2: Tesla K80\r\nGPU 3: Tesla K80\r\nGPU 4: Tesla K80\r\nGPU 5: Tesla K80\r\nGPU 6: Tesla K80\r\nGPU 7: Tesla K80\r\n\r\nNvidia driver version: 384.145\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.15.3\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.1\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch.distributed as dist\r\nimport torch as t\r\n\r\nimport argparse\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n            ""--rank"",\r\n            type=int,\r\n            help=""the rank of proc""\r\n            )\r\n    args = parser.parse_args()\r\n    dist.init_process_group(""nccl"", rank=args.rank, world_size=2)\r\n    tmp = [t.randn(5).cuda()] * 2\r\n    tensor = t.ones(5).cuda()\r\n    dist.all_gather(tmp, tensor)\r\n    print(tmp)\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n"
18648,"Step 1: Secretly add return_counts to unique, and refactor unique_dim for performanceStack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #18661 Step 7: remove _unique\r\n* #18655 Step 6: Rename _unique2 to unique and add int? dim\r\n* #18654 Step 5: remove _unque_dim in favor of unique_dim\r\n* #18651 Step 4: add support for unique with dim=None\r\n* #18650 Step 3: Add support for return_counts to torch.unique for dim not None\r\n* #18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim\r\n* **#18648 Step 1: Secretly add return_counts to unique, and refactor unique_dim for performance**\r\n\r\n`unique` is fragile, previously I tried to change it in #18391 and #17097, they all pass OSS tests but finally get reverted due to internal failure. My previous work of refactoring unique #18459 is based on #18391, and after #18391 get reverted, I could not work on #18459. To continue working on #18459, #18391, and #17097 without worrying about internal failures, I am suggesting the following steps for the improvements of `unique` and `unique_dim`. @soumith Please take this and there is no need to put #18391 back.\r\n\r\nThe motivation is basically to move forward as much as possible without causing any internal failures. So I will try to divide it into steps and sort from low probability of internal failure to high probability. (I don't know what the internal failure is, so I have to guess). Let's merge these PR stack one by one until we enounter internal failure.\r\n\r\nStep 1: Create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and keep `_unique` and `_unique_dim` unchanged. The backend of these two functions and `_unique` and `_unique_dim` are all the same, the only difference is the temporary ones support `return_counts` but not the `_unique` and `_unique_dim`. Step one is mostly #18391 + #18459. The cuda8 errors has been fixed. At this point, there is no user visible API change, so no docs are updated. `torch.unique` does not support `return_counts` yet, and `return_counts` is tested through the newly added temporary operators. This step just added two new ATen operators, so there shouldn't be any internal failure.\r\n\r\nStep 2: Rename `_unique_dim2_temporary_will_remove_soon` to `unique_dim`. This should cause no internal failure either, because no change to existing operators. The only thing to worry about is to delete `unique_dim` from python side because we don't want users to use it. At this point, C++ users now have `return_counts` support for `unique_dim`.\r\n\r\nStep 3: Update the docs of `torch.unique` and use `unique_dim` inside `torch.unique` to support `return_counts` In the docs, we should say `torch.unique` with None dim support does not support `return_counts` yet. This might cause internal failure.\r\n\r\nStep 4: Rename `_unique2_temporary_will_remove_soon` to `_unique2` and use `_unique2` inside `torch.unique` to support `return_counts`. Update the docs saying that `torch.unique` with None dim now support `return_counts`. This might cause internal failure.\r\n\r\nStep 5: Remove `_unique_dim`. This might cause internal failure.\r\n\r\nStep 6: Rename `_unique2` to `unique`, add optional `dim` argument to make it looks like the signature of Python's `torch.unique`. Inside `torch.unique`, use `unique` and get rid of `unique_dim`. Unbind `unique_dim` totally from Python at codegen. This is likely to cause internal fail.\r\n\r\nStep 7: Remove `_unique`. This is very likely to cause internal failure.\r\n\r\nThis PR\r\n======\r\n\r\nThis PR is for step 1. This create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and implement `return_counts` inside them and do refactor for performance improvements.\r\n\r\nPlease review @ngimel @VitalyFedyunin. They are mostly copied from #18391 and #18459, so the review should be easy.\r\n\r\nBelow is a benchmark on a tensor of shape `torch.Size([15320, 2])`:\r\n\r\nBefore\r\n---------\r\n\r\n\r\n\r\n```\r\n1.0.1\r\n192 \xb5s \xb1 1.61 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n548 ms \xb1 3.39 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n\r\n\r\n```\r\n1.0.1\r\n226 \xb5s \xb1 929 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n302 \xb5s \xb1 7.06 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\nAfter\r\n-------\r\n\r\n\r\n\r\n```\r\n1.1.0a0+83ab8ac\r\n190 \xb5s \xb1 2.14 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n237 \xb5s \xb1 1.23 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n219 \xb5s \xb1 2.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n263 \xb5s \xb1 1.15 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n\r\n\r\n```\r\n1.1.0a0+83ab8ac\r\n232 \xb5s \xb1 2.21 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n301 \xb5s \xb1 1.65 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n264 \xb5s \xb1 7.67 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n339 \xb5s \xb1 9.2 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\n\nDifferential Revision: [D14730905](https://our.internmc.facebook.com/intern/diff/D14730905)",open source,VitalyFedyunin,"Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* #18661 Step 7: remove _unique\r\n* #18655 Step 6: Rename _unique2 to unique and add int? dim\r\n* #18654 Step 5: remove _unque_dim in favor of unique_dim\r\n* #18651 Step 4: add support for unique with dim=None\r\n* #18650 Step 3: Add support for return_counts to torch.unique for dim not None\r\n* #18649 Step 2: Rename _unique_dim2_temporary_will_remove_soon to unique_dim\r\n* **#18648 Step 1: Secretly add return_counts to unique, and refactor unique_dim for performance**\r\n\r\n`unique` is fragile, previously I tried to change it in #18391 and #17097, they all pass OSS tests but finally get reverted due to internal failure. My previous work of refactoring unique #18459 is based on #18391, and after #18391 get reverted, I could not work on #18459. To continue working on #18459, #18391, and #17097 without worrying about internal failures, I am suggesting the following steps for the improvements of `unique` and `unique_dim`. @soumith Please take this and there is no need to put #18391 back.\r\n\r\nThe motivation is basically to move forward as much as possible without causing any internal failures. So I will try to divide it into steps and sort from low probability of internal failure to high probability. (I don't know what the internal failure is, so I have to guess). Let's merge these PR stack one by one until we enounter internal failure.\r\n\r\nStep 1: Create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and keep `_unique` and `_unique_dim` unchanged. The backend of these two functions and `_unique` and `_unique_dim` are all the same, the only difference is the temporary ones support `return_counts` but not the `_unique` and `_unique_dim`. Step one is mostly #18391 + #18459. The cuda8 errors has been fixed. At this point, there is no user visible API change, so no docs are updated. `torch.unique` does not support `return_counts` yet, and `return_counts` is tested through the newly added temporary operators. This step just added two new ATen operators, so there shouldn't be any internal failure.\r\n\r\nStep 2: Rename `_unique_dim2_temporary_will_remove_soon` to `unique_dim`. This should cause no internal failure either, because no change to existing operators. The only thing to worry about is to delete `unique_dim` from python side because we don't want users to use it. At this point, C++ users now have `return_counts` support for `unique_dim`.\r\n\r\nStep 3: Update the docs of `torch.unique` and use `unique_dim` inside `torch.unique` to support `return_counts` In the docs, we should say `torch.unique` with None dim support does not support `return_counts` yet. This might cause internal failure.\r\n\r\nStep 4: Rename `_unique2_temporary_will_remove_soon` to `_unique2` and use `_unique2` inside `torch.unique` to support `return_counts`. Update the docs saying that `torch.unique` with None dim now support `return_counts`. This might cause internal failure.\r\n\r\nStep 5: Remove `_unique_dim`. This might cause internal failure.\r\n\r\nStep 6: Rename `_unique2` to `unique`, add optional `dim` argument to make it looks like the signature of Python's `torch.unique`. Inside `torch.unique`, use `unique` and get rid of `unique_dim`. Unbind `unique_dim` totally from Python at codegen. This is likely to cause internal fail.\r\n\r\nStep 7: Remove `_unique`. This is very likely to cause internal failure.\r\n\r\nThis PR\r\n======\r\n\r\nThis PR is for step 1. This create two new ATen operators, `_unique2_temporary_will_remove_soon` and `_unique_dim2_temporary_will_remove_soon` and implement `return_counts` inside them and do refactor for performance improvements.\r\n\r\nPlease review @ngimel @VitalyFedyunin. They are mostly copied from #18391 and #18459, so the review should be easy.\r\n\r\nBelow is a benchmark on a tensor of shape `torch.Size([15320, 2])`:\r\n\r\nBefore\r\n---------\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False); torch.cuda.synchronize()\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True); torch.cuda.synchronize()\r\n```\r\n\r\n```\r\n1.0.1\r\n192 \xb5s \xb1 1.61 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n548 ms \xb1 3.39 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(sorted=True, return_inverse=False); torch.cuda.synchronize()\r\n%timeit a.unique(sorted=True, return_inverse=True); torch.cuda.synchronize()\r\n```\r\n\r\n```\r\n1.0.1\r\n226 \xb5s \xb1 929 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n302 \xb5s \xb1 7.06 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\nAfter\r\n-------\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False); torch.cuda.synchronize()\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True); torch.cuda.synchronize()\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=False, return_counts=True); torch.cuda.synchronize()\r\n%timeit torch._unique_dim2_temporary_will_remove_soon(a, dim=0, sorted=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()\r\n```\r\n\r\n```\r\n1.1.0a0+83ab8ac\r\n190 \xb5s \xb1 2.14 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n237 \xb5s \xb1 1.23 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n219 \xb5s \xb1 2.3 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n263 \xb5s \xb1 1.15 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n```python\r\nprint(torch.__version__)\r\n%timeit a.unique(sorted=True, return_inverse=False); torch.cuda.synchronize()\r\n%timeit a.unique(sorted=True, return_inverse=True); torch.cuda.synchronize()\r\n%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=False, return_counts=True); torch.cuda.synchronize()\r\n%timeit torch._unique2_temporary_will_remove_soon(a, sorted=True, return_inverse=True, return_counts=True); torch.cuda.synchronize()\r\n```\r\n\r\n```\r\n1.1.0a0+83ab8ac\r\n232 \xb5s \xb1 2.21 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n301 \xb5s \xb1 1.65 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n264 \xb5s \xb1 7.67 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n339 \xb5s \xb1 9.2 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\n\nDifferential Revision: [D14730905](https://our.internmc.facebook.com/intern/diff/D14730905)","python\r\nprint(torch.__version__)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False); torch.cuda.synchronize()\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True); torch.cuda.synchronize()\r\n"
18619,Fail in repeated evaluation of 2nd derivative of a custom autograd function## \U0001f41b Bug\r\n\r\nRepeated evaluation of a 2nd derivative of a custom autograd function gives incorrect results.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nThe output is:\r\n\r\n```\r\ntensor(1.) tensor(1.)\r\ntensor(1.) tensor(-0.)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe results should be the same on repeated evaluation.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 9.8 (stretch)\r\nGCC version: (Homebrew gcc 5.5.0_4) 5.5.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: NVS 310\r\n\r\nNvidia driver version: 390.87\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.1\r\n[pip] torch==1.0.1.post2\r\n[conda] Could not collect,high priority|module: double backwards|module: autograd|triaged,ezyang,"## \U0001f41b Bug\r\n\r\nRepeated evaluation of a 2nd derivative of a custom autograd function gives incorrect results.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\n\r\ndef mult1(x):\r\n    return x.prod(dim=-1).prod(dim=-1)\r\n\r\n\r\nclass Mult(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        y = mult1(x)\r\n        ctx.save_for_backward(x, y)\r\n        return y\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        x, y = ctx.saved_tensors\r\n        return (grad_output * y)[:, None, None] / x\r\n\r\n\r\nmult2 = Mult.apply\r\n\r\n\r\ndef check_gradgrad_repeated(x, y):\r\n    gy, = torch.autograd.grad(y[0], x, create_graph=True)\r\n    ggy_1, = torch.autograd.grad(gy[0, 0, 0], x, retain_graph=True)\r\n    gy, = torch.autograd.grad(y[0], x, create_graph=True)\r\n    ggy_2, = torch.autograd.grad(gy[0, 0, 0], x, retain_graph=True)\r\n    print(ggy_1[0, 0, 1], ggy_2[0, 0, 1])\r\n\r\n\r\nx = torch.ones(2, 4, 4).requires_grad_()\r\ncheck_gradgrad_repeated(x, mult1(x))\r\ncheck_gradgrad_repeated(x, mult2(x))\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\ntensor(1.) tensor(1.)\r\ntensor(1.) tensor(-0.)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe results should be the same on repeated evaluation.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 9.8 (stretch)\r\nGCC version: (Homebrew gcc 5.5.0_4) 5.5.0\r\nCMake version: version 3.13.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: NVS 310\r\n\r\nNvidia driver version: 390.87\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.1\r\n[pip] torch==1.0.1.post2\r\n[conda] Could not collect","python\r\nimport torch\r\n\r\n\r\ndef mult1(x):\r\n    return x.prod(dim=-1).prod(dim=-1)\r\n\r\n\r\nclass Mult(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        y = mult1(x)\r\n        ctx.save_for_backward(x, y)\r\n        return y\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        x, y = ctx.saved_tensors\r\n        return (grad_output * y)[:, None, None] / x\r\n\r\n\r\nmult2 = Mult.apply\r\n\r\n\r\ndef check_gradgrad_repeated(x, y):\r\n    gy, = torch.autograd.grad(y[0], x, create_graph=True)\r\n    ggy_1, = torch.autograd.grad(gy[0, 0, 0], x, retain_graph=True)\r\n    gy, = torch.autograd.grad(y[0], x, create_graph=True)\r\n    ggy_2, = torch.autograd.grad(gy[0, 0, 0], x, retain_graph=True)\r\n    print(ggy_1[0, 0, 1], ggy_2[0, 0, 1])\r\n\r\n\r\nx = torch.ones(2, 4, 4).requires_grad_()\r\ncheck_gradgrad_repeated(x, mult1(x))\r\ncheck_gradgrad_repeated(x, mult2(x))\r\n"
18617,Broken loading of weights for jitted RNN## \U0001f41b Bug\r\n\r\nIncorrect loading of weights for jitted RNN modules when using CUDA. Weights seem randomly changing from time to time. Model is broken and outputs garbage. Loading model with `map_location='cpu'` fix the problem.\r\n\r\n## To Reproduce\r\n\r\nout\r\n```\r\nweight_hh_l0 0.17591434717178345\r\nbias_ih_l0 0.17516206204891205\r\nbias_hh_l0 0.16948461532592773\r\nweight_ih_l0_reverse 1.2479848861694336\r\nweight_hh_l0_reverse 1.129560947418213\r\nbias_ih_l0_reverse 0.164781391620636\r\nbias_hh_l0_reverse 0.1713828146457672\r\nweight_ih_l1 0.8811493515968323\r\nweight_hh_l1 0.17583367228507996\r\nbias_ih_l1 0.16150619089603424\r\nbias_hh_l1 0.17185932397842407\r\nweight_ih_l1_reverse 0.17613038420677185\r\nweight_hh_l1_reverse 0.1766989678144455\r\nbias_ih_l1_reverse 0.17309224605560303\r\nbias_hh_l1_reverse 0.17430740594863892\r\n```\r\n## Expected behavior\r\n\r\nWeights correctly loading and model works\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[pip] torchfile==0.1.0\r\n[pip] torchnet==0.0.4\r\n[pip] torchvision==0.2.2\r\n[pip] warpctc-pytorch==0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] faiss-cpu                 1.4.0            py36_cuda0.0_1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py36h90e4bf4_5\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] pytorch                   1.0.1           py3.6_cuda9.0.176_cudnn7.4.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181206 py3.6_cuda9.2.148_cudnn7.4.1_0  [cuda92]  pytorch\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n[conda] torchnet                  0.0.4                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n[conda] warpctc-pytorch           0.1                      pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n,needs reproduction|oncall: jit,driazati,"## \U0001f41b Bug\r\n\r\nIncorrect loading of weights for jitted RNN modules when using CUDA. Weights seem randomly changing from time to time. Model is broken and outputs garbage. Loading model with `map_location='cpu'` fix the problem.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\ndevice = torch.device('cuda')\r\n\r\nrnn = nn.GRU(input_size=256,\r\n    hidden_size=128,\r\n    num_layers=2,\r\n    batch_first=False,\r\n    dropout=0.1,\r\n    bidirectional=True)\r\n\r\nrnn.eval()\r\nrnn = rnn.to(device)\r\n\r\nwith torch.no_grad():\r\n    traced_rnn = torch.jit.trace(rnn, torch.rand((50, 10, 256), dtype=torch.float32).to(device))\r\n\r\nparams = {}\r\n\r\nfor k, v in traced_rnn.named_parameters():\r\n    params[k] = v\r\n    \r\ntorch.jit.save(traced_rnn, 'traced.pt')\r\ntraced_rnn = torch.jit.load('traced.pt')\r\n\r\nfor k, v in traced_rnn.named_parameters():\r\n    diff = torch.max(torch.abs(params[k] - v)).item()\r\n    \r\n    if diff > 0:\r\n        print(k, diff)\r\n```\r\nout\r\n```\r\nweight_hh_l0 0.17591434717178345\r\nbias_ih_l0 0.17516206204891205\r\nbias_hh_l0 0.16948461532592773\r\nweight_ih_l0_reverse 1.2479848861694336\r\nweight_hh_l0_reverse 1.129560947418213\r\nbias_ih_l0_reverse 0.164781391620636\r\nbias_hh_l0_reverse 0.1713828146457672\r\nweight_ih_l1 0.8811493515968323\r\nweight_hh_l1 0.17583367228507996\r\nbias_ih_l1 0.16150619089603424\r\nbias_hh_l1 0.17185932397842407\r\nweight_ih_l1_reverse 0.17613038420677185\r\nweight_hh_l1_reverse 0.1766989678144455\r\nbias_ih_l1_reverse 0.17309224605560303\r\nbias_hh_l1_reverse 0.17430740594863892\r\n```\r\n## Expected behavior\r\n\r\nWeights correctly loading and model works\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.0.1.post2\r\n[pip] torchfile==0.1.0\r\n[pip] torchnet==0.0.4\r\n[pip] torchvision==0.2.2\r\n[pip] warpctc-pytorch==0.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] faiss-cpu                 1.4.0            py36_cuda0.0_1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py36h90e4bf4_5\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] pytorch                   1.0.1           py3.6_cuda9.0.176_cudnn7.4.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181206 py3.6_cuda9.2.148_cudnn7.4.1_0  [cuda92]  pytorch\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n[conda] torchnet                  0.0.4                    pypi_0    pypi\r\n[conda] torchvision               0.2.2                      py_3    pytorch\r\n[conda] warpctc-pytorch           0.1                      pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nfrom torch import nn\r\n\r\ndevice = torch.device('cuda')\r\n\r\nrnn = nn.GRU(input_size=256,\r\n    hidden_size=128,\r\n    num_layers=2,\r\n    batch_first=False,\r\n    dropout=0.1,\r\n    bidirectional=True)\r\n\r\nrnn.eval()\r\nrnn = rnn.to(device)\r\n\r\nwith torch.no_grad():\r\n    traced_rnn = torch.jit.trace(rnn, torch.rand((50, 10, 256), dtype=torch.float32).to(device))\r\n\r\nparams = {}\r\n\r\nfor k, v in traced_rnn.named_parameters():\r\n    params[k] = v\r\n    \r\ntorch.jit.save(traced_rnn, 'traced.pt')\r\ntraced_rnn = torch.jit.load('traced.pt')\r\n\r\nfor k, v in traced_rnn.named_parameters():\r\n    diff = torch.max(torch.abs(params[k] - v)).item()\r\n    \r\n    if diff > 0:\r\n        print(k, diff)\r\n"
18616,"[JIT] Bug with indexing when passing lists instead of tensors## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThere is a mismatch in semantics between torchscript and pytorch when indexing with a list of integers.\r\nIn PyTorch (which follows numpy semantics), the list is converted into a `int64` tensor, and advanced indexing is performed.\r\nIn torchscript, the list is treated as a tuple (in pytorch semantics), and it dispatches to several calls to `select`.\r\n\r\nSo\r\n\r\nbecomes in TorchScript\r\n\r\nwhich is not semantics-preserving.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nBy inspecting `f.code`, we see that it is performing the equivalent of `x[1][2]`:\r\n\r\nIt should instead dispatch to `index_fill_` or `index_copy` (or more generally, `index_put_`).\r\n\r\nNote that the same (wrong) behavior happens for selection:\r\n\r\n\r\nNote that if instead we pass `int64` tensors for the indices, everything work as expected.\r\n\r\nI'm using PyTorch nightly 1.0.0.dev20190321\r\n\r\n## Context\r\n\r\nThis is potentially related to https://github.com/pytorch/pytorch/issues/14332, but the situation presented in #14332 gives a compiler error, while the one here silently fails\n\ncc @suo",oncall: jit|triaged|jit-backlog,efaust,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nThere is a mismatch in semantics between torchscript and pytorch when indexing with a list of integers.\r\nIn PyTorch (which follows numpy semantics), the list is converted into a `int64` tensor, and advanced indexing is performed.\r\nIn torchscript, the list is treated as a tuple (in pytorch semantics), and it dispatches to several calls to `select`.\r\n\r\nSo\r\n```python\r\nx[[1, 2]]\r\n```\r\nbecomes in TorchScript\r\n```python\r\nx[1, 2]\r\n```\r\nwhich is not semantics-preserving.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nIn [4]: @torch.jit.script\r\n   ...: def f(x):\r\n   ...:     x[[1, 2]] = 1\r\n   ...:     return x\r\n   ...:\r\n\r\nIn [5]: x = torch.zeros(4, 4)\r\n\r\nIn [6]: f(x)\r\nOut[6]:\r\ntensor([[0., 0., 0., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 0., 0., 0.],\r\n        [0., 0., 0., 0.]])\r\n\r\nIn [7]: x = torch.zeros(4, 4)\r\n\r\nIn [8]: x[[1, 2]] = 1\r\n\r\nIn [9]: x\r\nOut[9]:\r\ntensor([[0., 0., 0., 0.],\r\n        [1., 1., 1., 1.],\r\n        [1., 1., 1., 1.],\r\n        [0., 0., 0., 0.]])\r\n```\r\n\r\nBy inspecting `f.code`, we see that it is performing the equivalent of `x[1][2]`:\r\n```python\r\ndef forward(self,\r\n    x: Tensor) -> Tensor:\r\n  _0 = torch.select(torch.select(x, 0, 1), 0, 2)\r\n  _1 = torch.copy_(_0, 1)\r\n  return x\r\n```\r\nIt should instead dispatch to `index_fill_` or `index_copy` (or more generally, `index_put_`).\r\n\r\nNote that the same (wrong) behavior happens for selection:\r\n```python\r\nIn [14]: @torch.jit.script\r\n    ...: def g(x):\r\n    ...:     return x[[1, 2]]\r\n    ...:\r\n\r\nIn [15]: a = torch.arange(16).reshape(4,4)\r\n\r\nIn [16]: a\r\nOut[16]:\r\ntensor([[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11],\r\n        [12, 13, 14, 15]])\r\n\r\nIn [17]: g(x)\r\nOut[17]: tensor(1.)\r\n\r\nIn [18]: print(g.code)\r\ndef forward(self,\r\n    x: Tensor) -> Tensor:\r\n  _0 = torch.select(torch.select(x, 0, 1), 0, 2)\r\n  return _0\r\n```\r\n\r\nNote that if instead we pass `int64` tensors for the indices, everything work as expected.\r\n\r\nI'm using PyTorch nightly 1.0.0.dev20190321\r\n\r\n## Context\r\n\r\nThis is potentially related to https://github.com/pytorch/pytorch/issues/14332, but the situation presented in #14332 gives a compiler error, while the one here silently fails\n\ncc @suo","python\r\nx[[1, 2]]\r\n"
18518,"""shape '[32, 3]' is invalid for input of size 6144"" when initializing Conv2d layer in C++ frontend## \U0001f41b Bug\r\n\r\nWhen using `torch::nn::init::orthogonal_()` on a `torch::nn::Conv2d` layer's weights parameter, it fails with the following message:\r\n```\r\nshape '[32, 3]' is invalid for input of size 6144\r\n```\r\n\r\nIn this case, my convolution layer has a shape of `[32, 3, 8, 8]`, which when multiplied out is 6144 weights.\r\n\r\nUsing the same initialization function on a `Linear` or `GRU` layer works fine.\r\n\r\n## To Reproduce\r\n\r\n\r\n## Expected behavior\r\n\r\nIn the Python frontend it correctly initializes the parameter, so I would expect that to happen.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.1\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): ccmake .. (choose options for building libtorch), make -j8, sudo make install\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: GTX 1050Ti\r\n - Any other relevant information: Using the PyTorch C++ frontend",module: cpp,yf225,"## \U0001f41b Bug\r\n\r\nWhen using `torch::nn::init::orthogonal_()` on a `torch::nn::Conv2d` layer's weights parameter, it fails with the following message:\r\n```\r\nshape '[32, 3]' is invalid for input of size 6144\r\n```\r\n\r\nIn this case, my convolution layer has a shape of `[32, 3, 8, 8]`, which when multiplied out is 6144 weights.\r\n\r\nUsing the same initialization function on a `Linear` or `GRU` layer works fine.\r\n\r\n## To Reproduce\r\n```c++\r\nauto conv_layer = torch::nn::Conv2d(nn::Conv2dOptions(3, 32, 8).stride(4));\r\ntorch::nn::init::orthogonal_(conv_layer->named_parameters()[""weight""]);\r\n```\r\n\r\n## Expected behavior\r\n\r\nIn the Python frontend it correctly initializes the parameter, so I would expect that to happen.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.1\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): ccmake .. (choose options for building libtorch), make -j8, sudo make install\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration: GTX 1050Ti\r\n - Any other relevant information: Using the PyTorch C++ frontend","c++\r\nauto conv_layer = torch::nn::Conv2d(nn::Conv2dOptions(3, 32, 8).stride(4));\r\ntorch::nn::init::orthogonal_(conv_layer->named_parameters()[""weight""]);\r\n"
18459,"Refactor CUDA implementation of unique and unique_dim to improve performance of unique_dimFixes: https://github.com/pytorch/pytorch/issues/18405\r\n\r\nThis is based on https://github.com/pytorch/pytorch/pull/18391, I suggest first wait until #18391 get merged and then I will rebase.\r\n\r\ncc: @ngimel \r\n\r\n# Benchmark\r\n\r\nBenchmark is on a tensor of shape `torch.Size([15320, 2])`\r\n\r\n## Before:\r\n\r\n\r\n```\r\n200 \xb5s \xb1 3.14 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n1.04 s \xb1 21.4 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n1.03 s \xb1 15 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n1.07 s \xb1 27.8 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n```\r\n227 \xb5s \xb1 2.38 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n291 \xb5s \xb1 2.34 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n249 \xb5s \xb1 1.56 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n316 \xb5s \xb1 2.64 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n## After\r\n\r\n\r\n```\r\n187 \xb5s \xb1 4.67 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n248 \xb5s \xb1 3.9 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n212 \xb5s \xb1 906 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n274 \xb5s \xb1 1.8 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n```\r\n226 \xb5s \xb1 763 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n294 \xb5s \xb1 1.03 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n253 \xb5s \xb1 3.5 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n323 \xb5s \xb1 1.52 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```",open source,VitalyFedyunin,"Fixes: https://github.com/pytorch/pytorch/issues/18405\r\n\r\nThis is based on https://github.com/pytorch/pytorch/pull/18391, I suggest first wait until #18391 get merged and then I will rebase.\r\n\r\ncc: @ngimel \r\n\r\n# Benchmark\r\n\r\nBenchmark is on a tensor of shape `torch.Size([15320, 2])`\r\n\r\n## Before:\r\n\r\n```python\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n```\r\n200 \xb5s \xb1 3.14 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n1.04 s \xb1 21.4 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n1.03 s \xb1 15 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n1.07 s \xb1 27.8 ms per loop (mean \xb1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n```python\r\n%timeit a.unique(sorted=True, return_inverse=False, return_counts=False)\r\n%timeit a.unique(sorted=True, return_inverse=True, return_counts=False)\r\n%timeit a.unique(sorted=True, return_inverse=False, return_counts=True)\r\n%timeit a.unique(sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n```\r\n227 \xb5s \xb1 2.38 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n291 \xb5s \xb1 2.34 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n249 \xb5s \xb1 1.56 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n316 \xb5s \xb1 2.64 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\n## After\r\n\r\n```python\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n```\r\n187 \xb5s \xb1 4.67 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n248 \xb5s \xb1 3.9 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n212 \xb5s \xb1 906 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n274 \xb5s \xb1 1.8 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n```python\r\n%timeit a.unique(sorted=True, return_inverse=False, return_counts=False)\r\n%timeit a.unique(sorted=True, return_inverse=True, return_counts=False)\r\n%timeit a.unique(sorted=True, return_inverse=False, return_counts=True)\r\n%timeit a.unique(sorted=True, return_inverse=True, return_counts=True)\r\n```\r\n```\r\n226 \xb5s \xb1 763 ns per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n294 \xb5s \xb1 1.03 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n253 \xb5s \xb1 3.5 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n323 \xb5s \xb1 1.52 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n```","python\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=False)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=False, return_counts=True)\r\n%timeit a.unique(dim=0, sorted=True, return_inverse=True, return_counts=True)\r\n"
18408,"_unique_dim does not support zero size tensorUnique without dim works well\r\n\r\n\r\nBut unique with dim does not work\r\n\r\n\r\nI suggest changing the behavior to:\r\nif `dim` refers to a dimension with size 0, and other dimensions have size > 0, then the output and reverse_indicies should be the same empty tensor as input. If `dim` refers to a dimension with size > 0, or there are other 0 sized dimensions, then reject the input with an error because whether ""nothing == nothing"" is not defined.",triaged,zasdfgbnm,"Unique without dim works well\r\n```python\r\n>>> a = torch.empty(5, 0)\r\n\r\n>>> a\r\ntensor([], size=(5, 0))\r\n\r\n>>> a.unique(return_inverse=True)\r\n(tensor([]), tensor([], size=(5, 0), dtype=torch.int64))\r\n```\r\n\r\nBut unique with dim does not work\r\n```python\r\n>>> a.unique(dim=1, return_inverse=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/gaoxiang/anaconda3/lib/python3.7/site-packages/torch/tensor.py"", line 351, in unique\r\n    dim=dim\r\nRuntimeError: cannot reshape tensor of 0 elements into shape [0, -1]\r\n\r\n>>> a.unique(dim=0, return_inverse=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/gaoxiang/anaconda3/lib/python3.7/site-packages/torch/tensor.py"", line 351, in unique\r\n    dim=dim\r\nRuntimeError: cannot reshape tensor of 0 elements into shape [-1, 0]\r\n```\r\n\r\nI suggest changing the behavior to:\r\nif `dim` refers to a dimension with size 0, and other dimensions have size > 0, then the output and reverse_indicies should be the same empty tensor as input. If `dim` refers to a dimension with size > 0, or there are other 0 sized dimensions, then reject the input with an error because whether ""nothing == nothing"" is not defined.","python\r\n>>> a = torch.empty(5, 0)\r\n\r\n>>> a\r\ntensor([], size=(5, 0))\r\n\r\n>>> a.unique(return_inverse=True)\r\n(tensor([]), tensor([], size=(5, 0), dtype=torch.int64))\r\n"
18398,"Build from source in virtualenv causes libiomp5.so load failure## \U0001f41b Bug\r\n\r\nBuild from source in virtualenv causes error below on Ubuntu 18.04.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\ngit clone --recursive https://github.com/pytorch/pytorch.git\r\nvirtualenv -p python3 pytorch_virtualenv\r\nsource ./pytorch_virtualenv/bin/activate\r\npython3 -m pip install future leveldb numpy protobuf pydot python-gflags pyyaml scikit-image setuptools six hypothesis typing tqdm\r\npython3 -m pip install --verbose ./pytorch/.\r\npython3 -c ""import pydoc; pydoc.locate('torch.nn.modules.conv.Conv1d')""\r\ndeactivate\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 347, in safeimport\r\n    module = __import__(path)\r\n  File ""/home/xxxxxxxxxx/Desktop/pytorch_virtualenv/lib/python3.6/site-packages/torch/__init__.py"", line 79, in <module>\r\n    from torch._C import *\r\nImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 1588, in locate\r\n    nextmodule = safeimport('.'.join(parts[:n+1]), forceload)\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 362, in safeimport\r\n    raise ErrorDuringImport(path, sys.exc_info())\r\npydoc.ErrorDuringImport: problem in torch - ImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.1.0a0+32d0e7e\r\n[conda] Could not collect\r\n```\r\n\r\n----\r\n\r\nNotes from #18836\r\n\r\n## \U0001f41b Bug\r\n\r\nBuilding pytorch from master (`python setup.py bdist_wheel`) does not\r\ninstall libraries (`libiomp5.so`,`libmklml_gnu.so` or `libmklml_intel.so`). However these libraries are built and located on `third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib`\r\n\r\n## To Reproduce\r\n\r\n1. `git clone --recursive https://github.com/pytorch`\r\n2. `python setupy.py bdist_wheel`\r\n3. `pip install torch-1.1.0a0+b80a4fa-cp37-cp37m-linux_x86_64.whl`\r\n\r\nError:\r\n``` bash\r\nIn [1]: import torch                                                                                                                                                                                                                           \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-eb42ca6e4af3> in <module>\r\n----> 1 import torch\r\n\r\n/data/frameworks/pytorch/lib/python3.7/site-packages/torch/__init__.py in <module>\r\n     77 del _dl_flags\r\n     78 \r\n---> 79 from torch._C import *\r\n     80 \r\n     81 __all__ += [name for name in dir(_C)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n```\r\nIn [1]: import torch                                                                                                                                                                                                                           \r\n\r\nIn [2]: \r\n```\r\nTo import torch successfully\r\n\r\n## Environment\r\n\r\n\r\nAdding the folder containing these libraries to the environment variable LD_LIBRARY_PATH fix the problem.\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",module: build|triaged|module: mkldnn|module: openmp|has workaround,rgommers,"## \U0001f41b Bug\r\n\r\nBuild from source in virtualenv causes error below on Ubuntu 18.04.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\ngit clone --recursive https://github.com/pytorch/pytorch.git\r\nvirtualenv -p python3 pytorch_virtualenv\r\nsource ./pytorch_virtualenv/bin/activate\r\npython3 -m pip install future leveldb numpy protobuf pydot python-gflags pyyaml scikit-image setuptools six hypothesis typing tqdm\r\npython3 -m pip install --verbose ./pytorch/.\r\npython3 -c ""import pydoc; pydoc.locate('torch.nn.modules.conv.Conv1d')""\r\ndeactivate\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 347, in safeimport\r\n    module = __import__(path)\r\n  File ""/home/xxxxxxxxxx/Desktop/pytorch_virtualenv/lib/python3.6/site-packages/torch/__init__.py"", line 79, in <module>\r\n    from torch._C import *\r\nImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 1588, in locate\r\n    nextmodule = safeimport('.'.join(parts[:n+1]), forceload)\r\n  File ""/usr/lib/python3.6/pydoc.py"", line 362, in safeimport\r\n    raise ErrorDuringImport(path, sys.exc_info())\r\npydoc.ErrorDuringImport: problem in torch - ImportError: libiomp5.so: cannot open shared object file: No such file or directory\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.1.0a0+32d0e7e\r\n[conda] Could not collect\r\n```\r\n\r\n----\r\n\r\nNotes from #18836\r\n\r\n## \U0001f41b Bug\r\n\r\nBuilding pytorch from master (`python setup.py bdist_wheel`) does not\r\ninstall libraries (`libiomp5.so`,`libmklml_gnu.so` or `libmklml_intel.so`). However these libraries are built and located on `third_party/ideep/mkl-dnn/external/mklml_lnx_2019.0.3.20190220/lib`\r\n\r\n## To Reproduce\r\n\r\n1. `git clone --recursive https://github.com/pytorch`\r\n2. `python setupy.py bdist_wheel`\r\n3. `pip install torch-1.1.0a0+b80a4fa-cp37-cp37m-linux_x86_64.whl`\r\n\r\nError:\r\n``` bash\r\nIn [1]: import torch                                                                                                                                                                                                                           \r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-eb42ca6e4af3> in <module>\r\n----> 1 import torch\r\n\r\n/data/frameworks/pytorch/lib/python3.7/site-packages/torch/__init__.py in <module>\r\n     77 del _dl_flags\r\n     78 \r\n---> 79 from torch._C import *\r\n     80 \r\n     81 __all__ += [name for name in dir(_C)\r\nImportError: libmklml_intel.so: cannot open shared object file: No such file or directory\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n```\r\nIn [1]: import torch                                                                                                                                                                                                                           \r\n\r\nIn [2]: \r\n```\r\nTo import torch successfully\r\n\r\n## Environment\r\n\r\n```bash\r\nCollecting environment information...\r\nPyTorch version: 1.1.0a0+b80a4fa\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.105\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 418.56\r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0a0+b80a4fa\r\n[pip3] torchvision==0.2.3\r\n```\r\nAdding the folder containing these libraries to the environment variable LD_LIBRARY_PATH fix the problem.\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh",bash\r\nCollecting environment information...\r\nPyTorch version: 1.1.0a0+b80a4fa\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.105\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 418.56\r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.5.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.1.0a0+b80a4fa\r\n[pip3] torchvision==0.2.3\r\n
18320,"JITed program is getting stuck## \U0001f41b Bug\r\n\r\nI have a program that's JITed using both script and trace. While the non-JITed version executes without any problem, JITed version gets stuck in a while loop.\r\n\r\n## To Reproduce\r\nBelow snippet is the complete script (it's a variant of PyTorch Chatbot example given in the doc). \r\nSo the issue I have is with the `wrapper` function I guess. It's not returning control and getting stuck in the while loop inside the `wrapper` function itself\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nNon-JITed version returns the token in less than a second and I would expect the same from JITed version. \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.0.1               py3.7_cpu_2    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n\r\n",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\nI have a program that's JITed using both script and trace. While the non-JITed version executes without any problem, JITed version gets stuck in a while loop.\r\n\r\n## To Reproduce\r\nBelow snippet is the complete script (it's a variant of PyTorch Chatbot example given in the doc). \r\nSo the issue I have is with the `wrapper` function I guess. It's not returning control and getting stuck in the while loop inside the `wrapper` function itself\r\n\r\n```python\r\nimport json\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nPAD_token = 0  # Used for padding short sentences\r\nSOS_token = 1  # Start-of-sentence token\r\nEOS_token = 2  # End-of-sentence token\r\n\r\n# TODO: `.to(device=device)` for all tensors\r\n\r\n\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, hidden_size, n_layers=1, dropout=0):\r\n        super().__init__()\r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        self.embedding = nn.Embedding(voc.num_words, hidden_size)\r\n        self.gru = nn.GRU(\r\n            hidden_size, hidden_size, n_layers,\r\n            dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\r\n\r\n    def forward(self, input_seq, input_lengths, hidden=None):\r\n        embedded = self.embedding(input_seq)\r\n        packed = nn.utils.rnn.pack_padded_sequence(\r\n            embedded, input_lengths)\r\n        outputs, hidden = self.gru(packed, hidden)\r\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\r\n        t1 = outputs[:, :, :self.hidden_size]\r\n        t2 = outputs[:, :, self.hidden_size:]\r\n        outputs = t1 + t2\r\n        return outputs, hidden\r\n\r\n\r\nclass Attn(nn.Module):\r\n    def __init__(self, hidden_size):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n\r\n    def forward(self, hidden, encoder_output):\r\n        attn_energies = torch.sum(hidden * encoder_output, dim=2)\r\n        attn_energies = attn_energies.t()\r\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\r\n\r\n\r\nclass LuongAttnDecoderRNN(nn.Module):\r\n    def __init__(\r\n            self, hidden_size,\r\n            output_size, n_layers=1, dropout=0.1):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n        self.output_size = output_size\r\n        self.n_layers = n_layers\r\n        self.dropout = dropout\r\n\r\n        # Define layers\r\n        self.embedding = nn.Embedding(voc.num_words, hidden_size)\r\n        self.embedding_dropout = nn.Dropout(dropout)\r\n        self.gru = nn.GRU(\r\n            hidden_size, hidden_size, n_layers,\r\n            dropout=(0 if n_layers == 1 else dropout))\r\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\r\n        self.out = nn.Linear(hidden_size, output_size)\r\n        self.attn = Attn(hidden_size)\r\n\r\n    def forward(self, input_seq, last_hidden, encoder_outputs):\r\n        embedded = self.embedding(input_seq)\r\n        embedded = self.embedding_dropout(embedded)\r\n        rnn_output, hidden = self.gru(embedded, last_hidden)\r\n        attn_weights = self.attn(rnn_output, encoder_outputs)\r\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\r\n        rnn_output = rnn_output.squeeze(0)\r\n        context = context.squeeze(1)\r\n        concat_input = torch.cat((rnn_output, context), 1)\r\n        concat_output = torch.tanh(self.concat(concat_input))\r\n        output = self.out(concat_output)\r\n        output = F.softmax(output, dim=1)\r\n        return output, hidden\r\n\r\n\r\nclass Voc:\r\n    def __init__(self, name):\r\n        self.name = name\r\n        self.trimmed = False\r\n        self.word2index = {}\r\n        self.word2count = {}\r\n        self.index2word = {\r\n            PAD_token: ""PAD"",\r\n            SOS_token: ""SOS"",\r\n            EOS_token: ""EOS""}\r\n        self.num_words = 3  # Count SOS, EOS, PAD\r\n\r\n\r\nvoc = Voc(name=None)\r\nvoc.num_words = 7826  # TODO - change this hardcoding after debugging\r\n\r\n\r\nhidden_size = 500\r\nencoder_n_layers = 2\r\ndecoder_n_layers = 2\r\ndropout = 0.1\r\nMAX_LENGTH = 10\r\n\r\nencoder = EncoderRNN(hidden_size, encoder_n_layers, dropout)\r\nencoder.eval()\r\nseq = torch.ones((MAX_LENGTH, 1), dtype=torch.long)\r\nseq_length = torch.tensor([seq.size()[0]])\r\ntraced_encoder = torch.jit.trace(encoder, (seq, seq_length))\r\n\r\ndecoder = LuongAttnDecoderRNN(\r\n    hidden_size, voc.num_words, decoder_n_layers, dropout)\r\ndecoder.eval()\r\ntest_encoder_outputs, test_encoder_hidden = traced_encoder(seq, seq_length)\r\ntest_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\r\ntest_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\r\ntraced_decoder = torch.jit.trace(\r\n    decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\r\n\r\n\r\n@torch.jit.script\r\ndef wrapper(input_seq, input_length):\r\n    PAD_token = 0\r\n    SOS_token = 1\r\n    EOS_token = 2\r\n    max_length = 10\r\n    n_layers = 2\r\n    e_outputs, e_hidden = traced_encoder(input_seq, input_length)\r\n    d_hidden = e_hidden[:n_layers]\r\n    d_input = torch.ones(1, 1, dtype=torch.long)\r\n    d_input *= SOS_token\r\n    #TODO - put EOS check somehwo\r\n    all_tokens = torch.zeros([0], dtype=torch.long)\r\n    while max_length > 0:\r\n        max_length -= 1\r\n        d_output, d_hidden = traced_decoder(d_input, d_hidden, e_outputs)\r\n        _, d_input = torch.max(d_output, dim=1)\r\n        all_tokens = torch.cat((all_tokens, d_input), dim=0)\r\n        d_input = torch.unsqueeze(d_input, 0)\r\n    return all_tokens\r\n\r\n\r\ndef run():\r\n    indexes_batch = [[787, 572, 2]]  # ""hello sir + EOS""\r\n    lengths = torch.tensor([3])\r\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\r\n    tokens = wrapper(input_batch, lengths)\r\n    print(tokens)\r\n\r\n\r\nrun()\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nNon-JITed version returns the token in less than a second and I would expect the same from JITed version. \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-cpu               1.0.1               py3.7_cpu_2    pytorch\r\n[conda] torchvision-cpu           0.2.2                      py_3    pytorch\r\n\r\n","python\r\nimport json\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nPAD_token = 0  # Used for padding short sentences\r\nSOS_token = 1  # Start-of-sentence token\r\nEOS_token = 2  # End-of-sentence token\r\n\r\n# TODO: `.to(device=device)` for all tensors\r\n\r\n\r\nclass EncoderRNN(nn.Module):\r\n    def __init__(self, hidden_size, n_layers=1, dropout=0):\r\n        super().__init__()\r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        self.embedding = nn.Embedding(voc.num_words, hidden_size)\r\n        self.gru = nn.GRU(\r\n            hidden_size, hidden_size, n_layers,\r\n            dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\r\n\r\n    def forward(self, input_seq, input_lengths, hidden=None):\r\n        embedded = self.embedding(input_seq)\r\n        packed = nn.utils.rnn.pack_padded_sequence(\r\n            embedded, input_lengths)\r\n        outputs, hidden = self.gru(packed, hidden)\r\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\r\n        t1 = outputs[:, :, :self.hidden_size]\r\n        t2 = outputs[:, :, self.hidden_size:]\r\n        outputs = t1 + t2\r\n        return outputs, hidden\r\n\r\n\r\nclass Attn(nn.Module):\r\n    def __init__(self, hidden_size):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n\r\n    def forward(self, hidden, encoder_output):\r\n        attn_energies = torch.sum(hidden * encoder_output, dim=2)\r\n        attn_energies = attn_energies.t()\r\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\r\n\r\n\r\nclass LuongAttnDecoderRNN(nn.Module):\r\n    def __init__(\r\n            self, hidden_size,\r\n            output_size, n_layers=1, dropout=0.1):\r\n        super().__init__()\r\n        self.hidden_size = hidden_size\r\n        self.output_size = output_size\r\n        self.n_layers = n_layers\r\n        self.dropout = dropout\r\n\r\n        # Define layers\r\n        self.embedding = nn.Embedding(voc.num_words, hidden_size)\r\n        self.embedding_dropout = nn.Dropout(dropout)\r\n        self.gru = nn.GRU(\r\n            hidden_size, hidden_size, n_layers,\r\n            dropout=(0 if n_layers == 1 else dropout))\r\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\r\n        self.out = nn.Linear(hidden_size, output_size)\r\n        self.attn = Attn(hidden_size)\r\n\r\n    def forward(self, input_seq, last_hidden, encoder_outputs):\r\n        embedded = self.embedding(input_seq)\r\n        embedded = self.embedding_dropout(embedded)\r\n        rnn_output, hidden = self.gru(embedded, last_hidden)\r\n        attn_weights = self.attn(rnn_output, encoder_outputs)\r\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\r\n        rnn_output = rnn_output.squeeze(0)\r\n        context = context.squeeze(1)\r\n        concat_input = torch.cat((rnn_output, context), 1)\r\n        concat_output = torch.tanh(self.concat(concat_input))\r\n        output = self.out(concat_output)\r\n        output = F.softmax(output, dim=1)\r\n        return output, hidden\r\n\r\n\r\nclass Voc:\r\n    def __init__(self, name):\r\n        self.name = name\r\n        self.trimmed = False\r\n        self.word2index = {}\r\n        self.word2count = {}\r\n        self.index2word = {\r\n            PAD_token: ""PAD"",\r\n            SOS_token: ""SOS"",\r\n            EOS_token: ""EOS""}\r\n        self.num_words = 3  # Count SOS, EOS, PAD\r\n\r\n\r\nvoc = Voc(name=None)\r\nvoc.num_words = 7826  # TODO - change this hardcoding after debugging\r\n\r\n\r\nhidden_size = 500\r\nencoder_n_layers = 2\r\ndecoder_n_layers = 2\r\ndropout = 0.1\r\nMAX_LENGTH = 10\r\n\r\nencoder = EncoderRNN(hidden_size, encoder_n_layers, dropout)\r\nencoder.eval()\r\nseq = torch.ones((MAX_LENGTH, 1), dtype=torch.long)\r\nseq_length = torch.tensor([seq.size()[0]])\r\ntraced_encoder = torch.jit.trace(encoder, (seq, seq_length))\r\n\r\ndecoder = LuongAttnDecoderRNN(\r\n    hidden_size, voc.num_words, decoder_n_layers, dropout)\r\ndecoder.eval()\r\ntest_encoder_outputs, test_encoder_hidden = traced_encoder(seq, seq_length)\r\ntest_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\r\ntest_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\r\ntraced_decoder = torch.jit.trace(\r\n    decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\r\n\r\n\r\n@torch.jit.script\r\ndef wrapper(input_seq, input_length):\r\n    PAD_token = 0\r\n    SOS_token = 1\r\n    EOS_token = 2\r\n    max_length = 10\r\n    n_layers = 2\r\n    e_outputs, e_hidden = traced_encoder(input_seq, input_length)\r\n    d_hidden = e_hidden[:n_layers]\r\n    d_input = torch.ones(1, 1, dtype=torch.long)\r\n    d_input *= SOS_token\r\n    #TODO - put EOS check somehwo\r\n    all_tokens = torch.zeros([0], dtype=torch.long)\r\n    while max_length > 0:\r\n        max_length -= 1\r\n        d_output, d_hidden = traced_decoder(d_input, d_hidden, e_outputs)\r\n        _, d_input = torch.max(d_output, dim=1)\r\n        all_tokens = torch.cat((all_tokens, d_input), dim=0)\r\n        d_input = torch.unsqueeze(d_input, 0)\r\n    return all_tokens\r\n\r\n\r\ndef run():\r\n    indexes_batch = [[787, 572, 2]]  # ""hello sir + EOS""\r\n    lengths = torch.tensor([3])\r\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\r\n    tokens = wrapper(input_batch, lengths)\r\n    print(tokens)\r\n\r\n\r\nrun()\r\n\r\n"
18312,"[jit] Trace perform weirdly on BatchNorm## \U0001f41b Bug\r\n\r\nI have use jit to trace a model which is based on ResNet. When the batch size of the input is 1, the result is correct. However, if the batch size is not 1, the result have a little difference. And I think it's caused by the different behavior of BatchNorm.\r\n\r\n## To Reproduce\r\nI use the code below.\r\n\r\n\r\n## Expected behavior\r\n\r\nWith the input shape of (3,3,224,224), the result should be 3*1000=3000, but it's just around 2000.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.13.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\n\r\n[pip3] numpy==1.14.3\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.0",oncall: jit,ailzhang,"## \U0001f41b Bug\r\n\r\nI have use jit to trace a model which is based on ResNet. When the batch size of the input is 1, the result is correct. However, if the batch size is not 1, the result have a little difference. And I think it's caused by the different behavior of BatchNorm.\r\n\r\n## To Reproduce\r\nI use the code below.\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\nresnet = torchvision.models.resnet18(True)\r\nresnet.eval()\r\ntraced_net = torch.jit.trace(resnet,torch.randn((3,3,224,224)))\r\n\r\ntraced_net.eval()\r\n\r\ndef check(inp):\r\n    out1 = resnet(inp)\r\n    out2 = traced_net(inp)\r\n    return torch.sum(out1==out2)\r\n\r\nfor i in range(5):\r\n    print(check(torch.randn((1,3,224,224))))\r\nfor i in range(5):\r\n    print(check(torch.randn((3,3,224,224))))\r\n\r\n#output\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(2009)\r\n#tensor(2066)\r\n#tensor(2119)\r\n#tensor(2049)\r\n#tensor(2023)\r\n```\r\n\r\n## Expected behavior\r\n\r\nWith the input shape of (3,3,224,224), the result should be 3*1000=3000, but it's just around 2000.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.13.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\n\r\n[pip3] numpy==1.14.3\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.0","python\r\nimport torch\r\nimport torchvision\r\n\r\nresnet = torchvision.models.resnet18(True)\r\nresnet.eval()\r\ntraced_net = torch.jit.trace(resnet,torch.randn((3,3,224,224)))\r\n\r\ntraced_net.eval()\r\n\r\ndef check(inp):\r\n    out1 = resnet(inp)\r\n    out2 = traced_net(inp)\r\n    return torch.sum(out1==out2)\r\n\r\nfor i in range(5):\r\n    print(check(torch.randn((1,3,224,224))))\r\nfor i in range(5):\r\n    print(check(torch.randn((3,3,224,224))))\r\n\r\n#output\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(1000)\r\n#tensor(2009)\r\n#tensor(2066)\r\n#tensor(2119)\r\n#tensor(2049)\r\n#tensor(2023)\r\n"
18300,"NCCL backend fails when calling broadcast from different threads## \U0001f41b Bug\r\n\r\nWhen I use the NCCL backend, `torch.distributed.broadcast()` results in a `NCCL error` when called from multiple threads (each thread uses a separate process group). `broadcast()` works fine from multiple threads when using the Gloo backend; NCCL backend works fine in the same environment when using a single thread.\r\n\r\n## To Reproduce\r\n\r\nCode to reproduce:\r\n\r\n\r\n\r\nCommands to reproduce the error (on the same multi-GPU server):\r\n\r\n\r\nError trace:\r\n\r\n\r\nThese commands work as expected:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nNo runtime error / tensors successfully broadcasted among participating ranks.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+056cfaf\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-PCIE-16GB\r\nGPU 1: Tesla V100-PCIE-16GB\r\nGPU 2: Tesla V100-PCIE-16GB\r\nGPU 3: Tesla V100-PCIE-16GB\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.3.2\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.0.0a0+056cfaf\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] torch                     1.0.0a0+056cfaf           <pip>\r\n[conda] torchtext                 0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```",high priority|oncall: distributed|triaged|module: nccl,mrshenli,"## \U0001f41b Bug\r\n\r\nWhen I use the NCCL backend, `torch.distributed.broadcast()` results in a `NCCL error` when called from multiple threads (each thread uses a separate process group). `broadcast()` works fine from multiple threads when using the Gloo backend; NCCL backend works fine in the same environment when using a single thread.\r\n\r\n## To Reproduce\r\n\r\nCode to reproduce:\r\n\r\n```python\r\nimport argparse\r\nimport os\r\nimport threading\r\nimport time\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\nNUM_TRIALS = 20\r\n\r\ndef receive_tensor_helper(tensor, src_rank, group, tag, num_iterations,\r\n                          intra_server_broadcast):\r\n    for i in range(num_iterations):\r\n        if intra_server_broadcast:\r\n            dist.broadcast(tensor=tensor, group=group, src=src_rank)\r\n        else:\r\n            dist.recv(tensor=tensor, src=src_rank, tag=tag)\r\n    print(""Done with tensor size %s"" % tensor.size())\r\n\r\ndef send_tensor_helper(tensor, dst_rank, group, tag, num_iterations,\r\n                       intra_server_broadcast):\r\n    for i in range(num_iterations):\r\n        if intra_server_broadcast:\r\n            dist.broadcast(tensor=tensor, group=group, src=1-dst_rank)\r\n        else:\r\n            dist.send(tensor=tensor, dst=dst_rank, tag=tag)\r\n    print(""Done with tensor size %s"" % tensor.size())\r\n\r\ndef start_helper_thread(func, args):\r\n    helper_thread = threading.Thread(target=func,\r\n                                     args=tuple(args))\r\n    helper_thread.start()\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(\r\n        description='Test lightweight communication library')\r\n    parser.add_argument(""--backend"", type=str, default='gloo',\r\n                        help=""Backend"")\r\n    parser.add_argument('-s', ""--send"", action='store_true',\r\n                        help=""Send tensor (if not specified, will receive tensor)"")\r\n    parser.add_argument(""--master_addr"", required=True, type=str,\r\n                        help=""IP address of master"")\r\n    parser.add_argument(""--use_helper_threads"", action='store_true',\r\n                        help=""Use multiple threads"")\r\n    parser.add_argument(""--rank"", required=True, type=int,\r\n                        help=""Rank of current worker"")\r\n    parser.add_argument('-p', ""--master_port"", default=12345,\r\n                        help=""Port used to communicate tensors"")\r\n    parser.add_argument(""--intra_server_broadcast"", action='store_true',\r\n                        help=""Broadcast within a server"")\r\n\r\n    args = parser.parse_args()\r\n\r\n    num_ranks_in_server = 1\r\n    if args.intra_server_broadcast:\r\n        num_ranks_in_server = 2\r\n    local_rank = args.rank % num_ranks_in_server\r\n    torch.cuda.set_device(local_rank)\r\n\r\n    os.environ['MASTER_ADDR'] = args.master_addr\r\n    os.environ['MASTER_PORT'] = str(args.master_port)\r\n    world_size = 2\r\n    dist.init_process_group(args.backend, rank=args.rank, world_size=world_size)\r\n\r\n    tensor_sizes = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\r\n\r\n    groups = []\r\n    for tag in range(len(tensor_sizes)):\r\n        if args.intra_server_broadcast:\r\n            group = dist.new_group([0, 1])\r\n            groups.append(group)\r\n\r\n    for tag, tensor_size in enumerate(tensor_sizes):\r\n        if args.intra_server_broadcast:\r\n            group = groups[tag]\r\n        else:\r\n            group = None\r\n\r\n        if args.send:\r\n            if args.intra_server_broadcast:\r\n                tensor = torch.tensor(range(tensor_size), dtype=torch.float32).cuda(\r\n                    local_rank)\r\n            else:\r\n                tensor = torch.tensor(range(tensor_size), dtype=torch.float32).cpu()\r\n            if args.use_helper_threads:\r\n                start_helper_thread(send_tensor_helper,\r\n                                    [tensor, 1-args.rank,\r\n                                     group, tag, NUM_TRIALS,\r\n                                     args.intra_server_broadcast])\r\n            else:\r\n                send_tensor_helper(tensor, 1-args.rank, group, tag,\r\n                                   NUM_TRIALS, args.intra_server_broadcast)\r\n        else:\r\n            if args.intra_server_broadcast:\r\n                tensor = torch.zeros((tensor_size,), dtype=torch.float32).cuda(\r\n                    local_rank)\r\n            else:\r\n                tensor = torch.zeros((tensor_size,), dtype=torch.float32).cpu()\r\n            if args.use_helper_threads:\r\n                start_helper_thread(receive_tensor_helper,\r\n                                    [tensor, 1-args.rank,\r\n                                     group, tag, NUM_TRIALS,\r\n                                     args.intra_server_broadcast])\r\n            else:\r\n                receive_tensor_helper(tensor, 1-args.rank, group, tag,\r\n                                      NUM_TRIALS, args.intra_server_broadcast)\r\n```\r\n\r\nCommands to reproduce the error (on the same multi-GPU server):\r\n```bash\r\npython harness.py --master_addr localhost --rank 0 --intra_server_broadcast --backend nccl --use_helper_threads --send\r\npython harness.py --master_addr localhost --rank 1 --intra_server_broadcast --backend nccl --use_helper_threads\r\n```\r\n\r\nError trace:\r\n```bash\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.6/threading.py"", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File ""/opt/conda/lib/python3.6/threading.py"", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""harness.py"", line 15, in receive_tensor_helper\r\n    dist.broadcast(tensor=tensor, group=group, src=src_rank)\r\n  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 741, in broadcast\r\n    work = group.broadcast([tensor], opts)\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:260, unhandled system error\r\n```\r\n\r\nThese commands work as expected:\r\n```bash\r\npython harness.py --master_addr localhost --rank 0 --intra_server_broadcast --backend gloo --use_helper_threads --send\r\npython harness.py --master_addr localhost --rank 1 --intra_server_broadcast --backend gloo --use_helper_threads\r\n```\r\n```bash\r\npython harness.py --master_addr localhost --rank 0 --intra_server_broadcast --backend nccl --send\r\npython harness.py --master_addr localhost --rank 1 --intra_server_broadcast --backend nccl\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo runtime error / tensors successfully broadcasted among participating ranks.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+056cfaf\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-PCIE-16GB\r\nGPU 1: Tesla V100-PCIE-16GB\r\nGPU 2: Tesla V100-PCIE-16GB\r\nGPU 3: Tesla V100-PCIE-16GB\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.3.2\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.0.0a0+056cfaf\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] torch                     1.0.0a0+056cfaf           <pip>\r\n[conda] torchtext                 0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```","python\r\nimport argparse\r\nimport os\r\nimport threading\r\nimport time\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\nNUM_TRIALS = 20\r\n\r\ndef receive_tensor_helper(tensor, src_rank, group, tag, num_iterations,\r\n                          intra_server_broadcast):\r\n    for i in range(num_iterations):\r\n        if intra_server_broadcast:\r\n            dist.broadcast(tensor=tensor, group=group, src=src_rank)\r\n        else:\r\n            dist.recv(tensor=tensor, src=src_rank, tag=tag)\r\n    print(""Done with tensor size %s"" % tensor.size())\r\n\r\ndef send_tensor_helper(tensor, dst_rank, group, tag, num_iterations,\r\n                       intra_server_broadcast):\r\n    for i in range(num_iterations):\r\n        if intra_server_broadcast:\r\n            dist.broadcast(tensor=tensor, group=group, src=1-dst_rank)\r\n        else:\r\n            dist.send(tensor=tensor, dst=dst_rank, tag=tag)\r\n    print(""Done with tensor size %s"" % tensor.size())\r\n\r\ndef start_helper_thread(func, args):\r\n    helper_thread = threading.Thread(target=func,\r\n                                     args=tuple(args))\r\n    helper_thread.start()\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(\r\n        description='Test lightweight communication library')\r\n    parser.add_argument(""--backend"", type=str, default='gloo',\r\n                        help=""Backend"")\r\n    parser.add_argument('-s', ""--send"", action='store_true',\r\n                        help=""Send tensor (if not specified, will receive tensor)"")\r\n    parser.add_argument(""--master_addr"", required=True, type=str,\r\n                        help=""IP address of master"")\r\n    parser.add_argument(""--use_helper_threads"", action='store_true',\r\n                        help=""Use multiple threads"")\r\n    parser.add_argument(""--rank"", required=True, type=int,\r\n                        help=""Rank of current worker"")\r\n    parser.add_argument('-p', ""--master_port"", default=12345,\r\n                        help=""Port used to communicate tensors"")\r\n    parser.add_argument(""--intra_server_broadcast"", action='store_true',\r\n                        help=""Broadcast within a server"")\r\n\r\n    args = parser.parse_args()\r\n\r\n    num_ranks_in_server = 1\r\n    if args.intra_server_broadcast:\r\n        num_ranks_in_server = 2\r\n    local_rank = args.rank % num_ranks_in_server\r\n    torch.cuda.set_device(local_rank)\r\n\r\n    os.environ['MASTER_ADDR'] = args.master_addr\r\n    os.environ['MASTER_PORT'] = str(args.master_port)\r\n    world_size = 2\r\n    dist.init_process_group(args.backend, rank=args.rank, world_size=world_size)\r\n\r\n    tensor_sizes = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\r\n\r\n    groups = []\r\n    for tag in range(len(tensor_sizes)):\r\n        if args.intra_server_broadcast:\r\n            group = dist.new_group([0, 1])\r\n            groups.append(group)\r\n\r\n    for tag, tensor_size in enumerate(tensor_sizes):\r\n        if args.intra_server_broadcast:\r\n            group = groups[tag]\r\n        else:\r\n            group = None\r\n\r\n        if args.send:\r\n            if args.intra_server_broadcast:\r\n                tensor = torch.tensor(range(tensor_size), dtype=torch.float32).cuda(\r\n                    local_rank)\r\n            else:\r\n                tensor = torch.tensor(range(tensor_size), dtype=torch.float32).cpu()\r\n            if args.use_helper_threads:\r\n                start_helper_thread(send_tensor_helper,\r\n                                    [tensor, 1-args.rank,\r\n                                     group, tag, NUM_TRIALS,\r\n                                     args.intra_server_broadcast])\r\n            else:\r\n                send_tensor_helper(tensor, 1-args.rank, group, tag,\r\n                                   NUM_TRIALS, args.intra_server_broadcast)\r\n        else:\r\n            if args.intra_server_broadcast:\r\n                tensor = torch.zeros((tensor_size,), dtype=torch.float32).cuda(\r\n                    local_rank)\r\n            else:\r\n                tensor = torch.zeros((tensor_size,), dtype=torch.float32).cpu()\r\n            if args.use_helper_threads:\r\n                start_helper_thread(receive_tensor_helper,\r\n                                    [tensor, 1-args.rank,\r\n                                     group, tag, NUM_TRIALS,\r\n                                     args.intra_server_broadcast])\r\n            else:\r\n                receive_tensor_helper(tensor, 1-args.rank, group, tag,\r\n                                      NUM_TRIALS, args.intra_server_broadcast)\r\n"
18180,"TypeError:  _queue_reduction() when using DistributedDataParallel to train model with unused parameters## \U0001f41b Bug\r\n\r\nHi, I have tried to use `torch.nn.parallel.DistributedDataParallel` to train a model on multi GPUs. My model *shares* weights between several ""child networks"". In each training step, I want to train only one child network and update its related parameters in the shared model, and other parameters are remain unchanged.\r\nThe code can run on single GPU. However, I met ""TypeError:  _queue_reduction()"" when using `DistributedDataParallel` as the parallel model.\r\nI have found that issue #13273 also report the same error, and I have tried to run the code on PyTorch-nightly build and use `apex.parallel.DistributedDataParallel`, but the error still occurs.\r\nI want to know how to fix this error, and how to train the model with unused parameters on multi-GPUs.\r\nThanks.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Copy the code snippet into **reproduce.py**, and run ``python reproduce.py``.\r\n\r\nThe code snippet to reproduce the error:\r\n\r\n\r\n\r\n2. I also tried the `LegacyDistributedDataParallel` from [here](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/legacy_distributed_data_parallel.py), but it ran out of memory after several epochs and it seems that a GPU memory leak occurred.\r\nRun steps:\r\n    1. Copy [legacy_distributed_data_parallel.py](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/legacy_distributed_data_parallel.py) and [distributed_utils.py](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/distributed_utils.py) to the same place of `reproduce.py`\r\n    2. Run `python reproduce.py --dp lddp`.\r\n\r\n## Expected behavior\r\n\r\n1. Output of `python reproduce.py`:\r\n```\r\n| rank=1, size=2, device_name=cuda:1\r\n| rank=0, size=2, device_name=cuda:0\r\nepoch=0, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=1, batch_num=1, x_shape=torch.Size([1, 3])\r\nepoch=0, rank=0, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=421.31MiB, MaxMem=459.68MiB, CachedMem=462.12MiB\r\n## GPU memory usage: rank=cuda:1, CurrentMem=421.31MiB, MaxMem=459.68MiB, CachedMem=462.12MiB\r\nepoch=0, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\nepoch=0, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=459.70MiB, MaxMem=498.09MiB, CachedMem=540.75MiB\r\n## GPU memory usage: rank=cuda:1, CurrentMem=459.70MiB, MaxMem=498.09MiB, CachedMem=540.75MiB\r\nProcess Process-1:\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 475, in _queue_reduction\r\n    self.device_ids)\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\r\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\r\n\r\nInvoked with: <torch.distributed.ProcessGroupGloo object at 0x7f66f15265a8>, [[None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ -555026.2500, -1591406.2500, -1114738.2500, -1381190.5000],\r\n       device='cuda:0')]], [0]\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 475, in _queue_reduction\r\n    self.device_ids)\r\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\r\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\r\n\r\nInvoked with: <torch.distributed.ProcessGroupGloo object at 0x7f66f15266c0>, [[None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1'), tensor([-6.5016e+05, -2.6875e+02, -1.0882e+06, -4.0550e+06], device='cuda:1')]], [1]\r\njoin\r\njoin done\r\njoin\r\njoin done\r\nall done\r\nTime passed: 10.873415s\r\n```\r\n\r\n2. Output of `python reproduce.py --dp lddp`:\r\n\r\n```\r\n| rank=0, size=2, device_name=cuda:0\r\nepoch=0, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\n| rank=1, size=2, device_name=cuda:1\r\nepoch=0, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=0, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=459.56MiB, MaxMem=459.56MiB, CachedMem=460.12MiB\r\nepoch=0, rank=1, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=459.56MiB, MaxMem=459.56MiB, CachedMem=460.12MiB\r\nepoch=0, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=536.20MiB, MaxMem=536.20MiB, CachedMem=536.75MiB\r\nepoch=0, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=536.20MiB, MaxMem=536.20MiB, CachedMem=536.75MiB\r\nepoch=0, rank=0, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=612.85MiB, MaxMem=612.85MiB, CachedMem=613.38MiB\r\nepoch=0, rank=1, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=612.85MiB, MaxMem=612.85MiB, CachedMem=613.38MiB\r\nepoch=0, rank=0, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=689.49MiB, MaxMem=689.49MiB, CachedMem=690.00MiB\r\n\r\n... after several steps ...\r\n\r\nepoch=50, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10436.11MiB, MaxMem=10436.11MiB, CachedMem=10474.75MiB\r\nepoch=50, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10436.11MiB, MaxMem=10436.11MiB, CachedMem=10474.75MiB\r\nepoch=50, rank=0, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10474.48MiB, MaxMem=10474.48MiB, CachedMem=10513.12MiB\r\nepoch=50, rank=1, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10474.48MiB, MaxMem=10474.48MiB, CachedMem=10513.12MiB\r\nepoch=50, rank=1, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10512.86MiB, MaxMem=10512.86MiB, CachedMem=10551.50MiB\r\nepoch=50, rank=0, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10512.86MiB, MaxMem=10512.86MiB, CachedMem=10551.50MiB\r\nepoch=51, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10551.23MiB, MaxMem=10551.23MiB, CachedMem=10589.88MiB\r\nepoch=51, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10551.23MiB, MaxMem=10551.23MiB, CachedMem=10551.62MiB\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CUDA out of memory. Tried to allocate 38.25 MiB (GPU 0; 11.90 GiB total capacity; 10.30 GiB already allocated; 16.06 MiB free; 364.50 KiB cached)\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/legacy_distributed_data_parallel.py"", line 163, in reduction_fn\r\n    all_reduce(buffered_params)\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/legacy_distributed_data_parallel.py"", line 117, in all_reduce\r\n    distributed_utils.all_reduce(buffer, self.process_group)\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/distributed_utils.py"", line 128, in all_reduce\r\n    return dist.all_reduce(tensor, group=group)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 838, in all_reduce\r\n    work.wait()\r\nRuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [127.0.0.1]:41612\r\njoin\r\njoin done\r\njoin\r\njoin done\r\nall done\r\nTime passed: 34.389752s\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n",high priority|oncall: distributed|triaged,pietern,"## \U0001f41b Bug\r\n\r\nHi, I have tried to use `torch.nn.parallel.DistributedDataParallel` to train a model on multi GPUs. My model *shares* weights between several ""child networks"". In each training step, I want to train only one child network and update its related parameters in the shared model, and other parameters are remain unchanged.\r\nThe code can run on single GPU. However, I met ""TypeError:  _queue_reduction()"" when using `DistributedDataParallel` as the parallel model.\r\nI have found that issue #13273 also report the same error, and I have tried to run the code on PyTorch-nightly build and use `apex.parallel.DistributedDataParallel`, but the error still occurs.\r\nI want to know how to fix this error, and how to train the model with unused parameters on multi-GPUs.\r\nThanks.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Copy the code snippet into **reproduce.py**, and run ``python reproduce.py``.\r\n\r\nThe code snippet to reproduce the error:\r\n\r\n```python\r\n#! /usr/bin/python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport argparse\r\nimport gc\r\nimport os\r\nimport time\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn.functional as F\r\n\r\nfrom torch.multiprocessing import Process\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.nn.parallel import DistributedDataParallel, DataParallel\r\nfrom torch.utils.data.distributed import DistributedSampler\r\n\r\n\r\ndef show_gpu_memory(device=None):\r\n    if device is None:\r\n        device = torch.cuda.current_device()\r\n    print('## GPU memory usage: rank={}, CurrentMem={:.2f}MiB, MaxMem={:.2f}MiB, CachedMem={:.2f}MiB'.format(\r\n        device,\r\n        torch.cuda.memory_allocated(device) / (1 << 20),\r\n        torch.cuda.max_memory_allocated(device) / (1 << 20),\r\n        torch.cuda.memory_cached(device) / (1 << 20)\r\n    ), flush=True)\r\n\r\n\r\nclass CustomDataset(torch.utils.data.Dataset):\r\n    def __init__(self, data_list):\r\n        self.data_list = data_list\r\n\r\n    def __len__(self):\r\n        return len(self.data_list)\r\n\r\n    def __getitem__(self, item):\r\n        return self.data_list[item]\r\n\r\n\r\nclass Child(torch.nn.Module):\r\n    def __init__(self, lin_in, linear, lin_out):\r\n        super().__init__()\r\n        self.lin_in = lin_in\r\n        self.linear = linear\r\n        self.lin_out = lin_out\r\n\r\n    def forward(self, x):\r\n        return self.lin_out(self.linear(self.lin_in(x)))\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.linears = torch.nn.ModuleList([\r\n            torch.nn.Linear(2000, 5000)     # 38.14MiB\r\n            for _ in range(10)\r\n        ])\r\n        self.lin_in = torch.nn.Linear(3, 2000)\r\n        self.lin_out = torch.nn.Linear(5000, 4)\r\n\r\n    @property\r\n    def num_linears(self):\r\n        return len(self.linears)\r\n\r\n    def get_child(self, i):\r\n        return Child(self.lin_in, self.linears[i % self.num_linears], self.lin_out)\r\n\r\n\r\ndef get_dataset():\r\n    data_list = [{""x"": torch.FloatTensor(3).random_(), ""y"": torch.FloatTensor(4).random_()} for i in range(10)]\r\n    return CustomDataset(data_list)\r\n\r\n\r\ndef train(args, rank, size, device_name):\r\n    print('| rank={}, size={}, device_name={}'.format(rank, size, device_name))\r\n    dataset = get_dataset()\r\n    distributed_sampler = DistributedSampler(dataset, size, rank)\r\n    train_loader = DataLoader(dataset, pin_memory=True, sampler=distributed_sampler)\r\n    device = torch.device(device_name)\r\n    model = Model()\r\n    model.to(device)\r\n\r\n    # optimizer = torch.optim.Adam(model.parameters())\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\r\n\r\n    children = [None for _ in range(model.num_linears)]\r\n\r\n    num_updates = 0\r\n    for epoch in range(100):\r\n        model.train()\r\n        for batch_num, batch in enumerate(train_loader):\r\n            print('epoch={}, rank={}, batch_num={}, x_shape={}'.format(\r\n                epoch, rank, batch_num, batch[""x""].shape))\r\n            show_gpu_memory(device)\r\n            optimizer.zero_grad()\r\n\r\n            # Create child from model.\r\n            if not args.cache_child:\r\n                child = model.get_child(num_updates)\r\n            else:\r\n                child = children[num_updates % len(children)]\r\n                if child is None:\r\n                    child = model.get_child(num_updates)\r\n                    children[num_updates % len(children)] = child\r\n\r\n            # Parallel child\r\n            if args.dp == 'ddp':\r\n                child = DistributedDataParallel(child, device_ids=[device])\r\n            elif args.dp == 'dp':\r\n                child = DataParallel(child, device_ids=[device])\r\n            elif args.dp == 'lddp':\r\n                from legacy_distributed_data_parallel import LegacyDistributedDataParallel\r\n                child = LegacyDistributedDataParallel(module=child, world_size=2)\r\n            elif args.dp == 'none':\r\n                pass\r\n            else:\r\n                raise ValueError('Unknown data-parallel model {!r}'.format(args.dp))\r\n\r\n            pred = child(batch['x'].to(device))\r\n            exp = batch['y'].to(device)\r\n            loss = F.binary_cross_entropy_with_logits(exp, pred)\r\n            loss.backward()\r\n            del loss\r\n\r\n            optimizer.step()\r\n            num_updates += 1\r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n    print('Final GPU memory usage:')\r\n    show_gpu_memory(device)\r\n\r\n\r\ndef init_processes(args, rank, size, fn, ddp_args, device):\r\n    """""" Initialize the distributed environment. """"""\r\n    os.environ['MASTER_ADDR'] = ddp_args['master_addr']\r\n    os.environ['MASTER_PORT'] = ddp_args['master_port']\r\n    os.environ['WORLD_SIZE'] = str(size)\r\n    os.environ['RANK'] = str(rank)\r\n    dist.init_process_group(ddp_args['backend'], rank=rank, world_size=size)\r\n    fn(args, rank, size, device)\r\n    print(""done, rank={}"".format(rank))\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--dp', choices=['dp', 'ddp', 'lddp', 'none'], default='ddp',\r\n                        help='data-parallel model, default is %(default)r')\r\n    parser.add_argument('--no-cache-child', action='store_false', dest='cache_child', default=True,\r\n                        help='disable cache child')\r\n    args = parser.parse_args()\r\n\r\n    start_time = time.time()\r\n    ddp_args = {""master_addr"": ""127.0.0.1"", ""master_port"": ""29500"", ""backend"": ""gloo""}\r\n    devices = [""cuda:0"", ""cuda:1""]\r\n    size = len(devices)\r\n    processes = []\r\n    for rank, device in enumerate(devices):\r\n        p = Process(target=init_processes, args=(args, rank, size, train, ddp_args, device))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    while not any(p.exitcode is not None for p in processes):  # all worker processes have finished\r\n        time.sleep(0.2)\r\n\r\n    for p in processes:\r\n        print(""join"")\r\n        p.join()\r\n        print(""join done"")\r\n    print(""all done"")\r\n\r\n    print('Time passed: {:.6f}s'.format(time.time() - start_time))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n\r\n2. I also tried the `LegacyDistributedDataParallel` from [here](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/legacy_distributed_data_parallel.py), but it ran out of memory after several epochs and it seems that a GPU memory leak occurred.\r\nRun steps:\r\n    1. Copy [legacy_distributed_data_parallel.py](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/legacy_distributed_data_parallel.py) and [distributed_utils.py](https://github.com/pytorch/fairseq/blob/v0.6.2/fairseq/distributed_utils.py) to the same place of `reproduce.py`\r\n    2. Run `python reproduce.py --dp lddp`.\r\n\r\n## Expected behavior\r\n\r\n1. Output of `python reproduce.py`:\r\n```\r\n| rank=1, size=2, device_name=cuda:1\r\n| rank=0, size=2, device_name=cuda:0\r\nepoch=0, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=1, batch_num=1, x_shape=torch.Size([1, 3])\r\nepoch=0, rank=0, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=421.31MiB, MaxMem=459.68MiB, CachedMem=462.12MiB\r\n## GPU memory usage: rank=cuda:1, CurrentMem=421.31MiB, MaxMem=459.68MiB, CachedMem=462.12MiB\r\nepoch=0, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\nepoch=0, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=459.70MiB, MaxMem=498.09MiB, CachedMem=540.75MiB\r\n## GPU memory usage: rank=cuda:1, CurrentMem=459.70MiB, MaxMem=498.09MiB, CachedMem=540.75MiB\r\nProcess Process-1:\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 475, in _queue_reduction\r\n    self.device_ids)\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\r\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\r\n\r\nInvoked with: <torch.distributed.ProcessGroupGloo object at 0x7f66f15265a8>, [[None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ -555026.2500, -1591406.2500, -1114738.2500, -1381190.5000],\r\n       device='cuda:0')]], [0]\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 445, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 475, in _queue_reduction\r\n    self.device_ids)\r\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\r\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\r\n\r\nInvoked with: <torch.distributed.ProcessGroupGloo object at 0x7f66f15266c0>, [[None, tensor([[0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.],\r\n        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1'), tensor([-6.5016e+05, -2.6875e+02, -1.0882e+06, -4.0550e+06], device='cuda:1')]], [1]\r\njoin\r\njoin done\r\njoin\r\njoin done\r\nall done\r\nTime passed: 10.873415s\r\n```\r\n\r\n2. Output of `python reproduce.py --dp lddp`:\r\n\r\n```\r\n| rank=0, size=2, device_name=cuda:0\r\nepoch=0, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\n| rank=1, size=2, device_name=cuda:1\r\nepoch=0, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=382.80MiB, MaxMem=382.80MiB, CachedMem=383.50MiB\r\nepoch=0, rank=0, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=459.56MiB, MaxMem=459.56MiB, CachedMem=460.12MiB\r\nepoch=0, rank=1, batch_num=1, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=459.56MiB, MaxMem=459.56MiB, CachedMem=460.12MiB\r\nepoch=0, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=536.20MiB, MaxMem=536.20MiB, CachedMem=536.75MiB\r\nepoch=0, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=536.20MiB, MaxMem=536.20MiB, CachedMem=536.75MiB\r\nepoch=0, rank=0, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=612.85MiB, MaxMem=612.85MiB, CachedMem=613.38MiB\r\nepoch=0, rank=1, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=612.85MiB, MaxMem=612.85MiB, CachedMem=613.38MiB\r\nepoch=0, rank=0, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=689.49MiB, MaxMem=689.49MiB, CachedMem=690.00MiB\r\n\r\n... after several steps ...\r\n\r\nepoch=50, rank=0, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10436.11MiB, MaxMem=10436.11MiB, CachedMem=10474.75MiB\r\nepoch=50, rank=1, batch_num=2, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10436.11MiB, MaxMem=10436.11MiB, CachedMem=10474.75MiB\r\nepoch=50, rank=0, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10474.48MiB, MaxMem=10474.48MiB, CachedMem=10513.12MiB\r\nepoch=50, rank=1, batch_num=3, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10474.48MiB, MaxMem=10474.48MiB, CachedMem=10513.12MiB\r\nepoch=50, rank=1, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10512.86MiB, MaxMem=10512.86MiB, CachedMem=10551.50MiB\r\nepoch=50, rank=0, batch_num=4, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10512.86MiB, MaxMem=10512.86MiB, CachedMem=10551.50MiB\r\nepoch=51, rank=1, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:1, CurrentMem=10551.23MiB, MaxMem=10551.23MiB, CachedMem=10589.88MiB\r\nepoch=51, rank=0, batch_num=0, x_shape=torch.Size([1, 3])\r\n## GPU memory usage: rank=cuda:0, CurrentMem=10551.23MiB, MaxMem=10551.23MiB, CachedMem=10551.62MiB\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CUDA out of memory. Tried to allocate 38.25 MiB (GPU 0; 11.90 GiB total capacity; 10.30 GiB already allocated; 16.06 MiB free; 364.50 KiB cached)\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""/home/yangfan/anaconda3/envs/py36-torch10/lib/python3.6/multiprocessing/process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""reproduce.py"", line 141, in init_processes\r\n    fn(args, rank, size, device)\r\n  File ""reproduce.py"", line 123, in train\r\n    loss.backward()\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/tensor.py"", line 102, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/legacy_distributed_data_parallel.py"", line 163, in reduction_fn\r\n    all_reduce(buffered_params)\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/legacy_distributed_data_parallel.py"", line 117, in all_reduce\r\n    distributed_utils.all_reduce(buffer, self.process_group)\r\n  File ""/home/yangfan/Jobs/FairSeqDist/examples/DebugDistPy/distributed_utils.py"", line 128, in all_reduce\r\n    return dist.all_reduce(tensor, group=group)\r\n  File ""/home/yangfan/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 838, in all_reduce\r\n    work.wait()\r\nRuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:572] Connection closed by peer [127.0.0.1]:41612\r\njoin\r\njoin done\r\njoin\r\njoin done\r\nall done\r\nTime passed: 34.389752s\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n","python\r\n#! /usr/bin/python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport argparse\r\nimport gc\r\nimport os\r\nimport time\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn.functional as F\r\n\r\nfrom torch.multiprocessing import Process\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.nn.parallel import DistributedDataParallel, DataParallel\r\nfrom torch.utils.data.distributed import DistributedSampler\r\n\r\n\r\ndef show_gpu_memory(device=None):\r\n    if device is None:\r\n        device = torch.cuda.current_device()\r\n    print('## GPU memory usage: rank={}, CurrentMem={:.2f}MiB, MaxMem={:.2f}MiB, CachedMem={:.2f}MiB'.format(\r\n        device,\r\n        torch.cuda.memory_allocated(device) / (1 << 20),\r\n        torch.cuda.max_memory_allocated(device) / (1 << 20),\r\n        torch.cuda.memory_cached(device) / (1 << 20)\r\n    ), flush=True)\r\n\r\n\r\nclass CustomDataset(torch.utils.data.Dataset):\r\n    def __init__(self, data_list):\r\n        self.data_list = data_list\r\n\r\n    def __len__(self):\r\n        return len(self.data_list)\r\n\r\n    def __getitem__(self, item):\r\n        return self.data_list[item]\r\n\r\n\r\nclass Child(torch.nn.Module):\r\n    def __init__(self, lin_in, linear, lin_out):\r\n        super().__init__()\r\n        self.lin_in = lin_in\r\n        self.linear = linear\r\n        self.lin_out = lin_out\r\n\r\n    def forward(self, x):\r\n        return self.lin_out(self.linear(self.lin_in(x)))\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.linears = torch.nn.ModuleList([\r\n            torch.nn.Linear(2000, 5000)     # 38.14MiB\r\n            for _ in range(10)\r\n        ])\r\n        self.lin_in = torch.nn.Linear(3, 2000)\r\n        self.lin_out = torch.nn.Linear(5000, 4)\r\n\r\n    @property\r\n    def num_linears(self):\r\n        return len(self.linears)\r\n\r\n    def get_child(self, i):\r\n        return Child(self.lin_in, self.linears[i % self.num_linears], self.lin_out)\r\n\r\n\r\ndef get_dataset():\r\n    data_list = [{""x"": torch.FloatTensor(3).random_(), ""y"": torch.FloatTensor(4).random_()} for i in range(10)]\r\n    return CustomDataset(data_list)\r\n\r\n\r\ndef train(args, rank, size, device_name):\r\n    print('| rank={}, size={}, device_name={}'.format(rank, size, device_name))\r\n    dataset = get_dataset()\r\n    distributed_sampler = DistributedSampler(dataset, size, rank)\r\n    train_loader = DataLoader(dataset, pin_memory=True, sampler=distributed_sampler)\r\n    device = torch.device(device_name)\r\n    model = Model()\r\n    model.to(device)\r\n\r\n    # optimizer = torch.optim.Adam(model.parameters())\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\r\n\r\n    children = [None for _ in range(model.num_linears)]\r\n\r\n    num_updates = 0\r\n    for epoch in range(100):\r\n        model.train()\r\n        for batch_num, batch in enumerate(train_loader):\r\n            print('epoch={}, rank={}, batch_num={}, x_shape={}'.format(\r\n                epoch, rank, batch_num, batch[""x""].shape))\r\n            show_gpu_memory(device)\r\n            optimizer.zero_grad()\r\n\r\n            # Create child from model.\r\n            if not args.cache_child:\r\n                child = model.get_child(num_updates)\r\n            else:\r\n                child = children[num_updates % len(children)]\r\n                if child is None:\r\n                    child = model.get_child(num_updates)\r\n                    children[num_updates % len(children)] = child\r\n\r\n            # Parallel child\r\n            if args.dp == 'ddp':\r\n                child = DistributedDataParallel(child, device_ids=[device])\r\n            elif args.dp == 'dp':\r\n                child = DataParallel(child, device_ids=[device])\r\n            elif args.dp == 'lddp':\r\n                from legacy_distributed_data_parallel import LegacyDistributedDataParallel\r\n                child = LegacyDistributedDataParallel(module=child, world_size=2)\r\n            elif args.dp == 'none':\r\n                pass\r\n            else:\r\n                raise ValueError('Unknown data-parallel model {!r}'.format(args.dp))\r\n\r\n            pred = child(batch['x'].to(device))\r\n            exp = batch['y'].to(device)\r\n            loss = F.binary_cross_entropy_with_logits(exp, pred)\r\n            loss.backward()\r\n            del loss\r\n\r\n            optimizer.step()\r\n            num_updates += 1\r\n    gc.collect()\r\n    torch.cuda.empty_cache()\r\n    print('Final GPU memory usage:')\r\n    show_gpu_memory(device)\r\n\r\n\r\ndef init_processes(args, rank, size, fn, ddp_args, device):\r\n    """""" Initialize the distributed environment. """"""\r\n    os.environ['MASTER_ADDR'] = ddp_args['master_addr']\r\n    os.environ['MASTER_PORT'] = ddp_args['master_port']\r\n    os.environ['WORLD_SIZE'] = str(size)\r\n    os.environ['RANK'] = str(rank)\r\n    dist.init_process_group(ddp_args['backend'], rank=rank, world_size=size)\r\n    fn(args, rank, size, device)\r\n    print(""done, rank={}"".format(rank))\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--dp', choices=['dp', 'ddp', 'lddp', 'none'], default='ddp',\r\n                        help='data-parallel model, default is %(default)r')\r\n    parser.add_argument('--no-cache-child', action='store_false', dest='cache_child', default=True,\r\n                        help='disable cache child')\r\n    args = parser.parse_args()\r\n\r\n    start_time = time.time()\r\n    ddp_args = {""master_addr"": ""127.0.0.1"", ""master_port"": ""29500"", ""backend"": ""gloo""}\r\n    devices = [""cuda:0"", ""cuda:1""]\r\n    size = len(devices)\r\n    processes = []\r\n    for rank, device in enumerate(devices):\r\n        p = Process(target=init_processes, args=(args, rank, size, train, ddp_args, device))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    while not any(p.exitcode is not None for p in processes):  # all worker processes have finished\r\n        time.sleep(0.2)\r\n\r\n    for p in processes:\r\n        print(""join"")\r\n        p.join()\r\n        print(""join done"")\r\n    print(""all done"")\r\n\r\n    print('Time passed: {:.6f}s'.format(time.time() - start_time))\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n"
18126,"IndexError while trying to save torchscript## \U0001f41b Bug\r\n\r\nI have a python script to save a mixture of both traced and  scripted `torchscript` as `.pt`. But on execution, it's thrown index error. I wrapped the `@script` functions into a ScriptModule class and everything is working fine now.\r\n\r\n## To Reproduce\r\n\r\n1. I am using a CharRNN model saved using `torch.jit.trace` -> code is available [here](https://colab.research.google.com/drive/1PquhW_ziRAEvRwPEDd-gE55-6r6L9Pde)\r\n2. Trace model is then loaded into this script and mixing up with `@script` functions\r\n\r\n3. Executing above script will throw `IndexError: _Map_base::at`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe .pt file should be saved. Because I wrapped the above code within a `ScriptModule` class and that's working\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20190307\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] torch==1.0.0.dev20190307\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-nightly-cpu       1.0.0.dev20190307     py3.7_cpu_0    pytorch\r\n",oncall: jit,driazati,"## \U0001f41b Bug\r\n\r\nI have a python script to save a mixture of both traced and  scripted `torchscript` as `.pt`. But on execution, it's thrown index error. I wrapped the `@script` functions into a ScriptModule class and everything is working fine now.\r\n\r\n## To Reproduce\r\n\r\n1. I am using a CharRNN model saved using `torch.jit.trace` -> code is available [here](https://colab.research.google.com/drive/1PquhW_ziRAEvRwPEDd-gE55-6r6L9Pde)\r\n2. Trace model is then loaded into this script and mixing up with `@script` functions\r\n```python\r\nimport torch\r\nimport string\r\n\r\ndebug = True\r\ndevice = 'cpu' if not torch.cuda.is_available() else 'cuda'\r\ntraced_generator = torch.jit.load('CharRNN_model.pt', device)\r\n\r\n\r\n@torch.jit.script\r\ndef post_processing(output):\r\n    output_dist = output.squeeze().div(0.8).exp()\r\n    prob = torch.multinomial(output_dist, 1)[0]\r\n    return prob\r\n\r\n\r\n@torch.jit.script\r\ndef seq_loop(inputs, hidden, prediction_seq_length: int):\r\n    out = []\r\n    i = 0\r\n    while i < prediction_seq_length:\r\n        output, hidden = traced_generator(inputs, hidden)\r\n        inputs = post_processing(output)\r\n        out.append(inputs)\r\n        i += 1\r\n    return out\r\n\r\ntorch.jit.save(seq_loop, 'pytorch_issue.pt')\r\n```\r\n3. Executing above script will throw `IndexError: _Map_base::at`\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe .pt file should be saved. Because I wrapped the above code within a `ScriptModule` class and that's working\r\n```python\r\nimport torch\r\nimport string\r\n\r\ndebug = True\r\ndevice = 'cpu' if not torch.cuda.is_available() else 'cuda'\r\ntraced_generator = torch.jit.load('CharRNN_model.pt', device)\r\n\r\ndef str2int(string_data):\r\n    return [all_characters.index(c) for c in string_data]\r\n\r\ndef int2str(int_data):\r\n    return ''.join([all_characters[i] for i in int_data])\r\n\r\nclass ScriptModuleWrapper(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.traced_generator = traced_generator\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, inputs, hidden):    \r\n        out = []\r\n        i = 0\r\n        n_layers = 2\r\n        hidden_size = 300\r\n        prediction_seq_length = 200\r\n        while i < prediction_seq_length:\r\n            output, hidden = self.traced_generator(inputs, hidden)\r\n            inputs = self.post_processing(output)\r\n            out.append(inputs)\r\n            i += 1\r\n        return out\r\n\r\n    @torch.jit.script_method\r\n    def post_processing(self, output):\r\n        output_dist = output.squeeze().div(0.8).exp()\r\n        prob = torch.multinomial(output_dist, 1)[0]\r\n        return prob\r\n\r\nfilename = 'CharRNN_pipeline.pt'\r\nprint('Saving pipeline to {}'.format(filename))\r\nScriptModuleWrapper().save(filename)\r\n```\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20190307\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.2\r\n[pip] torch==1.0.0.dev20190307\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch-nightly-cpu       1.0.0.dev20190307     py3.7_cpu_0    pytorch\r\n","python\r\nimport torch\r\nimport string\r\n\r\ndebug = True\r\ndevice = 'cpu' if not torch.cuda.is_available() else 'cuda'\r\ntraced_generator = torch.jit.load('CharRNN_model.pt', device)\r\n\r\n\r\n@torch.jit.script\r\ndef post_processing(output):\r\n    output_dist = output.squeeze().div(0.8).exp()\r\n    prob = torch.multinomial(output_dist, 1)[0]\r\n    return prob\r\n\r\n\r\n@torch.jit.script\r\ndef seq_loop(inputs, hidden, prediction_seq_length: int):\r\n    out = []\r\n    i = 0\r\n    while i < prediction_seq_length:\r\n        output, hidden = traced_generator(inputs, hidden)\r\n        inputs = post_processing(output)\r\n        out.append(inputs)\r\n        i += 1\r\n    return out\r\n\r\ntorch.jit.save(seq_loop, 'pytorch_issue.pt')\r\n"
17998,"[feature request] Support bidirectional RNNs in C++ API## \U0001f41b Bug\r\n\r\nC++ API `torch::nn::GRU` fails when `bidirectional=true`. (It works fine when `false`).\r\n\r\nError occurs [here](https://github.com/pytorch/pytorch/blob/42acae5406a024a7eca67bb119bab09222fd2fe6/aten/src/ATen/native/RNN.cpp#L135)\r\n\r\nI tried with different options; the changing the number of layers from 1 to 2 and switching `batch_first` parameter, but it yielded the same outcome.\r\n\r\n## To Reproduce\r\n\r\n**C++ Source**\r\n\r\n\r\n**CMakeLists.txt**\r\n\r\n\r\n**Error Message**\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Odd number of params or hiddens given to a bidirectional RNN (pair_vec at /pytorch/aten/src/ATen/native/RNN.cpp:135)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f185aad22b1 in /pytorch-test/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f185aad1bea in /pytorch-test/libtorch/lib/libc10.so)\r\nframe #2: <unknown function> + 0xa4ed11 (0x7f185b739d11 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #3: at::native::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x272 (0x7f185b7417b2 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #4: at::TypeDefault::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0xc9 (0x7f185ba1e879 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #5: torch::autograd::VariableType::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0x344 (0x7f18656c28d4 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #6: <unknown function> + 0xb03182 (0x7f18659f5182 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #7: std::_Function_handler<std::tuple<at::Tensor, at::Tensor> (at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool), std::tuple<at::Tensor, at::Tensor> (*)(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool)>::_M_invoke(std::_Any_data const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x34 (0x7f18659f5e14 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #8: torch::nn::detail::RNNImplBase<torch::nn::GRUImpl>::generic_forward(std::function<std::tuple<at::Tensor, at::Tensor> (at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool)>, at::Tensor const&, at::Tensor) + 0xd9 (0x7f18659f6869 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #9: torch::nn::GRUImpl::forward(at::Tensor const&, at::Tensor) + 0x56 (0x7f18659f59f6 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #10: Net::forward(at::Tensor) + 0x59 (0x423071 in ./test-app)\r\nframe #11: main + 0xba (0x42033e in ./test-app)\r\nframe #12: __libc_start_main + 0xf0 (0x7f185a166830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #13: _start + 0x29 (0x41f979 in ./test-app)\r\n```\r\n\r\n## Expected behavior\r\n\r\nGRU should work when `bidirectional=true` as well as `bidirectional=false`.\r\n\r\n## Environment\r\n\r\nlibtorch version: latest, downloaded from https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip at `Wed Mar 13 21:47:52 UTC 2019`\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\n## Additional context\r\n",module: cpp|module: rnn|triaged,pbelevich,"## \U0001f41b Bug\r\n\r\nC++ API `torch::nn::GRU` fails when `bidirectional=true`. (It works fine when `false`).\r\n\r\nError occurs [here](https://github.com/pytorch/pytorch/blob/42acae5406a024a7eca67bb119bab09222fd2fe6/aten/src/ATen/native/RNN.cpp#L135)\r\n\r\nI tried with different options; the changing the number of layers from 1 to 2 and switching `batch_first` parameter, but it yielded the same outcome.\r\n\r\n## To Reproduce\r\n\r\n**C++ Source**\r\n```C++\r\n#include <torch/torch.h>\r\n\r\nint BATCH_SIZE = 32;\r\nint SEQ_LEN = 50;\r\nint FEAT_DIM = 64;\r\nbool bidirectional = true;  // it works when this is `false`\r\n\r\nstruct Net : torch::nn::Module {\r\n  torch::nn::GRU gru{nullptr};\r\n  Net() {\r\n    auto opt = torch::nn::GRUOptions(FEAT_DIM, 128);\r\n    opt.bidirectional(bidirectional);\r\n    opt.batch_first(true);\r\n    gru = register_module(""gru"", torch::nn::GRU(opt));\r\n  }\r\n  torch::Tensor forward(torch::Tensor x) {\r\n      return gru->forward(x).output;\r\n  }\r\n};\r\n\r\nint main() {\r\n  auto net = std::make_shared<Net>();\r\n  net->forward(torch::rand({BATCH_SIZE, SEQ_LEN, FEAT_DIM}));\r\n}\r\n```\r\n\r\n**CMakeLists.txt**\r\n```cmake\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\nproject(example-app)\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\nadd_executable(test-app test.cpp)\r\ntarget_link_libraries(test-app ""${TORCH_LIBRARIES}"")\r\nset_property(TARGET test-app PROPERTY CXX_STANDARD 11)\r\n```\r\n\r\n**Error Message**\r\n```\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  Odd number of params or hiddens given to a bidirectional RNN (pair_vec at /pytorch/aten/src/ATen/native/RNN.cpp:135)\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f185aad22b1 in /pytorch-test/libtorch/lib/libc10.so)\r\nframe #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7f185aad1bea in /pytorch-test/libtorch/lib/libc10.so)\r\nframe #2: <unknown function> + 0xa4ed11 (0x7f185b739d11 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #3: at::native::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x272 (0x7f185b7417b2 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #4: at::TypeDefault::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0xc9 (0x7f185ba1e879 in /pytorch-test/libtorch/lib/libcaffe2.so)\r\nframe #5: torch::autograd::VariableType::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0x344 (0x7f18656c28d4 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #6: <unknown function> + 0xb03182 (0x7f18659f5182 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #7: std::_Function_handler<std::tuple<at::Tensor, at::Tensor> (at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool), std::tuple<at::Tensor, at::Tensor> (*)(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool)>::_M_invoke(std::_Any_data const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x34 (0x7f18659f5e14 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #8: torch::nn::detail::RNNImplBase<torch::nn::GRUImpl>::generic_forward(std::function<std::tuple<at::Tensor, at::Tensor> (at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool)>, at::Tensor const&, at::Tensor) + 0xd9 (0x7f18659f6869 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #9: torch::nn::GRUImpl::forward(at::Tensor const&, at::Tensor) + 0x56 (0x7f18659f59f6 in /pytorch-test/libtorch/lib/libtorch.so.1)\r\nframe #10: Net::forward(at::Tensor) + 0x59 (0x423071 in ./test-app)\r\nframe #11: main + 0xba (0x42033e in ./test-app)\r\nframe #12: __libc_start_main + 0xf0 (0x7f185a166830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #13: _start + 0x29 (0x41f979 in ./test-app)\r\n```\r\n\r\n## Expected behavior\r\n\r\nGRU should work when `bidirectional=true` as well as `bidirectional=false`.\r\n\r\n## Environment\r\n\r\nlibtorch version: latest, downloaded from https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip at `Wed Mar 13 21:47:52 UTC 2019`\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\n## Additional context\r\n","C++\r\n#include <torch/torch.h>\r\n\r\nint BATCH_SIZE = 32;\r\nint SEQ_LEN = 50;\r\nint FEAT_DIM = 64;\r\nbool bidirectional = true;  // it works when this is `false`\r\n\r\nstruct Net : torch::nn::Module {\r\n  torch::nn::GRU gru{nullptr};\r\n  Net() {\r\n    auto opt = torch::nn::GRUOptions(FEAT_DIM, 128);\r\n    opt.bidirectional(bidirectional);\r\n    opt.batch_first(true);\r\n    gru = register_module(""gru"", torch::nn::GRU(opt));\r\n  }\r\n  torch::Tensor forward(torch::Tensor x) {\r\n      return gru->forward(x).output;\r\n  }\r\n};\r\n\r\nint main() {\r\n  auto net = std::make_shared<Net>();\r\n  net->forward(torch::rand({BATCH_SIZE, SEQ_LEN, FEAT_DIM}));\r\n}\r\n"
17970,"Binary not operator causes crash when Jit module is executed on different device## \U0001f41b Bug\r\n\r\nWhen the binary not operator `~` is used in a module, which is then Jit traced, it causes a crash when the resulting Jit module is moved to and executed on a different device type (e.g. CPU -> GPU). \r\nThis crash only occurs in the Jit module, but not in eager mode. \r\nUsing `1-tensor` to negate the ByteTensor does not show the same issue in Jit. \r\n\r\n\r\n## To Reproduce\r\n\r\n\r\nThe error console output is:\r\n```\r\nTraceback (most recent call last):\r\n  File ""issue.py"", line 32, in <module>\r\n    not_jit.cuda()(inp.cuda())  # Fails\r\n  File ""/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1347, in forward\r\n    return self._get_method('forward')(*args, **kwargs)\r\nRuntimeError: \r\nexpected type CPUByteType but got CUDAByteType (compute_types at /home/lorenwel/git/pytorch/aten/src/ATen/native/TensorIterator.cpp:134)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6c (0x7f6df917b02c in /home/lorenwel/git/pytorch/torch/lib/libc10.so)\r\nframe #1: at::TensorIterator::compute_types() + 0xcb1 (0x7f6dd0db09b1 in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #2: at::TensorIterator::Builder::build() + 0x5c (0x7f6dd0db679c in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #3: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&) + 0x30a (0x7f6dd0db762a in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #4: at::native::sub_out(at::Tensor&, at::Tensor const&, at::Tensor const&, c10::Scalar) + 0xb2 (0x7f6dd0c31af2 in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #5: at::TypeDefault::sub_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 0x8d (0x7f6dd0fb771d in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #6: torch::autograd::VariableType::sub_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 0x306 (0x7f6dd3894b06 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #7: <unknown function> + 0x5f20a0 (0x7f6dd3b5a0a0 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #8: <unknown function> + 0x626ab5 (0x7f6dd3b8eab5 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #9: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x31 (0x7f6dd3b88ed1 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #10: <unknown function> + 0x60b0d3 (0x7f6dd3b730d3 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #11: <unknown function> + 0x3cc9c8 (0x7f6dffa789c8 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\nframe #12: <unknown function> + 0x3adc76 (0x7f6dffa59c76 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\nframe #13: <unknown function> + 0x10eb46 (0x7f6dff7bab46 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #17: python() [0x5381b4]\r\nframe #20: python() [0x574417]\r\nframe #25: python() [0x574417]\r\nframe #29: python() [0x5381b4]\r\nframe #31: python() [0x57cb45]\r\nframe #33: python() [0x574417]\r\nframe #35: python() [0x5e8ba2]\r\nframe #40: __libc_start_main + 0xe7 (0x7f6e09502b97 in /lib/x86_64-linux-gnu/libc.so.6)\r\n:\r\noperation failed in interpreter:\r\nissue.py(12): forward\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py(477): _slow_forward\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py(487): __call__\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py(636): trace\r\nissue.py(26): <module>\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt should not crash.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script]:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+743fdbd\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.78\r\ncuDNN version: Probably one of the following:\r\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\n - PyTorch Version 1.0.1\r\n - Ubuntu 18.04\r\n - Compiled from source with `python setup.py install`\r\n - Python 3.6.8\r\n - CUDA 10.0, cuDNN 7.4.2\r\n - Nvidia RTX 2080\r\n\r\n## Additional context\r\n\r\nThe same issue also occurs when loading the failing jit module into libtorch and executing it there. \r\n\n\ncc @suo",oncall: jit|triaged,Krovatkin,"## \U0001f41b Bug\r\n\r\nWhen the binary not operator `~` is used in a module, which is then Jit traced, it causes a crash when the resulting Jit module is moved to and executed on a different device type (e.g. CPU -> GPU). \r\nThis crash only occurs in the Jit module, but not in eager mode. \r\nUsing `1-tensor` to negate the ByteTensor does not show the same issue in Jit. \r\n\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass OneMinus(nn.Module):\r\n  def forward(self, inp):\r\n    mask = inp > 0.5\r\n    return inp[1-mask]\r\n\r\nclass Not(nn.Module):\r\n  def forward(self, inp):\r\n    mask = inp > 0.5\r\n    return inp[~mask]\r\n\r\ninp = torch.rand((8,))\r\n\r\n# Eager\r\nout = OneMinus()(inp)   # Works\r\nout = Not()(inp)    # Works\r\n\r\nout = OneMinus().cuda()(inp.cuda())   # Works\r\nout = Not().cuda()(inp.cuda())  # Works\r\n\r\n\r\n# Jit\r\noneminus_jit = torch.jit.trace(OneMinus(), inp)\r\nnot_jit = torch.jit.trace(Not(), inp)\r\n\r\noneminus_jit(inp)   # Works\r\nnot_jit(inp)    # Works\r\n\r\noneminus_jit.cuda()(inp.cuda())   # Works\r\nnot_jit.cuda()(inp.cuda())  # Fails\r\n\r\n\r\n# Jit Trace with Cuda\r\nnot_jit_cuda = torch.jit.trace(Not().cuda(), inp.cuda())\r\nnot_jit_cuda(inp.cuda())  # Works\r\nnot_jit_cuda.cpu()(inp)  # Fails\r\n```\r\nThe error console output is:\r\n```\r\nTraceback (most recent call last):\r\n  File ""issue.py"", line 32, in <module>\r\n    not_jit.cuda()(inp.cuda())  # Fails\r\n  File ""/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1347, in forward\r\n    return self._get_method('forward')(*args, **kwargs)\r\nRuntimeError: \r\nexpected type CPUByteType but got CUDAByteType (compute_types at /home/lorenwel/git/pytorch/aten/src/ATen/native/TensorIterator.cpp:134)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6c (0x7f6df917b02c in /home/lorenwel/git/pytorch/torch/lib/libc10.so)\r\nframe #1: at::TensorIterator::compute_types() + 0xcb1 (0x7f6dd0db09b1 in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #2: at::TensorIterator::Builder::build() + 0x5c (0x7f6dd0db679c in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #3: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&) + 0x30a (0x7f6dd0db762a in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #4: at::native::sub_out(at::Tensor&, at::Tensor const&, at::Tensor const&, c10::Scalar) + 0xb2 (0x7f6dd0c31af2 in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #5: at::TypeDefault::sub_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 0x8d (0x7f6dd0fb771d in /home/lorenwel/git/pytorch/torch/lib/libcaffe2.so)\r\nframe #6: torch::autograd::VariableType::sub_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 0x306 (0x7f6dd3894b06 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #7: <unknown function> + 0x5f20a0 (0x7f6dd3b5a0a0 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #8: <unknown function> + 0x626ab5 (0x7f6dd3b8eab5 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #9: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x31 (0x7f6dd3b88ed1 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #10: <unknown function> + 0x60b0d3 (0x7f6dd3b730d3 in /home/lorenwel/git/pytorch/torch/lib/libtorch.so.1)\r\nframe #11: <unknown function> + 0x3cc9c8 (0x7f6dffa789c8 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\nframe #12: <unknown function> + 0x3adc76 (0x7f6dffa59c76 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\nframe #13: <unknown function> + 0x10eb46 (0x7f6dff7bab46 in /home/lorenwel/git/pytorch/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #17: python() [0x5381b4]\r\nframe #20: python() [0x574417]\r\nframe #25: python() [0x574417]\r\nframe #29: python() [0x5381b4]\r\nframe #31: python() [0x57cb45]\r\nframe #33: python() [0x574417]\r\nframe #35: python() [0x5e8ba2]\r\nframe #40: __libc_start_main + 0xe7 (0x7f6e09502b97 in /lib/x86_64-linux-gnu/libc.so.6)\r\n:\r\noperation failed in interpreter:\r\nissue.py(12): forward\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py(477): _slow_forward\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py(487): __call__\r\n/home/lorenwel/venv/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py(636): trace\r\nissue.py(26): <module>\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt should not crash.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script]:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+743fdbd\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.78\r\ncuDNN version: Probably one of the following:\r\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\n - PyTorch Version 1.0.1\r\n - Ubuntu 18.04\r\n - Compiled from source with `python setup.py install`\r\n - Python 3.6.8\r\n - CUDA 10.0, cuDNN 7.4.2\r\n - Nvidia RTX 2080\r\n\r\n## Additional context\r\n\r\nThe same issue also occurs when loading the failing jit module into libtorch and executing it there. \r\n\n\ncc @suo","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass OneMinus(nn.Module):\r\n  def forward(self, inp):\r\n    mask = inp > 0.5\r\n    return inp[1-mask]\r\n\r\nclass Not(nn.Module):\r\n  def forward(self, inp):\r\n    mask = inp > 0.5\r\n    return inp[~mask]\r\n\r\ninp = torch.rand((8,))\r\n\r\n# Eager\r\nout = OneMinus()(inp)   # Works\r\nout = Not()(inp)    # Works\r\n\r\nout = OneMinus().cuda()(inp.cuda())   # Works\r\nout = Not().cuda()(inp.cuda())  # Works\r\n\r\n\r\n# Jit\r\noneminus_jit = torch.jit.trace(OneMinus(), inp)\r\nnot_jit = torch.jit.trace(Not(), inp)\r\n\r\noneminus_jit(inp)   # Works\r\nnot_jit(inp)    # Works\r\n\r\noneminus_jit.cuda()(inp.cuda())   # Works\r\nnot_jit.cuda()(inp.cuda())  # Fails\r\n\r\n\r\n# Jit Trace with Cuda\r\nnot_jit_cuda = torch.jit.trace(Not().cuda(), inp.cuda())\r\nnot_jit_cuda(inp.cuda())  # Works\r\nnot_jit_cuda.cpu()(inp)  # Fails\r\n"
17913,"Bug in CosineAnnealingLR (division by zero)## \U0001f41b Bug\r\n\r\nThe `CosineAnnealingLR` throws a `ZeroDivisionError` after being applied for more than one epoch.\r\nAlso, the learning rates returned by the `CosineAnnealingLR` do not match the internal learning rates of the passed `optimizer`.\r\n\r\n## To Reproduce\r\n\r\n\r\n## Expected behavior\r\n\r\nUsing the legacy implementation, no exception is raised and the learning rates of the scheduler and optimizer are equal:\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): `1.0.0.dev20190312`\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): conda nightlies\r\n - Python version: 3.7\r\n\r\n## Additional context\r\n\r\nThis issue might to be related to the refactoring of all schedulers done [here](https://github.com/pytorch/pytorch/pull/14010).\r\n\r\nCC @chandlerzuo @ezyang \r\n",high priority|module: optimizer|triaged|module: regression,ezyang|chandlerzuo,"## \U0001f41b Bug\r\n\r\nThe `CosineAnnealingLR` throws a `ZeroDivisionError` after being applied for more than one epoch.\r\nAlso, the learning rates returned by the `CosineAnnealingLR` do not match the internal learning rates of the passed `optimizer`.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.optim.lr_scheduler import _LRScheduler\r\nimport math\r\n\r\n# Test new scheduler\r\nmodel = nn.Linear(10, 2)\r\noptimizer = optim.SGD(model.parameters(), lr=1.)\r\nsteps = 10\r\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\r\n\r\ntry:\r\n    for epoch in range(2):\r\n        for idx in range(steps):\r\n            scheduler.step()\r\n            print(scheduler.get_lr())\r\n            print(optimizer.param_groups[0]['lr'])\r\nexcept ZeroDivisionError as e:\r\n    print(e)\r\n\r\n> [1.0]\r\n1.0\r\n[0.9516553824444453]\r\n0.9755282581475768\r\n[0.8386590697412432]\r\n0.9045084971874737\r\n[0.6968044012954536]\r\n0.7938926261462366\r\n[0.5395961100811341]\r\n0.6545084971874737\r\n[0.38196601125010515]\r\n0.5\r\n[0.23872875703131577]\r\n0.34549150281252633\r\n[0.12295598939793909]\r\n0.2061073738537635\r\n[0.04424211972088169]\r\n0.09549150281252633\r\n[0.006271407734229157]\r\n0.024471741852423234\r\n[0.0]\r\n0.0\r\nfloat division by zero\r\n```\r\n\r\n## Expected behavior\r\n\r\nUsing the legacy implementation, no exception is raised and the learning rates of the scheduler and optimizer are equal:\r\n```python\r\n# Compare with legacy implementation\r\nclass LegacyCosineAnnealingLR(_LRScheduler):\r\n    r""""""Set the learning rate of each parameter group using a cosine annealing\r\n    schedule, where :math:`\\eta_{max}` is set to the initial lr and\r\n    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\r\n\r\n    .. math::\r\n\r\n        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\r\n        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\r\n\r\n    When last_epoch=-1, sets initial lr as lr.\r\n\r\n    It has been proposed in\r\n    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\r\n    implements the cosine annealing part of SGDR, and not the restarts.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        T_max (int): Maximum number of iterations.\r\n        eta_min (float): Minimum learning rate. Default: 0.\r\n        last_epoch (int): The index of last epoch. Default: -1.\r\n\r\n    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\r\n        https://arxiv.org/abs/1608.03983\r\n    """"""\r\n\r\n    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\r\n        self.T_max = T_max\r\n        self.eta_min = eta_min\r\n        super(LegacyCosineAnnealingLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        return [self.eta_min + (base_lr - self.eta_min) *\r\n                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\r\n                for base_lr in self.base_lrs]\r\n\r\n\r\nmodel = nn.Linear(10, 2)\r\noptimizer = optim.SGD(model.parameters(), lr=1.)\r\nsteps = 10\r\nscheduler = LegacyCosineAnnealingLR(optimizer, steps)\r\n\r\nfor epoch in range(2):\r\n    for idx in range(steps):\r\n        scheduler.step()\r\n        print(scheduler.get_lr())\r\n        print(optimizer.param_groups[0]['lr'])\r\n\r\n> [1.0]\r\n1.0\r\n[0.9755282581475768]\r\n0.9755282581475768\r\n[0.9045084971874737]\r\n0.9045084971874737\r\n[0.7938926261462366]\r\n0.7938926261462366\r\n[0.6545084971874737]\r\n0.6545084971874737\r\n[0.5]\r\n0.5\r\n[0.34549150281252633]\r\n0.34549150281252633\r\n[0.2061073738537635]\r\n0.2061073738537635\r\n[0.09549150281252633]\r\n0.09549150281252633\r\n[0.024471741852423234]\r\n0.024471741852423234\r\n[0.0]\r\n0.0\r\n[0.024471741852423123]\r\n0.024471741852423123\r\n[0.09549150281252622]\r\n0.09549150281252622\r\n[0.20610737385376338]\r\n0.20610737385376338\r\n[0.3454915028125262]\r\n0.3454915028125262\r\n[0.4999999999999999]\r\n0.4999999999999999\r\n[0.6545084971874736]\r\n0.6545084971874736\r\n[0.7938926261462365]\r\n0.7938926261462365\r\n[0.9045084971874737]\r\n0.9045084971874737\r\n[0.9755282581475768]\r\n0.9755282581475768\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): `1.0.0.dev20190312`\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): conda nightlies\r\n - Python version: 3.7\r\n\r\n## Additional context\r\n\r\nThis issue might to be related to the refactoring of all schedulers done [here](https://github.com/pytorch/pytorch/pull/14010).\r\n\r\nCC @chandlerzuo @ezyang \r\n","python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.optim.lr_scheduler import _LRScheduler\r\nimport math\r\n\r\n# Test new scheduler\r\nmodel = nn.Linear(10, 2)\r\noptimizer = optim.SGD(model.parameters(), lr=1.)\r\nsteps = 10\r\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, steps)\r\n\r\ntry:\r\n    for epoch in range(2):\r\n        for idx in range(steps):\r\n            scheduler.step()\r\n            print(scheduler.get_lr())\r\n            print(optimizer.param_groups[0]['lr'])\r\nexcept ZeroDivisionError as e:\r\n    print(e)\r\n\r\n> [1.0]\r\n1.0\r\n[0.9516553824444453]\r\n0.9755282581475768\r\n[0.8386590697412432]\r\n0.9045084971874737\r\n[0.6968044012954536]\r\n0.7938926261462366\r\n[0.5395961100811341]\r\n0.6545084971874737\r\n[0.38196601125010515]\r\n0.5\r\n[0.23872875703131577]\r\n0.34549150281252633\r\n[0.12295598939793909]\r\n0.2061073738537635\r\n[0.04424211972088169]\r\n0.09549150281252633\r\n[0.006271407734229157]\r\n0.024471741852423234\r\n[0.0]\r\n0.0\r\nfloat division by zero\r\n"
17881,"Weak Script functions/modules are not compiled in tracingWhen a weak script module is traced, there's no hook to compile it into a `ScriptModule`, so the code in the module gets traced instead of compiled and inlined.\r\n\r\nExample\r\n\r\nOutputs\r\n```\r\ngraph(%input : Float(4, 4),\r\n      %weight : Float(4, 4),\r\n      %bias : Float(4)):\r\n  %3 : Float(4!, 4!) = aten::t(%weight), scope: Linear\r\n  %4 : int = prim::Constant[value=1](), scope: Linear\r\n  %5 : int = prim::Constant[value=1](), scope: Linear\r\n  %6 : Float(4, 4) = aten::addmm(%bias, %input, %3, %4, %5), scope: Linear\r\n  return (%6)\r\n\r\ngraph(%x : Tensor,\r\n      %1 : Tensor,\r\n      %2 : Tensor):\r\n  %5 : int = prim::Constant[value=1]()\r\n  %4 : None = prim::Constant()\r\n  %3 : int = prim::Constant[value=2]()\r\n  %6 : int = aten::dim(%x)\r\n  %7 : bool = aten::eq(%6, %3)\r\n  %8 : bool = prim::If(%7)\r\n    block0():\r\n      %9 : bool = aten::__isnot__(%2, %4)\r\n      -> (%9)\r\n    block1():\r\n      -> (%7)\r\n  %ret : Tensor = prim::If(%8)\r\n    block0():\r\n      %bias.2 : Tensor = prim::unchecked_unwrap_optional(%2)\r\n      %12 : Tensor = aten::t(%1)\r\n      %ret.1 : Tensor = aten::addmm(%bias.2, %x, %12, %5, %5)\r\n      -> (%ret.1)\r\n    block1():\r\n      %14 : Tensor = aten::t(%1)\r\n      %output.1 : Tensor = aten::matmul(%x, %14)\r\n      %16 : bool = aten::__isnot__(%2, %4)\r\n      %output : Tensor = prim::If(%16)\r\n        block0():\r\n          %bias.3 : Tensor = prim::unchecked_unwrap_optional(%2)\r\n          %output.2 : Tensor = aten::add_(%output.1, %bias.3, %5)\r\n          -> (%output.2)\r\n        block1():\r\n          -> (%output.1)\r\n      -> (%output)\r\n  return (%ret)\r\n```\r\n\r\n",oncall: jit|low priority,eellison,"When a weak script module is traced, there's no hook to compile it into a `ScriptModule`, so the code in the module gets traced instead of compiled and inlined.\r\n\r\nExample\r\n```python\r\nimport torch\r\n\r\nlinear = torch.nn.Linear(4, 4)\r\ndef fn(x):\r\n    return linear(x)\r\n\r\ninput = torch.randn(4, 4, requires_grad=False)\r\ntraced = torch.jit.trace(linear, (input))\r\nprint(traced.graph)\r\n\r\nclass M(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.linear = torch.nn.Linear(4, 4)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\nprint(M().graph)\r\n```\r\nOutputs\r\n```\r\ngraph(%input : Float(4, 4),\r\n      %weight : Float(4, 4),\r\n      %bias : Float(4)):\r\n  %3 : Float(4!, 4!) = aten::t(%weight), scope: Linear\r\n  %4 : int = prim::Constant[value=1](), scope: Linear\r\n  %5 : int = prim::Constant[value=1](), scope: Linear\r\n  %6 : Float(4, 4) = aten::addmm(%bias, %input, %3, %4, %5), scope: Linear\r\n  return (%6)\r\n\r\ngraph(%x : Tensor,\r\n      %1 : Tensor,\r\n      %2 : Tensor):\r\n  %5 : int = prim::Constant[value=1]()\r\n  %4 : None = prim::Constant()\r\n  %3 : int = prim::Constant[value=2]()\r\n  %6 : int = aten::dim(%x)\r\n  %7 : bool = aten::eq(%6, %3)\r\n  %8 : bool = prim::If(%7)\r\n    block0():\r\n      %9 : bool = aten::__isnot__(%2, %4)\r\n      -> (%9)\r\n    block1():\r\n      -> (%7)\r\n  %ret : Tensor = prim::If(%8)\r\n    block0():\r\n      %bias.2 : Tensor = prim::unchecked_unwrap_optional(%2)\r\n      %12 : Tensor = aten::t(%1)\r\n      %ret.1 : Tensor = aten::addmm(%bias.2, %x, %12, %5, %5)\r\n      -> (%ret.1)\r\n    block1():\r\n      %14 : Tensor = aten::t(%1)\r\n      %output.1 : Tensor = aten::matmul(%x, %14)\r\n      %16 : bool = aten::__isnot__(%2, %4)\r\n      %output : Tensor = prim::If(%16)\r\n        block0():\r\n          %bias.3 : Tensor = prim::unchecked_unwrap_optional(%2)\r\n          %output.2 : Tensor = aten::add_(%output.1, %bias.3, %5)\r\n          -> (%output.2)\r\n        block1():\r\n          -> (%output.1)\r\n      -> (%output)\r\n  return (%ret)\r\n```\r\n\r\n","python\r\nimport torch\r\n\r\nlinear = torch.nn.Linear(4, 4)\r\ndef fn(x):\r\n    return linear(x)\r\n\r\ninput = torch.randn(4, 4, requires_grad=False)\r\ntraced = torch.jit.trace(linear, (input))\r\nprint(traced.graph)\r\n\r\nclass M(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.linear = torch.nn.Linear(4, 4)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\nprint(M().graph)\r\n"
17819,"Dilated Convolution 2D runs a lot slower in C++ TorchScript## \U0001f41b Bug\r\n\r\nIf a model contains dilated convolution and uses JIT trace to export to TorchScript Module, the exported module will run slower in C++ inference than in Python on CPU.\r\nFor an internal model the slow down is significant, where Python inference takes ~10s with `OMP_NUM_THREADS=2`, and C++ inference takes ~31s.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Use the following code to export a ResNet101 model with modified dilation:\r\n\r\n2. Use the following code for Python inference profiling:\r\n\r\nThe output is\r\n```\r\n$ OMP_NUM_THREADS=1 python inference.py traced_resnet101_dilation.pt \r\ninference took 1764.76ms\r\ninference took 1369.52ms\r\ninference took 1363.28ms\r\ninference took 1358.34ms\r\ninference took 1354.14ms\r\n```\r\nThe profile tracing file is:\r\n[profile_python_cpu.json.txt](https://github.com/pytorch/pytorch/files/2947609/profile_python_cpu.json.txt)\r\n\r\nThe profile tracing screenshot is:\r\n![image](https://user-images.githubusercontent.com/1560135/54057937-62e2c380-41c2-11e9-81de-22b9680aa301.png)\r\n\r\n3. Use the following code for C++ inference profiling:\r\n\r\nThe output is:\r\n```\r\n$ OMP_NUM_THREADS=1 ./inference torch_cpu_slowdown/traced_resnet101_dilation.pt \r\nInference took 1871.63ms\r\nInference took 1580.98ms\r\nInference took 1588.3ms\r\nInference took 1574.94ms\r\nInference took 1570.36ms\r\n```\r\nThe profile tracing file is:\r\n[profile_cpp_cpu.json.txt](https://github.com/pytorch/pytorch/files/2947614/profile_cpp_cpu.json.txt)\r\n\r\nThe profile tracing screenshot is:\r\n![image](https://user-images.githubusercontent.com/1560135/54057918-4e9ec680-41c2-11e9-934f-2b021e3c995e.png)\r\n\r\n## Expected behavior\r\n\r\nThe Python and C++ inference using TorchScript Module should have similar performance. However, as the results from step 2 and 3, the dilation convolution layers runs ~29ms on Python but ~291ms on C++. There is a 10x slow down happening on C++ inference.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): I think it's 1.0, we are building from commit a3f600e.\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 2.7.6\r\n - CUDA/cuDNN version: 10.0.130 / 7.4.1.5\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nWe have an internal model which suffers significant slow down on CPU due to the slow dilation convolution on C++ inference.\r\n",oncall: jit,yf225,"## \U0001f41b Bug\r\n\r\nIf a model contains dilated convolution and uses JIT trace to export to TorchScript Module, the exported module will run slower in C++ inference than in Python on CPU.\r\nFor an internal model the slow down is significant, where Python inference takes ~10s with `OMP_NUM_THREADS=2`, and C++ inference takes ~31s.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Use the following code to export a ResNet101 model with modified dilation:\r\n```python\r\nimport torch\r\nfrom torchvision import models\r\nimport time\r\n\r\nnet = models.resnet101(pretrained=True).cpu()\r\n# artificially create a dilation layer \r\nnet.conv1.dilation=(3, 3)\r\nnet = net.eval()\r\n\r\n# 4batch size\r\nexample = torch.rand(4, 3, 224, 224).cpu()\r\n\r\nwith torch.no_grad():\r\n    traced_script_module = torch.jit.trace(net, example)\r\n\r\ntraced_script_module.save('traced_resnet101_dilation.pt')\r\n```\r\n2. Use the following code for Python inference profiling:\r\n```python\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport sys\r\nimport time\r\nimport torch\r\n\r\ntraced_script_module = torch.jit.load(sys.argv[1])\r\nexample = torch.rand(4, 3, 224, 224).cpu()\r\n\r\nwith torch.no_grad():\r\n    for _ in range(5):\r\n        start_ts = time.time()\r\n        with torch.autograd.profiler.profile() as profile:\r\n            traced_script_module(example)\r\n        end_ts = time.time()\r\n        print('inference took {:.2f}ms'.format((end_ts - start_ts) * 1000))\r\n\r\n        profile.export_chrome_trace('profile_python_cpu.json')\r\n```\r\nThe output is\r\n```\r\n$ OMP_NUM_THREADS=1 python inference.py traced_resnet101_dilation.pt \r\ninference took 1764.76ms\r\ninference took 1369.52ms\r\ninference took 1363.28ms\r\ninference took 1358.34ms\r\ninference took 1354.14ms\r\n```\r\nThe profile tracing file is:\r\n[profile_python_cpu.json.txt](https://github.com/pytorch/pytorch/files/2947609/profile_python_cpu.json.txt)\r\n\r\nThe profile tracing screenshot is:\r\n![image](https://user-images.githubusercontent.com/1560135/54057937-62e2c380-41c2-11e9-81de-22b9680aa301.png)\r\n\r\n3. Use the following code for C++ inference profiling:\r\n```cpp\r\n#include <torch/torch.h>\r\n#include <torch/script.h>\r\n\r\n#include <torch/csrc/autograd/profiler.h>\r\n\r\nvoid benchmark(const char* model_name) {\r\n    torch::NoGradGuard no_grad;\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\r\n    module->to(torch::kCPU);\r\n\r\n    std::vector<torch::jit::IValue> batch;\r\n    batch.push_back(torch::randn({4, 3, 224, 224}).set_requires_grad(false).cpu());\r\n\r\n    for (int i=0; i<5; i++) {\r\n        std::chrono::high_resolution_clock::time_point begin = std::chrono::high_resolution_clock::now();\r\n\r\n        {\r\n            torch::autograd::profiler::RecordProfile profiler(""profile_cpp_cpu.json"");\r\n            module->forward(batch);\r\n        }\r\n\r\n        std::chrono::high_resolution_clock::time_point end = std::chrono::high_resolution_clock::now();\r\n        std::cout << ""Inference took "" << std::chrono::duration_cast<std::chrono::microseconds>(end - begin).count() / 1000.0 << ""ms\\n"";\r\n    }\r\n}\r\n\r\nint main(int argc, const char* argv[]) {\r\n    benchmark(argv[1]);\r\n}\r\n```\r\nThe output is:\r\n```\r\n$ OMP_NUM_THREADS=1 ./inference torch_cpu_slowdown/traced_resnet101_dilation.pt \r\nInference took 1871.63ms\r\nInference took 1580.98ms\r\nInference took 1588.3ms\r\nInference took 1574.94ms\r\nInference took 1570.36ms\r\n```\r\nThe profile tracing file is:\r\n[profile_cpp_cpu.json.txt](https://github.com/pytorch/pytorch/files/2947614/profile_cpp_cpu.json.txt)\r\n\r\nThe profile tracing screenshot is:\r\n![image](https://user-images.githubusercontent.com/1560135/54057918-4e9ec680-41c2-11e9-934f-2b021e3c995e.png)\r\n\r\n## Expected behavior\r\n\r\nThe Python and C++ inference using TorchScript Module should have similar performance. However, as the results from step 2 and 3, the dilation convolution layers runs ~29ms on Python but ~291ms on C++. There is a 10x slow down happening on C++ inference.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): I think it's 1.0, we are building from commit a3f600e.\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 2.7.6\r\n - CUDA/cuDNN version: 10.0.130 / 7.4.1.5\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\nWe have an internal model which suffers significant slow down on CPU due to the slow dilation convolution on C++ inference.\r\n","python\r\nimport torch\r\nfrom torchvision import models\r\nimport time\r\n\r\nnet = models.resnet101(pretrained=True).cpu()\r\n# artificially create a dilation layer \r\nnet.conv1.dilation=(3, 3)\r\nnet = net.eval()\r\n\r\n# 4batch size\r\nexample = torch.rand(4, 3, 224, 224).cpu()\r\n\r\nwith torch.no_grad():\r\n    traced_script_module = torch.jit.trace(net, example)\r\n\r\ntraced_script_module.save('traced_resnet101_dilation.pt')\r\n"
17751,Integers to negative integer powers are handled incorrectly (both CPU and CUDA)\r\n\r\nNumPy handles it nicely:\r\n\r\n\r\n\r\nPyTorch on CUDA freaks out irrecoverably:\r\n\r\n\r\n\r\nPyTorch on CPU crashes with a C++ runtime error:\r\n\r\n,high priority|module: bootcamp|module: cuda|module: error checking|triaged|small,skrah,"```python\r\n    Python 3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 19:16:44) \r\n    [GCC 7.3.0] on linux\r\n    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n    >>> import torch\r\n    >>> torch.__version__\r\n    '1.0.0.dev20190114'\r\n    >>>\r\n```\r\n\r\nNumPy handles it nicely:\r\n\r\n```python\r\n    >>> 2 ** torch.tensor(-1).numpy()\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n    ValueError: Integers to negative integer powers are not allowed.\r\n    >>>\r\n```\r\n\r\nPyTorch on CUDA freaks out irrecoverably:\r\n\r\n```python\r\n    >>> 2 ** torch.tensor(-1, device='cuda')\r\n    /opt/conda/conda-bld/pytorch-nightly_1547458468907/work/aten/src/THC/THCNumerics.cuh:27: T powi(T, T) [with T = long]: block: [0,0,0], thread: [0,0,0] Assertion `THCNumerics<T>::ge(b, 0)` failed.\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n      File "".../anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 66, in __repr__\r\n        return torch._tensor_str._str(self)\r\n      File "".../anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 277, in _str\r\n        tensor_str = _tensor_str(self, indent)\r\n      File "".../anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 195, in _tensor_str\r\n        formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n      File "".../anaconda3/lib/python3.6/site-packages/torch/_tensor_str.py"", line 80, in __init__\r\n        value_str = '{}'.format(value)\r\n      File "".../anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 380, in __format__\r\n        return self.item().__format__(format_spec)\r\n    RuntimeError: CUDA error: device-side assert triggered\r\n    >>>2 ** torch.tensor(2, device='cuda')\r\n    Traceback (most recent call last):\r\n      File ""<stdin>"", line 1, in <module>\r\n    RuntimeError: CUDA error: device-side assert triggered\r\n```\r\n\r\nPyTorch on CPU crashes with a C++ runtime error:\r\n\r\n```python\r\n    >>> 2 ** torch.tensor(-1)\r\n    terminate called after throwing an instance of 'std::runtime_error'\r\n      what():  invalid argument 1: Integers to negative integer powers are not allowed at /opt/conda/conda-bld/pytorch-nightly_1547458468907/work/aten/src/TH/generic/THTensorApply.hpp:167\r\n    Aborted\r\n    $\r\n```","python\r\n    Python 3.6.7 |Anaconda custom (64-bit)| (default, Oct 23 2018, 19:16:44) \r\n    [GCC 7.3.0] on linux\r\n    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n    >>> import torch\r\n    >>> torch.__version__\r\n    '1.0.0.dev20190114'\r\n    >>>\r\n"
17669,"[jit] Bad optional access during training for ScriptModule## \U0001f41b Bug\r\n\r\nThere appears to be some regression recently, the following script produce a error saying bad optional access, it used to work several days ago. \r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nit gives error like:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test_toy.py"", line 67, in <module>\r\n    e_losses += train_epoch(script_model, opt, criterion)\r\n  File ""test_toy.py"", line 46, in train_epoch\r\n    y_hat = model(x_batch)\r\n  File ""/scratch/wanchaol/local/pytorch/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: bad optional access\r\n```\r\n\r\nneed to bisect and see whats going on",oncall: jit,ailzhang,"## \U0001f41b Bug\r\n\r\nThere appears to be some regression recently, the following script produce a error saying bad optional access, it used to work several days ago. \r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass ScriptClassifier(torch.jit.ScriptModule):\r\n\r\n    def __init__(self):\r\n        super(ScriptClassifier, self).__init__()\r\n        self.fc1 = nn.Linear(50, 50)\r\n        self.relu1 = nn.ReLU()\r\n        self.dout = nn.Dropout(0.2)\r\n        self.fc2 = nn.Linear(50, 100)\r\n        self.prelu = nn.PReLU(1)\r\n        self.out = nn.Linear(100, 1)\r\n        self.out_act = nn.Sigmoid()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input_):\r\n        a1 = self.fc1(input_)\r\n        h1 = self.relu1(a1)\r\n        dout = self.dout(h1)\r\n        a2 = self.fc2(dout)\r\n        h2 = self.prelu(a2)\r\n        a3 = self.out(h2)\r\n        y = self.out_act(a3)\r\n        return y\r\n\r\n\r\nscript_model = ScriptClassifier()\r\n\r\nopt = torch.optim.Adam(script_model.parameters(), lr=0.001, betas=(0.9, 0.999))\r\ncriterion = nn.BCELoss()\r\n\r\n\r\ndef train_epoch(model, opt, criterion, batch_size=50):\r\n    model.train()\r\n    losses = []\r\n    for beg_i in range(0, X.size(0), batch_size):\r\n        x_batch = X[beg_i:beg_i + batch_size, :]\r\n        y_batch = Y[beg_i:beg_i + batch_size, :]\r\n        #x_batch = Variable(x_batch)\r\n        #y_batch = Variable(y_batch)\r\n\r\n        opt.zero_grad()\r\n        # (1) Forward\r\n        y_hat = model(x_batch)\r\n        # (2) Compute diff\r\n        loss = criterion(y_hat, y_batch)\r\n        # (3) Compute gradients\r\n        loss.backward()\r\n        # (4) update weights\r\n        opt.step()\r\n        losses.append(loss.data.numpy())\r\n    return losses\r\n\r\n\r\nX1 = torch.randn(1000, 50)\r\nX2 = torch.randn(1000, 50) + 1.5\r\nX = torch.cat([X1, X2], dim=0)\r\nY1 = torch.zeros(1000, 1)\r\nY2 = torch.ones(1000, 1)\r\nY = torch.cat([Y1, Y2], dim=0)\r\n\r\ne_losses = []\r\nnum_epochs = 20\r\nfor e in range(num_epochs):\r\n    e_losses += train_epoch(script_model, opt, criterion)\r\n```\r\n\r\nit gives error like:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test_toy.py"", line 67, in <module>\r\n    e_losses += train_epoch(script_model, opt, criterion)\r\n  File ""test_toy.py"", line 46, in train_epoch\r\n    y_hat = model(x_batch)\r\n  File ""/scratch/wanchaol/local/pytorch/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: bad optional access\r\n```\r\n\r\nneed to bisect and see whats going on","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass ScriptClassifier(torch.jit.ScriptModule):\r\n\r\n    def __init__(self):\r\n        super(ScriptClassifier, self).__init__()\r\n        self.fc1 = nn.Linear(50, 50)\r\n        self.relu1 = nn.ReLU()\r\n        self.dout = nn.Dropout(0.2)\r\n        self.fc2 = nn.Linear(50, 100)\r\n        self.prelu = nn.PReLU(1)\r\n        self.out = nn.Linear(100, 1)\r\n        self.out_act = nn.Sigmoid()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input_):\r\n        a1 = self.fc1(input_)\r\n        h1 = self.relu1(a1)\r\n        dout = self.dout(h1)\r\n        a2 = self.fc2(dout)\r\n        h2 = self.prelu(a2)\r\n        a3 = self.out(h2)\r\n        y = self.out_act(a3)\r\n        return y\r\n\r\n\r\nscript_model = ScriptClassifier()\r\n\r\nopt = torch.optim.Adam(script_model.parameters(), lr=0.001, betas=(0.9, 0.999))\r\ncriterion = nn.BCELoss()\r\n\r\n\r\ndef train_epoch(model, opt, criterion, batch_size=50):\r\n    model.train()\r\n    losses = []\r\n    for beg_i in range(0, X.size(0), batch_size):\r\n        x_batch = X[beg_i:beg_i + batch_size, :]\r\n        y_batch = Y[beg_i:beg_i + batch_size, :]\r\n        #x_batch = Variable(x_batch)\r\n        #y_batch = Variable(y_batch)\r\n\r\n        opt.zero_grad()\r\n        # (1) Forward\r\n        y_hat = model(x_batch)\r\n        # (2) Compute diff\r\n        loss = criterion(y_hat, y_batch)\r\n        # (3) Compute gradients\r\n        loss.backward()\r\n        # (4) update weights\r\n        opt.step()\r\n        losses.append(loss.data.numpy())\r\n    return losses\r\n\r\n\r\nX1 = torch.randn(1000, 50)\r\nX2 = torch.randn(1000, 50) + 1.5\r\nX = torch.cat([X1, X2], dim=0)\r\nY1 = torch.zeros(1000, 1)\r\nY2 = torch.ones(1000, 1)\r\nY = torch.cat([Y1, Y2], dim=0)\r\n\r\ne_losses = []\r\nnum_epochs = 20\r\nfor e in range(num_epochs):\r\n    e_losses += train_epoch(script_model, opt, criterion)\r\n"
17662,torch.arange dtype mismatch with and without jit\r\n\r\nprints\r\n\r\n```\r\ntorch.int64\r\ntorch.float32\r\n```,oncall: jit,eellison,```python\r\nimport torch\r\n\r\ndef f():\r\n    return torch.arange(5)\r\n\r\nprint(f().dtype)\r\nprint(torch.jit.script(f)().dtype)\r\n```\r\n\r\nprints\r\n\r\n```\r\ntorch.int64\r\ntorch.float32\r\n```,python\r\nimport torch\r\n\r\ndef f():\r\n    return torch.arange(5)\r\n\r\nprint(f().dtype)\r\nprint(torch.jit.script(f)().dtype)\r\n
17652,"Unable to jit.script `a[0].item() == 1`## \U0001f41b Bug\r\n\r\n\r\n\r\ngives error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 3, in <module>\r\n    @torch.jit.script\r\n  File ""/home/gaoxiang/.virtualenvs/pt/lib/python3.7/site-packages/torch/jit/__init__.py"", line 734, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\narguments for call are not valid:\r\n  \r\n  for operator aten::eq(Tensor self, Tensor other) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Scalar other) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Tensor other, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Scalar other, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Device a, Device b) -> bool:\r\n  expected a value of type Device for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(string a, string b) -> bool:\r\n  expected a value of type string for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int[] a, int[] b) -> bool:\r\n  expected a value of type int[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float[] a, float[] b) -> bool:\r\n  expected a value of type float[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor[] a, Tensor[] b) -> bool:\r\n  expected a value of type Tensor[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(bool[] a, bool[] b) -> bool:\r\n  expected a value of type bool[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int a, int b) -> bool:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float a, float b) -> bool:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int a, float b) -> bool:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float a, int b) -> bool:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator eq(float a, Tensor b) -> Tensor:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator eq(int a, Tensor b) -> Tensor:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\nfor call at:\r\n@torch.jit.script\r\ndef f(a):\r\n    return a[0].item() == 1\r\n           ~~~~~~~~~~~~~~~~ <--- HERE\r\n```",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return a[0].item() == 1\r\n```\r\n\r\ngives error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 3, in <module>\r\n    @torch.jit.script\r\n  File ""/home/gaoxiang/.virtualenvs/pt/lib/python3.7/site-packages/torch/jit/__init__.py"", line 734, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(obj))\r\nRuntimeError: \r\narguments for call are not valid:\r\n  \r\n  for operator aten::eq(Tensor self, Tensor other) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Scalar other) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Tensor other, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor self, Scalar other, *, Tensor out) -> Tensor:\r\n  expected a value of type Tensor for argument 'self' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Device a, Device b) -> bool:\r\n  expected a value of type Device for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(string a, string b) -> bool:\r\n  expected a value of type string for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int[] a, int[] b) -> bool:\r\n  expected a value of type int[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float[] a, float[] b) -> bool:\r\n  expected a value of type float[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(Tensor[] a, Tensor[] b) -> bool:\r\n  expected a value of type Tensor[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(bool[] a, bool[] b) -> bool:\r\n  expected a value of type bool[] for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int a, int b) -> bool:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float a, float b) -> bool:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(int a, float b) -> bool:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator aten::eq(float a, int b) -> bool:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator eq(float a, Tensor b) -> Tensor:\r\n  expected a value of type float for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\n  \r\n  for operator eq(int a, Tensor b) -> Tensor:\r\n  expected a value of type int for argument 'a' but found Scalar\r\n  @torch.jit.script\r\n  def f(a):\r\n      return a[0].item() == 1\r\n             ~~~~~~~~~ <--- HERE\r\nfor call at:\r\n@torch.jit.script\r\ndef f(a):\r\n    return a[0].item() == 1\r\n           ~~~~~~~~~~~~~~~~ <--- HERE\r\n```",python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef f(a):\r\n    return a[0].item() == 1\r\n
17650,"[jit] incompleteInferType failed to handle type Tuple[GenericList]## \U0001f41b Bug\r\n\r\nWhen passing a Tuple[GenericList] to jit script module in C++, it goes to line 145 in [incompleteInferType](https://github.com/pytorch/pytorch/blob/06c8aa7a3bbd91cda2fd6255ec82aad21fa1c0d5/aten/src/ATen/core/type.cpp#L145).\r\n\r\nThen in fmap function call, incompleteInferType(GenericeList) is called recursively, then an `AT_ERROR(""Type cannot be accurately recovered from this IValue."")` is thrown because GenericList is not handled directly in this function.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. create a jit function in python\r\n```py\r\n@torch.jit.script\r\ndef MyScriptFun1(input1:Tuple[List[int]]) -> Tuple[List[int]]:\r\n    return input1\r\n```\r\n2. load and call the function in C++ api.\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): latest master\r\n - OS (e.g., Linux): Windows 10\r\n",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\nWhen passing a Tuple[GenericList] to jit script module in C++, it goes to line 145 in [incompleteInferType](https://github.com/pytorch/pytorch/blob/06c8aa7a3bbd91cda2fd6255ec82aad21fa1c0d5/aten/src/ATen/core/type.cpp#L145).\r\n\r\nThen in fmap function call, incompleteInferType(GenericeList) is called recursively, then an `AT_ERROR(""Type cannot be accurately recovered from this IValue."")` is thrown because GenericList is not handled directly in this function.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. create a jit function in python\r\n```py\r\n@torch.jit.script\r\ndef MyScriptFun1(input1:Tuple[List[int]]) -> Tuple[List[int]]:\r\n    return input1\r\n```\r\n2. load and call the function in C++ api.\r\n```c\r\n    auto generic_list = torch::jit::GenericList::create({1});\r\n    auto tuple_generic_list = torch::jit::Tuple::create({ generic_list });\r\n\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(tuple_generic_list);\r\n    // Execute the model and turn its output into a tensor.\r\n    try {\r\n        auto outputs = m->forward(inputs);\r\n    }\r\n    catch (std::exception& e)\r\n    {\r\n        std::string msg(e.what());\r\n    } \r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): latest master\r\n - OS (e.g., Linux): Windows 10\r\n",c\r\n    auto generic_list = torch::jit::GenericList::create({1});\r\n    auto tuple_generic_list = torch::jit::Tuple::create({ generic_list });\r\n\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(tuple_generic_list);\r\n    // Execute the model and turn its output into a tensor.\r\n    try {\r\n        auto outputs = m->forward(inputs);\r\n    }\r\n    catch (std::exception& e)\r\n    {\r\n        std::string msg(e.what());\r\n    } \r\n
17501,"Tensor unfold backward is slow## \U0001f41b Bug\r\n\r\nGiven the following code, the forward pass is very fast but the backward pass is unreasonably slow. I had a look at the backward pass code and while I am not an expert, I think it could be made significantly faster by writing a custom kernel instead of allocating index arrays, especially since `index_add_` seems to be so slow.\r\n\r\n\r\n\r\n```\r\n--------------------------------------------------------------------------------\r\n  Environment Summary\r\n--------------------------------------------------------------------------------\r\nPyTorch 1.0.0.dev20181221 not compiled w/ CUDA\r\nRunning with Python 3.6 and\r\n\r\n`pip list` truncated output:\r\nUnable to fetch\r\n--------------------------------------------------------------------------------\r\n  cProfile output\r\n--------------------------------------------------------------------------------\r\n         1983 function calls (1400 primitive calls) in 8.831 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 79 to 15 due to restriction <15>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    8.216    8.216    8.216    8.216 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n        1    0.315    0.315    0.315    0.315 {built-in method conv2d}\r\n        1    0.218    0.218    0.218    0.218 {built-in method rand}\r\n        1    0.078    0.078    0.078    0.078 {method 'sum' of 'torch._C._TensorBase' objects}\r\n    102/1    0.001    0.000    0.002    0.002 /anaconda3/lib/python3.6/abc.py:196(__subclasscheck__)\r\n        1    0.000    0.000    0.001    0.001 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py:17(__init__)\r\n        2    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/init.py:178(_calculate_fan_in_and_fan_out)\r\n    98/30    0.000    0.000    0.001    0.000 /anaconda3/lib/python3.6/typing.py:1145(__subclasscheck__)\r\n      202    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:70(__contains__)\r\n        1    0.000    0.000    8.831    8.831 minimal.py:1(<module>)\r\n        1    0.000    0.000    0.000    0.000 {method 'unfold' of 'torch._C._TensorBase' objects}\r\n       85    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:58(__iter__)\r\n    196/5    0.000    0.000    0.002    0.000 {built-in method builtins.issubclass}\r\n       70    0.000    0.000    0.000    0.000 {method '__subclasses__' of 'type' objects}\r\n       21    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:537(__setattr__)\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n  autograd profiler output (CPU mode)\r\n--------------------------------------------------------------------------------\r\n        top 15 events sorted by cpu_time_total\r\n\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                            CPU time        CUDA time            Calls        CPU total       CUDA total\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nUnfoldBackward                             7823869.000us          0.000us                1    7823869.000us          0.000us\r\nindex_add_                                 7222056.000us          0.000us                1    7222056.000us          0.000us\r\ncontiguous                                  370504.000us          0.000us                1     370504.000us          0.000us\r\nclone                                       370501.000us          0.000us                1     370501.000us          0.000us\r\nMkldnnConvolutionBackward                   257265.000us          0.000us                1     257265.000us          0.000us\r\nmkldnn_convolution_backward                 257256.000us          0.000us                1     257256.000us          0.000us\r\nmkldnn_convolution_backward_weights         257251.000us          0.000us                1     257251.000us          0.000us\r\nconv2d                                      249499.000us          0.000us                1     249499.000us          0.000us\r\nconvolution                                 249495.000us          0.000us                1     249495.000us          0.000us\r\n_convolution                                249494.000us          0.000us                1     249494.000us          0.000us\r\nmkldnn_convolution                          249477.000us          0.000us                1     249477.000us          0.000us\r\ncontiguous                                  170065.000us          0.000us                1     170065.000us          0.000us\r\nclone                                       170061.000us          0.000us                1     170061.000us          0.000us\r\nsum                                          71872.000us          0.000us                1      71872.000us          0.000us\r\n_th_arange                                   19876.000us          0.000us                1      19876.000us          0.000us\r\n```\r\n\r\n## To Reproduce\r\n\r\nSee code above\r\n\r\n## Expected behavior\r\n\r\nIt should be faster\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181221\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h6b9c3cc_5\r\n[conda] mkl_fft                   1.0.6            py36hb8a8100_0\r\n[conda] mkl_random                1.0.1            py36h5d10147_1\r\n[conda] pytorch                   1.0.0                   py3.6_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181221         py3.6_0    pytorch\r\n[conda] torchaudio                0.2                       <pip>\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n## Additional context\r\n\r\nMy real application needs `c=200` but it takes more than 10 minutes to run (i stopped at that point). Let me know if the same operation can be done more efficiently with some other method.\r\n\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel",high priority|module: performance|module: autograd|triaged|module: derivatives,nikitaved,"## \U0001f41b Bug\r\n\r\nGiven the following code, the forward pass is very fast but the backward pass is unreasonably slow. I had a look at the backward pass code and while I am not an expert, I think it could be made significantly faster by writing a custom kernel instead of allocating index arrays, especially since `index_add_` seems to be so slow.\r\n\r\n```python\r\nimport torch\r\nimport time\r\n\r\nlayer = torch.nn.Conv2d(3, 1, 3)\r\ninp = torch.rand(10, 3, 1000, 1000)\r\n\r\nout = layer(inp)\r\nc = 5\r\n\r\nres = out.unfold(3, c, 1)\r\nres.sum().backward()\r\n\r\n```\r\n\r\n```\r\n--------------------------------------------------------------------------------\r\n  Environment Summary\r\n--------------------------------------------------------------------------------\r\nPyTorch 1.0.0.dev20181221 not compiled w/ CUDA\r\nRunning with Python 3.6 and\r\n\r\n`pip list` truncated output:\r\nUnable to fetch\r\n--------------------------------------------------------------------------------\r\n  cProfile output\r\n--------------------------------------------------------------------------------\r\n         1983 function calls (1400 primitive calls) in 8.831 seconds\r\n\r\n   Ordered by: internal time\r\n   List reduced from 79 to 15 due to restriction <15>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    8.216    8.216    8.216    8.216 {method 'run_backward' of 'torch._C._EngineBase' objects}\r\n        1    0.315    0.315    0.315    0.315 {built-in method conv2d}\r\n        1    0.218    0.218    0.218    0.218 {built-in method rand}\r\n        1    0.078    0.078    0.078    0.078 {method 'sum' of 'torch._C._TensorBase' objects}\r\n    102/1    0.001    0.000    0.002    0.002 /anaconda3/lib/python3.6/abc.py:196(__subclasscheck__)\r\n        1    0.000    0.000    0.001    0.001 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py:17(__init__)\r\n        2    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/init.py:178(_calculate_fan_in_and_fan_out)\r\n    98/30    0.000    0.000    0.001    0.000 /anaconda3/lib/python3.6/typing.py:1145(__subclasscheck__)\r\n      202    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:70(__contains__)\r\n        1    0.000    0.000    8.831    8.831 minimal.py:1(<module>)\r\n        1    0.000    0.000    0.000    0.000 {method 'unfold' of 'torch._C._TensorBase' objects}\r\n       85    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/_weakrefset.py:58(__iter__)\r\n    196/5    0.000    0.000    0.002    0.000 {built-in method builtins.issubclass}\r\n       70    0.000    0.000    0.000    0.000 {method '__subclasses__' of 'type' objects}\r\n       21    0.000    0.000    0.000    0.000 /anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py:537(__setattr__)\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n  autograd profiler output (CPU mode)\r\n--------------------------------------------------------------------------------\r\n        top 15 events sorted by cpu_time_total\r\n\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                                            CPU time        CUDA time            Calls        CPU total       CUDA total\r\n---------------------------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nUnfoldBackward                             7823869.000us          0.000us                1    7823869.000us          0.000us\r\nindex_add_                                 7222056.000us          0.000us                1    7222056.000us          0.000us\r\ncontiguous                                  370504.000us          0.000us                1     370504.000us          0.000us\r\nclone                                       370501.000us          0.000us                1     370501.000us          0.000us\r\nMkldnnConvolutionBackward                   257265.000us          0.000us                1     257265.000us          0.000us\r\nmkldnn_convolution_backward                 257256.000us          0.000us                1     257256.000us          0.000us\r\nmkldnn_convolution_backward_weights         257251.000us          0.000us                1     257251.000us          0.000us\r\nconv2d                                      249499.000us          0.000us                1     249499.000us          0.000us\r\nconvolution                                 249495.000us          0.000us                1     249495.000us          0.000us\r\n_convolution                                249494.000us          0.000us                1     249494.000us          0.000us\r\nmkldnn_convolution                          249477.000us          0.000us                1     249477.000us          0.000us\r\ncontiguous                                  170065.000us          0.000us                1     170065.000us          0.000us\r\nclone                                       170061.000us          0.000us                1     170061.000us          0.000us\r\nsum                                          71872.000us          0.000us                1      71872.000us          0.000us\r\n_th_arange                                   19876.000us          0.000us                1      19876.000us          0.000us\r\n```\r\n\r\n## To Reproduce\r\n\r\nSee code above\r\n\r\n## Expected behavior\r\n\r\nIt should be faster\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181221\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h6b9c3cc_5\r\n[conda] mkl_fft                   1.0.6            py36hb8a8100_0\r\n[conda] mkl_random                1.0.1            py36h5d10147_1\r\n[conda] pytorch                   1.0.0                   py3.6_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181221         py3.6_0    pytorch\r\n[conda] torchaudio                0.2                       <pip>\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n## Additional context\r\n\r\nMy real application needs `c=200` but it takes more than 10 minutes to run (i stopped at that point). Let me know if the same operation can be done more efficiently with some other method.\r\n\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen @VitalyFedyunin @ngimel","python\r\nimport torch\r\nimport time\r\n\r\nlayer = torch.nn.Conv2d(3, 1, 3)\r\ninp = torch.rand(10, 3, 1000, 1000)\r\n\r\nout = layer(inp)\r\nc = 5\r\n\r\nres = out.unfold(3, c, 1)\r\nres.sum().backward()\r\n\r\n"
17449,"[JIT] encodeRHS val.isInt() ASSERT FAILED during fusion## \U0001f41b Bug\r\n\r\nFusing the following function crashes the interpreter\r\n\r\n\r\nwith the following error\r\n```\r\nfmassa-mbp:~ fmassa$ python tst4.py\r\nTraceback (most recent call last):\r\n  File ""tst4.py"", line 30, in <module>\r\n    box_iou(torch.rand(4, 4), torch.rand(5, 4))\r\n  File ""/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: val.isInt() ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch-nightly_1550985177969/work/torch/csrc/jit/fuser/codegen.cpp:220, please report a bug to PyTorch. (encodeRHS at /Users/distiller/project/conda/conda-bld/pytorch-nightly_1550985177969/work/torch/csrc/jit/fuser/codegen.cpp:220)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x10879d6e7 in libc10.dylib)\r\nframe #1: torch::jit::fuser::generateKernel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::Graph const&, std::__1::vector<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const>, std::__1::allocator<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const> > > const&, std::__1::vector<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const>, std::__1::allocator<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const> > > const&, bool) + 20806 (0x10ed300c6 in libtorch.1.dylib)\r\nframe #2: torch::jit::fuser::compileKernel(torch::jit::fuser::KernelSpec const&, torch::jit::fuser::ArgSpec const&, std::__1::vector<long long, std::__1::allocator<long long> > const&, c10::Device) + 3696 (0x10ed1e980 in libtorch.1.dylib)\r\nframe #3: torch::jit::fuser::runFusion(long long, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 1632 (0x10ed26830 in libtorch.1.dylib)\r\nframe #4: torch::jit::runFusion(long long, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 18 (0x10ec2f5f2 in libtorch.1.dylib)\r\nframe #5: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_1::operator()(torch::jit::Node const*) const::'lambda'(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&), std::__1::allocator<torch::jit::(anonymous namespace)::$_1::operator()(torch::jit::Node const*) const::'lambda'(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>, int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 45 (0x10ec2ff8d in libtorch.1.dylib)\r\nframe #6: torch::jit::InterpreterStateImpl::runImpl(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 245 (0x10eb7b015 in libtorch.1.dylib)\r\nframe #7: torch::jit::InterpreterStateImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 28 (0x10eb7354c in libtorch.1.dylib)\r\nframe #8: torch::jit::GraphExecutorImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 4274 (0x10eb4ffa2 in libtorch.1.dylib)\r\nframe #9: torch::jit::script::Method::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 216 (0x1047f66e8 in libtorch_python.dylib)\r\nframe #10: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, torch::jit::tuple_slice, pybind11::kwargs) + 73 (0x1047f65a9 in libtorch_python.dylib)\r\nframe #11: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_17, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_17&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 340 (0x1047f62b4 in libtorch_python.dylib)\r\nframe #12: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3324 (0x10439785c in libtorch_python.dylib)\r\n<omitting python frames>\r\nframe #32: start + 1 (0x7fff59ab1ed9 in libdyld.dylib)\r\n```\r\n\r\nI'm using latest nightly `1.0.0.dev20190223`",oncall: jit,wanchaol,"## \U0001f41b Bug\r\n\r\nFusing the following function crashes the interpreter\r\n\r\n```python\r\nimport torch\r\n\r\ntorch._C._jit_override_can_fuse_on_cpu(True)\r\n\r\n@torch.jit.script\r\ndef box_iou(box1, box2, eps:float=1e-5):\r\n    # box1: [N, 4], box2: [M, 4]\r\n    x1, y1, w1, h1 = box1.unsqueeze(1).unbind(2)\r\n    x2, y2, w2, h2 = box2.unbind(1)\r\n\r\n    xi = torch.max(x1, x2)  # Intersection\r\n    yi = torch.max(y1, y2)\r\n\r\n    wi = torch.clamp(torch.min(x1 + w1, x2 + w2) - xi, min=0)\r\n    hi = torch.clamp(torch.min(y1 + h1, y2 + h2) - yi, min=0)\r\n    return wi, hi\r\n\r\nbox_iou(torch.rand(4, 4), torch.rand(5, 4))\r\nprint(box_iou.graph_for(torch.rand(4, 4), torch.rand(5, 4)))\r\n```\r\nwith the following error\r\n```\r\nfmassa-mbp:~ fmassa$ python tst4.py\r\nTraceback (most recent call last):\r\n  File ""tst4.py"", line 30, in <module>\r\n    box_iou(torch.rand(4, 4), torch.rand(5, 4))\r\n  File ""/Users/fmassa/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\nRuntimeError: val.isInt() ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch-nightly_1550985177969/work/torch/csrc/jit/fuser/codegen.cpp:220, please report a bug to PyTorch. (encodeRHS at /Users/distiller/project/conda/conda-bld/pytorch-nightly_1550985177969/work/torch/csrc/jit/fuser/codegen.cpp:220)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x10879d6e7 in libc10.dylib)\r\nframe #1: torch::jit::fuser::generateKernel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, torch::jit::Graph const&, std::__1::vector<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const>, std::__1::allocator<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const> > > const&, std::__1::vector<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const>, std::__1::allocator<std::__1::pair<torch::jit::Value const*, torch::jit::fuser::TensorDesc const> > > const&, bool) + 20806 (0x10ed300c6 in libtorch.1.dylib)\r\nframe #2: torch::jit::fuser::compileKernel(torch::jit::fuser::KernelSpec const&, torch::jit::fuser::ArgSpec const&, std::__1::vector<long long, std::__1::allocator<long long> > const&, c10::Device) + 3696 (0x10ed1e980 in libtorch.1.dylib)\r\nframe #3: torch::jit::fuser::runFusion(long long, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 1632 (0x10ed26830 in libtorch.1.dylib)\r\nframe #4: torch::jit::runFusion(long long, std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 18 (0x10ec2f5f2 in libtorch.1.dylib)\r\nframe #5: std::__1::__function::__func<torch::jit::(anonymous namespace)::$_1::operator()(torch::jit::Node const*) const::'lambda'(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&), std::__1::allocator<torch::jit::(anonymous namespace)::$_1::operator()(torch::jit::Node const*) const::'lambda'(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>, int (std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&)>::operator()(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 45 (0x10ec2ff8d in libtorch.1.dylib)\r\nframe #6: torch::jit::InterpreterStateImpl::runImpl(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 245 (0x10eb7b015 in libtorch.1.dylib)\r\nframe #7: torch::jit::InterpreterStateImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 28 (0x10eb7354c in libtorch.1.dylib)\r\nframe #8: torch::jit::GraphExecutorImpl::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 4274 (0x10eb4ffa2 in libtorch.1.dylib)\r\nframe #9: torch::jit::script::Method::run(std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >&) + 216 (0x1047f66e8 in libtorch_python.dylib)\r\nframe #10: torch::jit::invokeScriptMethodFromPython(torch::jit::script::Method&, torch::jit::tuple_slice, pybind11::kwargs) + 73 (0x1047f65a9 in libtorch_python.dylib)\r\nframe #11: void pybind11::cpp_function::initialize<torch::jit::script::initJitScriptBindings(_object*)::$_17, pybind11::object, pybind11::args, pybind11::kwargs, pybind11::name, pybind11::is_method, pybind11::sibling>(torch::jit::script::initJitScriptBindings(_object*)::$_17&&, pybind11::object (*)(pybind11::args, pybind11::kwargs), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) + 340 (0x1047f62b4 in libtorch_python.dylib)\r\nframe #12: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3324 (0x10439785c in libtorch_python.dylib)\r\n<omitting python frames>\r\nframe #32: start + 1 (0x7fff59ab1ed9 in libdyld.dylib)\r\n```\r\n\r\nI'm using latest nightly `1.0.0.dev20190223`","python\r\nimport torch\r\n\r\ntorch._C._jit_override_can_fuse_on_cpu(True)\r\n\r\n@torch.jit.script\r\ndef box_iou(box1, box2, eps:float=1e-5):\r\n    # box1: [N, 4], box2: [M, 4]\r\n    x1, y1, w1, h1 = box1.unsqueeze(1).unbind(2)\r\n    x2, y2, w2, h2 = box2.unbind(1)\r\n\r\n    xi = torch.max(x1, x2)  # Intersection\r\n    yi = torch.max(y1, y2)\r\n\r\n    wi = torch.clamp(torch.min(x1 + w1, x2 + w2) - xi, min=0)\r\n    hi = torch.clamp(torch.min(y1 + h1, y2 + h2) - yi, min=0)\r\n    return wi, hi\r\n\r\nbox_iou(torch.rand(4, 4), torch.rand(5, 4))\r\nprint(box_iou.graph_for(torch.rand(4, 4), torch.rand(5, 4)))\r\n"
17357,"cuda runtime error (3): we're not detecting bad forks## \U0001f41b Bug\r\n\r\nI am not sure if this is reproducible for every environment, but I hit the following error when trying to set cuda device in processes. What is weird is that the error disappears if I remove the line `x = torch.rand(20, 2).cuda()` right after the for loop.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap\r\n    self.run()\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""dist_bug.py"", line 5, in run\r\n    torch.cuda.set_device(rank)\r\n  File ""/home/shenli/project/pytorch/torch/cuda/__init__.py"", line 265, in set_device\r\n    torch._C._cuda_setDevice(device)\r\nRuntimeError: cuda runtime error (3) : initialization error at ../torch/csrc/cuda/Module.cpp:33\r\n```\r\n\r\n**Edit:**  based on the discussion below, the solution should be fixing the ""bad fork"" error detection, which is duplicated with #17359. \r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0a0+63214b5\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.1.0a0+63214b5\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.1 144\r\n[conda] mkl-include 2019.1 144\r\n[conda] mkl_fft 1.0.6 py37hd81dba3_0\r\n[conda] mkl_random 1.0.2 py37hd81dba3_0\r\n[conda] torch 1.1.0a0+63214b5 dev_0\r\n",module: cuda|triaged,zhangguanheng66,"## \U0001f41b Bug\r\n\r\nI am not sure if this is reproducible for every environment, but I hit the following error when trying to set cuda device in processes. What is weird is that the error disappears if I remove the line `x = torch.rand(20, 2).cuda()` right after the for loop.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap\r\n    self.run()\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""dist_bug.py"", line 5, in run\r\n    torch.cuda.set_device(rank)\r\n  File ""/home/shenli/project/pytorch/torch/cuda/__init__.py"", line 265, in set_device\r\n    torch._C._cuda_setDevice(device)\r\nRuntimeError: cuda runtime error (3) : initialization error at ../torch/csrc/cuda/Module.cpp:33\r\n```\r\n\r\n**Edit:**  based on the discussion below, the solution should be fixing the ""bad fork"" error detection, which is duplicated with #17359. \r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nfrom torch.multiprocessing import Process\r\n\r\ndef run(rank):\r\n    torch.cuda.set_device(rank)\r\n\r\nif __name__ == ""__main__"":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        # it would work fine without the line below\r\n        x = torch.rand(20, 2).cuda()\r\n        p = Process(target=run, args=(rank,))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0a0+63214b5\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.1.0a0+63214b5\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.1 144\r\n[conda] mkl-include 2019.1 144\r\n[conda] mkl_fft 1.0.6 py37hd81dba3_0\r\n[conda] mkl_random 1.0.2 py37hd81dba3_0\r\n[conda] torch 1.1.0a0+63214b5 dev_0\r\n","python\r\nimport torch\r\nfrom torch.multiprocessing import Process\r\n\r\ndef run(rank):\r\n    torch.cuda.set_device(rank)\r\n\r\nif __name__ == ""__main__"":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        # it would work fine without the line below\r\n        x = torch.rand(20, 2).cuda()\r\n        p = Process(target=run, args=(rank,))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n"
17356,"DistributedDataParallel throws RuntimeError for sparse embeddings## \U0001f41b Bug\r\n\r\nUsing `nn.Embedding(n, d, sparse=True)` in `DistributedDataParallel` models hits the following error in the backward pass.\r\n\r\nCC: @glample  @soumith \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap\r\n    self.run()\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""dist_bug.py"", line 32, in init_processes\r\n    y.mean().backward()\r\n  File ""/home/shenli/project/pytorch/torch/tensor.py"", line 106, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/shenli/project/pytorch/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/shenli/project/pytorch/torch/nn/parallel/distributed.py"", line 450, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/shenli/project/pytorch/torch/nn/parallel/distributed.py"", line 480, in _queue_reduction\r\n    self.device_ids)\r\nRuntimeError: sparse tensors do not have is_contiguous (is_contiguous at ../aten/src/ATen/SparseTensorImpl.cpp:47)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f1225ae6b65 in /home/shenli/project/pytorch/torch/lib/libc10.so)\r\nframe #1: at::SparseTensorImpl::is_contiguous() const + 0x7d (0x7f122629262d in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #2: at::native::contiguous(at::Tensor const&) + 0x20 (0x7f1226434310 in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #3: at::TypeDefault::contiguous(at::Tensor const&) const + 0x50 (0x7f122667cf80 in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #4: torch::autograd::VariableType::contiguous(at::Tensor const&) const + 0x10f (0x7f12238c9e0f in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #5: <unknown function> + 0x4a4fe8 (0x7f123bb9ffe8 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #6: c10d::queueReduction(c10d::ProcessGroup&, std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<long, std::allocator<long> > const&) + 0x44e (0x7f123bd0f73e in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #7: <unknown function> + 0x61272b (0x7f123bd0d72b in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #8: <unknown function> + 0x11a513 (0x7f123b815513 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #18: torch::autograd::PyFunctionPostHook::operator()(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) + 0xc8 (0x7f123ba45d28 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #19: torch::autograd::Engine::evaluate_function(torch::autograd::FunctionTask&) + 0x1574 (0x7f12233beea4 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #20: torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) + 0xc0 (0x7f12233bfab0 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #21: torch::autograd::Engine::thread_init(int) + 0x81 (0x7f12233bcaa1 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #22: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7f123ba335ea in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #23: <unknown function> + 0xb8678 (0x7f123c6ad678 in /home/shenli/local/miniconda/lib/libstdc++.so.6)\r\nframe #24: <unknown function> + 0x7e25 (0x7f1253a59e25 in /lib64/libpthread.so.0)\r\nframe #25: clone + 0x6d (0x7f1253783bad in /lib64/libc.so.6)\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0a0+63214b5\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.1.0a0+63214b5\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] torch                     1.1.0a0+63214b5           dev_0    <develop>\r\n\r\n",oncall: distributed,mrshenli,"## \U0001f41b Bug\r\n\r\nUsing `nn.Embedding(n, d, sparse=True)` in `DistributedDataParallel` models hits the following error in the backward pass.\r\n\r\nCC: @glample  @soumith \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 297, in _bootstrap\r\n    self.run()\r\n  File ""/home/shenli/local/miniconda/lib/python3.7/multiprocessing/process.py"", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""dist_bug.py"", line 32, in init_processes\r\n    y.mean().backward()\r\n  File ""/home/shenli/project/pytorch/torch/tensor.py"", line 106, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File ""/home/shenli/project/pytorch/torch/autograd/__init__.py"", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File ""/home/shenli/project/pytorch/torch/nn/parallel/distributed.py"", line 450, in distributed_data_parallel_hook\r\n    self._queue_reduction(bucket_idx)\r\n  File ""/home/shenli/project/pytorch/torch/nn/parallel/distributed.py"", line 480, in _queue_reduction\r\n    self.device_ids)\r\nRuntimeError: sparse tensors do not have is_contiguous (is_contiguous at ../aten/src/ATen/SparseTensorImpl.cpp:47)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f1225ae6b65 in /home/shenli/project/pytorch/torch/lib/libc10.so)\r\nframe #1: at::SparseTensorImpl::is_contiguous() const + 0x7d (0x7f122629262d in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #2: at::native::contiguous(at::Tensor const&) + 0x20 (0x7f1226434310 in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #3: at::TypeDefault::contiguous(at::Tensor const&) const + 0x50 (0x7f122667cf80 in /home/shenli/project/pytorch/torch/lib/libcaffe2.so)\r\nframe #4: torch::autograd::VariableType::contiguous(at::Tensor const&) const + 0x10f (0x7f12238c9e0f in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #5: <unknown function> + 0x4a4fe8 (0x7f123bb9ffe8 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #6: c10d::queueReduction(c10d::ProcessGroup&, std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<long, std::allocator<long> > const&) + 0x44e (0x7f123bd0f73e in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #7: <unknown function> + 0x61272b (0x7f123bd0d72b in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #8: <unknown function> + 0x11a513 (0x7f123b815513 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #18: torch::autograd::PyFunctionPostHook::operator()(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&, std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) + 0xc8 (0x7f123ba45d28 in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #19: torch::autograd::Engine::evaluate_function(torch::autograd::FunctionTask&) + 0x1574 (0x7f12233beea4 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #20: torch::autograd::Engine::thread_main(torch::autograd::GraphTask*) + 0xc0 (0x7f12233bfab0 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #21: torch::autograd::Engine::thread_init(int) + 0x81 (0x7f12233bcaa1 in /home/shenli/project/pytorch/torch/lib/libtorch.so.1)\r\nframe #22: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7f123ba335ea in /home/shenli/project/pytorch/torch/lib/libtorch_python.so)\r\nframe #23: <unknown function> + 0xb8678 (0x7f123c6ad678 in /home/shenli/local/miniconda/lib/libstdc++.so.6)\r\nframe #24: <unknown function> + 0x7e25 (0x7f1253a59e25 in /lib64/libpthread.so.0)\r\nframe #25: clone + 0x6d (0x7f1253783bad in /lib64/libc.so.6)\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.multiprocessing import Process\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = nn.Embedding(10, 3, sparse=True)\r\n        self.net = nn.Linear(2, 3)\r\n\r\n    def forward(self, x):\r\n        return self.net(x) + self.embedding(torch.tensor(0).cuda())\r\n\r\n\r\ndef init_processes(rank, size, backend='gloo'):\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    torch.distributed.init_process_group(backend, rank=rank, world_size=size)\r\n    torch.cuda.set_device(rank)\r\n    model = nn.parallel.DistributedDataParallel(Model().cuda(), device_ids=[rank], output_device=0)\r\n    x = torch.rand(20, 2).cuda()\r\n    y = model(x)\r\n    y.mean().backward()\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.1.0a0+63214b5\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.88\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: \r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] torch==1.1.0a0+63214b5\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] torch                     1.1.0a0+63214b5           dev_0    <develop>\r\n\r\n","python\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.multiprocessing import Process\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = nn.Embedding(10, 3, sparse=True)\r\n        self.net = nn.Linear(2, 3)\r\n\r\n    def forward(self, x):\r\n        return self.net(x) + self.embedding(torch.tensor(0).cuda())\r\n\r\n\r\ndef init_processes(rank, size, backend='gloo'):\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    torch.distributed.init_process_group(backend, rank=rank, world_size=size)\r\n    torch.cuda.set_device(rank)\r\n    model = nn.parallel.DistributedDataParallel(Model().cuda(), device_ids=[rank], output_device=0)\r\n    x = torch.rand(20, 2).cuda()\r\n    y = model(x)\r\n    y.mean().backward()\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n"
17291,"Exceptions have a hardcoded message`raise` statements in the JIT always raise with the string ""Exception"". They should instead use the user supplied arguments\r\n\r\nExample\r\n\r\nOutputs the graph\r\n```\r\ngraph(%x : int):\r\n  %3 : string = prim::Constant[value=""Exception""]()\r\n  %1 : int = prim::Constant[value=2]()\r\n  %5 : int = prim::Constant[value=3]()\r\n  %2 : bool = aten::gt(%x, %1)\r\n   = prim::If(%2)\r\n    block0():\r\n       = prim::RaiseException(%3)\r\n      -> ()\r\n    block1():\r\n      -> ()\r\n  %6 : int = aten::add(%x, %5)\r\n  return (%6)\r\n```\n\ncc @ezyang @gchanan @zou3519 @suo @gmagogsfm",high priority|oncall: jit|weeks,gmagogsfm,"`raise` statements in the JIT always raise with the string ""Exception"". They should instead use the user supplied arguments\r\n\r\nExample\r\n```python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (int) -> int\r\n    if x > 2:\r\n        raise RuntimeError(""bad input, {} is too high"".format(x))\r\n    return x + 3\r\n\r\nprint(fn.graph)\r\nprint(fn(4))\r\n```\r\nOutputs the graph\r\n```\r\ngraph(%x : int):\r\n  %3 : string = prim::Constant[value=""Exception""]()\r\n  %1 : int = prim::Constant[value=2]()\r\n  %5 : int = prim::Constant[value=3]()\r\n  %2 : bool = aten::gt(%x, %1)\r\n   = prim::If(%2)\r\n    block0():\r\n       = prim::RaiseException(%3)\r\n      -> ()\r\n    block1():\r\n      -> ()\r\n  %6 : int = aten::add(%x, %5)\r\n  return (%6)\r\n```\n\ncc @ezyang @gchanan @zou3519 @suo @gmagogsfm","python\r\n@torch.jit.script\r\ndef fn(x):\r\n    # type: (int) -> int\r\n    if x > 2:\r\n        raise RuntimeError(""bad input, {} is too high"".format(x))\r\n    return x + 3\r\n\r\nprint(fn.graph)\r\nprint(fn(4))\r\n"
17271,"torch.exp on CPU is incorrect for tensors with more than 2^31 or more elementsReported by Ben Graham\r\n\r\n\r\n\r\nstderr shows\r\n\r\n```\r\nIntel MKL ERROR: Parameter 1 was incorrect on entry to vsExp.\r\n```\r\n\r\nThe other operations implemented by MKL's VML (sin, cos, etc.) are likely to be broken as well.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/0fc03d155a91cbca2c9e23d23463f69fd254c0a3/aten/src/ATen/cpu/vml.h#L140-L158",high priority,cpuhrsch,"Reported by Ben Graham\r\n\r\n```python\r\nimport torch\r\nprint(torch.__version__) #1.0.0.dev20190207\r\na=torch.ones(2**31,dtype=torch.float32)\r\nprint(a)             #tensor([1., 1., 1.,  ..., 1., 1., 1.])\r\nprint(torch.sign(a)) #tensor([1., 1., 1.,  ..., 1., 1., 1.])\r\nprint(torch.exp(a))  #tensor([0., 0., 0.,  ..., 0., 0., 0.])  THIS IS WRONG\r\n```\r\n\r\nstderr shows\r\n\r\n```\r\nIntel MKL ERROR: Parameter 1 was incorrect on entry to vsExp.\r\n```\r\n\r\nThe other operations implemented by MKL's VML (sin, cos, etc.) are likely to be broken as well.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/0fc03d155a91cbca2c9e23d23463f69fd254c0a3/aten/src/ATen/cpu/vml.h#L140-L158","python\r\nimport torch\r\nprint(torch.__version__) #1.0.0.dev20190207\r\na=torch.ones(2**31,dtype=torch.float32)\r\nprint(a)             #tensor([1., 1., 1.,  ..., 1., 1., 1.])\r\nprint(torch.sign(a)) #tensor([1., 1., 1.,  ..., 1., 1., 1.])\r\nprint(torch.exp(a))  #tensor([0., 0., 0.,  ..., 0., 0., 0.])  THIS IS WRONG\r\n"
17185,"Confusing behavior with *= operator with torch.expand## \U0001f41b Bug\r\n\r\nI am observing confusing (and probably errornous behavior) when using *= tensor operator with expand function. I don't think this should be an expected behavior, but if it is, there should at least be a user warning. Below is a minimally reproducible example:\r\n\r\n## To Reproduce\r\ncode input:\r\n\r\noutput:\r\n\r\ncode input:\r\n\r\noutput:\r\n\r\ncode input:\r\n\r\noutput:\r\n\r\ncode input:\r\n\r\noutput:\r\n\r\n\r\n## Environment\r\n\r\n",triage review,zou3519,"## \U0001f41b Bug\r\n\r\nI am observing confusing (and probably errornous behavior) when using *= tensor operator with expand function. I don't think this should be an expected behavior, but if it is, there should at least be a user warning. Below is a minimally reproducible example:\r\n\r\n## To Reproduce\r\ncode input:\r\n```python\r\nimport torch\r\na = torch.tensor([1,2,3]).view(1,3)\r\nb = torch.tensor([4,5]).view(2,1)\r\nprint(a)\r\nprint(b)\r\nprint(a*b) # directly multiplying will trigger the correct broadcast\r\n```\r\noutput:\r\n```bash\r\ntensor([[1, 2, 3]])\r\ntensor([[4],\r\n        [5]])\r\ntensor([[ 4,  8, 12],\r\n        [ 5, 10, 15]])\r\n```\r\ncode input:\r\n```python\r\nprint(a.expand(2,3)*b.expand(2,3)) # explicitly using expand to broadcast followed by * is also fine\r\n```\r\noutput:\r\n```bash\r\ntensor([[ 4,  8, 12],\r\n        [ 5, 10, 15]])\r\n```\r\ncode input:\r\n```python\r\na *= b # trying to broadcast with *= doesn't work\r\n```\r\noutput:\r\n```bash\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nRuntimeError: output with shape [1, 3] doesn't match the broadcast shape [2, 3]\r\n```\r\ncode input:\r\n```python\r\na = a.expand(2,3) # explicitly broadcasting with expand\r\nb = b.expand(2,3) # explicitly broadcasting with expand\r\na *= b # this produces weird values that I don't understand\r\nprint(a)\r\n```\r\noutput:\r\n```bash\r\ntensor([[20, 40, 60],\r\n        [20, 40, 60]])\r\n```\r\n\r\n## Environment\r\n```bash\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.2\r\nGCC version: Could not collect\r\nCMake version: version 3.10.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.11.3\r\n[pip3] torch==0.4.0\r\n[pip3] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl-service               1.1.2                    py35_3\r\n[conda] mkl_fft                   1.0.6            py35hb8a8100_0\r\n[conda] mkl_random                1.0.1            py35h5d10147_1\r\n[conda] pytorch                   1.0.0                   py3.5_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n","python\r\nimport torch\r\na = torch.tensor([1,2,3]).view(1,3)\r\nb = torch.tensor([4,5]).view(2,1)\r\nprint(a)\r\nprint(b)\r\nprint(a*b) # directly multiplying will trigger the correct broadcast\r\n"
17140,"[jit] Error: 'List must contain only a single type' when loading Python model into C++## \U0001f41b Bug\r\nI have generated and saved a model in Python using `torch.git.save()`. Now I am attempting to load it into my C++ implementation for inference, however I get the error:\r\n\r\n```\r\n> terminate called after throwing an instance of 'torch::jit::script::ErrorReport'\r\n>   what():  \r\n> Lists must contain only a single type, expected: Float(1, 1083, 11) but found Float(1, 4332, 11) instead:\r\n> op_version_set = 0\r\n> def forward(self,\r\n>     input_1: Tensor) -> Tensor:\r\n>   _0 = [CONSTANTS.c0, CONSTANTS.c1, CONSTANTS.c2]\r\n>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n>   return torch.cat(_0, 1)\r\n> \r\n> Aborted (core dumped)\r\n```\r\n\r\n## To Reproduce\r\n\r\nIt does not specify the code source of the failure, however I am thinking it could be this (following) forward function on my Python implementation due to the similarities with the error report:\r\n\r\n\r\n\r\nI think it is important to notice that on the error the ""expected Float(1,*1083*,11)"" id exactly 4 times larger than ""found Float(1, *4332*, 11)""\r\n\r\n## Expected behavior\r\n\r\nThe model should be loaded successfully for inference.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 Ti\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.12.1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2018.0.3 1\r\n[conda] pytorch 1.0.1 py3.6_cuda10.0.130_cudnn7.4.2_2 pytorch\r\n[conda] torchvision 0.2.1 py_2 pytorch\r\n",oncall: jit,suo,"## \U0001f41b Bug\r\nI have generated and saved a model in Python using `torch.git.save()`. Now I am attempting to load it into my C++ implementation for inference, however I get the error:\r\n\r\n```\r\n> terminate called after throwing an instance of 'torch::jit::script::ErrorReport'\r\n>   what():  \r\n> Lists must contain only a single type, expected: Float(1, 1083, 11) but found Float(1, 4332, 11) instead:\r\n> op_version_set = 0\r\n> def forward(self,\r\n>     input_1: Tensor) -> Tensor:\r\n>   _0 = [CONSTANTS.c0, CONSTANTS.c1, CONSTANTS.c2]\r\n>        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n>   return torch.cat(_0, 1)\r\n> \r\n> Aborted (core dumped)\r\n```\r\n\r\n## To Reproduce\r\n\r\nIt does not specify the code source of the failure, however I am thinking it could be this (following) forward function on my Python implementation due to the similarities with the error report:\r\n\r\n```python\r\nclass YOLOv3(nn.Module):\r\n\r\n    def __init__(self, anchors, anch_mask, n_classes, ignore_thre=0.7):\r\n        """"""\r\n        Initialization of YOLOv3 class.\r\n        Args:\r\n            config_model (dict): used in YOLOLayer.\r\n            ignore_thre (float): used in YOLOLayer.\r\n        """"""\r\n        super(YOLOv3, self).__init__()\r\n        self.loss_list = []\r\n        self.module_list= create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre)\r\n        self.length = len(self.module_list) \r\n\r\n    def forward(self, x, targets=None):\r\n        """"""\r\n        Forward path of YOLOv3.\r\n        Args:\r\n            x (torch.Tensor) : input data whose shape is :math:`(N, C, H, W)`, \\\r\n                where N, C are batchsize and num. of channels.\r\n            targets (torch.Tensor) : label array whose shape is :math:`(N, 50, 5)`\r\n\r\n        Returns:\r\n            training:\r\n                output (torch.Tensor): loss tensor for backpropagation.\r\n            test:\r\n                output (torch.Tensor): concatenated detection results.\r\n        """"""\r\n        train = targets is not None\r\n        output = []\r\n        route_layers = []\r\n        for i in range(self.length):\r\n            # yolo layers\r\n            if i == 14 or i == 22 or i == 28:\r\n                if train:\r\n                    x, *loss_dict = self.module_list[i](x, targets)\r\n                    #for name, loss in zip(['xy', 'wh', 'conf', 'cls', 'l2'] , loss_dict):\r\n                    self.loss_list += loss_dict\r\n                else:\r\n                    x = self.module_list[i](x)\r\n                output.append(x)\r\n            else:\r\n                x = self.module_list[i](x)\r\n\r\n            # route layers\r\n            if i == 6 or i == 8 or i == 12 or i == 20:\r\n                route_layers.append(x)\r\n            if i == 14:\r\n                x = route_layers[2]\r\n            if i == 22:  # yolo 2nd\r\n                x = route_layers[3]\r\n            if i == 16:\r\n                x = torch.cat((x, route_layers[1]), 1)\r\n            if i == 24:\r\n                x = torch.cat((x, route_layers[0]), 1)\r\n\r\n            i+=1\r\n        if train:\r\n            return sum(output)\r\n        else:\r\n            return torch.cat(output, 1)\r\n```\r\n\r\nI think it is important to notice that on the error the ""expected Float(1,*1083*,11)"" id exactly 4 times larger than ""found Float(1, *4332*, 11)""\r\n\r\n## Expected behavior\r\n\r\nThe model should be loaded successfully for inference.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 Ti\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.12.1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2018.0.3 1\r\n[conda] pytorch 1.0.1 py3.6_cuda10.0.130_cudnn7.4.2_2 pytorch\r\n[conda] torchvision 0.2.1 py_2 pytorch\r\n","python\r\nclass YOLOv3(nn.Module):\r\n\r\n    def __init__(self, anchors, anch_mask, n_classes, ignore_thre=0.7):\r\n        """"""\r\n        Initialization of YOLOv3 class.\r\n        Args:\r\n            config_model (dict): used in YOLOLayer.\r\n            ignore_thre (float): used in YOLOLayer.\r\n        """"""\r\n        super(YOLOv3, self).__init__()\r\n        self.loss_list = []\r\n        self.module_list= create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre)\r\n        self.length = len(self.module_list) \r\n\r\n    def forward(self, x, targets=None):\r\n        """"""\r\n        Forward path of YOLOv3.\r\n        Args:\r\n            x (torch.Tensor) : input data whose shape is :math:`(N, C, H, W)`, \\\r\n                where N, C are batchsize and num. of channels.\r\n            targets (torch.Tensor) : label array whose shape is :math:`(N, 50, 5)`\r\n\r\n        Returns:\r\n            training:\r\n                output (torch.Tensor): loss tensor for backpropagation.\r\n            test:\r\n                output (torch.Tensor): concatenated detection results.\r\n        """"""\r\n        train = targets is not None\r\n        output = []\r\n        route_layers = []\r\n        for i in range(self.length):\r\n            # yolo layers\r\n            if i == 14 or i == 22 or i == 28:\r\n                if train:\r\n                    x, *loss_dict = self.module_list[i](x, targets)\r\n                    #for name, loss in zip(['xy', 'wh', 'conf', 'cls', 'l2'] , loss_dict):\r\n                    self.loss_list += loss_dict\r\n                else:\r\n                    x = self.module_list[i](x)\r\n                output.append(x)\r\n            else:\r\n                x = self.module_list[i](x)\r\n\r\n            # route layers\r\n            if i == 6 or i == 8 or i == 12 or i == 20:\r\n                route_layers.append(x)\r\n            if i == 14:\r\n                x = route_layers[2]\r\n            if i == 22:  # yolo 2nd\r\n                x = route_layers[3]\r\n            if i == 16:\r\n                x = torch.cat((x, route_layers[1]), 1)\r\n            if i == 24:\r\n                x = torch.cat((x, route_layers[0]), 1)\r\n\r\n            i+=1\r\n        if train:\r\n            return sum(output)\r\n        else:\r\n            return torch.cat(output, 1)\r\n"
17136,"Customize the printing of namedtuple returnFixes https://github.com/pytorch/pytorch/issues/17112\r\n\r\nnow gives\r\n```\r\n>>> import torch\r\n>>> print(""good"", torch.randn(5,5,5).max(1))\r\ngood torch.return_types.max(\r\nvalues=tensor([[ 1.2821,  1.8063,  1.8075,  1.3082, -0.1267],\r\n        [ 0.3437,  0.7353,  1.2619,  0.7557,  1.6662],\r\n        [ 0.8583,  1.8906,  1.0246,  1.7598,  1.1184],\r\n        [ 1.7821,  0.0230,  0.9452,  1.0318,  1.0823],\r\n        [ 0.4116, -0.0379, -0.1843,  1.4129,  1.8796]]),\r\nindices=tensor([[4, 4, 3, 2, 1],\r\n        [1, 2, 4, 1, 1],\r\n        [2, 4, 0, 2, 1],\r\n        [0, 2, 0, 3, 1],\r\n        [0, 4, 4, 4, 4]]))\r\n>>> print(""terrible"", torch.randn(5,5,10).max(1))\r\nterrible torch.return_types.max(\r\nvalues=tensor([[ 2.1272,  1.3664,  2.2067,  1.3974, -0.0883,  1.2505,  1.0074,  1.1217,\r\n          0.3849,  0.6936],\r\n        [ 0.6288, -0.4560,  1.2748,  1.5482,  1.2777,  1.6874,  0.7151,  0.6041,\r\n          1.3572,  1.6232],\r\n        [ 1.6703,  1.0075,  1.6480,  2.2839,  1.3390,  0.4938,  1.6449,  1.7628,\r\n          0.8141,  2.5714],\r\n        [ 0.7079,  1.8677,  3.2478,  1.5591,  2.4870,  0.8635, -0.1450,  1.6923,\r\n          1.4924,  1.6298],\r\n        [ 2.4056,  0.8002,  0.9317,  0.7455,  0.7866,  2.1191,  0.3492,  1.2095,\r\n          1.8637,  1.7470]]),\r\nindices=tensor([[1, 1, 0, 0, 0, 0, 3, 4, 4, 4],\r\n        [4, 2, 2, 1, 2, 2, 3, 1, 1, 3],\r\n        [0, 3, 3, 0, 2, 1, 4, 1, 0, 1],\r\n        [4, 1, 3, 0, 3, 2, 0, 1, 4, 3],\r\n        [1, 0, 3, 2, 1, 0, 0, 1, 0, 1]]))\r\n>>> print(""not as good"", torch.randn(5,5,500).max(1))\r\nnot as good torch.return_types.max(\r\nvalues=tensor([[ 0.3877,  0.7873,  1.8701,  ...,  0.5971,  1.6103, -0.3435],\r\n        [ 1.1300,  2.2418,  1.4239,  ...,  1.3943,  0.3872,  1.6475],\r\n        [ 2.0656,  1.3136,  0.9896,  ...,  2.3918,  0.8226,  1.0517],\r\n        [ 1.1054,  0.9945,  1.0561,  ...,  2.1039,  1.1524,  3.0304],\r\n        [ 1.5041,  2.2809,  1.0883,  ...,  0.8504,  2.4774,  1.1041]]),\r\nindices=tensor([[4, 3, 1,  ..., 1, 4, 0],\r\n        [4, 4, 4,  ..., 3, 0, 3],\r\n        [3, 0, 1,  ..., 2, 2, 4],\r\n        [0, 1, 1,  ..., 4, 2, 2],\r\n        [1, 0, 4,  ..., 2, 0, 2]]))\r\n>>> print (""old behaviour = gold standard"")\r\nold behaviour = gold standard\r\n>>> print(tuple(torch.randn(5,5,5).max(1)))\r\n(tensor([[ 1.1908,  1.1807,  1.3151,  1.7184,  0.3556],\r\n        [ 0.3798,  0.9213,  0.3001,  1.3087,  2.2419],\r\n        [ 1.4233,  1.4814,  1.9900,  1.7744,  1.3059],\r\n        [ 1.0026, -0.0330,  1.3061,  1.8730,  2.0685],\r\n        [ 1.3041,  1.6458,  1.3449,  1.8948,  3.6206]]), tensor([[0, 4, 3, 4, 0],\r\n        [1, 1, 4, 0, 4],\r\n        [4, 1, 0, 3, 3],\r\n        [1, 2, 1, 4, 0],\r\n        [3, 3, 0, 3, 3]]))\r\n>>> print(tuple(torch.randn(5,5,10).max(1)))\r\n(tensor([[-0.1232,  0.8275,  0.6732,  1.1223,  0.8247,  1.2851,  1.6009,  1.9979,\r\n          1.9109,  0.7313],\r\n        [ 0.2260,  0.5922,  1.6928,  0.6024,  2.1158,  3.0619,  0.5653,  0.7426,\r\n          0.8316,  0.6346],\r\n        [ 0.4319,  0.2231,  0.5255,  1.7620,  1.1657,  0.8875,  0.5782,  0.6506,\r\n          0.5032,  1.7097],\r\n        [ 0.4137,  1.7265,  1.4260,  2.0301,  1.2244,  0.7128,  2.6345,  0.7230,\r\n          1.3553,  1.6508],\r\n        [ 1.0684,  1.7195,  1.4068,  0.7076, -0.0242,  0.8474,  0.8754,  1.7108,\r\n          0.2188,  1.1584]]), tensor([[0, 1, 3, 4, 2, 3, 4, 2, 1, 0],\r\n        [1, 4, 0, 0, 3, 2, 0, 0, 3, 3],\r\n        [2, 3, 1, 1, 4, 0, 1, 4, 4, 4],\r\n        [0, 4, 1, 3, 2, 0, 2, 0, 3, 1],\r\n        [1, 0, 0, 0, 0, 3, 3, 3, 2, 0]]))\r\n>>> print(tuple(torch.randn(5,5,500).max(1)))\r\n(tensor([[0.9395, 1.5572, 1.8797,  ..., 2.0494, 0.8202, 0.9623],\r\n        [1.7937, 0.7225, 1.8836,  ..., 0.7927, 1.4976, 1.1813],\r\n        [0.8558, 1.6943, 1.4192,  ..., 0.8327, 1.9661, 0.4197],\r\n        [1.2993, 1.4995, 0.9357,  ..., 0.7810, 1.3030, 2.6216],\r\n        [1.4206, 1.8315, 1.0338,  ..., 1.4312, 1.3198, 1.5233]]), tensor([[0, 4, 3,  ..., 3, 0, 2],\r\n        [0, 1, 0,  ..., 0, 4, 3],\r\n        [3, 4, 3,  ..., 3, 0, 0],\r\n        [3, 2, 3,  ..., 1, 2, 1],\r\n        [1, 2, 4,  ..., 3, 1, 3]]))\r\n```",open source,VitalyFedyunin,"Fixes https://github.com/pytorch/pytorch/issues/17112\r\n```python\r\nprint(""good"", torch.randn(5,5,5).max(1))\r\nprint(""terrible"", torch.randn(5,5,10).max(1))\r\nprint(""not as good"", torch.randn(5,5,500).max(1))\r\nprint (""old behaviour = gold standard"")\r\nprint(tuple(torch.randn(5,5,5).max(1)))\r\nprint(tuple(torch.randn(5,5,10).max(1)))\r\nprint(tuple(torch.randn(5,5,500).max(1)))\r\n```\r\nnow gives\r\n```\r\n>>> import torch\r\n>>> print(""good"", torch.randn(5,5,5).max(1))\r\ngood torch.return_types.max(\r\nvalues=tensor([[ 1.2821,  1.8063,  1.8075,  1.3082, -0.1267],\r\n        [ 0.3437,  0.7353,  1.2619,  0.7557,  1.6662],\r\n        [ 0.8583,  1.8906,  1.0246,  1.7598,  1.1184],\r\n        [ 1.7821,  0.0230,  0.9452,  1.0318,  1.0823],\r\n        [ 0.4116, -0.0379, -0.1843,  1.4129,  1.8796]]),\r\nindices=tensor([[4, 4, 3, 2, 1],\r\n        [1, 2, 4, 1, 1],\r\n        [2, 4, 0, 2, 1],\r\n        [0, 2, 0, 3, 1],\r\n        [0, 4, 4, 4, 4]]))\r\n>>> print(""terrible"", torch.randn(5,5,10).max(1))\r\nterrible torch.return_types.max(\r\nvalues=tensor([[ 2.1272,  1.3664,  2.2067,  1.3974, -0.0883,  1.2505,  1.0074,  1.1217,\r\n          0.3849,  0.6936],\r\n        [ 0.6288, -0.4560,  1.2748,  1.5482,  1.2777,  1.6874,  0.7151,  0.6041,\r\n          1.3572,  1.6232],\r\n        [ 1.6703,  1.0075,  1.6480,  2.2839,  1.3390,  0.4938,  1.6449,  1.7628,\r\n          0.8141,  2.5714],\r\n        [ 0.7079,  1.8677,  3.2478,  1.5591,  2.4870,  0.8635, -0.1450,  1.6923,\r\n          1.4924,  1.6298],\r\n        [ 2.4056,  0.8002,  0.9317,  0.7455,  0.7866,  2.1191,  0.3492,  1.2095,\r\n          1.8637,  1.7470]]),\r\nindices=tensor([[1, 1, 0, 0, 0, 0, 3, 4, 4, 4],\r\n        [4, 2, 2, 1, 2, 2, 3, 1, 1, 3],\r\n        [0, 3, 3, 0, 2, 1, 4, 1, 0, 1],\r\n        [4, 1, 3, 0, 3, 2, 0, 1, 4, 3],\r\n        [1, 0, 3, 2, 1, 0, 0, 1, 0, 1]]))\r\n>>> print(""not as good"", torch.randn(5,5,500).max(1))\r\nnot as good torch.return_types.max(\r\nvalues=tensor([[ 0.3877,  0.7873,  1.8701,  ...,  0.5971,  1.6103, -0.3435],\r\n        [ 1.1300,  2.2418,  1.4239,  ...,  1.3943,  0.3872,  1.6475],\r\n        [ 2.0656,  1.3136,  0.9896,  ...,  2.3918,  0.8226,  1.0517],\r\n        [ 1.1054,  0.9945,  1.0561,  ...,  2.1039,  1.1524,  3.0304],\r\n        [ 1.5041,  2.2809,  1.0883,  ...,  0.8504,  2.4774,  1.1041]]),\r\nindices=tensor([[4, 3, 1,  ..., 1, 4, 0],\r\n        [4, 4, 4,  ..., 3, 0, 3],\r\n        [3, 0, 1,  ..., 2, 2, 4],\r\n        [0, 1, 1,  ..., 4, 2, 2],\r\n        [1, 0, 4,  ..., 2, 0, 2]]))\r\n>>> print (""old behaviour = gold standard"")\r\nold behaviour = gold standard\r\n>>> print(tuple(torch.randn(5,5,5).max(1)))\r\n(tensor([[ 1.1908,  1.1807,  1.3151,  1.7184,  0.3556],\r\n        [ 0.3798,  0.9213,  0.3001,  1.3087,  2.2419],\r\n        [ 1.4233,  1.4814,  1.9900,  1.7744,  1.3059],\r\n        [ 1.0026, -0.0330,  1.3061,  1.8730,  2.0685],\r\n        [ 1.3041,  1.6458,  1.3449,  1.8948,  3.6206]]), tensor([[0, 4, 3, 4, 0],\r\n        [1, 1, 4, 0, 4],\r\n        [4, 1, 0, 3, 3],\r\n        [1, 2, 1, 4, 0],\r\n        [3, 3, 0, 3, 3]]))\r\n>>> print(tuple(torch.randn(5,5,10).max(1)))\r\n(tensor([[-0.1232,  0.8275,  0.6732,  1.1223,  0.8247,  1.2851,  1.6009,  1.9979,\r\n          1.9109,  0.7313],\r\n        [ 0.2260,  0.5922,  1.6928,  0.6024,  2.1158,  3.0619,  0.5653,  0.7426,\r\n          0.8316,  0.6346],\r\n        [ 0.4319,  0.2231,  0.5255,  1.7620,  1.1657,  0.8875,  0.5782,  0.6506,\r\n          0.5032,  1.7097],\r\n        [ 0.4137,  1.7265,  1.4260,  2.0301,  1.2244,  0.7128,  2.6345,  0.7230,\r\n          1.3553,  1.6508],\r\n        [ 1.0684,  1.7195,  1.4068,  0.7076, -0.0242,  0.8474,  0.8754,  1.7108,\r\n          0.2188,  1.1584]]), tensor([[0, 1, 3, 4, 2, 3, 4, 2, 1, 0],\r\n        [1, 4, 0, 0, 3, 2, 0, 0, 3, 3],\r\n        [2, 3, 1, 1, 4, 0, 1, 4, 4, 4],\r\n        [0, 4, 1, 3, 2, 0, 2, 0, 3, 1],\r\n        [1, 0, 0, 0, 0, 3, 3, 3, 2, 0]]))\r\n>>> print(tuple(torch.randn(5,5,500).max(1)))\r\n(tensor([[0.9395, 1.5572, 1.8797,  ..., 2.0494, 0.8202, 0.9623],\r\n        [1.7937, 0.7225, 1.8836,  ..., 0.7927, 1.4976, 1.1813],\r\n        [0.8558, 1.6943, 1.4192,  ..., 0.8327, 1.9661, 0.4197],\r\n        [1.2993, 1.4995, 0.9357,  ..., 0.7810, 1.3030, 2.6216],\r\n        [1.4206, 1.8315, 1.0338,  ..., 1.4312, 1.3198, 1.5233]]), tensor([[0, 4, 3,  ..., 3, 0, 2],\r\n        [0, 1, 0,  ..., 0, 4, 3],\r\n        [3, 4, 3,  ..., 3, 0, 0],\r\n        [3, 2, 3,  ..., 1, 2, 1],\r\n        [1, 2, 4,  ..., 3, 1, 3]]))\r\n```","python\r\nprint(""good"", torch.randn(5,5,5).max(1))\r\nprint(""terrible"", torch.randn(5,5,10).max(1))\r\nprint(""not as good"", torch.randn(5,5,500).max(1))\r\nprint (""old behaviour = gold standard"")\r\nprint(tuple(torch.randn(5,5,5).max(1)))\r\nprint(tuple(torch.randn(5,5,10).max(1)))\r\nprint(tuple(torch.randn(5,5,500).max(1)))\r\n"
17106,"[JIT] Scripting arguments for call are not valid## \U0001f41b Bug\r\n\r\nI am converting my model to TorchScript in order to be able to save it and load it in C++. After solving various errors that arose along the way, I am left with a  _RuntimeError:  arguments for call are not valid:_ which I do not know where it originates exactly.\r\n\r\n## To Reproduce\r\n\r\nThe latest reference to my code comes from a calling to this function:\r\n\r\nwhere\r\n\r\n\r\n\r\nand\r\n\r\n\r\nproduces this error:\r\n```\r\n> RuntimeError: \r\n> arguments for call are not valid:\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int? size=<default>, float[]? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float[]? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int[]? size=<default>, float[]? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float[]? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int? size=<default>, float? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int[]? size=<default>, float? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n> for call at:\r\n> @weak_script_method\r\n> def forward(self, input):\r\n>     warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>     return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>            ~~~~~~~~~~~~~ <--- HERE\r\n\r\nand the traceback is:\r\n\r\n> Traceback (most recent call last):\r\n>   File ""train.py"", line 204, in <module>\r\n>     main()\r\n>   File ""train.py"", line 80, in main\r\n>     model = YOLOv3(anchors, anch_mask, n_classes, ignore_thre=ignore_thre)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/Documents/yolotorch/training_implementation/models/yolov3.py"", line 294, in __init__\r\n>     self.module_list= create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1123, in __setattr__\r\n>     super(ScriptModule, self).__setattr__(attr, _ConstModuleList(value))\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1355, in __init__\r\n>     module = _make_strong(module)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1254, in _make_strong\r\n>     proxy = WeakScriptModuleProxy(mod, stubs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1191, in __init__\r\n>     _create_methods_from_stubs(self, stubs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 913, in _create_methods_from_stubs\r\n>     self._create_methods(defs, rcbs, defaults)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe code should produce a ScriptModule which I should be able to save. \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 Ti\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.12.1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n",oncall: jit,driazati,"## \U0001f41b Bug\r\n\r\nI am converting my model to TorchScript in order to be able to save it and load it in C++. After solving various errors that arose along the way, I am left with a  _RuntimeError:  arguments for call are not valid:_ which I do not know where it originates exactly.\r\n\r\n## To Reproduce\r\n\r\nThe latest reference to my code comes from a calling to this function:\r\n```python\r\n\r\ndef create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre):\r\n    mlist = nn.ModuleList()\r\n    mlist.append(add_conv(in_ch=3, out_ch=32, ksize=3, stride=1))\r\n    mlist.append(add_conv(in_ch=32, out_ch=64, ksize=3, stride=2))\r\n    mlist.append(resblock(ch=64))\r\n    mlist.append(add_conv(in_ch=64, out_ch=128, ksize=3, stride=2))\r\n    mlist.append(resblock(ch=128, nblocks=2))\r\n    ...\r\n```\r\nwhere\r\n\r\n```python\r\ndef add_conv(in_ch, out_ch, ksize, stride):\r\n    stage = nn.Sequential()\r\n    pad = (ksize - 1) // 2\r\n    stage.add_module('conv', nn.Conv2d(in_channels=in_ch,\r\n                                       out_channels=out_ch, kernel_size=ksize, stride=stride,\r\n                                       padding=pad, bias=False))\r\n    stage.add_module('batch_norm', nn.BatchNorm2d(out_ch))\r\n    stage.add_module('leaky', nn.LeakyReLU(0.1))\r\n    return stage\r\n```\r\n\r\nand\r\n\r\n```python\r\nclass resblock(ScriptModule):\r\n\r\n    __constants__ = [""nblocks"",""ch"",""shortcut""]\r\n\r\n    def __init__(self, ch, nblocks=1, shortcut=True):\r\n\r\n        super().__init__()\r\n        self.shortcut = shortcut\r\n        self.nblocks = nblocks\r\n        self.ch = ch\r\n        self.module_list = nn.ModuleList()\r\n        self.blockt1=add_conv(self.ch, self.ch//2, 1, 1)\r\n        self.blockt2=add_conv(self.ch//2, self.ch, 3, 1)\r\n        for _ in range(nblocks):\r\n            resblock_one = nn.ModuleList()\r\n            self.blockt1\r\n            self.blockt2\r\n            self.module_list.append(resblock_one)\r\n\r\n    @script_method\r\n    def forward(self, x):\r\n        for _ in range(self.nblocks):#in_ch, out_ch, ksize, stride\r\n            h = x\r\n            h = self.blockt1(h)\r\n            h = self.blockt2(h)\r\n            x = x + h if self.shortcut else h\r\n        return x\r\n```\r\nproduces this error:\r\n```\r\n> RuntimeError: \r\n> arguments for call are not valid:\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int? size=<default>, float[]? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float[]? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int[]? size=<default>, float[]? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float[]? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int? size=<default>, float? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n>   \r\n>   for operator aten::__interpolate(Tensor input, int[]? size=<default>, float? scale_factor=<default>, string mode=<default>, bool? align_corners=<default>) -> Tensor:\r\n>   expected a value of type float? for argument 'scale_factor' but found int\r\n>   @weak_script_method\r\n>   def forward(self, input):\r\n>       warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>       return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>                                              ~~~~~~~~~~ <--- HERE\r\n> for call at:\r\n> @weak_script_method\r\n> def forward(self, input):\r\n>     warnings.warn(""nn.{} is deprecated. Use nn.functional.interpolate instead."".format(self.name))\r\n>     return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners)\r\n>            ~~~~~~~~~~~~~ <--- HERE\r\n\r\nand the traceback is:\r\n\r\n> Traceback (most recent call last):\r\n>   File ""train.py"", line 204, in <module>\r\n>     main()\r\n>   File ""train.py"", line 80, in main\r\n>     model = YOLOv3(anchors, anch_mask, n_classes, ignore_thre=ignore_thre)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/Documents/yolotorch/training_implementation/models/yolov3.py"", line 294, in __init__\r\n>     self.module_list= create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1123, in __setattr__\r\n>     super(ScriptModule, self).__setattr__(attr, _ConstModuleList(value))\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1355, in __init__\r\n>     module = _make_strong(module)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1254, in _make_strong\r\n>     proxy = WeakScriptModuleProxy(mod, stubs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 951, in init_then_register\r\n>     original_init(self, *args, **kwargs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 1191, in __init__\r\n>     _create_methods_from_stubs(self, stubs)\r\n>   File ""/home/.conda/envs/python36_ocv_pytorch/lib/python3.6/site-packages/torch/jit/__init__.py"", line 913, in _create_methods_from_stubs\r\n>     self._create_methods(defs, rcbs, defaults)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe code should produce a ScriptModule which I should be able to save. \r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070 Ti\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.12.1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] pytorch                   1.0.1           py3.6_cuda10.0.130_cudnn7.4.2_2    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n","python\r\n\r\ndef create_yolov3_modules(anchors, anch_mask, n_classes, ignore_thre):\r\n    mlist = nn.ModuleList()\r\n    mlist.append(add_conv(in_ch=3, out_ch=32, ksize=3, stride=1))\r\n    mlist.append(add_conv(in_ch=32, out_ch=64, ksize=3, stride=2))\r\n    mlist.append(resblock(ch=64))\r\n    mlist.append(add_conv(in_ch=64, out_ch=128, ksize=3, stride=2))\r\n    mlist.append(resblock(ch=128, nblocks=2))\r\n    ...\r\n"
17095,"libtorch elevated memory usage.## \U0001f41b Bug\r\n\r\nReported by @dawnwch over here: https://github.com/pytorch/pytorch/issues/16255#issuecomment-462730809\r\n\r\nOriginal message pasted below.\r\n\r\ncc: @yf225 can you help get to the bottom of this?\r\n\r\nHello, I still have the problem by using the preview builds(1.0.0.dev20190211).\r\nI use `torch::cuda::cudnn_is_available()` to see if CuDNN being linked with libtorch, and the result is `True`.\r\nThe model I use is [https://github.com/dawnwch/for-a-libtorch-question](url) (There are ReflectionPad2d, Conv2d, InstanceNorml2d, ReLU,ConvTraansposed2d and Tanh in my model)and the test code is\r\n\r\nThe error is\r\n```\r\nCUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.91 GiB total capacity; 10.07 GiB already allocated; 121.38 MiB free; 10.18 MiB cached) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:236)\r\n```\r\n\r\nHere is the gpu memory usage I test.\r\n![image](https://user-images.githubusercontent.com/35034417/52633408-3effcb00-2eff-11e9-9628-94ff4d8d18be.png)\r\nHope for your replay and thanks for your help.\r\n",high priority|module: cpp,yf225,"## \U0001f41b Bug\r\n\r\nReported by @dawnwch over here: https://github.com/pytorch/pytorch/issues/16255#issuecomment-462730809\r\n\r\nOriginal message pasted below.\r\n\r\ncc: @yf225 can you help get to the bottom of this?\r\n\r\nHello, I still have the problem by using the preview builds(1.0.0.dev20190211).\r\nI use `torch::cuda::cudnn_is_available()` to see if CuDNN being linked with libtorch, and the result is `True`.\r\nThe model I use is [https://github.com/dawnwch/for-a-libtorch-question](url) (There are ReflectionPad2d, Conv2d, InstanceNorml2d, ReLU,ConvTraansposed2d and Tanh in my model)and the test code is\r\n```cpp\r\n#include <torch/script.h>\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n#include <memory>\r\n\r\nint main(int argc, const char* argv[]) {\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n    assert(module != nullptr);\r\n    torch::DeviceType device;\r\n    device = torch::kCUDA;\r\n    module->to(device);\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(torch::ones({1, 3, 1920, 1080}).to(device));\r\n    at::Tensor output = module->forward(inputs).toTensor();\r\n    std::cout << ""Done"" << std::endl;\r\n    return 0;\r\n}\r\n```\r\nThe error is\r\n```\r\nCUDA out of memory. Tried to allocate 1.11 GiB (GPU 0; 10.91 GiB total capacity; 10.07 GiB already allocated; 121.38 MiB free; 10.18 MiB cached) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:236)\r\n```\r\n\r\nHere is the gpu memory usage I test.\r\n![image](https://user-images.githubusercontent.com/35034417/52633408-3effcb00-2eff-11e9-9628-94ff4d8d18be.png)\r\nHope for your replay and thanks for your help.\r\n","cpp\r\n#include <torch/script.h>\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n#include <memory>\r\n\r\nint main(int argc, const char* argv[]) {\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(argv[1]);\r\n    assert(module != nullptr);\r\n    torch::DeviceType device;\r\n    device = torch::kCUDA;\r\n    module->to(device);\r\n    std::vector<torch::jit::IValue> inputs;\r\n    inputs.push_back(torch::ones({1, 3, 1920, 1080}).to(device));\r\n    at::Tensor output = module->forward(inputs).toTensor();\r\n    std::cout << ""Done"" << std::endl;\r\n    return 0;\r\n}\r\n"
16939,"multinomial performance regressed 2x from 0.4.1 to 1.0.1## \U0001f41b Bug\r\n\r\n```\r\nPyTorch version:  0.4.1\r\nMKL:  True\r\nIterations:  48000\r\nThreads:  1\r\nSize:  256\r\ntorch.multinomial(a, 1): 0.318154849\r\n```\r\n\r\n```\r\nPyTorch version:  1.0.1.post2\r\nMKL:  True\r\nIterations:  48000\r\nThreads:  1\r\nSize:  256\r\ntorch.multinomial(a, 1): 0.640500445\r\n```\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nExpect same or better performance with torch 1.0.1.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.1\r\nGCC version: Could not collect\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-nlp==0.3.7.post1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.6            py37h27c97d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.0.1                   py3.7_2    pytorch\r\n[conda] pytorch-nlp               0.3.7.post1              pypi_0    pypi\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n",module: performance,ifedan,"## \U0001f41b Bug\r\n\r\n```\r\nPyTorch version:  0.4.1\r\nMKL:  True\r\nIterations:  48000\r\nThreads:  1\r\nSize:  256\r\ntorch.multinomial(a, 1): 0.318154849\r\n```\r\n\r\n```\r\nPyTorch version:  1.0.1.post2\r\nMKL:  True\r\nIterations:  48000\r\nThreads:  1\r\nSize:  256\r\ntorch.multinomial(a, 1): 0.640500445\r\n```\r\n\r\n## To Reproduce\r\n\r\n```python3\r\nimport torch\r\nfrom timeit import timeit\r\n\r\n# Some defaults\r\nnumber = 48000\r\nn = 256\r\ntorch.set_grad_enabled(False)\r\nprint('PyTorch version: ', torch.__version__)\r\nprint('MKL: ', torch.backends.mkl.is_available())\r\nprint('Iterations: ', number)\r\nprint('Threads: ', torch.get_num_threads())\r\nprint('Size: ', n)\r\n\r\na = torch.rand(256)\r\n\r\n# Test for performance\r\nkwargs = {'globals': globals(), 'number': number}\r\nprint('torch.multinomial(a, 1):', timeit(""torch.multinomial(a, 1)"", **kwargs))\r\n```\r\n\r\n## Expected behavior\r\n\r\nExpect same or better performance with torch 1.0.1.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.1\r\nGCC version: Could not collect\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.8.0\r\n[pip] pytorch-nlp==0.3.7.post1\r\n[pip] torch==1.0.1.post2\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] mkl-service               1.1.2            py37hfbe908c_5  \r\n[conda] mkl_fft                   1.0.6            py37h27c97d8_0  \r\n[conda] mkl_random                1.0.2            py37h27c97d8_0  \r\n[conda] pytorch                   1.0.1                   py3.7_2    pytorch\r\n[conda] pytorch-nlp               0.3.7.post1              pypi_0    pypi\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```\r\n","python3\r\nimport torch\r\nfrom timeit import timeit\r\n\r\n# Some defaults\r\nnumber = 48000\r\nn = 256\r\ntorch.set_grad_enabled(False)\r\nprint('PyTorch version: ', torch.__version__)\r\nprint('MKL: ', torch.backends.mkl.is_available())\r\nprint('Iterations: ', number)\r\nprint('Threads: ', torch.get_num_threads())\r\nprint('Size: ', n)\r\n\r\na = torch.rand(256)\r\n\r\n# Test for performance\r\nkwargs = {'globals': globals(), 'number': number}\r\nprint('torch.multinomial(a, 1):', timeit(""torch.multinomial(a, 1)"", **kwargs))\r\n"
16830,pytorch cpp api the derivative for 'target' is not implementedCode to reproduce the error. Is it a bug on my machine?\r\n\r\nMain\r\n\r\n\r\nCMake\r\n\r\n\r\nThe Python equivalent of this code can be fixed by retaining the graph. May this be my mistake. How can I keep the graph in the backward pass?,module: cpp|triaged,glaringlee,"Code to reproduce the error. Is it a bug on my machine?\r\n\r\nMain\r\n```cpp\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n\r\n\t// Layer.\r\n\ttorch::nn::Linear linear1(5, 1);\r\n\ttorch::nn::Linear linear2(5, 1);\r\n\r\n\t// Optimizer.\r\n\ttorch::optim::Adam opt(linear1->parameters(), torch::optim::AdamOptions(0.001));\r\n\r\n\t// Input.\r\n\ttorch::Tensor in = torch::randn({1, 5});\r\n\r\n\t// Output. Have one of these tensors. The first one wont work.\r\n\ttorch::Tensor desired_out = linear2->forward(in); // raises the error ""the derivative for 'target' is not implemented""\r\n\t//torch::Tensor desired_out = torch::ones({1, 1}); // works perfectly fine\r\n\r\n\t// Training loop.\r\n\tstd::cout << ""initial: "" << linear1->forward(in) << std::endl;\r\n\r\n\tfor (int i = 0; i < 1000; i++) {\r\n\r\n\t\topt.zero_grad();\r\n\t\ttorch::Tensor out = linear1->forward(in);\r\n\t\ttorch::Tensor loss = torch::mse_loss(out, desired_out);\r\n\t\tloss.backward();\r\n\t\topt.step();\t\r\n\t}\r\n\r\n\tstd::cout << ""desired: "" << desired_out << std::endl;\r\n\tstd::cout << ""trained: "" << linear1->forward(in) << std::endl;\r\n\r\n\treturn 0;\r\n}\r\n```\r\n\r\nCMake\r\n```cmake\r\ncmake_minimum_required(VERSION 3.11 FATAL_ERROR)\r\n\r\nfind_package(Torch REQUIRED)\r\n\r\ninclude_directories(${Torch_INCLUDE_DIRS})\r\n\r\nadd_executable(main main.cpp)\r\ntarget_link_libraries(main ${TORCH_LIBRARIES})\r\n```\r\n\r\nThe Python equivalent of this code can be fixed by retaining the graph. May this be my mistake. How can I keep the graph in the backward pass?","cpp\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n\r\n\t// Layer.\r\n\ttorch::nn::Linear linear1(5, 1);\r\n\ttorch::nn::Linear linear2(5, 1);\r\n\r\n\t// Optimizer.\r\n\ttorch::optim::Adam opt(linear1->parameters(), torch::optim::AdamOptions(0.001));\r\n\r\n\t// Input.\r\n\ttorch::Tensor in = torch::randn({1, 5});\r\n\r\n\t// Output. Have one of these tensors. The first one wont work.\r\n\ttorch::Tensor desired_out = linear2->forward(in); // raises the error ""the derivative for 'target' is not implemented""\r\n\t//torch::Tensor desired_out = torch::ones({1, 1}); // works perfectly fine\r\n\r\n\t// Training loop.\r\n\tstd::cout << ""initial: "" << linear1->forward(in) << std::endl;\r\n\r\n\tfor (int i = 0; i < 1000; i++) {\r\n\r\n\t\topt.zero_grad();\r\n\t\ttorch::Tensor out = linear1->forward(in);\r\n\t\ttorch::Tensor loss = torch::mse_loss(out, desired_out);\r\n\t\tloss.backward();\r\n\t\topt.step();\t\r\n\t}\r\n\r\n\tstd::cout << ""desired: "" << desired_out << std::endl;\r\n\tstd::cout << ""trained: "" << linear1->forward(in) << std::endl;\r\n\r\n\treturn 0;\r\n}\r\n"
16742,"Training flag in the JIT script module is not saved in the serialization## \U0001f41b Bug\r\n\r\nEven we call the eval() before the serialization, the loaded model's training flag is still True.\r\n\r\n## To Reproduce\r\n\r\n\r\nThe output is\r\n```\r\na's Training: False\r\nb's Training: True\r\na.d's Training: False\r\nb.d's Training: True\r\na(input):  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\r\nb(input):  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\r\n```\r\n\r\ncc: @suo @jamesr66a @zdevito @pritamdamania87 ",oncall: jit,suo,"## \U0001f41b Bug\r\n\r\nEven we call the eval() before the serialization, the loaded model's training flag is still True.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\n\r\nclass TestModule(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(TestModule, self).__init__()\r\n        self.d = torch.nn.Dropout()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        return self.d(input)\r\n\r\na = TestModule()\r\na.eval()\r\ntorch.jit.save(a, ""test_training_flag.pt"")\r\n\r\nb = torch.jit.load(""test_training_flag.pt"")\r\nprint(""a's Training: {}\\nb's Training: {}"".format(a.training, b.training))\r\nprint(""a.d's Training: {}\\nb.d's Training: {}"".format(a.d.training, b.d.training))\r\n\r\ninput = torch.zeros([10])\r\nprint(""a(input): "", a(input))\r\nprint(""b(input): "", b(input))\r\n```\r\n\r\nThe output is\r\n```\r\na's Training: False\r\nb's Training: True\r\na.d's Training: False\r\nb.d's Training: True\r\na(input):  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\r\nb(input):  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\r\n```\r\n\r\ncc: @suo @jamesr66a @zdevito @pritamdamania87 ","python\r\nimport torch\r\n\r\nclass TestModule(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(TestModule, self).__init__()\r\n        self.d = torch.nn.Dropout()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input):\r\n        return self.d(input)\r\n\r\na = TestModule()\r\na.eval()\r\ntorch.jit.save(a, ""test_training_flag.pt"")\r\n\r\nb = torch.jit.load(""test_training_flag.pt"")\r\nprint(""a's Training: {}\\nb's Training: {}"".format(a.training, b.training))\r\nprint(""a.d's Training: {}\\nb.d's Training: {}"".format(a.d.training, b.d.training))\r\n\r\ninput = torch.zeros([10])\r\nprint(""a(input): "", a(input))\r\nprint(""b(input): "", b(input))\r\n"
16741,"enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory## \U0001f41b Bug\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nWe have a C++ script using PyTorch C++ api to test a model's performance , and when loading the saved model by our PyTorch script and calling this line:\r\n\r\n\r\n\r\nwe got core dump error as described above.Please help.\r\n\r\nWe have the following included libraries in our C++ scripts:\r\n\r\n\r\n\r\n## Environment\r\n\r\nThe C compiler identification is GNU 5.4.0\r\nThe CXX compiler identification is GNU 5.4.0\r\nUbuntu 16.04\r\nPytorch 1.0.0\r\ncuda 10.0\r\ncudnn:7.4.2\r\nPython 3.6\r\nConda installed Pytorch\r\n\r\n\r\n\r\n",oncall: jit,houseroad,"## \U0001f41b Bug\r\n\r\n```python\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  [enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f15338da111 in xxx/projects/libtorch/lib/libc10.so)\r\nframe #1: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x49 (0x7f15338d9f29 in /xxx/projects/libtorch/lib/libc10.so)\r\nframe #2: caffe2::serialize::PyTorchStreamReader::valid(char const*) + 0x6b (0x7f156be4c54b in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #3: caffe2::serialize::PyTorchStreamReader::init() + 0x9d (0x7f156be4e31d in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #4: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >) + 0x3b (0x7f156be4fd7b in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #5: <unknown function> + 0x6797de (0x7f157741e7de in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>) + 0x95 (0x7f1577421f65 in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #7: torch::jit::load(std::string const&, c10::optional<c10::Device>) + 0x60 (0x7f1577422100 in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #8: main + 0x125 (0x432291 in ./build/infer)\r\nframe #9: __libc_start_main + 0xf0 (0x7f1532f78830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x430eb9 in ./build/infer)\r\n\r\nAborted (core dumped)\r\n```\r\n\r\n## To Reproduce\r\n\r\nWe have a C++ script using PyTorch C++ api to test a model's performance , and when loading the saved model by our PyTorch script and calling this line:\r\n\r\n```cpp\r\nstd::shared_ptr<torch::jit::script::Module> model = torch::jit::load(""model.pt"");\r\n```\r\n\r\nwe got core dump error as described above.Please help.\r\n\r\nWe have the following included libraries in our C++ scripts:\r\n\r\n```cpp\r\n#include <chrono>\r\n#include <stdio.h>\r\n#include <torch/torch.h>\r\n#include <torch/script.h>\r\n```\r\n\r\n## Environment\r\n\r\nThe C compiler identification is GNU 5.4.0\r\nThe CXX compiler identification is GNU 5.4.0\r\nUbuntu 16.04\r\nPytorch 1.0.0\r\ncuda 10.0\r\ncudnn:7.4.2\r\nPython 3.6\r\nConda installed Pytorch\r\n\r\n\r\n\r\n","python\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  [enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory\r\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f15338da111 in xxx/projects/libtorch/lib/libc10.so)\r\nframe #1: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x49 (0x7f15338d9f29 in /xxx/projects/libtorch/lib/libc10.so)\r\nframe #2: caffe2::serialize::PyTorchStreamReader::valid(char const*) + 0x6b (0x7f156be4c54b in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #3: caffe2::serialize::PyTorchStreamReader::init() + 0x9d (0x7f156be4e31d in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #4: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >) + 0x3b (0x7f156be4fd7b in /xxx/projects/libtorch/lib/libcaffe2.so)\r\nframe #5: <unknown function> + 0x6797de (0x7f157741e7de in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #6: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>) + 0x95 (0x7f1577421f65 in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #7: torch::jit::load(std::string const&, c10::optional<c10::Device>) + 0x60 (0x7f1577422100 in /xxx/projects/libtorch/lib/libtorch.so.1)\r\nframe #8: main + 0x125 (0x432291 in ./build/infer)\r\nframe #9: __libc_start_main + 0xf0 (0x7f1532f78830 in /lib/x86_64-linux-gnu/libc.so.6)\r\nframe #10: _start + 0x29 (0x430eb9 in ./build/infer)\r\n\r\nAborted (core dumped)\r\n"
16713,[JIT] Assign parameter in nested module## \u2753 Questions and Help\r\n\r\nIs it currently possible with TorchScript to change an argument of a module class within a module?\r\n\r\n**e.g.**\r\n\r\n\r\n\r\nI currently get an error saying I can not access a JIT module attribute (or something along those lines).\r\n\r\nIs there a workaround for this?,needs reproduction|oncall: jit,suo,"## \u2753 Questions and Help\r\n\r\nIs it currently possible with TorchScript to change an argument of a module class within a module?\r\n\r\n**e.g.**\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\nclass LinearScheduler(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['i', 'drop_values']\r\n    def __init__(self, module, start_value, stop_value, nr_steps):\r\n        super(LinearScheduler, self).__init__()\r\n        self.module = module\r\n        self.i = 0\r\n        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=nr_steps).tolist()\r\n\r\n    def forward(self, x):\r\n        return self.module(x)\r\n\r\n    def step(self):\r\n        if self.i < len(self.drop_values):\r\n            self.module.drop_prob = self.drop_values[self.i]\r\n\r\n        self.i += 1\r\n```\r\n\r\nI currently get an error saying I can not access a JIT module attribute (or something along those lines).\r\n\r\nIs there a workaround for this?","python\r\nimport torch\r\nimport numpy as np\r\n\r\n\r\nclass LinearScheduler(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['i', 'drop_values']\r\n    def __init__(self, module, start_value, stop_value, nr_steps):\r\n        super(LinearScheduler, self).__init__()\r\n        self.module = module\r\n        self.i = 0\r\n        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=nr_steps).tolist()\r\n\r\n    def forward(self, x):\r\n        return self.module(x)\r\n\r\n    def step(self):\r\n        if self.i < len(self.drop_values):\r\n            self.module.drop_prob = self.drop_values[self.i]\r\n\r\n        self.i += 1\r\n"
16699,"RuntimeError: block->inputs().size() >= num_initializers ASSERT FAILED## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. initialize a model object by  model = Model()\r\n2. read model state by model_state = torch.load(os.path.join(args.serialization_dir, 'best.th'), map_location=torch.device('cpu'))\r\n3. load model state by model.load_state_dict(model_state)\r\n4. torch.onnx.export(model=model, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nFailed to export model using export_onnx\r\n```\r\nTraceback (most recent call last):\r\n  File ""scripts/export_pytorch_script.py"", line 87, in <module>\r\n    sys.exit(main())\r\n  File ""scripts/export_pytorch_script.py"", line 83, in main\r\n    torch.onnx.export(model=model._transformer, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 27, in export\r\n    return utils.export(*args, **kwargs)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/utils.py"", line 104, in export\r\n    operator_export_type=operator_export_type)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/utils.py"", line 292, in _export\r\n    proto, export_map = graph._export_onnx(params, _onnx_opset_version, defer_weight_export, operator_export_type)\r\nRuntimeError: block->inputs().size() >= num_initializers ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1549065344879/work/torch/csrc/jit/export.cpp:307, please report a bug to PyTorch. (EncodeBlock at /opt/conda/conda-bld/pytorch-nightly_1549065344879/work/torch/csrc/jit/export.cpp:307)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f1059cd08b5 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x56523c (0x7f10579ef23c in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #2: <unknown function> + 0x5658ae (0x7f10579ef8ae in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #3: torch::jit::export_onnx(std::shared_ptr<torch::jit::Graph> const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, long, bool, torch::onnx::OperatorExportTypes) + 0x37 (0x7f10579efa77 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0x3aea18 (0x7f108cfe8a18 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x113c36 (0x7f108cd4dc36 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #28: __libc_start_main + 0xf5 (0x7f109ec94445 in /lib64/libc.so.6)\r\n```\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20190201\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.0.0.dev20190201 py3.7_cuda9.0.176_cudnn7.4.1_0    pytorch\r\n[conda] pytorch-pretrained-bert   0.3.0                    pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: jit|module: onnx,houseroad,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. initialize a model object by  model = Model()\r\n2. read model state by model_state = torch.load(os.path.join(args.serialization_dir, 'best.th'), map_location=torch.device('cpu'))\r\n3. load model state by model.load_state_dict(model_state)\r\n4. torch.onnx.export(model=model, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\ndummy_input = torch.ones(1, 14, 200, dtype=torch.float)\r\ndummy_mask = torch.ones(1, 14, dtype=torch.float)\r\ntorch.onnx.export(model=model._transformer, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n```\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nFailed to export model using export_onnx\r\n```\r\nTraceback (most recent call last):\r\n  File ""scripts/export_pytorch_script.py"", line 87, in <module>\r\n    sys.exit(main())\r\n  File ""scripts/export_pytorch_script.py"", line 83, in main\r\n    torch.onnx.export(model=model._transformer, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/__init__.py"", line 27, in export\r\n    return utils.export(*args, **kwargs)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/utils.py"", line 104, in export\r\n    operator_export_type=operator_export_type)\r\n  File ""/anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/onnx/utils.py"", line 292, in _export\r\n    proto, export_map = graph._export_onnx(params, _onnx_opset_version, defer_weight_export, operator_export_type)\r\nRuntimeError: block->inputs().size() >= num_initializers ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1549065344879/work/torch/csrc/jit/export.cpp:307, please report a bug to PyTorch. (EncodeBlock at /opt/conda/conda-bld/pytorch-nightly_1549065344879/work/torch/csrc/jit/export.cpp:307)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f1059cd08b5 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x56523c (0x7f10579ef23c in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #2: <unknown function> + 0x5658ae (0x7f10579ef8ae in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #3: torch::jit::export_onnx(std::shared_ptr<torch::jit::Graph> const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, long, bool, torch::onnx::OperatorExportTypes) + 0x37 (0x7f10579efa77 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0x3aea18 (0x7f108cfe8a18 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0x113c36 (0x7f108cd4dc36 in /anaconda3/envs/pytorch-nightly/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #28: __libc_start_main + 0xf5 (0x7f109ec94445 in /lib64/libc.so.6)\r\n```\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20190201\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 3.6.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-16GB\r\nGPU 1: Tesla P100-PCIE-16GB\r\n\r\nNvidia driver version: 396.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37ha843d7b_0\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0\r\n[conda] pytorch-nightly           1.0.0.dev20190201 py3.7_cuda9.0.176_cudnn7.4.1_0    pytorch\r\n[conda] pytorch-pretrained-bert   0.3.0                    pypi_0    pypi\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\ndummy_input = torch.ones(1, 14, 200, dtype=torch.float)\r\ndummy_mask = torch.ones(1, 14, dtype=torch.float)\r\ntorch.onnx.export(model=model._transformer, args=(dummy_input, dummy_mask), f=args.output_pt, verbose=True)\r\n"
16664,"JIT Script module support for pad_packed_sequence and pack_padded_sequence## \U0001f680 Feature\r\nUsers would like to implement LSTM module as jit script module. However, `pack_padded_sequence` and `pad_packed_sequence` does not support it yet.\r\n\r\nThe code to reproduce the problem:\r\n\r\n\r\n\r\ncc: @jamesr66a @pritamdamania ",oncall: jit,driazati,"## \U0001f680 Feature\r\nUsers would like to implement LSTM module as jit script module. However, `pack_padded_sequence` and `pad_packed_sequence` does not support it yet.\r\n\r\nThe code to reproduce the problem:\r\n\r\n```python\r\nclass Model(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\r\n        self.lstm = nn.LSTM(EMBEDDING_SIZE, LSTM_SIZE, num_layers=1, bidirectional=True)\r\n\r\n    @jit.script_method\r\n    def forward(self, tokens, seq_lengths):\r\n        embedded = self.embedding(tokens)\r\n        rnn_input = pack_padded_sequence(\r\n            embedded,\r\n            seq_lengths.int(), True, True,\r\n        )\r\n        rep, unused_state = self.lstm(rnn_input)\r\n        unpacked, _ = pad_packed_sequence(\r\n            rep,\r\n            batch_first=True,\r\n            padding_value=0.0,\r\n            total_length=embedded.size(1),\r\n        )\r\n        return unpacked\r\n```\r\n\r\ncc: @jamesr66a @pritamdamania ","python\r\nclass Model(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\r\n        self.lstm = nn.LSTM(EMBEDDING_SIZE, LSTM_SIZE, num_layers=1, bidirectional=True)\r\n\r\n    @jit.script_method\r\n    def forward(self, tokens, seq_lengths):\r\n        embedded = self.embedding(tokens)\r\n        rnn_input = pack_padded_sequence(\r\n            embedded,\r\n            seq_lengths.int(), True, True,\r\n        )\r\n        rep, unused_state = self.lstm(rnn_input)\r\n        unpacked, _ = pad_packed_sequence(\r\n            rep,\r\n            batch_first=True,\r\n            padding_value=0.0,\r\n            total_length=embedded.size(1),\r\n        )\r\n        return unpacked\r\n"
16610,"Potential instability in ConvTranspose2d with cudnn## \U0001f41b Bug\r\n\r\nTogether with @fmassa, we observed a potential instability when I run the following snippet with CUDNN. Without CUDNN, it works as expected.\r\nIt's also worth noticing that this used to work on Pytorch 0.4.1, and that it works as expected on CPU as well.\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nRun the code. It requires a small pickle file containing one single minibatch of data (replacing it by a random tensor did not reproduce the problem). The file is linked here.\r\n[minibatch.pkl.zip](https://github.com/pytorch/pytorch/files/2818110/minibatch.pkl.zip)\r\n\r\n\r\nIf the line `torch.backends.cudnn.enabled = False` is not commented (and thus if I do not use CUDNN), the output is \r\n```\r\nLoss: 412.374939\r\nLoss: 105.106758\r\nLoss: 60.929550\r\nLoss: 56.290668\r\nLoss: 53.807034\r\nLoss: 51.205917\r\nLoss: 50.837688\r\nLoss: 50.152969\r\nLoss: 49.427326\r\nLoss: 49.931602\r\nLoss: 49.682564\r\nLoss: 49.036819\r\n```\r\n\r\nIf I comment that that line (which means that I am using CUDNN), the output is:\r\n```\r\nLoss: 412.365082\r\nLoss: 159.593903\r\nLoss: 139.744904\r\nLoss: 180.208984\r\nLoss: 216.828827\r\nLoss: 235.451126\r\nLoss: 242.588562\r\nLoss: 243.361099\r\nLoss: 248.088272\r\nLoss: 259.658417\r\nLoss: 307.006592\r\nLoss: 329.257568\r\nLoss: 342.992310\r\nLoss: 345.325562\r\nLoss: 391.979645\r\nLoss: 403.900879\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe two executions of the code (with and without CUDNN) should output approximately the same loss values.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 396.51\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37h14c3975_1    conda-forge\r\n[conda] mkl_random                1.0.2            py37h637b7d7_2    conda-forge\r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",high priority|module: dependency bug|module: cudnn|triaged,Baranowski,"## \U0001f41b Bug\r\n\r\nTogether with @fmassa, we observed a potential instability when I run the following snippet with CUDNN. Without CUDNN, it works as expected.\r\nIt's also worth noticing that this used to work on Pytorch 0.4.1, and that it works as expected on CPU as well.\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nRun the code. It requires a small pickle file containing one single minibatch of data (replacing it by a random tensor did not reproduce the problem). The file is linked here.\r\n[minibatch.pkl.zip](https://github.com/pytorch/pytorch/files/2818110/minibatch.pkl.zip)\r\n\r\n```python\r\nimport torch\r\nfrom torch import optim\r\nimport torch.nn as nn\r\nfrom torch.nn import functional as F\r\nimport pickle as pkl\r\n\r\ntorch.manual_seed(1)\r\n\r\n\r\nclass Decoder(nn.Module):\r\n    """""" VAE decoder """"""\r\n    def __init__(self, img_channels, latent_size):\r\n        super(Decoder, self).__init__()\r\n        self.latent_size = latent_size\r\n        self.img_channels = img_channels\r\n\r\n        self.fc1 = nn.Linear(latent_size, 1024)\r\n        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\r\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\r\n        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\r\n        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 6, stride=2)\r\n\r\n    def forward(self, x): \r\n        x = F.relu(self.fc1(x))\r\n        x = x.unsqueeze(-1).unsqueeze(-1)\r\n        x = F.relu(self.deconv1(x))\r\n        x = F.relu(self.deconv2(x))\r\n        x = F.relu(self.deconv3(x))\r\n        reconstruction = torch.sigmoid(self.deconv4(x))\r\n        return reconstruction\r\n\r\ntorch.backends.cudnn.enabled = False               \r\ncuda = torch.cuda.is_available()\r\ndevice = torch.device(""cuda"" if cuda else ""cpu"")\r\n\r\ndecoder = Decoder(3, 32).to(device)\r\ndecoder.train()\r\noptimizer = optim.Adam(decoder.parameters())\r\n\r\nwith open('minibatch.pkl', 'rb') as f:\r\n    data = pkl.load(f)\r\n\r\nfor batch_idx in range(500):\r\n    data = data.to(device)\r\n    mu = torch.randn((32, 32), device=device)\r\n    recon_batch = decoder(mu)\r\n    loss = .5 * (recon_batch - data).pow(2).sum(dim=(1,2,3)).sum()\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n    \r\n    if batch_idx % 20 == 0:\r\n        print(f'Loss: {loss.item()/len(data):.6f}')\r\n```\r\nIf the line `torch.backends.cudnn.enabled = False` is not commented (and thus if I do not use CUDNN), the output is \r\n```\r\nLoss: 412.374939\r\nLoss: 105.106758\r\nLoss: 60.929550\r\nLoss: 56.290668\r\nLoss: 53.807034\r\nLoss: 51.205917\r\nLoss: 50.837688\r\nLoss: 50.152969\r\nLoss: 49.427326\r\nLoss: 49.931602\r\nLoss: 49.682564\r\nLoss: 49.036819\r\n```\r\n\r\nIf I comment that that line (which means that I am using CUDNN), the output is:\r\n```\r\nLoss: 412.365082\r\nLoss: 159.593903\r\nLoss: 139.744904\r\nLoss: 180.208984\r\nLoss: 216.828827\r\nLoss: 235.451126\r\nLoss: 242.588562\r\nLoss: 243.361099\r\nLoss: 248.088272\r\nLoss: 259.658417\r\nLoss: 307.006592\r\nLoss: 329.257568\r\nLoss: 342.992310\r\nLoss: 345.325562\r\nLoss: 391.979645\r\nLoss: 403.900879\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe two executions of the code (with and without CUDNN) should output approximately the same loss values.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 396.51\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2019.1                      144\r\n[conda] mkl_fft                   1.0.10           py37h14c3975_1    conda-forge\r\n[conda] mkl_random                1.0.2            py37h637b7d7_2    conda-forge\r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport torch\r\nfrom torch import optim\r\nimport torch.nn as nn\r\nfrom torch.nn import functional as F\r\nimport pickle as pkl\r\n\r\ntorch.manual_seed(1)\r\n\r\n\r\nclass Decoder(nn.Module):\r\n    """""" VAE decoder """"""\r\n    def __init__(self, img_channels, latent_size):\r\n        super(Decoder, self).__init__()\r\n        self.latent_size = latent_size\r\n        self.img_channels = img_channels\r\n\r\n        self.fc1 = nn.Linear(latent_size, 1024)\r\n        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\r\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\r\n        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\r\n        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 6, stride=2)\r\n\r\n    def forward(self, x): \r\n        x = F.relu(self.fc1(x))\r\n        x = x.unsqueeze(-1).unsqueeze(-1)\r\n        x = F.relu(self.deconv1(x))\r\n        x = F.relu(self.deconv2(x))\r\n        x = F.relu(self.deconv3(x))\r\n        reconstruction = torch.sigmoid(self.deconv4(x))\r\n        return reconstruction\r\n\r\ntorch.backends.cudnn.enabled = False               \r\ncuda = torch.cuda.is_available()\r\ndevice = torch.device(""cuda"" if cuda else ""cpu"")\r\n\r\ndecoder = Decoder(3, 32).to(device)\r\ndecoder.train()\r\noptimizer = optim.Adam(decoder.parameters())\r\n\r\nwith open('minibatch.pkl', 'rb') as f:\r\n    data = pkl.load(f)\r\n\r\nfor batch_idx in range(500):\r\n    data = data.to(device)\r\n    mu = torch.randn((32, 32), device=device)\r\n    recon_batch = decoder(mu)\r\n    loss = .5 * (recon_batch - data).pow(2).sum(dim=(1,2,3)).sum()\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n    \r\n    if batch_idx % 20 == 0:\r\n        print(f'Loss: {loss.item()/len(data):.6f}')\r\n"
16532,"Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty)## \U0001f41b Bug\r\n\r\nGradient is missing when calling torch.autograd.grad wrapped inside a function on multiple GPU's. (eg computing wgan gradient penalty). Calling torch.autograd.grad inline (not wrapped in a function) on multiple GPU's returns expected behavior.\r\n\r\n\r\n## To Reproduce\r\n\r\nCode below:\r\n\r\n\r\n<br>\r\n\r\nOutput for a single GPU calling `gradient_penalty` function\r\n```\r\nSingle GPU Functional\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\nMultiGPU calling `gradient_penalty` function\r\n```\r\nMulti-GPU Functional\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0., 0., 0., 0.]], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\nMulti-GPU calling `autograd.grad` inline (not inside a function)\r\n```\r\nMulti-GPU Inline\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0'), tensor([0.], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\n## Expected behavior\r\n\r\nThe gradient should be accumulated when calling `autograd.grad` from inside another function. All outputs gradients from the script should be the same.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.9\r\n - OS (e.g., Linux): Ubuntu 16.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.0/6\r\n - GPU models and configuration:\r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\n\r\n",high priority|module: autograd|triaged,ezyang,"## \U0001f41b Bug\r\n\r\nGradient is missing when calling torch.autograd.grad wrapped inside a function on multiple GPU's. (eg computing wgan gradient penalty). Calling torch.autograd.grad inline (not wrapped in a function) on multiple GPU's returns expected behavior.\r\n\r\n\r\n## To Reproduce\r\n\r\nCode below:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ntorch.cuda.manual_seed_all(0)\r\ntorch.manual_seed(0)\r\n\r\n\r\ndef gradient_penalty(netD, x):\r\n    """"""Functional Gradient Calculation""""""\r\n    output = netD(x)\r\n    gradients = torch.autograd.grad(outputs=output, inputs=x,\r\n                                    grad_outputs=x.new_ones(output.size()),\r\n                                    create_graph=True, retain_graph=True)[0].mean()\r\n    return gradients\r\n\r\n\r\nnet = nn.Linear(4, 1).cuda()\r\nmultigpu_net = nn.DataParallel(net, [0, 1])\r\n\r\nx = torch.ones(2, 4, requires_grad=True).cuda()\r\n\r\nprint(""Single GPU Functional"")\r\nnet.zero_grad()\r\nloss = gradient_penalty(net, x)\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n\r\n\r\nprint(""\\nMulti-GPU Functional"")\r\nmultigpu_net.zero_grad()\r\nloss = gradient_penalty(multigpu_net, x)\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n\r\nprint(""\\nMulti-GPU Inline"")\r\nmultigpu_net.zero_grad()\r\noutput = multigpu_net(x)\r\n\r\n# Compute grad inline\r\nloss = torch.autograd.grad(outputs=output, inputs=x,\r\n                            grad_outputs=x.new_ones(output.size()),\r\n                            create_graph=True, retain_graph=True)[0].mean()\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n```\r\n\r\n<br>\r\n\r\nOutput for a single GPU calling `gradient_penalty` function\r\n```\r\nSingle GPU Functional\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\nMultiGPU calling `gradient_penalty` function\r\n```\r\nMulti-GPU Functional\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0., 0., 0., 0.]], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\nMulti-GPU calling `autograd.grad` inline (not inside a function)\r\n```\r\nMulti-GPU Inline\r\nLoss: -0.1287534236907959\r\nGrad: [tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0'), tensor([0.], device='cuda:0')]\r\n```\r\n\r\n<br>\r\n\r\n## Expected behavior\r\n\r\nThe gradient should be accumulated when calling `autograd.grad` from inside another function. All outputs gradients from the script should be the same.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.9\r\n - OS (e.g., Linux): Ubuntu 16.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.0/6\r\n - GPU models and configuration:\r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\n\r\n","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ntorch.cuda.manual_seed_all(0)\r\ntorch.manual_seed(0)\r\n\r\n\r\ndef gradient_penalty(netD, x):\r\n    """"""Functional Gradient Calculation""""""\r\n    output = netD(x)\r\n    gradients = torch.autograd.grad(outputs=output, inputs=x,\r\n                                    grad_outputs=x.new_ones(output.size()),\r\n                                    create_graph=True, retain_graph=True)[0].mean()\r\n    return gradients\r\n\r\n\r\nnet = nn.Linear(4, 1).cuda()\r\nmultigpu_net = nn.DataParallel(net, [0, 1])\r\n\r\nx = torch.ones(2, 4, requires_grad=True).cuda()\r\n\r\nprint(""Single GPU Functional"")\r\nnet.zero_grad()\r\nloss = gradient_penalty(net, x)\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n\r\n\r\nprint(""\\nMulti-GPU Functional"")\r\nmultigpu_net.zero_grad()\r\nloss = gradient_penalty(multigpu_net, x)\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n\r\nprint(""\\nMulti-GPU Inline"")\r\nmultigpu_net.zero_grad()\r\noutput = multigpu_net(x)\r\n\r\n# Compute grad inline\r\nloss = torch.autograd.grad(outputs=output, inputs=x,\r\n                            grad_outputs=x.new_ones(output.size()),\r\n                            create_graph=True, retain_graph=True)[0].mean()\r\nloss.backward()\r\nprint(""Loss:"", loss.item())\r\nprint(""Grad:"", [p.grad for p in net.parameters() if p.grad is not None])\r\n"
16492,"torch.jit.script is not getting type hint correctI don't think users would do odd things like below, but it would be nice to fix:\r\n\r\n\r\nprints\r\n\r\n```\r\ntyping.List[torch.Tensor]\r\n{'x': typing.List[torch.Tensor]}\r\ngraph(%x : (Tensor)) {\r\n  return (%x);\r\n}\r\n```\r\nbut it should be\r\n```\r\ntyping.List[torch.Tensor]\r\n{'x': typing.List[torch.Tensor]}\r\ngraph(%x : Tensor[]) {\r\n  return (%x);\r\n}\r\n```",oncall: jit|low priority,driazati,"I don't think users would do odd things like below, but it would be nice to fix:\r\n```python\r\nimport torch\r\nimport typing\r\nfrom typing import List\r\n\r\nTuple = {torch.Tensor : List[torch.Tensor]}\r\nprint(Tuple[torch.Tensor])\r\n\r\ndef f(x: Tuple[torch.Tensor]):\r\n    return x\r\n\r\n@torch.jit.script\r\ndef g(x: Tuple[torch.Tensor]):\r\n    return x\r\n\r\nprint(typing.get_type_hints(f))\r\nprint(g.graph)\r\n```\r\n\r\nprints\r\n\r\n```\r\ntyping.List[torch.Tensor]\r\n{'x': typing.List[torch.Tensor]}\r\ngraph(%x : (Tensor)) {\r\n  return (%x);\r\n}\r\n```\r\nbut it should be\r\n```\r\ntyping.List[torch.Tensor]\r\n{'x': typing.List[torch.Tensor]}\r\ngraph(%x : Tensor[]) {\r\n  return (%x);\r\n}\r\n```",python\r\nimport torch\r\nimport typing\r\nfrom typing import List\r\n\r\nTuple = {torch.Tensor : List[torch.Tensor]}\r\nprint(Tuple[torch.Tensor])\r\n\r\ndef f(x: Tuple[torch.Tensor]):\r\n    return x\r\n\r\n@torch.jit.script\r\ndef g(x: Tuple[torch.Tensor]):\r\n    return x\r\n\r\nprint(typing.get_type_hints(f))\r\nprint(g.graph)\r\n
15923,"Cannot use LBFGS in C++ (clang, torch 1.0.0)## \U0001f41b Bug\r\n\r\nI want to use the LBFGS optimizer in C++. My code is working fine in python and with SGD, but I get ``terminating with uncaught exception of type c10::Error: a leaf Variable that requires grad has been used in an in-place operation. (check_inplace at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/libtorch_2.7/pytorch/torch/csrc/autograd/VariableTypeUtils.h:49)`` in C++\r\n\r\n## To Reproduce\r\n\r\n### MWE:\r\n\r\nI compiled with:\r\n```\r\nclang++  -std=c++11 -I libtorch/include/torch/csrc/api/include -I libtorch/include  -Llibtorch/lib   -ltorch -lc10   -lcaffe2    ./test_pytorch.cc\r\n```\r\nEverything compiles fine.\r\n\r\n### Result:\r\n```\r\nlibc++abi.dylib: terminating with uncaught exception of type c10::Error: a leaf Variable that requires grad has been used in an in-place operation. (check_inplace at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/libtorch_2.7/pytorch/torch/csrc/autograd/VariableTypeUtils.h:49)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x110091f37 in libc10.dylib)\r\nframe #1: torch::autograd::check_inplace(at::Tensor const&) + 231 (0x10ed97137 in libtorch.1.dylib)\r\nframe #2: torch::autograd::VariableType::add_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 109 (0x10ef3a7cd in libtorch.1.dylib)\r\nframe #3: torch::optim::LBFGS::add_grad(at::Tensor const&, at::Tensor const&) + 389 (0x10f596b75 in libtorch.1.dylib)\r\nframe #4: torch::optim::LBFGS::step(std::__1::function<at::Tensor ()>) + 6214 (0x10f598566 in libtorch.1.dylib)\r\nframe #5: main + 1198 (0x10ec74c4e in a.out)\r\nframe #6: start + 1 (0x7fff7265ded9 in libdyld.dylib)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe cost should converge to some small value. It works with SGD and in python:\r\n\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.2\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] gpytorch                  0.1.0rc1                  <pip>\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl_fft                   1.0.6            py37hb8a8100_0\r\n[conda] mkl_random                1.0.1            py37h5d10147_1\r\n[conda] pytorch                   0.4.1           py37_cuda0.0_cudnn0.0_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20180929         py3.7_0    pytorch\r\n[conda] pytorch-utils             0.4                       <pip>\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n## Additional context\r\n```\r\notool -L ./a.out\r\n./a.out:\r\n\t@rpath/libtorch.1.dylib (compatibility version 1.0.0, current version 1.0.0)\r\n\t@rpath/libc10.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libcaffe2.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.4)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.200.5)\r\n```\r\n",module: cpp,yf225,"## \U0001f41b Bug\r\n\r\nI want to use the LBFGS optimizer in C++. My code is working fine in python and with SGD, but I get ``terminating with uncaught exception of type c10::Error: a leaf Variable that requires grad has been used in an in-place operation. (check_inplace at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/libtorch_2.7/pytorch/torch/csrc/autograd/VariableTypeUtils.h:49)`` in C++\r\n\r\n## To Reproduce\r\n\r\n### MWE:\r\n```c++\r\n#include <iostream>\r\n#include <torch/torch.h>\r\n\r\nint main()\r\n{\r\n    int n_actions = 10;\r\n    auto target = torch::randn({n_actions, 1});\r\n\r\n    std::vector<torch::Tensor> q_dot = {torch::randn({n_actions, 1}, torch::requires_grad())};\r\n    //torch::optim::SGD optimizer(q_dot, /*lr=*/0.01);\r\n    torch::optim::LBFGS optimizer(q_dot, 1);\r\n\r\n    auto cost = [&](){\r\n        optimizer.zero_grad();\r\n        auto d = torch::pow(q_dot[0] - target, 2).sum();\r\n        d.backward();\r\n        return d;\r\n    };\r\n\r\n    for (int i = 0; i < 50; ++i){\r\n        optimizer.step(cost);//for LBFGS\r\n        //cost(); optimizer.step(); // for SGD\r\n    }\r\n    std::cout << cost() << std::endl;\r\n}\r\n```\r\nI compiled with:\r\n```\r\nclang++  -std=c++11 -I libtorch/include/torch/csrc/api/include -I libtorch/include  -Llibtorch/lib   -ltorch -lc10   -lcaffe2    ./test_pytorch.cc\r\n```\r\nEverything compiles fine.\r\n\r\n### Result:\r\n```\r\nlibc++abi.dylib: terminating with uncaught exception of type c10::Error: a leaf Variable that requires grad has been used in an in-place operation. (check_inplace at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/libtorch_2.7/pytorch/torch/csrc/autograd/VariableTypeUtils.h:49)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x110091f37 in libc10.dylib)\r\nframe #1: torch::autograd::check_inplace(at::Tensor const&) + 231 (0x10ed97137 in libtorch.1.dylib)\r\nframe #2: torch::autograd::VariableType::add_(at::Tensor&, at::Tensor const&, c10::Scalar) const + 109 (0x10ef3a7cd in libtorch.1.dylib)\r\nframe #3: torch::optim::LBFGS::add_grad(at::Tensor const&, at::Tensor const&) + 389 (0x10f596b75 in libtorch.1.dylib)\r\nframe #4: torch::optim::LBFGS::step(std::__1::function<at::Tensor ()>) + 6214 (0x10f598566 in libtorch.1.dylib)\r\nframe #5: main + 1198 (0x10ec74c4e in a.out)\r\nframe #6: start + 1 (0x7fff7265ded9 in libdyld.dylib)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe cost should converge to some small value. It works with SGD and in python:\r\n```python\r\nimport time\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\n\r\n\r\nn_actions = 22\r\ndt = 0.01\r\n\r\njac_t = torch.randn(6, n_actions)\r\nstate = torch.randn(6, 1)\r\ntarget = torch.matmul(jac_t, torch.randn(n_actions, 1)) * dt + state\r\n\r\nprint(""target:"", target)\r\n\r\nt0 = time.perf_counter()\r\nq_dot = Variable(torch.randn(n_actions, 1), requires_grad=True)\r\nv = [q_dot]\r\noptimizer = torch.optim.LBFGS(v)#, lr=0.1)\r\nfor i in range(0, 10):\r\n    def cost():\r\n        optimizer.zero_grad()\r\n        next_state = torch.matmul(jac_t, q_dot) * dt + state\r\n        d = torch.pow(next_state - target, 2).sum()\r\n        d.backward()\r\n        return d\r\n    optimizer.step(cost)\r\n    d = cost()\r\n    if d < 1e-3:\r\n        break\r\nprint(time.perf_counter() - t0, "" s"")\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.2\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] gpytorch                  0.1.0rc1                  <pip>\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl_fft                   1.0.6            py37hb8a8100_0\r\n[conda] mkl_random                1.0.1            py37h5d10147_1\r\n[conda] pytorch                   0.4.1           py37_cuda0.0_cudnn0.0_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20180929         py3.7_0    pytorch\r\n[conda] pytorch-utils             0.4                       <pip>\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n## Additional context\r\n```\r\notool -L ./a.out\r\n./a.out:\r\n\t@rpath/libtorch.1.dylib (compatibility version 1.0.0, current version 1.0.0)\r\n\t@rpath/libc10.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libcaffe2.dylib (compatibility version 0.0.0, current version 0.0.0)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 400.9.4)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1252.200.5)\r\n```\r\n","c++\r\n#include <iostream>\r\n#include <torch/torch.h>\r\n\r\nint main()\r\n{\r\n    int n_actions = 10;\r\n    auto target = torch::randn({n_actions, 1});\r\n\r\n    std::vector<torch::Tensor> q_dot = {torch::randn({n_actions, 1}, torch::requires_grad())};\r\n    //torch::optim::SGD optimizer(q_dot, /*lr=*/0.01);\r\n    torch::optim::LBFGS optimizer(q_dot, 1);\r\n\r\n    auto cost = [&](){\r\n        optimizer.zero_grad();\r\n        auto d = torch::pow(q_dot[0] - target, 2).sum();\r\n        d.backward();\r\n        return d;\r\n    };\r\n\r\n    for (int i = 0; i < 50; ++i){\r\n        optimizer.step(cost);//for LBFGS\r\n        //cost(); optimizer.step(); // for SGD\r\n    }\r\n    std::cout << cost() << std::endl;\r\n}\r\n"
15867,"RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'## \U0001f41b Bug\r\n\r\nWhen randomly initializing a CUDA tensor with a manually-seeded generator using `Tensor.uniform_()`, the function throws the following exception:\r\n```\r\nRuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'\r\n```\r\n\r\nThis does not happen when the device is CPU; it works as expected then.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet below\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nThe function should not fail. The CUDA tensor `x` should be initialized using the manually-seeded generator.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: Tesla K40c\r\nNvidia driver version: 384.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nI installed PyTorch using pyenv and Pipenv.\r\n\r\n## Additional context\r\n\r\nThe `generator` argument to `Tensor.uniform_()` does not seem to be documented, despite being extremely useful.\r\n",high priority|module: cuda|triaged,fmassa,"## \U0001f41b Bug\r\n\r\nWhen randomly initializing a CUDA tensor with a manually-seeded generator using `Tensor.uniform_()`, the function throws the following exception:\r\n```\r\nRuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'\r\n```\r\n\r\nThis does not happen when the device is CPU; it works as expected then.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the code snippet below\r\n\r\n```python\r\nimport torch\r\ndevice = torch.device('cuda')\r\ngenerator = torch.manual_seed(123)\r\nx = torch.zeros(10, device=device)\r\nx.uniform_(-1.0, 1.0, generator=generator)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe function should not fail. The CUDA tensor `x` should be initialized using the manually-seeded generator.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: Tesla K40c\r\nNvidia driver version: 384.66\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nI installed PyTorch using pyenv and Pipenv.\r\n\r\n## Additional context\r\n\r\nThe `generator` argument to `Tensor.uniform_()` does not seem to be documented, despite being extremely useful.\r\n","python\r\nimport torch\r\ndevice = torch.device('cuda')\r\ngenerator = torch.manual_seed(123)\r\nx = torch.zeros(10, device=device)\r\nx.uniform_(-1.0, 1.0, generator=generator)\r\n"
15792,libtorch memory leak with torch::load optimizer## \U0001f41b Bug\r\n\r\ntorch::load optimizer does memory leak in CPU and GPU.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n1. run this snippet with conda pytorch 1.0.0 or latest libtorch zip\r\n1. you will see OOM or memory increasing in top\r\n1. I also tested model loading but it did not leak the memory\r\n\r\n## Expected behavior\r\n\r\ntorch::load should not leak it. maybe it should release previous optimizer states.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nI used libtorch with build-version 1.0.0 as above.\r\nOr conda's pytorch as follows\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (1.0.0)\r\n[pip] torch-autograd-solver (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[pip] warpctc-pytorch (0.1.1)\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nnone,high priority,goldsborough,"## \U0001f41b Bug\r\n\r\ntorch::load optimizer does memory leak in CPU and GPU.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```c++\r\n#include <torch/torch.h>\r\n\r\n// you can play with CUDA memory using .to(torch::Device(torch::kCUDA))\r\nint main() {\r\n    auto model = torch::nn::Linear(4096, 4096);\r\n    auto optimizer = torch::optim::Adam(model->parameters(), 0.01);\r\n\r\n    auto loss = model->forward(torch::zeros({1, 4096})).sum();\r\n    loss.backward();\r\n    optimizer.step();\r\n    optimizer.zero_grad();\r\n    torch::save(optimizer, ""/tmp/optimizer.pt"");\r\n    while (true) {\r\n        torch::load(optimizer, ""/tmp/optimizer.pt"");\r\n    }\r\n}\r\n```\r\n\r\n1. run this snippet with conda pytorch 1.0.0 or latest libtorch zip\r\n1. you will see OOM or memory increasing in top\r\n1. I also tested model loading but it did not leak the memory\r\n\r\n## Expected behavior\r\n\r\ntorch::load should not leak it. maybe it should release previous optimizer states.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nI used libtorch with build-version 1.0.0 as above.\r\nOr conda's pytorch as follows\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (1.0.0)\r\n[pip] torch-autograd-solver (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[pip] warpctc-pytorch (0.1.1)\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nnone","c++\r\n#include <torch/torch.h>\r\n\r\n// you can play with CUDA memory using .to(torch::Device(torch::kCUDA))\r\nint main() {\r\n    auto model = torch::nn::Linear(4096, 4096);\r\n    auto optimizer = torch::optim::Adam(model->parameters(), 0.01);\r\n\r\n    auto loss = model->forward(torch::zeros({1, 4096})).sum();\r\n    loss.backward();\r\n    optimizer.step();\r\n    optimizer.zero_grad();\r\n    torch::save(optimizer, ""/tmp/optimizer.pt"");\r\n    while (true) {\r\n        torch::load(optimizer, ""/tmp/optimizer.pt"");\r\n    }\r\n}\r\n"
15754,"[JIT] @script tensor.cpu() raises unknown builtin op error## \U0001f41b Bug\r\n\r\n`tensor.cpu()`, `tensor.cuda()` are not available in jit script methods despite `tensor.to(device=...)` is ok.\r\n\r\n```\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 4, in <module>\r\n    @torch.jit.script\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 691, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(fn))\r\nRuntimeError:\r\nunknown builtin op:\r\n@torch.jit.script\r\ndef test(tensor):\r\n    return tensor.cuda()\r\n           ~~~~~~~~~~~ <--- HERE\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nShould work same as `tensor.to(device='cpu')`\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n/usr/local/cuda-10.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: jit,eellison,"## \U0001f41b Bug\r\n\r\n`tensor.cpu()`, `tensor.cuda()` are not available in jit script methods despite `tensor.to(device=...)` is ok.\r\n\r\n```\r\n$ python test.py\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 4, in <module>\r\n    @torch.jit.script\r\n  File ""/home/qbx2/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py"", line 691, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(fn))\r\nRuntimeError:\r\nunknown builtin op:\r\n@torch.jit.script\r\ndef test(tensor):\r\n    return tensor.cuda()\r\n           ~~~~~~~~~~~ <--- HERE\r\n\r\n```\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\n\r\n@torch.jit.script\r\ndef test(tensor):\r\n    return tensor.cpu()\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould work same as `tensor.to(device='cpu')`\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n/usr/local/cuda-10.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",python\r\nimport torch\r\n\r\n\r\n@torch.jit.script\r\ndef test(tensor):\r\n    return tensor.cpu()\r\n
15671,"torch.einsum equation works in NumPy but not in Pytorch## Issue description\r\n\r\nI tried doing batchwise dot product across channels or rather pairwise similarity between all pairs of features for two sets of feature matrices using `torch.einsum`, but it fails. Could be a bug or is this to be expected?\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nIn NumPy i get\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 950M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.15.4)\r\n[pip3] torch (1.0.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect\r\n\n\ncc @mruberry @rgommers @vincentqb @vishwakftw @jianyuh @nikitaved @pearu",todo|triaged|enhancement|module: numpy|module: linear algebra,heitorschueroff,"## Issue description\r\n\r\nI tried doing batchwise dot product across channels or rather pairwise similarity between all pairs of features for two sets of feature matrices using `torch.einsum`, but it fails. Could be a bug or is this to be expected?\r\n\r\n## To Reproduce\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.randn(1, 3, 24, 20)\r\n>>> b = torch.randn(5, 3, 24, 20)\r\n>>> torch.einsum(""bijk, bilm -> bjklm"", a, b).shape\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/functional.py"", line 245, in einsum\r\n    return torch._C._VariableFunctions.einsum(equation, operands)\r\nRuntimeError: size of dimension does not match previous size, operand 1, dim 0\r\n```\r\n\r\n## Expected behavior\r\n\r\nIn NumPy i get\r\n```python\r\n>>> import numpy as np\r\n>>> a = np.random.rand(1, 3, 24, 20)\r\n>>> b = np.random.rand(5, 3, 24, 20)\r\n>>> np.einsum(""bijk, bilm -> bjklm"", a, b).shape\r\n(5, 24, 20, 24, 20)\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 950M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.15.4)\r\n[pip3] torch (1.0.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect\r\n\n\ncc @mruberry @rgommers @vincentqb @vishwakftw @jianyuh @nikitaved @pearu","python\r\n>>> import torch\r\n>>> a = torch.randn(1, 3, 24, 20)\r\n>>> b = torch.randn(5, 3, 24, 20)\r\n>>> torch.einsum(""bijk, bilm -> bjklm"", a, b).shape\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/functional.py"", line 245, in einsum\r\n    return torch._C._VariableFunctions.einsum(equation, operands)\r\nRuntimeError: size of dimension does not match previous size, operand 1, dim 0\r\n"
15638,"init_process_group() sometimes hangs (not stable) with pytorch 1.0.0## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ninit_process_group() hangs and it never returns even after some other workers can return. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nwith python 3.6.7 + pytorch 1.0.0, init_process_group() sometimes hangs and never returns.  \r\n\r\n### Code scripts\r\na.py\r\n\r\n\r\n### When it works\r\npython 2.7.12 + pytorch 0.4.1\r\n\r\nIf i run the scripts multiple times, it always succeeds.\r\n### When it does not work\r\n\r\nThe rank 0 is able to finish the function call of `init_process_group`, but the rank 1 never returns. Then, I use the gdb to attach the hanged process.\r\n```\r\n$ sudo gdb -p 40855\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type ""show copying""\r\nand ""show warranty"" for details.\r\nThis GDB was configured as ""x86_64-linux-gnu"".\r\nType ""show configuration"" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type ""help"".\r\nType ""apropos word"" to search for commands related to ""word"".\r\nAttaching to process 40855\r\nReading symbols from /raid/jianfw/anaconda3/bin/python3.6...done.\r\nReading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug/.build-id/ce/17e023542265fc11d9bc8f534bb4f070493d30.debug...done.\r\ndone.\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".\r\nReading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.23.so...done.\r\ndone.\r\nReading symbols from /lib/x86_64-linux-gnu/libdl.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libdl-2.23.so...done.\r\ndone.\r\n(gdb) where\r\n#0  0x00007f0ce5586c00 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:84\r\n#1  0x00007f0cde561e88 in c10d::tcputil::connect(std::string const&, unsigned short, bool, std::chrono::duration<long, std::ratio<1l, 1000l> > const&) ()\r\n   from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#2  0x00007f0cde55cef5 in c10d::TCPStore::TCPStore(std::string const&, unsigned short, bool) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#3  0x00007f0cde4f09f6 in void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::string const&, int, bool>::execute<pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >, , 0>(pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >&)::{lambda(pybind11::detail::value_and_holder&, std::string const&, int, bool)#1}, void, pybind11::detail::value_and_holder&, std::string const&, int, bool, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>(void pybind11::detail::initimpl::constructor<std::string const&, int, bool>::execute<pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >, , 0>(pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >&)::{lambda(pybind11::detail::value_and_holder&, std::string const&, int, bool)#1}&&, void (*)(pybind11::detail::value_and_holder&, std::string const&, int, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#4  0x00007f0cddff0e36 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#5  0x000055b391cbe3d4 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/methodobject.c:231\r\n#6  0x000055b391cbe7ef in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2313\r\n#7  0x000055b391cc3303 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2373\r\n#8  0x000055b391cbe1de in PyObject_Call () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2261\r\n#9  0x000055b391d1b78b in slot_tp_init () at /tmp/build/80754af9/python_1540319457073/work/Objects/typeobject.c:6420\r\n#10 0x000055b391d47f57 in type_call () at /tmp/build/80754af9/python_1540319457073/work/Objects/typeobject.c:915\r\n#11 0x000055b391cbe5bb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2331\r\n#12 0x000055b391d47d6e in call_function () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4861\r\n#13 0x000055b391d6a71a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3335\r\n#14 0x000055b391d4a860 in gen_send_ex (closing=0, exc=0, arg=0x0, gen=0x7f0ca53fed58) at /tmp/build/80754af9/python_1540319457073/work/Objects/genobject.c:189\r\n#15 gen_iternext (gen=0x7f0ca53fed58) at /tmp/build/80754af9/python_1540319457073/work/Objects/genobject.c:563\r\n#16 builtin_next () at /tmp/build/80754af9/python_1540319457073/work/Python/bltinmodule.c:1330\r\n#17 0x000055b391cbe311 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/methodobject.c:234\r\n#18 0x000055b391d47c1c in call_function () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4837\r\n#19 0x000055b391d6a71a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3335\r\n#20 0x000055b391d42ad9 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=2, defs=0x7f0ca578dd60, kwstep=2, kwcount=<optimized out>,\r\n    kwargs=0x7f0ce447eae8, kwnames=0x7f0ce447eae0, argcount=<optimized out>, args=0x55b393457df0, locals=0x0, globals=<optimized out>, _co=0x7f0ca54e8270)\r\n    at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4166\r\n#21 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4187\r\n#22 0x000055b391d43a06 in function_call () at /tmp/build/80754af9/python_1540319457073/work/Objects/funcobject.c:604\r\n#23 0x000055b391cbe1de in PyObject_Call () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2261\r\n#24 0x000055b391d6bd9a in do_call_core (kwdict=0x7f0ce58c0678, callargs=0x7f0ce594b048, func=0x7f0ca54f0a60) at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:5106\r\n#25 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3404\r\n#26 0x000055b391d42ad9 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=<optimized out>, kwargs=0x0, kwnames=0x0,\r\n    argcount=0, args=0x0, locals=0x7f0ce5901360, globals=0x7f0ce5901360, _co=0x7f0ce448e5d0) at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4166\r\n#27 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4187\r\n#28 0x000055b391d4387c in PyEval_EvalCode (co=co@entry=0x7f0ce448e5d0, globals=globals@entry=0x7f0ce5901360, locals=locals@entry=0x7f0ce5901360)\r\n    at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:731\r\n#29 0x000055b391dc4074 in run_mod () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:1025\r\n#30 0x000055b391dc4471 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:978\r\n#31 0x000055b391dc4673 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:419\r\n#32 0x000055b391dc477d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:81\r\n#33 0x000055b391dc81b0 in run_file (p_cf=0x7ffc8471a94c, filename=0x55b3933a6300 L""a.py"", fp=0x55b393433e20) at /tmp/build/80754af9/python_1540319457073/work/Modules/main.c:340\r\n#34 Py_Main () at /tmp/build/80754af9/python_1540319457073/work/Modules/main.c:811\r\n#35 0x000055b391c8fb4e in main () at /tmp/build/80754af9/python_1540319457073/work/Programs/python.c:69\r\n#36 0x00007f0ce51cc830 in __libc_start_main (main=0x55b391c8fa60 <main>, argc=4, argv=0x7ffc8471ab58, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>,\r\n    stack_end=0x7ffc8471ab48) at ../csu/libc-start.c:291\r\n#37 0x000055b391d711a8 in _start () at ../sysdeps/x86_64/elf/start.S:103\r\n```\r\nLooks like it hangs at the function call of `c10d::TCPStore::TCPStore(std::string const&, unsigned short, bool) ()`, `c10d::tcputil::connect(std::string const&, unsigned short, bool, std::chrono::duration<long, std::ratio<1l, 1000l> > const&) ()`\r\n\r\n\r\n## Expected behavior\r\nIt should return rather than stuck. \r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-SXM2-16GB\r\nGPU 1: Tesla P100-SXM2-16GB\r\nGPU 2: Tesla P100-SXM2-16GB\r\nGPU 3: Tesla P100-SXM2-16GB\r\nGPU 4: Tesla P100-SXM2-16GB\r\nGPU 5: Tesla P100-SXM2-16GB\r\nGPU 6: Tesla P100-SXM2-16GB\r\nGPU 7: Tesla P100-SXM2-16GB\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py36h90e4bf4_5\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] pytorch                   1.0.0           py3.6_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n",oncall: distributed,teng-li,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\ninit_process_group() hangs and it never returns even after some other workers can return. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nwith python 3.6.7 + pytorch 1.0.0, init_process_group() sometimes hangs and never returns.  \r\n\r\n### Code scripts\r\na.py\r\n```python\r\nimport torch\r\nimport torch.distributed as dist\r\nimport os\r\n\r\ndef get_mpi_rank():\r\n    return int(os.environ['RANK'])\r\n\r\ndef get_mpi_size():\r\n    return int(os.environ.get('WORLD_SIZE', '1'))\r\n\r\nrank = get_mpi_rank()\r\nworld_size = get_mpi_size()\r\n\r\ninit_param={'backend': 'nccl',\r\n        'init_method': 'env://',\r\n        'rank': rank,\r\n        'world_size': world_size}\r\n\r\nfrom pprint import pformat\r\nprint('before {} - {}\\n'.format(rank,\r\n    pformat(init_param)))\r\ndist.init_process_group(**init_param)\r\nprint('after {}'.format(rank))\r\n```\r\n\r\n### When it works\r\npython 2.7.12 + pytorch 0.4.1\r\n```bash\r\n$ python --version\r\nPython 2.7.12\r\n$ python -c 'import torch; print torch.__version__'\r\n0.4.1\r\n$ python -m torch.distributed.launch --nproc_per_node 2 a.py\r\nbefore 1 - {'backend': 'nccl', 'init_method': 'env://', 'rank': 1, 'world_size': 2}\r\n\r\nbefore 0 - {'backend': 'nccl', 'init_method': 'env://', 'rank': 0, 'world_size': 2}\r\n\r\nafter 0\r\nafter 1\r\n```\r\nIf i run the scripts multiple times, it always succeeds.\r\n### When it does not work\r\n```bash\r\n$ python --version\r\nPython 3.6.7 :: Anaconda, Inc.\r\n$ python -c 'import torch; print(torch.__version__)'\r\n1.0.0\r\n$ python -m torch.distributed.launch --nproc_per_node 2 a.py\r\nbefore 1 - {'backend': 'nccl', 'init_method': 'env://', 'rank': 1, 'world_size': 2}\r\n\r\nbefore 0 - {'backend': 'nccl', 'init_method': 'env://', 'rank': 0, 'world_size': 2}\r\n\r\nafter 0\r\n```\r\nThe rank 0 is able to finish the function call of `init_process_group`, but the rank 1 never returns. Then, I use the gdb to attach the hanged process.\r\n```\r\n$ sudo gdb -p 40855\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type ""show copying""\r\nand ""show warranty"" for details.\r\nThis GDB was configured as ""x86_64-linux-gnu"".\r\nType ""show configuration"" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type ""help"".\r\nType ""apropos word"" to search for commands related to ""word"".\r\nAttaching to process 40855\r\nReading symbols from /raid/jianfw/anaconda3/bin/python3.6...done.\r\nReading symbols from /lib/x86_64-linux-gnu/libpthread.so.0...Reading symbols from /usr/lib/debug/.build-id/ce/17e023542265fc11d9bc8f534bb4f070493d30.debug...done.\r\ndone.\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".\r\nReading symbols from /lib/x86_64-linux-gnu/libc.so.6...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libc-2.23.so...done.\r\ndone.\r\nReading symbols from /lib/x86_64-linux-gnu/libdl.so.2...Reading symbols from /usr/lib/debug//lib/x86_64-linux-gnu/libdl-2.23.so...done.\r\ndone.\r\n(gdb) where\r\n#0  0x00007f0ce5586c00 in __nanosleep_nocancel () at ../sysdeps/unix/syscall-template.S:84\r\n#1  0x00007f0cde561e88 in c10d::tcputil::connect(std::string const&, unsigned short, bool, std::chrono::duration<long, std::ratio<1l, 1000l> > const&) ()\r\n   from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#2  0x00007f0cde55cef5 in c10d::TCPStore::TCPStore(std::string const&, unsigned short, bool) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#3  0x00007f0cde4f09f6 in void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::string const&, int, bool>::execute<pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >, , 0>(pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >&)::{lambda(pybind11::detail::value_and_holder&, std::string const&, int, bool)#1}, void, pybind11::detail::value_and_holder&, std::string const&, int, bool, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>(void pybind11::detail::initimpl::constructor<std::string const&, int, bool>::execute<pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >, , 0>(pybind11::class_<c10d::TCPStore, std::shared_ptr<c10d::TCPStore> >&)::{lambda(pybind11::detail::value_and_holder&, std::string const&, int, bool)#1}&&, void (*)(pybind11::detail::value_and_holder&, std::string const&, int, bool), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#4  0x00007f0cddff0e36 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /raid/jianfw/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so\r\n#5  0x000055b391cbe3d4 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/methodobject.c:231\r\n#6  0x000055b391cbe7ef in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2313\r\n#7  0x000055b391cc3303 in _PyObject_Call_Prepend () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2373\r\n#8  0x000055b391cbe1de in PyObject_Call () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2261\r\n#9  0x000055b391d1b78b in slot_tp_init () at /tmp/build/80754af9/python_1540319457073/work/Objects/typeobject.c:6420\r\n#10 0x000055b391d47f57 in type_call () at /tmp/build/80754af9/python_1540319457073/work/Objects/typeobject.c:915\r\n#11 0x000055b391cbe5bb in _PyObject_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2331\r\n#12 0x000055b391d47d6e in call_function () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4861\r\n#13 0x000055b391d6a71a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3335\r\n#14 0x000055b391d4a860 in gen_send_ex (closing=0, exc=0, arg=0x0, gen=0x7f0ca53fed58) at /tmp/build/80754af9/python_1540319457073/work/Objects/genobject.c:189\r\n#15 gen_iternext (gen=0x7f0ca53fed58) at /tmp/build/80754af9/python_1540319457073/work/Objects/genobject.c:563\r\n#16 builtin_next () at /tmp/build/80754af9/python_1540319457073/work/Python/bltinmodule.c:1330\r\n#17 0x000055b391cbe311 in _PyCFunction_FastCallDict () at /tmp/build/80754af9/python_1540319457073/work/Objects/methodobject.c:234\r\n#18 0x000055b391d47c1c in call_function () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4837\r\n#19 0x000055b391d6a71a in _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3335\r\n#20 0x000055b391d42ad9 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=2, defs=0x7f0ca578dd60, kwstep=2, kwcount=<optimized out>,\r\n    kwargs=0x7f0ce447eae8, kwnames=0x7f0ce447eae0, argcount=<optimized out>, args=0x55b393457df0, locals=0x0, globals=<optimized out>, _co=0x7f0ca54e8270)\r\n    at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4166\r\n#21 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4187\r\n#22 0x000055b391d43a06 in function_call () at /tmp/build/80754af9/python_1540319457073/work/Objects/funcobject.c:604\r\n#23 0x000055b391cbe1de in PyObject_Call () at /tmp/build/80754af9/python_1540319457073/work/Objects/abstract.c:2261\r\n#24 0x000055b391d6bd9a in do_call_core (kwdict=0x7f0ce58c0678, callargs=0x7f0ce594b048, func=0x7f0ca54f0a60) at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:5106\r\n#25 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:3404\r\n#26 0x000055b391d42ad9 in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=<optimized out>, kwargs=0x0, kwnames=0x0,\r\n    argcount=0, args=0x0, locals=0x7f0ce5901360, globals=0x7f0ce5901360, _co=0x7f0ce448e5d0) at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4166\r\n#27 PyEval_EvalCodeEx () at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:4187\r\n#28 0x000055b391d4387c in PyEval_EvalCode (co=co@entry=0x7f0ce448e5d0, globals=globals@entry=0x7f0ce5901360, locals=locals@entry=0x7f0ce5901360)\r\n    at /tmp/build/80754af9/python_1540319457073/work/Python/ceval.c:731\r\n#29 0x000055b391dc4074 in run_mod () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:1025\r\n#30 0x000055b391dc4471 in PyRun_FileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:978\r\n#31 0x000055b391dc4673 in PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:419\r\n#32 0x000055b391dc477d in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python_1540319457073/work/Python/pythonrun.c:81\r\n#33 0x000055b391dc81b0 in run_file (p_cf=0x7ffc8471a94c, filename=0x55b3933a6300 L""a.py"", fp=0x55b393433e20) at /tmp/build/80754af9/python_1540319457073/work/Modules/main.c:340\r\n#34 Py_Main () at /tmp/build/80754af9/python_1540319457073/work/Modules/main.c:811\r\n#35 0x000055b391c8fb4e in main () at /tmp/build/80754af9/python_1540319457073/work/Programs/python.c:69\r\n#36 0x00007f0ce51cc830 in __libc_start_main (main=0x55b391c8fa60 <main>, argc=4, argv=0x7ffc8471ab58, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>,\r\n    stack_end=0x7ffc8471ab48) at ../csu/libc-start.c:291\r\n#37 0x000055b391d711a8 in _start () at ../sysdeps/x86_64/elf/start.S:103\r\n```\r\nLooks like it hangs at the function call of `c10d::TCPStore::TCPStore(std::string const&, unsigned short, bool) ()`, `c10d::tcputil::connect(std::string const&, unsigned short, bool, std::chrono::duration<long, std::ratio<1l, 1000l> > const&) ()`\r\n\r\n\r\n## Expected behavior\r\nIt should return rather than stuck. \r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-SXM2-16GB\r\nGPU 1: Tesla P100-SXM2-16GB\r\nGPU 2: Tesla P100-SXM2-16GB\r\nGPU 3: Tesla P100-SXM2-16GB\r\nGPU 4: Tesla P100-SXM2-16GB\r\nGPU 5: Tesla P100-SXM2-16GB\r\nGPU 6: Tesla P100-SXM2-16GB\r\nGPU 7: Tesla P100-SXM2-16GB\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py36h90e4bf4_5\r\n[conda] mkl_fft                   1.0.6            py36h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py36h4414c95_1\r\n[conda] pytorch                   1.0.0           py3.6_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n","python\r\nimport torch\r\nimport torch.distributed as dist\r\nimport os\r\n\r\ndef get_mpi_rank():\r\n    return int(os.environ['RANK'])\r\n\r\ndef get_mpi_size():\r\n    return int(os.environ.get('WORLD_SIZE', '1'))\r\n\r\nrank = get_mpi_rank()\r\nworld_size = get_mpi_size()\r\n\r\ninit_param={'backend': 'nccl',\r\n        'init_method': 'env://',\r\n        'rank': rank,\r\n        'world_size': world_size}\r\n\r\nfrom pprint import pformat\r\nprint('before {} - {}\\n'.format(rank,\r\n    pformat(init_param)))\r\ndist.init_process_group(**init_param)\r\nprint('after {}'.format(rank))\r\n"
15602,torch.norm on CPU gives incorrect results for large Tensors## \U0001f41b Bug\r\n\r\n\r\n\r\nOriginally reported at https://discuss.pytorch.org/t/output-of-torch-norm-x-depends-on-size-of-x-not-in-a-good-way/33299/8,high priority|cherry-picked,cpuhrsch,"## \U0001f41b Bug\r\n\r\n```python\r\n>>> import torch\r\n>>> x = torch.ones(40000)\r\n>>> torch.norm(x)\r\ntensor(266.0605)\r\n>>> torch.norm(x.to(dtype=torch.float32, device='cuda'))\r\ntensor(200., device='cuda:0')\r\n```\r\n\r\nOriginally reported at https://discuss.pytorch.org/t/output-of-torch-norm-x-depends-on-size-of-x-not-in-a-good-way/33299/8","python\r\n>>> import torch\r\n>>> x = torch.ones(40000)\r\n>>> torch.norm(x)\r\ntensor(266.0605)\r\n>>> torch.norm(x.to(dtype=torch.float32, device='cuda'))\r\ntensor(200., device='cuda:0')\r\n"
15568,"*Possible* bug in P2P (device-to-device) copy overlapped with work on GPU## \U0001f41b Bug\r\n \r\nFollowing very simple code prints value of 1 to output (tensor(1., device=\u2018cuda:1\u2019)); instead of 0, i\u2019d expect to see. Using default stream (instead of custom) \u2018fixes\u2019 the issue, but defeats the purpose of original code - copy from one GPU to several others at the same time (async), also overlapping with (unrelated) computations on these GPUs. *Would be happy for any other solutions which achieve this goal?* (likely i'm just doing it in a wrong way?)\r\n\r\nCan only guess that something odd happens with memory when \u201cx1 + 1\u201d starts executing (see nvprof) and at the same time device-to-device copy starts happening\r\n\r\ncross-referenced from https://discuss.pytorch.org/t/possible-bug-in-p2p-device-to-device-copy-overlapped-with-work-on-gpu/33033\r\n\r\n## To Reproduce\r\n\r\n\r\n## Expected behavior\r\n\r\nx2 should not be 'corrupted' - i.e. all elements should be zeros\r\n\r\n## Environment\r\n - PyTorch Version 1.0\r\n - OS Linux\r\n - built from src (as conda build seem not to have OMP multithreading? - separate issue)\r\n - Python 3.7.1 \r\n - CUDA/cuDNN version: 10\r\n - GPU models V100\r\n\r\n- Same issue happens using old pytorch version (0.4.1) from conda distribution, and CUDA 9 and different type of GPU - which likely means it's a 'feature' rather than bug (yet surprising). Sorry if so, please feel free to close.\r\n\r\n---\r\nnvprof output\r\n```\r\nStart Duration Grid Size Block Size Regs* SSMem* DSMem* Size Throughput SrcMemType DstMemType Device Context Stream Src Dev Src Ctx Dst Dev Dst Ctx Name\r\n6.69668s 2.0160us - - - - - 3.3528GB 2e+06GB/s Device - Tesla V100-SXM2 1 7 - - - - [CUDA memset]\r\n13.0215s 1.9520us - - - - - 3.3528GB 2e+06GB/s Device - Tesla V100-SXM2 2 18 - - - - [CUDA memset]\r\n13.0261s 4.7312ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [886]\r\n13.0292s 74.185ms - - - - - 3.3528GB 45.194GB/s Device Device Tesla V100-SXM2 1 28 Tesla V100-SXM2 1 Tesla V100-SXM2 2 [CUDA memcpy PtoP]\r\n13.0308s 5.0902ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [889]\r\n13.0359s 5.0761ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [902]\r\n13.0410s 5.0793ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [905]\r\n13.1045s 2.6560us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorEQOp<float, unsigned char>, unsigned char, float, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, float, float>, OffsetInfo<TensorEQOp<float, unsigned char>, float, unsigned int>, OffsetInfo<unsigned char, float, int=1>, float, float) [1014]\r\n13.1046s 1.5040us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_abs_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_abs_Float_Op) [1028]\r\n13.1047s 1.7280us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorNEValueOp<float, unsigned char>, unsigned char, float, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, float>, OffsetInfo<TensorNEValueOp<float, unsigned char>, unsigned char, unsigned int>, unsigned char, float) [1040]\r\n13.1048s 1.6630us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorBitAndOp, unsigned char, unsigned char, unsigned char, unsigned int, int=1, int=1, int=1>(OffsetInfo<TensorBitAndOp, unsigned char, unsigned int>, OffsetInfo<unsigned char, unsigned char, int=1>, OffsetInfo<unsigned char, unsigned char, int=1>, unsigned char, unsigned char) [1051]\r\n13.1048s 1.1840us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorNEValueOp<float, unsigned char>, unsigned char, float, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, float>, OffsetInfo<TensorNEValueOp<float, unsigned char>, unsigned char, unsigned int>, unsigned char, float) [1062]\r\n13.1048s 1.2160us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorBitAndOp, unsigned char, unsigned char, unsigned char, unsigned int, int=1, int=1, int=1>(OffsetInfo<TensorBitAndOp, unsigned char, unsigned int>, OffsetInfo<unsigned char, unsigned char, int=1>, OffsetInfo<unsigned char, unsigned char, int=1>, unsigned char, unsigned char) [1073]\r\n13.1049s 3.6480us (1 1 1) (1024 1 1) 30 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<unsigned char, unsigned int, long, thrust::identity, ReduceAdd, int=1>(TensorInfo<unsigned char, unsigned int>, unsigned int, long, long, thrust::identity, long*) [1086]\r\n13.1049s 3.2000us - - - - - 8B 2.3842MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1051s 1.9520us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at4cuda75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a321kernelPointwiseApply2IZN75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a36CopyOpIlhE5applyERNS_6TensorERKS6_EUlRlRKhE_lhjLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSF_IT1_SH_EESH_T [1101]\r\n13.1051s 1.3440us (1 1 1) (128 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__scan::InitAgent<thrust::cuda_cub::cub::ScanTileState<long, bool=1>, int>, thrust::cuda_cub::cub::ScanTileState<long, bool=1>, int>(bool=1, thrust::cuda_cub::cub::ScanTileState<long, bool=1>) [1121]\r\n13.1051s 7.0080us (1 1 1) (96 1 1) 42 0B 2.0781KB - - - - Tesla V100-SXM2 2 18 - - - - void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__scan::ScanAgent<thrust::device_ptr, thrust::device_ptr, thrust::plus, int, long, thrust::detail::integral_constant<bool, bool=0>>, thrust::device_ptr, thrust::device_ptr, thrust::plus, int, thrust::cuda_cub::cub::ScanTileState<long, bool=1>, thrust::cuda_cub::__scan::AddInitToExclusiveScan<long, thrust::plus>>(thrust::device_ptr, thrust::device_ptr, long, thrust::plus, int, long) [1126]\r\n13.1052s 1.7600us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorMaskedSelectOp<float, unsigned char, long>, unsigned char, long, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, long>, OffsetInfo<long, unsigned char, float>, OffsetInfo<TensorMaskedSelectOp<float, unsigned char, long>, unsigned char, unsigned int>, unsigned char, float) [1132]\r\n13.1052s 1.2480us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_abs_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_abs_Float_Op) [1146]\r\n13.1053s 1.8240us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at4cuda75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a321kernelPointwiseApply2IZN75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a36CopyOpIdfE5applyERNS_6TensorERKS6_EUlRdRKfE_dfjLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSF_IT1_SH_EESH_T [1159]\r\n13.1054s 5.6320us (1 1 1) (1024 1 1) 16 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<double, unsigned int, double, thrust::identity, ReduceMin, int=1>(TensorInfo<double, unsigned int>, unsigned int, double, double, thrust::identity, double*) [1172]\r\n13.1054s 1.8880us - - - - - 8B 4.0410MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1055s 1.3440us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply1<TensorFillOp, double, unsigned int, int=1>(OffsetInfo<TensorFillOp, double, unsigned int>, double, double) [1184]\r\n13.1055s 5.4080us (1 1 1) (1024 1 1) 16 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<double, unsigned int, double, thrust::identity, ReduceMax, int=1>(TensorInfo<double, unsigned int>, unsigned int, double, double, thrust::identity, double*) [1196]\r\n13.1055s 1.5040us - - - - - 8B 5.0727MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1056s 1.1840us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply1<TensorFillOp, double, unsigned int, int=1>(OffsetInfo<TensorFillOp, double, unsigned int>, double, double) [1208]\r\n13.1057s 1.5680us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_ceil_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_ceil_Float_Op) [1226]\r\n13.1057s 1.5680us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorNEOp<float, unsigned char>, unsigned char, float, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, float, float>, OffsetInfo<TensorNEOp<float, unsigned char>, float, unsigned int>, OffsetInfo<unsigned char, float, int=1>, float, float) [1238]\r\n13.1058s 1.6000us - - - - - 1B 610.35KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1059s 2.3680us (1 1 1) (128 1 1) 26 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi128ELi4EZNS0_17gpu_binary_kernelIZNS0_15div_kernel_implIdEEvRNS_14TensorIteratorEEUlddE_EEvS5_RKT_EUliE0_EEviT1 [1259]\r\n13.1060s 1.5360us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorGTValueOp<double, unsigned char>, unsigned char, double, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, double>, OffsetInfo<TensorGTValueOp<double, unsigned char>, unsigned char, unsigned int>, unsigned char, double) [1270]\r\n13.1060s 1.4720us - - - - - 1B 663.42KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1060s 1.2480us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorGTValueOp<double, unsigned char>, unsigned char, double, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, double>, OffsetInfo<TensorGTValueOp<double, unsigned char>, unsigned char, unsigned int>, unsigned char, double) [1289]\r\n13.1060s 1.4720us - - - - - 1B 663.42KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1061s 1.4720us - - - - - 4B 2.5915MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1061s 1.9520us - - - - - 4B 1.9543MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n```",triage review,mrshenli,"## \U0001f41b Bug\r\n \r\nFollowing very simple code prints value of 1 to output (tensor(1., device=\u2018cuda:1\u2019)); instead of 0, i\u2019d expect to see. Using default stream (instead of custom) \u2018fixes\u2019 the issue, but defeats the purpose of original code - copy from one GPU to several others at the same time (async), also overlapping with (unrelated) computations on these GPUs. *Would be happy for any other solutions which achieve this goal?* (likely i'm just doing it in a wrong way?)\r\n\r\nCan only guess that something odd happens with memory when \u201cx1 + 1\u201d starts executing (see nvprof) and at the same time device-to-device copy starts happening\r\n\r\ncross-referenced from https://discuss.pytorch.org/t/possible-bug-in-p2p-device-to-device-copy-overlapped-with-work-on-gpu/33033\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\n\r\nn = int(3e4)\r\n\r\nd0 = torch.device(\u2018cuda:0\u2019)\r\nx0 = torch.zeros(n, n, device=d0)\r\n\r\nd1 = torch.device(\u2018cuda:1\u2019)\r\nx1 = torch.zeros(n, n, device=d1)\r\nx1 + 1\r\nx1 + 1\r\n\r\ns = torch.cuda.Stream()\r\nwith torch.cuda.stream(s):\r\n    x2 = x0.to(d1)\r\n\r\ntorch.cuda.synchronize() # just in case, x2[0,0] should sync anyway; sync on device-1 (d1) doesn\u2019t help, using events don\u2019t help either, neither does time.sleep() - looks like x2 is already 'corrupted' by this point (see nvprof) \r\n\r\nprint(x2[0, 0])\r\n```\r\n\r\n## Expected behavior\r\n\r\nx2 should not be 'corrupted' - i.e. all elements should be zeros\r\n\r\n## Environment\r\n - PyTorch Version 1.0\r\n - OS Linux\r\n - built from src (as conda build seem not to have OMP multithreading? - separate issue)\r\n - Python 3.7.1 \r\n - CUDA/cuDNN version: 10\r\n - GPU models V100\r\n\r\n- Same issue happens using old pytorch version (0.4.1) from conda distribution, and CUDA 9 and different type of GPU - which likely means it's a 'feature' rather than bug (yet surprising). Sorry if so, please feel free to close.\r\n\r\n---\r\nnvprof output\r\n```\r\nStart Duration Grid Size Block Size Regs* SSMem* DSMem* Size Throughput SrcMemType DstMemType Device Context Stream Src Dev Src Ctx Dst Dev Dst Ctx Name\r\n6.69668s 2.0160us - - - - - 3.3528GB 2e+06GB/s Device - Tesla V100-SXM2 1 7 - - - - [CUDA memset]\r\n13.0215s 1.9520us - - - - - 3.3528GB 2e+06GB/s Device - Tesla V100-SXM2 2 18 - - - - [CUDA memset]\r\n13.0261s 4.7312ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [886]\r\n13.0292s 74.185ms - - - - - 3.3528GB 45.194GB/s Device Device Tesla V100-SXM2 1 28 Tesla V100-SXM2 1 Tesla V100-SXM2 2 [CUDA memcpy PtoP]\r\n13.0308s 5.0902ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [889]\r\n13.0359s 5.0761ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [902]\r\n13.0410s 5.0793ms (878907 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi512ELi1EZNS0_16gpu_unary_kernelIZNS0_17gpu_binary_kernelIZNS0_15add_kernel_implIfEEvRNS_14TensorIteratorEN3c106ScalarEEUlffE_EEvS6_RKT_EUlfE0_EEvS6_SC_EUliE_EEviT1 [905]\r\n13.1045s 2.6560us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorEQOp<float, unsigned char>, unsigned char, float, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, float, float>, OffsetInfo<TensorEQOp<float, unsigned char>, float, unsigned int>, OffsetInfo<unsigned char, float, int=1>, float, float) [1014]\r\n13.1046s 1.5040us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_abs_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_abs_Float_Op) [1028]\r\n13.1047s 1.7280us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorNEValueOp<float, unsigned char>, unsigned char, float, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, float>, OffsetInfo<TensorNEValueOp<float, unsigned char>, unsigned char, unsigned int>, unsigned char, float) [1040]\r\n13.1048s 1.6630us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorBitAndOp, unsigned char, unsigned char, unsigned char, unsigned int, int=1, int=1, int=1>(OffsetInfo<TensorBitAndOp, unsigned char, unsigned int>, OffsetInfo<unsigned char, unsigned char, int=1>, OffsetInfo<unsigned char, unsigned char, int=1>, unsigned char, unsigned char) [1051]\r\n13.1048s 1.1840us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorNEValueOp<float, unsigned char>, unsigned char, float, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, float>, OffsetInfo<TensorNEValueOp<float, unsigned char>, unsigned char, unsigned int>, unsigned char, float) [1062]\r\n13.1048s 1.2160us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorBitAndOp, unsigned char, unsigned char, unsigned char, unsigned int, int=1, int=1, int=1>(OffsetInfo<TensorBitAndOp, unsigned char, unsigned int>, OffsetInfo<unsigned char, unsigned char, int=1>, OffsetInfo<unsigned char, unsigned char, int=1>, unsigned char, unsigned char) [1073]\r\n13.1049s 3.6480us (1 1 1) (1024 1 1) 30 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<unsigned char, unsigned int, long, thrust::identity, ReduceAdd, int=1>(TensorInfo<unsigned char, unsigned int>, unsigned int, long, long, thrust::identity, long*) [1086]\r\n13.1049s 3.2000us - - - - - 8B 2.3842MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1051s 1.9520us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at4cuda75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a321kernelPointwiseApply2IZN75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a36CopyOpIlhE5applyERNS_6TensorERKS6_EUlRlRKhE_lhjLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSF_IT1_SH_EESH_T [1101]\r\n13.1051s 1.3440us (1 1 1) (128 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__scan::InitAgent<thrust::cuda_cub::cub::ScanTileState<long, bool=1>, int>, thrust::cuda_cub::cub::ScanTileState<long, bool=1>, int>(bool=1, thrust::cuda_cub::cub::ScanTileState<long, bool=1>) [1121]\r\n13.1051s 7.0080us (1 1 1) (96 1 1) 42 0B 2.0781KB - - - - Tesla V100-SXM2 2 18 - - - - void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__scan::ScanAgent<thrust::device_ptr, thrust::device_ptr, thrust::plus, int, long, thrust::detail::integral_constant<bool, bool=0>>, thrust::device_ptr, thrust::device_ptr, thrust::plus, int, thrust::cuda_cub::cub::ScanTileState<long, bool=1>, thrust::cuda_cub::__scan::AddInitToExclusiveScan<long, thrust::plus>>(thrust::device_ptr, thrust::device_ptr, long, thrust::plus, int, long) [1126]\r\n13.1052s 1.7600us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorMaskedSelectOp<float, unsigned char, long>, unsigned char, long, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, long>, OffsetInfo<long, unsigned char, float>, OffsetInfo<TensorMaskedSelectOp<float, unsigned char, long>, unsigned char, unsigned int>, unsigned char, float) [1132]\r\n13.1052s 1.2480us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_abs_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_abs_Float_Op) [1146]\r\n13.1053s 1.8240us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at4cuda75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a321kernelPointwiseApply2IZN75_GLOBAL__N__51_tmpxft_000055a3_00000000_11_Copy_compute_75_cpp1_ii_dd3fb9a36CopyOpIdfE5applyERNS_6TensorERKS6_EUlRdRKfE_dfjLi1ELi1ELi1EEEvNS0_6detail10TensorInfoIT0_T2_EENSF_IT1_SH_EESH_T [1159]\r\n13.1054s 5.6320us (1 1 1) (1024 1 1) 16 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<double, unsigned int, double, thrust::identity, ReduceMin, int=1>(TensorInfo<double, unsigned int>, unsigned int, double, double, thrust::identity, double*) [1172]\r\n13.1054s 1.8880us - - - - - 8B 4.0410MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1055s 1.3440us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply1<TensorFillOp, double, unsigned int, int=1>(OffsetInfo<TensorFillOp, double, unsigned int>, double, double) [1184]\r\n13.1055s 5.4080us (1 1 1) (1024 1 1) 16 0B 8.0000KB - - - - Tesla V100-SXM2 2 18 - - - - void kernelReduceAll<double, unsigned int, double, thrust::identity, ReduceMax, int=1>(TensorInfo<double, unsigned int>, unsigned int, double, double, thrust::identity, double*) [1196]\r\n13.1055s 1.5040us - - - - - 8B 5.0727MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1056s 1.1840us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply1<TensorFillOp, double, unsigned int, int=1>(OffsetInfo<TensorFillOp, double, unsigned int>, double, double) [1208]\r\n13.1057s 1.5680us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<Tensor_ceil_Float_Op, float, float, unsigned int, int=1, int=1>(OffsetInfo<float, unsigned int, int=1>, OffsetInfo<float, unsigned int, int=1>, unsigned int, Tensor_ceil_Float_Op) [1226]\r\n13.1057s 1.5680us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply3<TensorNEOp<float, unsigned char>, unsigned char, float, float, unsigned int, int=1, int=1, int=1>(OffsetInfo<unsigned char, float, float>, OffsetInfo<TensorNEOp<float, unsigned char>, float, unsigned int>, OffsetInfo<unsigned char, float, int=1>, float, float) [1238]\r\n13.1058s 1.6000us - - - - - 1B 610.35KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1059s 2.3680us (1 1 1) (128 1 1) 26 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - ZN2at6native18elementwise_kernelILi128ELi4EZNS0_17gpu_binary_kernelIZNS0_15div_kernel_implIdEEvRNS_14TensorIteratorEEUlddE_EEvS5_RKT_EUliE0_EEviT1 [1259]\r\n13.1060s 1.5360us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorGTValueOp<double, unsigned char>, unsigned char, double, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, double>, OffsetInfo<TensorGTValueOp<double, unsigned char>, unsigned char, unsigned int>, unsigned char, double) [1270]\r\n13.1060s 1.4720us - - - - - 1B 663.42KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1060s 1.2480us (1 1 1) (512 1 1) 16 0B 0B - - - - Tesla V100-SXM2 2 18 - - - - void kernelPointwiseApply2<TensorGTValueOp<double, unsigned char>, unsigned char, double, unsigned int, int=1, int=1>(OffsetInfo<unsigned char, unsigned char, double>, OffsetInfo<TensorGTValueOp<double, unsigned char>, unsigned char, unsigned int>, unsigned char, double) [1289]\r\n13.1060s 1.4720us - - - - - 1B 663.42KB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1061s 1.4720us - - - - - 4B 2.5915MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n13.1061s 1.9520us - - - - - 4B 1.9543MB/s Device Pageable Tesla V100-SXM2 2 18 - - - - [CUDA memcpy DtoH]\r\n```","python\r\nimport torch\r\n\r\nn = int(3e4)\r\n\r\nd0 = torch.device(\u2018cuda:0\u2019)\r\nx0 = torch.zeros(n, n, device=d0)\r\n\r\nd1 = torch.device(\u2018cuda:1\u2019)\r\nx1 = torch.zeros(n, n, device=d1)\r\nx1 + 1\r\nx1 + 1\r\n\r\ns = torch.cuda.Stream()\r\nwith torch.cuda.stream(s):\r\n    x2 = x0.to(d1)\r\n\r\ntorch.cuda.synchronize() # just in case, x2[0,0] should sync anyway; sync on device-1 (d1) doesn\u2019t help, using events don\u2019t help either, neither does time.sleep() - looks like x2 is already 'corrupted' by this point (see nvprof) \r\n\r\nprint(x2[0, 0])\r\n"
15511,"pdist with large inputs is giving illegal memory exception on CUDA## \U0001f41b Bug\r\n\r\npdist with large inputs is giving illegal memory exception on CUDA\r\n\r\nReported on the forums here: https://discuss.pytorch.org/t/does-pytorch-pdist-have-size-limit/32898\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",high priority|cherry-picked,gchanan,"## \U0001f41b Bug\r\n\r\npdist with large inputs is giving illegal memory exception on CUDA\r\n\r\nReported on the forums here: https://discuss.pytorch.org/t/does-pytorch-pdist-have-size-limit/32898\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport math\r\nimport torch #version 1.0\r\nimport torch.nn.functional as F\r\n\r\nsizes = [100, 1000, 3000, 5000, 7500, 8500, 9999, 10000]\r\nfor size in sizes:\r\n    print(""[{}] Start"".format(size))\r\n    a = torch.randn(size, 20).float().cuda()\r\n    print(""Before"")\r\n    # The operation\r\n    b = F.pdist(a, p=2)\r\n    print(""After"")\r\n    a = torch.randn(size, 20).float().cuda()\r\n    print(""---End---"")\r\n```\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python\r\nimport math\r\nimport torch #version 1.0\r\nimport torch.nn.functional as F\r\n\r\nsizes = [100, 1000, 3000, 5000, 7500, 8500, 9999, 10000]\r\nfor size in sizes:\r\n    print(""[{}] Start"".format(size))\r\n    a = torch.randn(size, 20).float().cuda()\r\n    print(""Before"")\r\n    # The operation\r\n    b = F.pdist(a, p=2)\r\n    print(""After"")\r\n    a = torch.randn(size, 20).float().cuda()\r\n    print(""---End---"")\r\n"
15505,"[ONNX] Exporting JIT script with warning subcall fails## \U0001f41b Bug\r\n\r\nONNX export fails when a JITted function emits a warning. I probably should fix the warning, but someone (who isn't me) who tries to export a popular open source model should probably not see\r\n```\r\nUnsupported prim::Constant kind: `s`. Send a bug report.\r\n```\r\nIt looks like the issue is that the warning string gets baked into the function during JIT compilation and hence exported.\r\n\r\nMWE:\r\n\r\n\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip`\r\n - Python version: 3.6\r\n\r\n\r\n## Additional context\r\n\r\nI'm trying to export the [waveglow](https://github.com/NVIDIA/waveglow/blob/master/glow_old.py) model.\r\n",oncall: jit|module: onnx,eellison,"## \U0001f41b Bug\r\n\r\nONNX export fails when a JITted function emits a warning. I probably should fix the warning, but someone (who isn't me) who tries to export a popular open source model should probably not see\r\n```\r\nUnsupported prim::Constant kind: `s`. Send a bug report.\r\n```\r\nIt looks like the issue is that the warning string gets baked into the function during JIT compilation and hence exported.\r\n\r\nMWE:\r\n\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef jit_warner(inp):\r\n    return torch.nn.functional.sigmoid(inp) # triggers a deprecation warning\r\n\r\n@torch.jit.script\r\ndef jit_ok(inp):\r\n    return torch.sigmoid(inp)\r\n\r\nclass JitWarner(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n    def forward(self, x):\r\n        return jit_warner(x)\r\n        # return jit_ok(x) # this works\r\n\r\ntorch.onnx.export(JitWarner(), torch.randn(42), '/dev/null')\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip`\r\n - Python version: 3.6\r\n\r\n\r\n## Additional context\r\n\r\nI'm trying to export the [waveglow](https://github.com/NVIDIA/waveglow/blob/master/glow_old.py) model.\r\n","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef jit_warner(inp):\r\n    return torch.nn.functional.sigmoid(inp) # triggers a deprecation warning\r\n\r\n@torch.jit.script\r\ndef jit_ok(inp):\r\n    return torch.sigmoid(inp)\r\n\r\nclass JitWarner(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n    def forward(self, x):\r\n        return jit_warner(x)\r\n        # return jit_ok(x) # this works\r\n\r\ntorch.onnx.export(JitWarner(), torch.randn(42), '/dev/null')\r\n"
15478,"JIT Tensor .to not functioning, confusing error message## \U0001f41b Bug\r\n\r\nWhen using `torch.jit.script_method` or `torch.jit.script` moving Tensors to GPU / CPU via the `.to()` method does not function. Also, the resulting error message might be wrong?\r\n\r\n## To Reproduce\r\n\r\nThis script\r\n\r\n\r\nRun with `PYTORCH_JIT=0 python jittest.py`, works as expected, and `PYTORCH_JIT=1 python jittest.py` gives error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""jittest.py"", line 4, in <module>\r\n    def test(device: str):\r\n  File ""/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py"", line 691, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(fn))\r\nRuntimeError:\r\narguments for call are not valid:\r\n\r\n  for operator aten::to(Tensor self, Tensor other, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected a value of type Tensor for argument 'other' but found string\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n                           ~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, int dtype, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected a value of type int for argument 'dtype' but found string\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n                           ~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, Device device, int dtype, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  argument dtype not provided.\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n      ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, *, int dtype=<default>, int layout=<default>, Device device=<default>, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected at most 0 arguments but found 1 positional arguments.\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n      ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\nfor call at:\r\n@torch.jit.script\r\ndef test(device: str):\r\n    torch.ones(1,2,3).to(device)\r\n    ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```\r\n## Expected behavior\r\nThe code should run without error\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h6b9c3cc_5\r\n[conda] mkl_fft                   1.0.6            py36hb8a8100_0\r\n[conda] mkl_random                1.0.1            py36h5d10147_1\r\n[conda] pytorch                   1.0.0                   py3.6_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181121         py3.6_0    pytorch\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n\r\n## Additional context\r\nWhy would `.to` expect a Tensor as an argument?",oncall: jit,highker,"## \U0001f41b Bug\r\n\r\nWhen using `torch.jit.script_method` or `torch.jit.script` moving Tensors to GPU / CPU via the `.to()` method does not function. Also, the resulting error message might be wrong?\r\n\r\n## To Reproduce\r\n\r\nThis script\r\n```python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test(device: str):\r\n    torch.ones(1,2,3).to(device)\r\n\r\ntest(""cpu"")\r\n```\r\n\r\nRun with `PYTORCH_JIT=0 python jittest.py`, works as expected, and `PYTORCH_JIT=1 python jittest.py` gives error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""jittest.py"", line 4, in <module>\r\n    def test(device: str):\r\n  File ""/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py"", line 691, in script\r\n    _jit_script_compile(mod, ast, _rcb, get_default_args(fn))\r\nRuntimeError:\r\narguments for call are not valid:\r\n\r\n  for operator aten::to(Tensor self, Tensor other, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected a value of type Tensor for argument 'other' but found string\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n                           ~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, int dtype, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected a value of type int for argument 'dtype' but found string\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n                           ~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, Device device, int dtype, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  argument dtype not provided.\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n      ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\n  for operator aten::to(Tensor self, *, int dtype=<default>, int layout=<default>, Device device=<default>, bool non_blocking=<default>, bool copy=<default>) -> Tensor:\r\n  expected at most 0 arguments but found 1 positional arguments.\r\n  @torch.jit.script\r\n  def test(device: str):\r\n      torch.ones(1,2,3).to(device)\r\n      ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n\r\nfor call at:\r\n@torch.jit.script\r\ndef test(device: str):\r\n    torch.ones(1,2,3).to(device)\r\n    ~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```\r\n## Expected behavior\r\nThe code should run without error\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.13.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-service               1.1.2            py37h6b9c3cc_5\r\n[conda] mkl_fft                   1.0.6            py36hb8a8100_0\r\n[conda] mkl_random                1.0.1            py36h5d10147_1\r\n[conda] pytorch                   1.0.0                   py3.6_1    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181121         py3.6_0    pytorch\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n\r\n## Additional context\r\nWhy would `.to` expect a Tensor as an argument?","python\r\nimport torch\r\n\r\n@torch.jit.script\r\ndef test(device: str):\r\n    torch.ones(1,2,3).to(device)\r\n\r\ntest(""cpu"")\r\n"
15435,"Distributed operations fail when computational graph is different across processes## Potential \U0001f41b Bug\r\n\r\nWhen a dynamic network containing distributed operations such as `distributed.all_reduce` is executed with `DistributedDataParallel`, the distributed operations will fail after the first forward / backward pass.\r\nWhen the back-end is `gloo`, this will take the form of an `'gloo::EnforceNotMet'` error being thrown, while with `nccl` there will be a deadlock.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\nHere `TestFunction` is a simple function that performs `all_reduce` both in the forward and the backward. This function is then wrapped into `TestModule`. `TestNet` contains two convolutions and a `TestModule`, but the second convolution is only called from the process with `rank == 0`. The network is wrapped in `DistributedDataParallel`, and the default process group is initialized as per pytorch documentation. The loop at the end of the script performs forward / backward passes with random input data.\r\n\r\nTo reproduce the issue simply launch the script with:\r\n`python -m torch.distributed.launch --nproc_per_node=2 test.py`\r\n\r\nThis will print:\r\n```\r\nforward_start\r\nforward_start\r\nforward_done\r\nforward_done\r\nbackward_start\r\nbackward_start\r\nbackward_done\r\nbackward_done\r\nforward_start\r\nforward_start\r\n```\r\nthen, depending on the back-end it will either remain stuck in a deadlock (nccl) or print an error message and crash (gloo).\r\n\r\nNote that this can be reproduced with any number of `nproc_per_node` (we tested from 2 to 8).\r\n\r\n## Expected behavior\r\n\r\nThe distributed operations should be able to complete properly, since they are always called by all processes, in the same order and with the same input data shapes.\r\nThe fact that the rest of the network being different has an effect on `all_reduce` is definitely unexpected and doesn't seem to be documented anywhere.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 396.37\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0  \r\n[conda] mkl_random                1.0.1            py37h4414c95_1  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n",oncall: distributed,teng-li,"## Potential \U0001f41b Bug\r\n\r\nWhen a dynamic network containing distributed operations such as `distributed.all_reduce` is executed with `DistributedDataParallel`, the distributed operations will fail after the first forward / backward pass.\r\nWhen the back-end is `gloo`, this will take the form of an `'gloo::EnforceNotMet'` error being thrown, while with `nccl` there will be a deadlock.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport argparse\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.distributed as distributed\r\nimport torch.autograd as autograd\r\n\r\n\r\nclass TestFunction(autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        print(""forward_start"")\r\n        distributed.all_reduce(x, distributed.ReduceOp.SUM)\r\n        torch.cuda.synchronize()\r\n        print(""forward_done"")\r\n        return x\r\n    \r\n    @staticmethod\r\n    def backward(ctx, dx):\r\n        print(""backward_start"")\r\n        distributed.all_reduce(dx, distributed.ReduceOp.SUM)\r\n        torch.cuda.synchronize()\r\n        print(""backward_done"")\r\n        return dx\r\n\r\n\r\nclass TestModule(nn.Module):\r\n    def forward(self, x):\r\n        return TestFunction.apply(x)\r\n\r\n\r\nclass TestNet(nn.Module):\r\n    def __init__(self, rank):\r\n        super(TestNet, self).__init__()\r\n        self.rank = rank\r\n        self.mod1 = nn.Conv2d(4, 4, 1)\r\n        self.mod2 = TestModule()\r\n        self.mod3 = nn.Conv2d(4, 4, 1)\r\n    \r\n    def forward(self, x):\r\n        x = self.mod1(x)\r\n        x = self.mod2(x)\r\n        if self.rank == 0:\r\n            x = self.mod3(x)\r\n        return x\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(""--local_rank"", type=int)\r\nargs = parser.parse_args()\r\n    \r\ndistributed.init_process_group(backend='nccl', init_method='env://') # backend='gloo' also fails\r\ndevice_id, device = args.local_rank, torch.device(args.local_rank)\r\nrank = distributed.get_rank()\r\ntorch.cuda.set_device(device_id)\r\n\r\nnet = TestNet(rank).cuda(device)\r\nnet = nn.parallel.DistributedDataParallel(net, device_ids=[device_id], output_device=device_id)\r\nnet.train()\r\n    \r\nfor i in range(100):    \r\n    x = torch.randn(4, 4, 4, 4).cuda()\r\n    x = net(x)\r\n    x.mean().backward()\r\n```\r\n\r\nHere `TestFunction` is a simple function that performs `all_reduce` both in the forward and the backward. This function is then wrapped into `TestModule`. `TestNet` contains two convolutions and a `TestModule`, but the second convolution is only called from the process with `rank == 0`. The network is wrapped in `DistributedDataParallel`, and the default process group is initialized as per pytorch documentation. The loop at the end of the script performs forward / backward passes with random input data.\r\n\r\nTo reproduce the issue simply launch the script with:\r\n`python -m torch.distributed.launch --nproc_per_node=2 test.py`\r\n\r\nThis will print:\r\n```\r\nforward_start\r\nforward_start\r\nforward_done\r\nforward_done\r\nbackward_start\r\nbackward_start\r\nbackward_done\r\nbackward_done\r\nforward_start\r\nforward_start\r\n```\r\nthen, depending on the back-end it will either remain stuck in a deadlock (nccl) or print an error message and crash (gloo).\r\n\r\nNote that this can be reproduced with any number of `nproc_per_node` (we tested from 2 to 8).\r\n\r\n## Expected behavior\r\n\r\nThe distributed operations should be able to complete properly, since they are always called by all processes, in the same order and with the same input data shapes.\r\nThe fact that the rest of the network being different has an effect on `all_reduce` is definitely unexpected and doesn't seem to be documented anywhere.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-SXM2-32GB\r\nGPU 1: Tesla V100-SXM2-32GB\r\nGPU 2: Tesla V100-SXM2-32GB\r\nGPU 3: Tesla V100-SXM2-32GB\r\nGPU 4: Tesla V100-SXM2-32GB\r\nGPU 5: Tesla V100-SXM2-32GB\r\nGPU 6: Tesla V100-SXM2-32GB\r\nGPU 7: Tesla V100-SXM2-32GB\r\n\r\nNvidia driver version: 396.37\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] mkl                       2018.0.3                      1  \r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0  \r\n[conda] mkl_random                1.0.1            py37h4414c95_1  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n","python\r\nimport argparse\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.distributed as distributed\r\nimport torch.autograd as autograd\r\n\r\n\r\nclass TestFunction(autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        print(""forward_start"")\r\n        distributed.all_reduce(x, distributed.ReduceOp.SUM)\r\n        torch.cuda.synchronize()\r\n        print(""forward_done"")\r\n        return x\r\n    \r\n    @staticmethod\r\n    def backward(ctx, dx):\r\n        print(""backward_start"")\r\n        distributed.all_reduce(dx, distributed.ReduceOp.SUM)\r\n        torch.cuda.synchronize()\r\n        print(""backward_done"")\r\n        return dx\r\n\r\n\r\nclass TestModule(nn.Module):\r\n    def forward(self, x):\r\n        return TestFunction.apply(x)\r\n\r\n\r\nclass TestNet(nn.Module):\r\n    def __init__(self, rank):\r\n        super(TestNet, self).__init__()\r\n        self.rank = rank\r\n        self.mod1 = nn.Conv2d(4, 4, 1)\r\n        self.mod2 = TestModule()\r\n        self.mod3 = nn.Conv2d(4, 4, 1)\r\n    \r\n    def forward(self, x):\r\n        x = self.mod1(x)\r\n        x = self.mod2(x)\r\n        if self.rank == 0:\r\n            x = self.mod3(x)\r\n        return x\r\n\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(""--local_rank"", type=int)\r\nargs = parser.parse_args()\r\n    \r\ndistributed.init_process_group(backend='nccl', init_method='env://') # backend='gloo' also fails\r\ndevice_id, device = args.local_rank, torch.device(args.local_rank)\r\nrank = distributed.get_rank()\r\ntorch.cuda.set_device(device_id)\r\n\r\nnet = TestNet(rank).cuda(device)\r\nnet = nn.parallel.DistributedDataParallel(net, device_ids=[device_id], output_device=device_id)\r\nnet.train()\r\n    \r\nfor i in range(100):    \r\n    x = torch.randn(4, 4, 4, 4).cuda()\r\n    x = net(x)\r\n    x.mean().backward()\r\n"
15272,"Cannot cast RNN initial hidden tensor to GPU (torch.jit)## \U0001f41b Bug\r\n\r\nWhen I try to cast a traced model (traced via `torch.jit.trace` in CPU) to GPU using `.cuda()` method, it do not cast initial hidden tensor of RNN to GPU.\r\n\r\n## To Reproduce\r\n\r\nIn order to reproduce the behavior run the script given below:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\n```\r\nRuntimeError: \r\nInput and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu (operator() at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/ATen/native/RNN.h:30)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7fa5555ffcc5 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x11b3cbf (0x7fa558f49cbf in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #2: at::native::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long>, at::$ensor const&) + 0xc8 (0x7fa558f3f678 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #3: at::CUDAFloatType::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long$, at::Tensor const&) const + 0xdc (0x7fa559010fec in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #4: torch::autograd::VariableType::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::A$rayRef<long>, at::Tensor const&) const + 0x49f (0x7fa55296dbff in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #5: <unknown function> + 0x11a5753 (0x7fa558f3b753 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #6: <unknown function> + 0x11a5ebe (0x7fa558f3bebe in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #7: <unknown function> + 0x5cbb46 (0x7fa555ddcb46 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #8: at::native::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x10e (0x7fa555dd747e in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #9: at::TypeDefault::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0xc9 (0x7fa555ff1b39 in /home/onur/miniconda2/envs/pytorch-1.0-for-iss\r\nues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #10: torch::autograd::VariableType::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0x348 (0x7fa552b0a538 in /home/onur/miniconda2/envs/pyt\r\norch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #11: <unknown function> + 0x5be4de (0x7fa552bbe4de in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #12: <unknown function> + 0x6802c8 (0x7fa552c802c8 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #13: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x22 (0x7fa552c7b622 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/\r\nlib/libtorch.so.1)\r\nframe #14: <unknown function> + 0x65e12c (0x7fa552c5e12c in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #15: <unknown function> + 0x3cf9c2 (0x7fa585b5a9c2 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #16: <unknown function> + 0x3ac433 (0x7fa585b37433 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #17: <unknown function> + 0x112176 (0x7fa58589d176 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #37: __libc_start_main + 0xf0 (0x7fa59775b830 in /lib/x86_64-linux-gnu/libc.so.6)\r\n:\r\noperation failed in interpreter:\r\n  _4 = int(n_width)\r\n  n_filters = ops.prim.NumToTensor(torch.size(x_2, 3))\r\n  _5 = int(n_filters)\r\n  x_3 = torch.contiguous(x_2)\r\n  x_4 = torch.view(x_3, [int(torch.mul(b, n_height)), _4, _5])\r\n  input = torch.permute(x_4, [1, 0, 2])\r\n  max_batch_size = ops.prim.NumToTensor(torch.size(input, 1))\r\n  hx = torch.zeros([2, int(max_batch_size), 50], dtype=6, layout=0, device=torch.device(""cpu""))\r\n  _6 = [self.renet1.rnn.weight_ih_l0, self.renet1.rnn.weight_hh_l0, self.renet1.rnn.bias_ih_l0, self.renet1.rnn.bias_hh_l0, self.renet1.rnn.weight_ih_l0_reverse, self.renet1.rnn.weight_hh_l0_reverse, self.renet1.\r\nrnn.bias_ih_l0_reverse, self.renet1.rnn.bias_hh_l0_reverse]\r\n  x_5, _7 = torch.gru(input, hx, _6, True, 1, 0., False, True, False)\r\n            ~~~~~~~~~ <--- HERE\r\n  x_6 = torch.permute(x_5, [1, 0, 2])\r\n  x = torch.view(x_6, [_1, _2, _3, -1])\r\n  return torch.permute(x, [0, 3, 1, 2])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 980M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```",oncall: jit,suo,"## \U0001f41b Bug\r\n\r\nWhen I try to cast a traced model (traced via `torch.jit.trace` in CPU) to GPU using `.cuda()` method, it do not cast initial hidden tensor of RNN to GPU.\r\n\r\n## To Reproduce\r\n\r\nIn order to reproduce the behavior run the script given below:\r\n\r\n```python\r\nimport copy\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torchvision import models\r\n\r\n\r\nclass ReNet(nn.Module):\r\n\r\n    def __init__(self, n_input, n_units):\r\n        super(ReNet, self).__init__()\r\n\r\n        self.rnn = nn.GRU(n_input, n_units,\r\n                          num_layers=1, batch_first=False,\r\n                          bidirectional=True)\r\n\r\n    def rnn_forward(self, x):\r\n\r\n        b, n_height, n_width, n_filters = x.size()\r\n\r\n        x = x.view(b * n_height, n_width, n_filters)\r\n        x = x.permute(1, 0, 2)\r\n        x, _ = self.rnn(x)\r\n        x = x.permute(1, 0, 2)\r\n        x = x.view(b, n_height, n_width, -1)\r\n\r\n        return x\r\n\r\n    def forward(self, x):\r\n                                       #b, nf, h, w\r\n        x = x.permute(0, 2, 3, 1)      #b, h, w, nf\r\n        x = self.rnn_forward(x)        #b, h, w, nf\r\n        x = x.permute(0, 3, 1, 2)      #b, nf, h, w\r\n\r\n        return x\r\n\r\n\r\nclass Architecture(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Architecture, self).__init__()\r\n\r\n        self.cnn = models.resnet50(pretrained=True)\r\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-5])\r\n\r\n        self.renet1 = ReNet(256, 50)\r\n\r\n    def forward(self, x):\r\n        x = self.cnn(x)\r\n        x = self.renet1(x)\r\n\r\n        return x\r\n\r\n\r\ndef trace(model, usegpu):\r\n    with torch.set_grad_enabled(False):\r\n        model.eval()\r\n\r\n        dummy_input = torch.randn(1, 3, 224, 224)\r\n        \r\n        if usegpu:\r\n            dummy_input = dummy_input.to(torch.device('cuda'))\r\n\r\n        traced_model = torch.jit.trace(model, dummy_input)\r\n\r\n    return traced_model\r\n\r\n\r\ntorch.manual_seed(13)\r\n\r\nmodel_cpu = Architecture()\r\ntraced_model_cpu = trace(model_cpu, False)\r\ntorch.jit.save(traced_model_cpu, ""model_cpu.pth"")\r\n\r\ntraced_model_loaded = torch.jit.load(""model_cpu.pth"")\r\n\r\ntraced_model_loaded = traced_model_loaded.cuda()\r\ngpu_dummy_input = torch.randn(1, 3, 224, 224).cuda()\r\n\r\ntraced_model_loaded(gpu_dummy_input)\r\n```\r\n\r\n## Expected behavior\r\n\r\n```\r\nRuntimeError: \r\nInput and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu (operator() at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/ATen/native/RNN.h:30)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7fa5555ffcc5 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0x11b3cbf (0x7fa558f49cbf in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #2: at::native::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long>, at::$ensor const&) + 0xc8 (0x7fa558f3f678 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #3: at::CUDAFloatType::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::ArrayRef<long$, at::Tensor const&) const + 0xdc (0x7fa559010fec in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #4: torch::autograd::VariableType::_cudnn_rnn(at::Tensor const&, c10::ArrayRef<at::Tensor>, long, at::Tensor const&, at::Tensor const&, at::Tensor const&, long, long, long, bool, double, bool, bool, c10::A$rayRef<long>, at::Tensor const&) const + 0x49f (0x7fa55296dbff in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #5: <unknown function> + 0x11a5753 (0x7fa558f3b753 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #6: <unknown function> + 0x11a5ebe (0x7fa558f3bebe in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2_gpu.so)\r\nframe #7: <unknown function> + 0x5cbb46 (0x7fa555ddcb46 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #8: at::native::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) + 0x10e (0x7fa555dd747e in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #9: at::TypeDefault::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0xc9 (0x7fa555ff1b39 in /home/onur/miniconda2/envs/pytorch-1.0-for-iss\r\nues/lib/python3.7/site-packages/torch/lib/libcaffe2.so)\r\nframe #10: torch::autograd::VariableType::gru(at::Tensor const&, at::Tensor const&, c10::ArrayRef<at::Tensor>, bool, long, double, bool, bool, bool) const + 0x348 (0x7fa552b0a538 in /home/onur/miniconda2/envs/pyt\r\norch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #11: <unknown function> + 0x5be4de (0x7fa552bbe4de in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #12: <unknown function> + 0x6802c8 (0x7fa552c802c8 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #13: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x22 (0x7fa552c7b622 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/\r\nlib/libtorch.so.1)\r\nframe #14: <unknown function> + 0x65e12c (0x7fa552c5e12c in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch.so.1)\r\nframe #15: <unknown function> + 0x3cf9c2 (0x7fa585b5a9c2 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #16: <unknown function> + 0x3ac433 (0x7fa585b37433 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #17: <unknown function> + 0x112176 (0x7fa58589d176 in /home/onur/miniconda2/envs/pytorch-1.0-for-issues/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #37: __libc_start_main + 0xf0 (0x7fa59775b830 in /lib/x86_64-linux-gnu/libc.so.6)\r\n:\r\noperation failed in interpreter:\r\n  _4 = int(n_width)\r\n  n_filters = ops.prim.NumToTensor(torch.size(x_2, 3))\r\n  _5 = int(n_filters)\r\n  x_3 = torch.contiguous(x_2)\r\n  x_4 = torch.view(x_3, [int(torch.mul(b, n_height)), _4, _5])\r\n  input = torch.permute(x_4, [1, 0, 2])\r\n  max_batch_size = ops.prim.NumToTensor(torch.size(input, 1))\r\n  hx = torch.zeros([2, int(max_batch_size), 50], dtype=6, layout=0, device=torch.device(""cpu""))\r\n  _6 = [self.renet1.rnn.weight_ih_l0, self.renet1.rnn.weight_hh_l0, self.renet1.rnn.bias_ih_l0, self.renet1.rnn.bias_hh_l0, self.renet1.rnn.weight_ih_l0_reverse, self.renet1.rnn.weight_hh_l0_reverse, self.renet1.\r\nrnn.bias_ih_l0_reverse, self.renet1.rnn.bias_hh_l0_reverse]\r\n  x_5, _7 = torch.gru(input, hx, _6, True, 1, 0., False, True, False)\r\n            ~~~~~~~~~ <--- HERE\r\n  x_6 = torch.permute(x_5, [1, 0, 2])\r\n  x = torch.view(x_6, [_1, _2, _3, -1])\r\n  return torch.permute(x, [0, 3, 1, 2])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 980M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```","python\r\nimport copy\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torchvision import models\r\n\r\n\r\nclass ReNet(nn.Module):\r\n\r\n    def __init__(self, n_input, n_units):\r\n        super(ReNet, self).__init__()\r\n\r\n        self.rnn = nn.GRU(n_input, n_units,\r\n                          num_layers=1, batch_first=False,\r\n                          bidirectional=True)\r\n\r\n    def rnn_forward(self, x):\r\n\r\n        b, n_height, n_width, n_filters = x.size()\r\n\r\n        x = x.view(b * n_height, n_width, n_filters)\r\n        x = x.permute(1, 0, 2)\r\n        x, _ = self.rnn(x)\r\n        x = x.permute(1, 0, 2)\r\n        x = x.view(b, n_height, n_width, -1)\r\n\r\n        return x\r\n\r\n    def forward(self, x):\r\n                                       #b, nf, h, w\r\n        x = x.permute(0, 2, 3, 1)      #b, h, w, nf\r\n        x = self.rnn_forward(x)        #b, h, w, nf\r\n        x = x.permute(0, 3, 1, 2)      #b, nf, h, w\r\n\r\n        return x\r\n\r\n\r\nclass Architecture(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Architecture, self).__init__()\r\n\r\n        self.cnn = models.resnet50(pretrained=True)\r\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-5])\r\n\r\n        self.renet1 = ReNet(256, 50)\r\n\r\n    def forward(self, x):\r\n        x = self.cnn(x)\r\n        x = self.renet1(x)\r\n\r\n        return x\r\n\r\n\r\ndef trace(model, usegpu):\r\n    with torch.set_grad_enabled(False):\r\n        model.eval()\r\n\r\n        dummy_input = torch.randn(1, 3, 224, 224)\r\n        \r\n        if usegpu:\r\n            dummy_input = dummy_input.to(torch.device('cuda'))\r\n\r\n        traced_model = torch.jit.trace(model, dummy_input)\r\n\r\n    return traced_model\r\n\r\n\r\ntorch.manual_seed(13)\r\n\r\nmodel_cpu = Architecture()\r\ntraced_model_cpu = trace(model_cpu, False)\r\ntorch.jit.save(traced_model_cpu, ""model_cpu.pth"")\r\n\r\ntraced_model_loaded = torch.jit.load(""model_cpu.pth"")\r\n\r\ntraced_model_loaded = traced_model_loaded.cuda()\r\ngpu_dummy_input = torch.randn(1, 3, 224, 224).cuda()\r\n\r\ntraced_model_loaded(gpu_dummy_input)\r\n"
15271,"Corrupted RNN parameters in models exported on GPU (torch.jit)## \U0001f41b Bug\r\n\r\n### Step 1\r\n\r\nI build a CNN + RNN based model and cast it to both CPU and GPU, then trace them using `torch.jit.trace` as below:\r\n\r\n\r\n\r\nAt this step, the parameters of all models (`model_cpu`, `model_gpu`, `traced_model_cpu`, `traced_model_gpu`) are exactly the same.\r\n\r\n### Step 2\r\n\r\nThen I save and load them using `torch.jit` as below:\r\n\r\n\r\n\r\nThe parameters of the RNN layers in `traced_model_gpu_loaded` is totally different from `model_cpu`, `model_gpu`, `traced_model_cpu`, `traced_model_gpu` and `traced_model_cpu_loaded`.\r\n\r\n## To Reproduce\r\n\r\nIn order to reproduce the behavior, run the script given below:\r\n\r\n\r\n\r\nThis script prints the parameter names (and a small portion of the values) that are different between CPU and GPU.\r\n\r\n## Expected behavior\r\n\r\nIt prints the following output which indicates that the parameters of RNN layers of the model are different between CPU and GPU:\r\n\r\n```\r\n        # PARAMETER :  renet1.rnn.weight_hh_l0\r\n        * GPU :  tensor([-0.5195, -0.7641,  0.7705,  0.6834,  0.0681], device='cuda:0')\r\n        * CPU :  tensor([ 0.0051,  0.0621,  0.0859, -0.0506, -0.1000])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_ih_l0\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([-0.0004,  0.0263,  0.0537, -0.0810,  0.0930])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_hh_l0\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([ 0.0439, -0.0063,  0.0250,  0.0784,  0.0408])\r\n\r\n        # PARAMETER :  renet1.rnn.weight_ih_l0_reverse\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([-0.0755, -0.0730, -0.0435, -0.0522, -0.0979])\r\n\r\n        # PARAMETER :  renet1.rnn.weight_hh_l0_reverse\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([ 0.0104,  0.1401, -0.0695,  0.0870, -0.0896])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_ih_l0_reverse\r\n        * GPU :  tensor([0.0165, 0.0165, 0.0383, 0.0383, 0.0000], device='cuda:0')\r\n        * CPU :  tensor([ 0.0596,  0.1129, -0.1282,  0.0738, -0.0051])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_hh_l0_reverse\r\n        * GPU :  tensor([ 0.0419,  0.0845, -0.0466, -0.1143, -0.0606], device='cuda:0')\r\n        * CPU :  tensor([-0.0844,  0.0941, -0.1149,  0.0188, -0.0276])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 980M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```",oncall: jit,houseroad,"## \U0001f41b Bug\r\n\r\n### Step 1\r\n\r\nI build a CNN + RNN based model and cast it to both CPU and GPU, then trace them using `torch.jit.trace` as below:\r\n\r\n```python\r\nmodel_cpu = Architecture()\r\nmodel_gpu = copy.deepcopy(model_cpu).to(torch.device(""cuda""))\r\n\r\ndummy_input = torch.randn(1, 3, 224, 224)\r\n\r\ntraced_model_cpu = torch.jit.trace(model_cpu, dummy_input)\r\ntraced_model_gpu = torch.jit.trace(model_gpu, dummy_input.to(torch.device('cuda')))\r\n```\r\n\r\nAt this step, the parameters of all models (`model_cpu`, `model_gpu`, `traced_model_cpu`, `traced_model_gpu`) are exactly the same.\r\n\r\n### Step 2\r\n\r\nThen I save and load them using `torch.jit` as below:\r\n\r\n```python\r\ntorch.jit.save(traced_model_cpu, ""model_cpu.pth"")\r\ntorch.jit.save(traced_model_gpu, ""model_gpu.pth"")\r\n\r\ntraced_model_cpu_loaded = torch.jit.load(""model_cpu.pth"")\r\ntraced_model_gpu_loaded = torch.jit.load(""model_gpu.pth"")\r\n```\r\n\r\nThe parameters of the RNN layers in `traced_model_gpu_loaded` is totally different from `model_cpu`, `model_gpu`, `traced_model_cpu`, `traced_model_gpu` and `traced_model_cpu_loaded`.\r\n\r\n## To Reproduce\r\n\r\nIn order to reproduce the behavior, run the script given below:\r\n\r\n```python\r\nimport copy\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torchvision import models\r\n\r\n\r\nclass ReNet(nn.Module):\r\n\r\n    def __init__(self, n_input, n_units):\r\n        super(ReNet, self).__init__()\r\n\r\n        self.rnn = nn.GRU(n_input, n_units,\r\n                          num_layers=1, batch_first=False,\r\n                          bidirectional=True)\r\n\r\n    def rnn_forward(self, x):\r\n\r\n        b, n_height, n_width, n_filters = x.size()\r\n\r\n        x = x.view(b * n_height, n_width, n_filters)\r\n        x = x.permute(1, 0, 2)\r\n        x, _ = self.rnn(x)\r\n        x = x.permute(1, 0, 2)\r\n        x = x.view(b, n_height, n_width, -1)\r\n\r\n        return x\r\n\r\n    def forward(self, x):\r\n                                       #b, nf, h, w\r\n        x = x.permute(0, 2, 3, 1)      #b, h, w, nf\r\n        x = self.rnn_forward(x)        #b, h, w, nf\r\n        x = x.permute(0, 3, 1, 2)      #b, nf, h, w\r\n\r\n        return x\r\n\r\n\r\nclass Architecture(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Architecture, self).__init__()\r\n\r\n        self.cnn = models.resnet50(pretrained=True)\r\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-5])\r\n\r\n        self.renet1 = ReNet(256, 50)\r\n\r\n    def forward(self, x):\r\n        x = self.cnn(x)\r\n        x = self.renet1(x)\r\n\r\n        return x\r\n\r\n\r\ndef compare_models(cpu_model, gpu_model):\r\n\r\n    is_identical = True\r\n\r\n    cpu_model_state_dict = cpu_model.state_dict()\r\n    gpu_model_state_dict = gpu_model.state_dict()\r\n\r\n    for param_key, cpu_params in cpu_model_state_dict.items():\r\n        gpu_params = gpu_model_state_dict[param_key]\r\n        _identical = torch.all(gpu_params == cpu_params.to(torch.device(""cuda"")))\r\n        if _identical.item() == 0:\r\n            print(""\\n\\t# PARAMETER : "", param_key)\r\n            print(""\\t* GPU : "", gpu_params.view(-1)[:5])\r\n            print(""\\t* CPU : "", cpu_params.view(-1)[:5])\r\n            is_identical = False\r\n\r\n    return is_identical\r\n\r\n\r\ndef trace(model, usegpu):\r\n    with torch.set_grad_enabled(False):\r\n        model.eval()\r\n\r\n        dummy_input = torch.randn(1, 3, 224, 224)\r\n        \r\n        if usegpu:\r\n            dummy_input = dummy_input.to(torch.device('cuda'))\r\n\r\n        traced_model = torch.jit.trace(model, dummy_input)\r\n\r\n    return traced_model\r\n\r\n\r\ntorch.manual_seed(13)\r\n\r\nmodel_cpu = Architecture()\r\nmodel_gpu = copy.deepcopy(model_cpu).to(torch.device(""cuda""))\r\n\r\nprint(""STEP 1 : "", compare_models(model_cpu, model_gpu))\r\n\r\ntraced_model_cpu = trace(model_cpu, False)\r\ntraced_model_gpu = trace(model_gpu, True)\r\nprint(""STEP 2 : "", compare_models(traced_model_cpu, traced_model_gpu))\r\nprint(""STEP 2 : "", compare_models(traced_model_gpu, model_gpu))\r\n\r\ntorch.jit.save(traced_model_cpu, ""model_cpu.pth"")\r\ntorch.jit.save(traced_model_gpu, ""model_gpu.pth"")\r\nprint(""STEP 3 : "", compare_models(traced_model_cpu, traced_model_gpu))\r\nprint(""STEP 3 : "", compare_models(traced_model_gpu, model_gpu))\r\n\r\ntraced_model_cpu_loaded = torch.jit.load(""model_cpu.pth"")\r\ntraced_model_gpu_loaded = torch.jit.load(""model_gpu.pth"")\r\nprint(""\\nSTEP 4 : "", compare_models(traced_model_cpu_loaded, model_cpu))\r\nprint(""\\nSTEP 4 : "", compare_models(traced_model_gpu_loaded, model_cpu))\r\n```\r\n\r\nThis script prints the parameter names (and a small portion of the values) that are different between CPU and GPU.\r\n\r\n## Expected behavior\r\n\r\nIt prints the following output which indicates that the parameters of RNN layers of the model are different between CPU and GPU:\r\n\r\n```\r\n        # PARAMETER :  renet1.rnn.weight_hh_l0\r\n        * GPU :  tensor([-0.5195, -0.7641,  0.7705,  0.6834,  0.0681], device='cuda:0')\r\n        * CPU :  tensor([ 0.0051,  0.0621,  0.0859, -0.0506, -0.1000])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_ih_l0\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([-0.0004,  0.0263,  0.0537, -0.0810,  0.0930])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_hh_l0\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([ 0.0439, -0.0063,  0.0250,  0.0784,  0.0408])\r\n\r\n        # PARAMETER :  renet1.rnn.weight_ih_l0_reverse\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([-0.0755, -0.0730, -0.0435, -0.0522, -0.0979])\r\n\r\n        # PARAMETER :  renet1.rnn.weight_hh_l0_reverse\r\n        * GPU :  tensor([0., 0., 0., 0., 0.], device='cuda:0')\r\n        * CPU :  tensor([ 0.0104,  0.1401, -0.0695,  0.0870, -0.0896])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_ih_l0_reverse\r\n        * GPU :  tensor([0.0165, 0.0165, 0.0383, 0.0383, 0.0000], device='cuda:0')\r\n        * CPU :  tensor([ 0.0596,  0.1129, -0.1282,  0.0738, -0.0051])\r\n\r\n        # PARAMETER :  renet1.rnn.bias_hh_l0_reverse\r\n        * GPU :  tensor([ 0.0419,  0.0845, -0.0466, -0.1143, -0.0606], device='cuda:0')\r\n        * CPU :  tensor([-0.0844,  0.0941, -0.1149,  0.0188, -0.0276])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.1 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 980M\r\nNvidia driver version: 396.44\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.6            py37hd81dba3_0  \r\n[conda] mkl_random                1.0.2            py37hd81dba3_0  \r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n```","python\r\nmodel_cpu = Architecture()\r\nmodel_gpu = copy.deepcopy(model_cpu).to(torch.device(""cuda""))\r\n\r\ndummy_input = torch.randn(1, 3, 224, 224)\r\n\r\ntraced_model_cpu = torch.jit.trace(model_cpu, dummy_input)\r\ntraced_model_gpu = torch.jit.trace(model_gpu, dummy_input.to(torch.device('cuda')))\r\n"
15135,Printing Tensor from traced module results in TypeError: rsplit() takes no keyword arguments## \U0001f41b Bug\r\nPython 2.7 errors out on printing tensor generate by traced module.\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nPrinted representation of the tensor contents as a string.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\nCMake version: version 3.5.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.225\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a_for_7.0.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (1.0.0)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl_fft                   1.0.4            py27h4414c95_1\r\n[conda] mkl_random                1.0.1            py27h4414c95_1\r\n[conda] pytorch                   1.0.0           py2.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n,oncall: jit,soumith,"## \U0001f41b Bug\r\nPython 2.7 errors out on printing tensor generate by traced module.\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-43405c9b8ee8> in <module>()\r\n----> 1 print features\r\n\r\n/.../lib/python2.7/site-packages/torch/tensor.pyc in __repr__(self)\r\n     67         else:\r\n     68             if hasattr(sys.stdout, 'encoding'):\r\n---> 69                 return torch._tensor_str._str(self).encode(\r\n     70                     sys.stdout.encoding or 'UTF-8', 'replace')\r\n     71             else:\r\n\r\n/.../lib/python2.7/site-packages/torch/_tensor_str.pyc in _str(self)\r\n    283         name = type(self.grad_fn).__name__\r\n    284         if name == 'CppFunction':\r\n--> 285             name = self.grad_fn.name().rsplit('::', maxsplit=1)[-1]\r\n    286         suffixes.append('grad_fn=<{}>'.format(name))\r\n    287     elif self.requires_grad:\r\n\r\nTypeError: rsplit() takes no keyword arguments\r\n```\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torchvision\r\n\r\nresnet = torchvision.models.resnet18()\r\nsample_image = torch.randn(1, 3, 224, 224)\r\nresnet_jit = torch.jit.trace(resnet, sample_image)\r\n\r\nfeatures = resnet_jit(sample_image)\r\nprint features\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nPrinted representation of the tensor contents as a string.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\nCMake version: version 3.5.2\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.225\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a_for_7.0.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (1.0.0)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl_fft                   1.0.4            py27h4414c95_1\r\n[conda] mkl_random                1.0.1            py27h4414c95_1\r\n[conda] pytorch                   1.0.0           py2.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                      py_2    pytorch\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","python-traceback\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-43405c9b8ee8> in <module>()\r\n----> 1 print features\r\n\r\n/.../lib/python2.7/site-packages/torch/tensor.pyc in __repr__(self)\r\n     67         else:\r\n     68             if hasattr(sys.stdout, 'encoding'):\r\n---> 69                 return torch._tensor_str._str(self).encode(\r\n     70                     sys.stdout.encoding or 'UTF-8', 'replace')\r\n     71             else:\r\n\r\n/.../lib/python2.7/site-packages/torch/_tensor_str.pyc in _str(self)\r\n    283         name = type(self.grad_fn).__name__\r\n    284         if name == 'CppFunction':\r\n--> 285             name = self.grad_fn.name().rsplit('::', maxsplit=1)[-1]\r\n    286         suffixes.append('grad_fn=<{}>'.format(name))\r\n    287     elif self.requires_grad:\r\n\r\nTypeError: rsplit() takes no keyword arguments\r\n"
15119,"[jit] warnings.warn is always printed## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n```\r\n/home/tunz/anaconda3/envs/torchdev/lib/python3.7/site-packages/torch/nn/modules/module.py:489: RuntimeWarning: Format options are not supported.\r\n  result = self.forward(*input, **kwargs)\r\n/home/tunz/anaconda3/envs/torchdev/lib/python3.7/site-packages/torch/nn/modules/module.py:489: RuntimeWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  result = self.forward(*input, **kwargs)\r\n```\r\n\r\n## Expected behavior\r\n\r\nno warnings\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+479481b\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 410.73\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n[conda] torch                     1.0.0a0+479481b           <pip>\r\n```\r\n## Additional context\r\n\r\nA warning about `dim` parameter is always printed even if `dim` is not None.\r\nhttps://github.com/pytorch/pytorch/blob/00a4c8d41cf8a7504a4300b3fc741d8a4ccc5795/torch/nn/functional.py#L1183-L1184\r\nhttps://github.com/pytorch/pytorch/blob/00a4c8d41cf8a7504a4300b3fc741d8a4ccc5795/torch/nn/functional.py#L1114-L1118\r\n\r\nAfter some debugging, I found that it's actually taking correct paths that don't have warnings. it's just printing every warnings at the start of each function regardless of actual paths. I guess some jit optimization pass is reordering `at::warn` op.",oncall: jit,zou3519,"## \U0001f41b Bug\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass M(torch.jit.ScriptModule):\r\n\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.softmax = nn.Softmax(dim=0)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, v):\r\n        return self.softmax(v)\r\n\r\ni = torch.Tensor(2)\r\nm = M()\r\no = m(i)\r\n```\r\n\r\n```\r\n/home/tunz/anaconda3/envs/torchdev/lib/python3.7/site-packages/torch/nn/modules/module.py:489: RuntimeWarning: Format options are not supported.\r\n  result = self.forward(*input, **kwargs)\r\n/home/tunz/anaconda3/envs/torchdev/lib/python3.7/site-packages/torch/nn/modules/module.py:489: RuntimeWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\r\n  result = self.forward(*input, **kwargs)\r\n```\r\n\r\n## Expected behavior\r\n\r\nno warnings\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+479481b\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 410.73\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n[conda] torch                     1.0.0a0+479481b           <pip>\r\n```\r\n## Additional context\r\n\r\nA warning about `dim` parameter is always printed even if `dim` is not None.\r\nhttps://github.com/pytorch/pytorch/blob/00a4c8d41cf8a7504a4300b3fc741d8a4ccc5795/torch/nn/functional.py#L1183-L1184\r\nhttps://github.com/pytorch/pytorch/blob/00a4c8d41cf8a7504a4300b3fc741d8a4ccc5795/torch/nn/functional.py#L1114-L1118\r\n\r\nAfter some debugging, I found that it's actually taking correct paths that don't have warnings. it's just printing every warnings at the start of each function regardless of actual paths. I guess some jit optimization pass is reordering `at::warn` op.","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass M(torch.jit.ScriptModule):\r\n\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.softmax = nn.Softmax(dim=0)\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, v):\r\n        return self.softmax(v)\r\n\r\ni = torch.Tensor(2)\r\nm = M()\r\no = m(i)\r\n"
15116,"torch.save does not work if nn.Module has partial JIT.## \U0001f41b Bug\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n```\r\nTypeError: can't pickle M objects\r\n```\r\n\r\n## Expected behavior\r\n\r\nSuccessfully saving the model.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 410.73\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] mkldnn                    0.14.0                        0    mingfeima\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n```\r\n## Additional context\r\n\r\nWhile `model.save(f)` works fine for standalone jit.ScriptModule, I cannot find a way to save the entire `torch.nn.Module` containing `torch.jit.ScriptModule`, and it seems like a bug that I cannot use `torch.save` for the case.\r\n",oncall: jit,zdevito,"## \U0001f41b Bug\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Sub(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Sub, self).__init__()\r\n        self.weight = nn.Parameter(torch.randn(2))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, thing):\r\n        return self.weight + thing\r\n\r\n\r\nclass M(torch.jit.ScriptModule):\r\n    __constants__ = ['mods']\r\n\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.mods = nn.ModuleList([Sub() for i in range(10)])\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, v):\r\n        for m in self.mods:\r\n            v = m(v)\r\n        return v\r\n\r\n\r\nclass Wrap(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Wrap, self).__init__()\r\n        self.m = M()\r\n\r\n    def forward(self, x):\r\n        return self.m(x)\r\n\r\nw = Wrap()\r\ntorch.save(w, './test')\r\n```\r\n```\r\nTypeError: can't pickle M objects\r\n```\r\n\r\n## Expected behavior\r\n\r\nSuccessfully saving the model.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 410.73\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.0.3\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda92              2.4.0                         1    pytorch\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl-include               2019.1                      144\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] mkldnn                    0.14.0                        0    mingfeima\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n```\r\n## Additional context\r\n\r\nWhile `model.save(f)` works fine for standalone jit.ScriptModule, I cannot find a way to save the entire `torch.nn.Module` containing `torch.jit.ScriptModule`, and it seems like a bug that I cannot use `torch.save` for the case.\r\n","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Sub(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(Sub, self).__init__()\r\n        self.weight = nn.Parameter(torch.randn(2))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, thing):\r\n        return self.weight + thing\r\n\r\n\r\nclass M(torch.jit.ScriptModule):\r\n    __constants__ = ['mods']\r\n\r\n    def __init__(self):\r\n        super(M, self).__init__()\r\n        self.mods = nn.ModuleList([Sub() for i in range(10)])\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, v):\r\n        for m in self.mods:\r\n            v = m(v)\r\n        return v\r\n\r\n\r\nclass Wrap(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Wrap, self).__init__()\r\n        self.m = M()\r\n\r\n    def forward(self, x):\r\n        return self.m(x)\r\n\r\nw = Wrap()\r\ntorch.save(w, './test')\r\n"
15048,"[JIT] undefined symbol: cuCtxGetCurrent## \U0001f41b Bug\r\n\r\njit_script_method, cuda, and skip connection with /2 occurs this bug.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n```\r\n$ python test_jit.py\r\ncpu: tensor([[0.3313, 0.2135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0486, 0.0308, 0.6912,\r\n         0.5355, 0.4852, 0.0000, 0.1473, 0.5116, 0.8522, 0.2075, 0.6240, 0.0820,\r\n         0.3366, 0.1562, 0.0000, 0.0000, 0.0000, 0.2413, 0.0000, 0.2346, 0.0000,\r\n         0.0000, 0.0773, 0.0000, 0.0000, 0.3382]],\r\n       grad_fn=<DifferentiableGraphBackward>)\r\npython3: symbol lookup error: /home/qbx2/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1: undefined symbol: cuCtxGetCurrent\r\n```\r\n\r\n## Expected behavior\r\n\r\nGPU result is expected to be same with the CPU result.\r\n\r\n## Environment\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n/usr/local/cuda-10.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n\r\n\r\n## Additional context\r\nDisabled USE_NCCL in tools/setup_helpers/nccl.py while build to bypass compile errors.\r\n\r\nEDIT: Rebuilt with USE_NCCL enabled but did not help :/",needs reproduction|oncall: jit,orionr,"## \U0001f41b Bug\r\n\r\njit_script_method, cuda, and skip connection with /2 occurs this bug.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Block(torch.jit.ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n\r\n        self.linear = nn.Linear(dim, dim)\r\n        self.relu = nn.ReLU()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        # x = x + self.linear(x)\r\n        # x = x / 2.\r\n        # return self.relu(x)\r\n        return self.relu((self.linear(x) + x) / 2.)\r\n\r\n\r\nm = Block(32)\r\nx = torch.randn(1, 32)\r\nprint('cpu:', m(x))\r\n\r\nm.cuda()\r\nx = x.cuda()\r\nprint('gpu:', m(x))\r\n```\r\n\r\n```\r\n$ python test_jit.py\r\ncpu: tensor([[0.3313, 0.2135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0486, 0.0308, 0.6912,\r\n         0.5355, 0.4852, 0.0000, 0.1473, 0.5116, 0.8522, 0.2075, 0.6240, 0.0820,\r\n         0.3366, 0.1562, 0.0000, 0.0000, 0.0000, 0.2413, 0.0000, 0.2346, 0.0000,\r\n         0.0000, 0.0773, 0.0000, 0.0000, 0.3382]],\r\n       grad_fn=<DifferentiableGraphBackward>)\r\npython3: symbol lookup error: /home/qbx2/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch.so.1: undefined symbol: cuCtxGetCurrent\r\n```\r\n\r\n## Expected behavior\r\n\r\nGPU result is expected to be same with the CPU result.\r\n\r\n## Environment\r\nPyTorch version: 1.0.0a0+db5d313\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.48\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/lib64/libcudnn.so.7.4.1\r\n/usr/local/cuda-10.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] magma-cuda100             2.4.0                         1    pytorch\r\n[conda] torch                     1.0.0a0+db5d313           <pip>\r\n\r\n\r\n## Additional context\r\nDisabled USE_NCCL in tools/setup_helpers/nccl.py while build to bypass compile errors.\r\n\r\nEDIT: Rebuilt with USE_NCCL enabled but did not help :/","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Block(torch.jit.ScriptModule):\r\n    def __init__(self, dim):\r\n        super().__init__()\r\n\r\n        self.linear = nn.Linear(dim, dim)\r\n        self.relu = nn.ReLU()\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        # x = x + self.linear(x)\r\n        # x = x / 2.\r\n        # return self.relu(x)\r\n        return self.relu((self.linear(x) + x) / 2.)\r\n\r\n\r\nm = Block(32)\r\nx = torch.randn(1, 32)\r\nprint('cpu:', m(x))\r\n\r\nm.cuda()\r\nx = x.cuda()\r\nprint('gpu:', m(x))\r\n"
15043,"[JIT] jit.trace fails with custom GRUs and CUDA when the sequence is longer## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen `jit.trace` is applied to a custom GRU with CUDA enabled, the call fails when seq_len is longer than 200. \r\n\r\nWith CUDA, the call succeeds with smaller seq_len such as 100 or 120. \r\nWith CPU, the call succeeds even with seq_len >= 200.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. Set `cuda_id = 0 (or any device)` and `seq_len = 200` in the following code\r\n\r\n### Error message\r\n```\r\ntorch.jit.TracingCheckError: Tracing failed sanity checks!\r\nEncountered an exception while running the trace with test inputs.\r\nException:\r\n        default_program(28): Error: Formal parameter space overflowed (4096 bytes max) in function kernel_399\r\n```\r\n\r\n### Code\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n`jit.trace` call succeeds.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n",oncall: jit,gqchen,"## \U0001f41b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen `jit.trace` is applied to a custom GRU with CUDA enabled, the call fails when seq_len is longer than 200. \r\n\r\nWith CUDA, the call succeeds with smaller seq_len such as 100 or 120. \r\nWith CPU, the call succeeds even with seq_len >= 200.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1. Set `cuda_id = 0 (or any device)` and `seq_len = 200` in the following code\r\n\r\n### Error message\r\n```\r\ntorch.jit.TracingCheckError: Tracing failed sanity checks!\r\nEncountered an exception while running the trace with test inputs.\r\nException:\r\n        default_program(28): Error: Formal parameter space overflowed (4096 bytes max) in function kernel_399\r\n```\r\n\r\n### Code\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\nclass GRU(nn.Module):\r\n    ''' GRU\r\n    Reference\r\n    https://discuss.pytorch.org/t/implementation-of-multiplicative-lstm/2328/9\r\n    https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L46\r\n    '''\r\n\r\n    def __init__(self, input_size, hidden_size, seq_len, batch_first=True):\r\n        super(GRU, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n\r\n        self.seq_len = seq_len\r\n\r\n        self.input_weights = nn.Linear(input_size, 3 * hidden_size)\r\n        self.hidden_weights = nn.Linear(hidden_size, 3 * hidden_size)\r\n\r\n    def step(self, input, hidden):\r\n        hx = hidden\r\n        w_ih = self.input_weights(input)\r\n        w_hh = self.hidden_weights(hx)\r\n\r\n        i_r, i_c, i_u = w_ih.chunk(3, 1)\r\n        h_r, h_c, h_u = w_hh.chunk(3, 1)\r\n\r\n        updategate = torch.sigmoid(i_u+h_u)\r\n        resetgate = torch.sigmoid(i_r+h_r)\r\n\r\n        new_state = torch.tanh(i_c + resetgate * h_c)\r\n\r\n        hy = (1.-updategate)*hx + updategate*new_state\r\n\r\n        return hy\r\n\r\n    def forward(self, input, hidden):\r\n\r\n        if self.batch_first:\r\n            input = input.transpose(0, 1)\r\n\r\n        # Main loop\r\n        output = []\r\n        for i in range(self.seq_len):\r\n            hidden = self.step(input[i], hidden)\r\n            output.append(hidden)\r\n\r\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\r\n\r\n        if self.batch_first:\r\n            output = output.transpose(0, 1)\r\n\r\n        return output, hidden\r\n\r\n\r\nif __name__ == '__main__':\r\n    input_size = 48\r\n    hidden_size = 48\r\n    batch_size = 20\r\n\r\n    cuda_id = 0\r\n    # cuda_id = None\r\n\r\n    seq_len = 200\r\n    # seq_len = 100\r\n\r\n    gru = GRU(input_size, hidden_size, seq_len, batch_first=True)\r\n\r\n    input = torch.rand(batch_size, seq_len, input_size)\r\n    hidden = torch.rand(batch_size, hidden_size)\r\n\r\n    if cuda_id is not None:\r\n        torch.cuda.set_device(cuda_id)\r\n        gru = gru.cuda()\r\n        input = input.cuda()\r\n        hidden = hidden.cuda()\r\n\r\n    traced_gru = torch.jit.trace(gru, (input, hidden))\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n`jit.trace` call succeeds.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/lib64/libcudnn.so\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.3                      1\r\n[conda] mkl_fft                   1.0.6            py37h7dd41cf_0\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] pytorch                   1.0.0           py3.7_cuda9.0.176_cudnn7.4.1_1    pytorch\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n","python\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\nclass GRU(nn.Module):\r\n    ''' GRU\r\n    Reference\r\n    https://discuss.pytorch.org/t/implementation-of-multiplicative-lstm/2328/9\r\n    https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py#L46\r\n    '''\r\n\r\n    def __init__(self, input_size, hidden_size, seq_len, batch_first=True):\r\n        super(GRU, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.batch_first = batch_first\r\n\r\n        self.seq_len = seq_len\r\n\r\n        self.input_weights = nn.Linear(input_size, 3 * hidden_size)\r\n        self.hidden_weights = nn.Linear(hidden_size, 3 * hidden_size)\r\n\r\n    def step(self, input, hidden):\r\n        hx = hidden\r\n        w_ih = self.input_weights(input)\r\n        w_hh = self.hidden_weights(hx)\r\n\r\n        i_r, i_c, i_u = w_ih.chunk(3, 1)\r\n        h_r, h_c, h_u = w_hh.chunk(3, 1)\r\n\r\n        updategate = torch.sigmoid(i_u+h_u)\r\n        resetgate = torch.sigmoid(i_r+h_r)\r\n\r\n        new_state = torch.tanh(i_c + resetgate * h_c)\r\n\r\n        hy = (1.-updategate)*hx + updategate*new_state\r\n\r\n        return hy\r\n\r\n    def forward(self, input, hidden):\r\n\r\n        if self.batch_first:\r\n            input = input.transpose(0, 1)\r\n\r\n        # Main loop\r\n        output = []\r\n        for i in range(self.seq_len):\r\n            hidden = self.step(input[i], hidden)\r\n            output.append(hidden)\r\n\r\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\r\n\r\n        if self.batch_first:\r\n            output = output.transpose(0, 1)\r\n\r\n        return output, hidden\r\n\r\n\r\nif __name__ == '__main__':\r\n    input_size = 48\r\n    hidden_size = 48\r\n    batch_size = 20\r\n\r\n    cuda_id = 0\r\n    # cuda_id = None\r\n\r\n    seq_len = 200\r\n    # seq_len = 100\r\n\r\n    gru = GRU(input_size, hidden_size, seq_len, batch_first=True)\r\n\r\n    input = torch.rand(batch_size, seq_len, input_size)\r\n    hidden = torch.rand(batch_size, hidden_size)\r\n\r\n    if cuda_id is not None:\r\n        torch.cuda.set_device(cuda_id)\r\n        gru = gru.cuda()\r\n        input = input.cuda()\r\n        hidden = hidden.cuda()\r\n\r\n    traced_gru = torch.jit.trace(gru, (input, hidden))\r\n"
14992,"[JIT] Trace->Script + Inplace causes shapes to be fixed where they should not## \U0001f41b Bug\r\n\r\nWhen tracing a scripted function creating a tensor and performing inplace operations, the shape seems to be fixed where I would think it should not. (This used to work in mid November in the context of https://github.com/facebookresearch/maskrcnn-benchmark/pull/138/ .)\r\nBelow there are two script functions that only differ in `print` vs. `masked_scatter_`.\r\nIndexing seemed to also produce the bug.\r\n\r\n## To reproduce\r\n\r\nRun the following (sorry not quite minimal, but at least self contained):\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nNot ""constify"" the shape passed into `aten::zeros` in either traced function. In particular, \r\n```\r\n%210 : int[] = prim::Constant[value=[50, 3, 14, 14]]()\r\n```\r\nseems not so good.\r\n\r\n## Environment\r\n\r\nThis was on master from December 7.\r\n\r\n## Additional context\r\n\r\nI briefly tried to minimize the example further, but the error seemed to disappear when I do it too much.\r\n",oncall: jit|cherry-picked,suo,"## \U0001f41b Bug\r\n\r\nWhen tracing a scripted function creating a tensor and performing inplace operations, the shape seems to be fixed where I would think it should not. (This used to work in mid November in the context of https://github.com/facebookresearch/maskrcnn-benchmark/pull/138/ .)\r\nBelow there are two script functions that only differ in `print` vs. `masked_scatter_`.\r\nIndexing seemed to also produce the bug.\r\n\r\n## To reproduce\r\n\r\nRun the following (sorry not quite minimal, but at least self contained):\r\n\r\n```python\r\nimport torch\r\nfrom typing import List\r\n\r\n@torch.jit.script\r\ndef merge_levels(levels, unmerged_results: List[torch.Tensor]):\r\n    first_result = unmerged_results[0]\r\n    dtype, device = first_result.dtype, first_result.device\r\n    res = torch.zeros((levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3)),\r\n                      dtype=dtype, device=device)\r\n    for l in range(len(unmerged_results)):\r\n        mask = (levels == l).view(-1, 1, 1, 1).expand(levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3))\r\n        res.masked_scatter_(mask, unmerged_results[l])\r\n    return res\r\n\r\n@torch.jit.script\r\ndef merge_levels_not(levels, unmerged_results: List[torch.Tensor]):\r\n    first_result = unmerged_results[0]\r\n    dtype, device = first_result.dtype, first_result.device\r\n    res = torch.zeros((levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3)),\r\n                      dtype=dtype, device=device)\r\n    print (""inside merge level not######"", levels.shape, res.shape)\r\n    for l in range(len(unmerged_results)):\r\n        mask = (levels == l).view(-1, 1, 1, 1).expand(levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3))\r\n        print (""lnot#######"", l, res.shape, mask.shape, unmerged_results[l].shape)\r\n    return res\r\n\r\ndef myscript1(levels):\r\n    unmerged_results = []\r\n    for level in range(5):\r\n        idx_in_level = torch.nonzero(levels == level).view(-1, 1, 1, 1).expand(-1, 3, 14, 14)\r\n        unmerged_results.append(idx_in_level)\r\n    \r\n    res = merge_levels_not(levels, unmerged_results)\r\n    return res\r\n\r\ndef myscript2(levels):\r\n    unmerged_results = []\r\n    for level in range(5):\r\n        idx_in_level = torch.nonzero(levels == level).view(-1, 1, 1, 1).expand(-1, 3, 14, 14)\r\n        unmerged_results.append(idx_in_level)\r\n\r\n    res = merge_levels(levels, unmerged_results)\r\n    return res\r\n\r\nlevels = torch.randint(0,5, (50,))\r\n\r\nscr = torch.jit.trace(myscript1, (levels,))\r\nscr2 = torch.jit.trace(myscript2, (levels,))\r\nprint (scr.graph_for(levels))\r\nprint (scr2.graph_for(levels))\r\nlevels2 = torch.randint(0,5, (55,))\r\n\r\nscr(levels2)\r\nscr2(levels2)\r\n```\r\n\r\n## Expected behavior\r\n\r\nNot ""constify"" the shape passed into `aten::zeros` in either traced function. In particular, \r\n```\r\n%210 : int[] = prim::Constant[value=[50, 3, 14, 14]]()\r\n```\r\nseems not so good.\r\n\r\n## Environment\r\n\r\nThis was on master from December 7.\r\n\r\n## Additional context\r\n\r\nI briefly tried to minimize the example further, but the error seemed to disappear when I do it too much.\r\n","python\r\nimport torch\r\nfrom typing import List\r\n\r\n@torch.jit.script\r\ndef merge_levels(levels, unmerged_results: List[torch.Tensor]):\r\n    first_result = unmerged_results[0]\r\n    dtype, device = first_result.dtype, first_result.device\r\n    res = torch.zeros((levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3)),\r\n                      dtype=dtype, device=device)\r\n    for l in range(len(unmerged_results)):\r\n        mask = (levels == l).view(-1, 1, 1, 1).expand(levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3))\r\n        res.masked_scatter_(mask, unmerged_results[l])\r\n    return res\r\n\r\n@torch.jit.script\r\ndef merge_levels_not(levels, unmerged_results: List[torch.Tensor]):\r\n    first_result = unmerged_results[0]\r\n    dtype, device = first_result.dtype, first_result.device\r\n    res = torch.zeros((levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3)),\r\n                      dtype=dtype, device=device)\r\n    print (""inside merge level not######"", levels.shape, res.shape)\r\n    for l in range(len(unmerged_results)):\r\n        mask = (levels == l).view(-1, 1, 1, 1).expand(levels.size(0), first_result.size(1),\r\n                       first_result.size(2), first_result.size(3))\r\n        print (""lnot#######"", l, res.shape, mask.shape, unmerged_results[l].shape)\r\n    return res\r\n\r\ndef myscript1(levels):\r\n    unmerged_results = []\r\n    for level in range(5):\r\n        idx_in_level = torch.nonzero(levels == level).view(-1, 1, 1, 1).expand(-1, 3, 14, 14)\r\n        unmerged_results.append(idx_in_level)\r\n    \r\n    res = merge_levels_not(levels, unmerged_results)\r\n    return res\r\n\r\ndef myscript2(levels):\r\n    unmerged_results = []\r\n    for level in range(5):\r\n        idx_in_level = torch.nonzero(levels == level).view(-1, 1, 1, 1).expand(-1, 3, 14, 14)\r\n        unmerged_results.append(idx_in_level)\r\n\r\n    res = merge_levels(levels, unmerged_results)\r\n    return res\r\n\r\nlevels = torch.randint(0,5, (50,))\r\n\r\nscr = torch.jit.trace(myscript1, (levels,))\r\nscr2 = torch.jit.trace(myscript2, (levels,))\r\nprint (scr.graph_for(levels))\r\nprint (scr2.graph_for(levels))\r\nlevels2 = torch.randint(0,5, (55,))\r\n\r\nscr(levels2)\r\nscr2(levels2)\r\n"
14848,"torch.nonzero slower than np.nonzero ## \U0001f41b Bug\r\n\r\n`torch.nonzero` is slower than `np.nonzero`. \r\n\r\nObject detection libraries such as [maskrcnn_benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) heavily use this function in order to select the proposals, which might decrease inference time. Also, critical parts of Pytorch, such as [indexing](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Indexing.cpp), rely on `torch.nonzero`.\r\n\r\n## To Reproduce\r\n\r\n1D tensor of size 512\r\n\r\n\r\nND tensor\r\n\r\n## Expected behavior\r\n\r\nCPU implementation of `torch.nonzero` should have a similar performance than `np.nonzero`, while GPU implementation should be faster (at least for high dimensional tensors)\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181024\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.3.2)\r\n[pip] numpy (1.15.4)\r\n[pip] pytorch-ignite (0.1.0)\r\n[pip] torch (1.0.0.dev20181024)\r\n[pip] torchaudio (0.1)\r\n[pip] torchtext (0.3.1)\r\n[pip] torchvision (0.2.1)\r\n[pip] torchvision-nightly (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181024 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n[conda] torchvision-nightly       0.2.1                     <pip>\r\n```\r\n",module: performance|module: bootcamp|triaged,glaringlee,"## \U0001f41b Bug\r\n\r\n`torch.nonzero` is slower than `np.nonzero`. \r\n\r\nObject detection libraries such as [maskrcnn_benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) heavily use this function in order to select the proposals, which might decrease inference time. Also, critical parts of Pytorch, such as [indexing](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Indexing.cpp), rely on `torch.nonzero`.\r\n\r\n## To Reproduce\r\n\r\n1D tensor of size 512\r\n```python\r\nimport numpy as np\r\nimport torch\r\ndata = np.random.randn(512)\r\nt_data = torch.as_tensor(data)\r\nct_data = torch.as_tensor(data, device='cuda')\r\n\r\n%timeit np.nonzero(data)\r\n4.02 \xb5s \xb1 54 ns per loop (mean \xb1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n%timeit torch.nonzero(t_data)\r\n23.7 \xb5s \xb1 269 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n%timeit torch.nonzero(ct_data)                                                                                                                                                                                                        \r\n31.6 \xb5s \xb1 148 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n```\r\n\r\nND tensor\r\n```python\r\nimport numpy as np\r\nimport torch\r\ndata = np.random.randn(16, 3, 512)\r\nt_data = torch.as_tensor(data)\r\nct_data = torch.as_tensor(data, device='cuda')\r\n\r\n%timeit np.nonzero(data)\r\n270 \xb5s \xb1 2.58 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit torch.nonzero(t_data)\r\n3.09 ms \xb1 181 \xb5s per loop (mean \xb1 std. dev. of 7 runs, 100 loops each)\r\n\r\n%timeit torch.nonzero(ct_data)                                                                                                                                                                                                        \r\n38.9 \xb5s \xb1 348 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n```\r\n## Expected behavior\r\n\r\nCPU implementation of `torch.nonzero` should have a similar performance than `np.nonzero`, while GPU implementation should be faster (at least for high dimensional tensors)\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.0.0.dev20181024\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.3.2)\r\n[pip] numpy (1.15.4)\r\n[pip] pytorch-ignite (0.1.0)\r\n[pip] torch (1.0.0.dev20181024)\r\n[pip] torchaudio (0.1)\r\n[pip] torchtext (0.3.1)\r\n[pip] torchvision (0.2.1)\r\n[pip] torchvision-nightly (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181024 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchtext                 0.3.1                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n[conda] torchvision-nightly       0.2.1                     <pip>\r\n```\r\n","python\r\nimport numpy as np\r\nimport torch\r\ndata = np.random.randn(512)\r\nt_data = torch.as_tensor(data)\r\nct_data = torch.as_tensor(data, device='cuda')\r\n\r\n%timeit np.nonzero(data)\r\n4.02 \xb5s \xb1 54 ns per loop (mean \xb1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n%timeit torch.nonzero(t_data)\r\n23.7 \xb5s \xb1 269 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n%timeit torch.nonzero(ct_data)                                                                                                                                                                                                        \r\n31.6 \xb5s \xb1 148 ns per loop (mean \xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n"
14344,"Unable to print half tensors## \U0001f41b Bug\r\n\r\nEarlier, a bug was reported (https://github.com/pytorch/pytorch/issues/12093) about not being able to print large half Tensors. Now, half Tensors cannot be printed at all.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nTensors should be printed, at least small ones.",high priority,fmassa,"## \U0001f41b Bug\r\n\r\nEarlier, a bug was reported (https://github.com/pytorch/pytorch/issues/12093) about not being able to print large half Tensors. Now, half Tensors cannot be printed at all.\r\n\r\n## To Reproduce\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.randn(1)\r\n>>> print(a.half())\r\nRuntimeError: s__th_eq is not implemented for type torch.HalfTensor\r\n```\r\n\r\n## Expected behavior\r\n\r\nTensors should be printed, at least small ones.",python\r\n>>> import torch\r\n>>> a = torch.randn(1)\r\n>>> print(a.half())\r\nRuntimeError: s__th_eq is not implemented for type torch.HalfTensor\r\n
14078,"Build failing at torch/lib/c10d/ProcessGroupMPI.cpp## \U0001f41b Bug\r\n\r\nPyTorch fails to finish building, with a possible bug (see below).\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): Master branch (1.0) \r\n - OS (e.g., Linux): Debian:Stretch\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): see above\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",oncall: distributed,teng-li,"## \U0001f41b Bug\r\n\r\nPyTorch fails to finish building, with a possible bug (see below).\r\n\r\n```bash \r\nScanning dependencies of target caffe2_observers\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/net_observer_reporter_print.cc.o\r\n\x1b[91m/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp: In destructor \u2018virtual c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()\u2019:\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: error: throw will always call terminate() [-Werror=terminate]\r\n         ""Attempted destruction of AsyncWork before work has completed"");\r\n                                                                       ^\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: note: in C++11 destructors default to noexcept\r\n\x1b[0m[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/function.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/accumulate_grad.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/ps_roi_pool_op.cc.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/observer_config.cc.o\r\n\x1b[91mcc1plus: all warnings being treated as errors\r\n\x1b[0m\x1b[91mmake[2]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o] Error 1\r\n\x1b[0m\x1b[91mmake[2]: *** Waiting for unfinished jobs....\r\n\x1b[0mcaffe2/torch/lib/c10d/CMakeFiles/c10d.dir/build.make:230: recipe for target 'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o' failed\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/basic_ops.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethod.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/perf_observer.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/roi_pool_f_op.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sample_as_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/comm.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/select_smooth_l1_loss_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/tensor.cpp.o\r\nCMakeFiles/Makefile2:7496: recipe for target 'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all' failed\r\n\x1b[91mmake[1]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all] Error 2\r\n\x1b[0m\x1b[91mmake[1]: *** Waiting for unfinished jobs....\r\n...\r\n...\r\n...\r\n[ 99%] Linking CXX shared library ../../lib/libtorch.so\r\n[ 99%] Built target torch\r\nMakefile:138: recipe for target 'all' failed\r\n\x1b[91mmake: *** [all] Error 2\r\n\x1b[0msetup.py::build_deps::run()\r\nFailed to run 'bash ../tools/build_pytorch_libs.sh --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n```\r\n\r\n## To Reproduce\r\n```bash \r\n cd /opt && git clone --recursive https://github.com/pytorch/pytorch \\\r\n    && cd pytorch && git submodule update --init && \\\r\n    cd /opt/pytorch/third_party/ideep/mkl-dnn && \\\r\n    git pull https://github.com/intel/mkl-dnn.git --no-commit  --rebase && \\\r\n    cd /opt/pytorch && \\\r\n    sed -i 's/""Use MKLDNN"" OFF/""Use MKLDNN"" ON /g' CMakeLists.txt && \\\r\n    sed -i 's/""Use DISTRIBUTED"" OFF/""Use DISTRIBUTED"" ON /g' CMakeLists.txt && \\\r\n    sed -i 's/for parallel code"" OFF/for parallel code"" ON /g' CMakeLists.txt && \\\r\n    PYTHON_EXECUTABLE=/opt/conda/bin/python \\\r\n    PYTHON_LIBRARY=/opt/conda/lib/libpython3.6m.so \\\r\n    PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m \\\r\n    FULL_CAFFE2=1 \\\r\n    USE_OPENMP=1 \\\r\n    USE_MKL=1 \\\r\n    USE_MKLDNN=1 \\\r\n    USE_MKLML=1 \\\r\n    USE_SYSTEM_EIGEN_INSTALL=1 \\\r\n    USE_ZMQ=1 \\\r\n    USE_DISTRIBUTED=1 \\\r\n    MKLDNN_LIBRARY=/usr/local/lib \\\r\n    MKLDNN_INCLUDE_DIR=/usr/local/include \\\r\n    MKLDNN_LIB_DIR=/usr/local/lib \\\r\n    python setup.py install && \\\r\n    cd /opt && rm -rf /opt/pytorch && \\\r\n    cd /usr/lib && sudo ldconfig\r\n```\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): Master branch (1.0) \r\n - OS (e.g., Linux): Debian:Stretch\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): see above\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n","bash \r\nScanning dependencies of target caffe2_observers\r\n[ 92%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/data_channels/DataChannelTCP.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/net_observer_reporter_print.cc.o\r\n\x1b[91m/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp: In destructor \u2018virtual c10d::ProcessGroupMPI::AsyncWork::~AsyncWork()\u2019:\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: error: throw will always call terminate() [-Werror=terminate]\r\n         ""Attempted destruction of AsyncWork before work has completed"");\r\n                                                                       ^\r\n/opt/pytorch/torch/lib/c10d/ProcessGroupMPI.cpp:154:71: note: in C++11 destructors default to noexcept\r\n\x1b[0m[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/function.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/accumulate_grad.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/ps_roi_pool_op.cc.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/observer_config.cc.o\r\n\x1b[91mcc1plus: all warnings being treated as errors\r\n\x1b[0m\x1b[91mmake[2]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o] Error 1\r\n\x1b[0m\x1b[91mmake[2]: *** Waiting for unfinished jobs....\r\n\x1b[0mcaffe2/torch/lib/c10d/CMakeFiles/c10d.dir/build.make:230: recipe for target 'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/ProcessGroupMPI.cpp.o' failed\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/basic_ops.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethod.cpp.o\r\n[ 93%] Building CXX object modules/observers/CMakeFiles/caffe2_observers.dir/perf_observer.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/roi_pool_f_op.cc.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/sample_as_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/lib/THD/CMakeFiles/THD.dir/base/init_methods/InitMethodEnv.cpp.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/comm.cpp.o\r\n[ 93%] Building CXX object modules/detectron/CMakeFiles/caffe2_detectron_ops.dir/select_smooth_l1_loss_op.cc.o\r\n[ 93%] Building CXX object caffe2/torch/CMakeFiles/torch.dir/csrc/autograd/functions/tensor.cpp.o\r\nCMakeFiles/Makefile2:7496: recipe for target 'caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all' failed\r\n\x1b[91mmake[1]: *** [caffe2/torch/lib/c10d/CMakeFiles/c10d.dir/all] Error 2\r\n\x1b[0m\x1b[91mmake[1]: *** Waiting for unfinished jobs....\r\n...\r\n...\r\n...\r\n[ 99%] Linking CXX shared library ../../lib/libtorch.so\r\n[ 99%] Built target torch\r\nMakefile:138: recipe for target 'all' failed\r\n\x1b[91mmake: *** [all] Error 2\r\n\x1b[0msetup.py::build_deps::run()\r\nFailed to run 'bash ../tools/build_pytorch_libs.sh --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'\r\n"
14057,"Unable to pickle torch dtype objects in Python 3.5## \U0001f41b Bug\r\n\r\nWhen pickling a `torch.dtype` object, Python 3.5 reports an obscure error ""can't pickle int objects"".\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nIn Python 3.6 one can pickle torch dtypes successfully.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.2.1 20181011 (Red Hat 8.2.1-4)\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 410.73\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (0.4.1.post2)\r\n[conda] pytorch                   0.4.1           py35_py27__9.0.176_7.1.2_2    pytorch\r\n```",high priority,ailzhang,"## \U0001f41b Bug\r\n\r\nWhen pickling a `torch.dtype` object, Python 3.5 reports an obscure error ""can't pickle int objects"".\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: import pickle\r\n\r\nIn [3]: with open('/tmp/a', 'wb') as f:\r\n   ...:     pickle.dump(torch.float32, f)\r\n   ...:     \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-769b4901f38c> in <module>()\r\n      1 with open('/tmp/a', 'wb') as f:\r\n----> 2     pickle.dump(torch.float32, f)\r\n      3 \r\n\r\n~/anaconda3/envs/tmp/lib/python3.5/copyreg.py in _reduce_ex(self, proto)\r\n     63     else:\r\n     64         if base is self.__class__:\r\n---> 65             raise TypeError(""can't pickle %s objects"" % base.__name__)\r\n     66         state = base(self)\r\n     67     args = (self.__class__, base, state)\r\n\r\nTypeError: can't pickle int objects\r\n```\r\n\r\n## Expected behavior\r\n\r\nIn Python 3.6 one can pickle torch dtypes successfully.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Fedora release 29 (Twenty Nine)\r\nGCC version: (GCC) 8.2.1 20181011 (Red Hat 8.2.1-4)\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 410.73\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (0.4.1.post2)\r\n[conda] pytorch                   0.4.1           py35_py27__9.0.176_7.1.2_2    pytorch\r\n```","python\r\nIn [1]: import torch\r\n\r\nIn [2]: import pickle\r\n\r\nIn [3]: with open('/tmp/a', 'wb') as f:\r\n   ...:     pickle.dump(torch.float32, f)\r\n   ...:     \r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-769b4901f38c> in <module>()\r\n      1 with open('/tmp/a', 'wb') as f:\r\n----> 2     pickle.dump(torch.float32, f)\r\n      3 \r\n\r\n~/anaconda3/envs/tmp/lib/python3.5/copyreg.py in _reduce_ex(self, proto)\r\n     63     else:\r\n     64         if base is self.__class__:\r\n---> 65             raise TypeError(""can't pickle %s objects"" % base.__name__)\r\n     66         state = base(self)\r\n     67     args = (self.__class__, base, state)\r\n\r\nTypeError: can't pickle int objects\r\n"
14033,"[Torchscript/C++] cuDNN linking issue with libtorch binaries causes model slowdownHi folks! I'm converting my Pytorch model on C++ via Torchscipt. But when I tested the inference time, it turned out that my C++ model is two times slower than my original Python model.\r\nWhat could be the reason for this behavior?  Or am I mistaken somewhere?\r\n\r\nUbuntu 16.04, CUDA: 9.1, cuDNN: 7.1.3\r\nNet is a Wide ResNet 50-2.\r\n\r\nConverting in Torchscript code:\r\n\r\n\r\nPython inference time test code:\r\n\r\n\r\nC++ inference time test code:\r\n\r\n\r\nAs a result I got 25 msec for Python and 51 msec for C++.",high priority|oncall: jit|module: build|cherry-picked,zou3519,"Hi folks! I'm converting my Pytorch model on C++ via Torchscipt. But when I tested the inference time, it turned out that my C++ model is two times slower than my original Python model.\r\nWhat could be the reason for this behavior?  Or am I mistaken somewhere?\r\n\r\nUbuntu 16.04, CUDA: 9.1, cuDNN: 7.1.3\r\nNet is a Wide ResNet 50-2.\r\n\r\nConverting in Torchscript code:\r\n```python\r\nnet = MyNet()\r\nnet.load_state_dict('model.pth')\r\nnet.train(False)\r\nexample = torch.rand(1, 3, 224, 224)\r\n\r\nwith torch.no_grad():\r\n    traced_script_module = torch.jit.trace(net, example)\r\n\r\ntraced_script_module.save('model.pt')\r\n```\r\n\r\nPython inference time test code:\r\n```python\r\nnet = MyNet()\r\nnet.load_state_dict('model.pth')\r\nnet.train(False)\r\nnet.cuda()\r\n\r\ntest_number = 1010\r\nbatch_num = 8\r\nskip_numbers = 10\r\n\r\nbatch = torch.randn(batch_num, 3, 224, 224).cuda()\r\n\r\ntimes = 0\r\n\r\nfor i in range(test_number):\r\n    start = int(round(time.time() * 1000))\r\n    with torch.no_grad():\r\n        _, __ = net(batch)\r\n    torch.cuda.synchronize()\r\n    end = int(round(time.time() * 1000))\r\n    \r\n    elapsed_mseconds = end - start\r\n\r\n    if i > skip_numbers:\r\n        times += elapsed_mseconds\r\n\r\nprint('Mean time: {} msec'.format(times / (test_number - skip_numbers)))\r\n```\r\n\r\nC++ inference time test code:\r\n```C++\r\n#include <torch/torch.h>\r\n#include <torch/script.h>\r\n\r\n#include <ATen/ATen.h>\r\n\r\n#include <iostream>\r\n#include <memory>\r\n#include <chrono>\r\n#include <ctime>\r\n\r\nvoid time_test(const char* model_name) {\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\r\n    module->to(at::kCUDA);\r\n\r\n    int batch_num = 8;\r\n    int test_number = 1010;\r\n    int skip_numbers = 10;\r\n\r\n    std::vector<torch::jit::IValue> batch;\r\n    batch.push_back(torch::randn({batch_num, 3, 224, 224}).to(at::kCUDA));\r\n\r\n    std::chrono::time_point<std::chrono::system_clock> start, end;\r\n\r\n    int times = 0;\r\n\r\n    for (int j = 0; j < test_number; ++j) {\r\n        start = std::chrono::system_clock::now();\r\n        auto output = module->forward(batch);\r\n\r\n        // synchronizing\r\n        auto out_elems = output.toTuple()->elements();\r\n        auto embed = out_elems[1].toTensor();\r\n        embed.to(at::kCPU);\r\n\r\n        end = std::chrono::system_clock::now();\r\n\r\n        long elapsed_mseconds = std::chrono::duration_cast<std::chrono::milliseconds>(end-start).count();\r\n\r\n        if (j > skip_numbers) {\r\n            times += elapsed_mseconds;\r\n        }\r\n    }\r\n\r\n    std::cout << ""Mean time: "" << times / (test_number - skip_numbers) << "" msec"" << '\\n';\r\n}\r\n\r\nint main(int argc, const char* argv[]) {\r\n    time_test(argv[1]);\r\n}\r\n```\r\n\r\nAs a result I got 25 msec for Python and 51 msec for C++.","python\r\nnet = MyNet()\r\nnet.load_state_dict('model.pth')\r\nnet.train(False)\r\nexample = torch.rand(1, 3, 224, 224)\r\n\r\nwith torch.no_grad():\r\n    traced_script_module = torch.jit.trace(net, example)\r\n\r\ntraced_script_module.save('model.pt')\r\n"
14026,"[JIT] TestJit.test_lstm_fusion_concat_cuda is flaky## \U0001f41b Bug\r\n\r\nPutting a loop with manual seed 0..99 aound in the test tells me it fails for 9 of those 100 seeds.\r\n\r\n## To Reproduce\r\n\r\nOn recent master, amend the test like this:\r\n\r\nand run.\r\n\r\n## Expected behavior\r\n\r\nNot to produce pesky CI bugs on my pull requests.\r\n\r\n## Environment\r\n\r\nrecent master\r\n\r\n## Additional context\r\n\r\nSeen on CI at #13985 , fixed there by renaming the test to go after the flaky one.\r\n",oncall: jit,t-vi,"## \U0001f41b Bug\r\n\r\nPutting a loop with manual seed 0..99 aound in the test tells me it fails for 9 of those 100 seeds.\r\n\r\n## To Reproduce\r\n\r\nOn recent master, amend the test like this:\r\n```python\r\ndiff --git a/test/test_jit.py b/test/test_jit.py\r\nindex a6fd537af..3ae4abe05 100644\r\n--- a/test/test_jit.py\r\n+++ b/test/test_jit.py\r\n@@ -688,9 +688,18 @@ class TestJit(JitTestCase):\r\n     @unittest.skipIf(not RUN_CUDA, ""fuser requires CUDA"")\r\n     @skipIfRocm\r\n     def test_lstm_fusion_concat_cuda(self):\r\n-        inputs = get_lstm_inputs('cuda')\r\n-        ge = self.checkTrace(LSTMCellC, inputs)\r\n-        self.assertExpectedGraph(ge.graph_for(*inputs))\r\n+        fails = 0\r\n+        for i in range(100):\r\n+            print ('\\r',i,'   ', end='')\r\n+            sys.stdout.flush()\r\n+            torch.manual_seed(i)\r\n+            try:\r\n+                inputs = get_lstm_inputs('cuda')\r\n+                ge = self.checkTrace(LSTMCellC, inputs)\r\n+                self.assertExpectedGraph(ge.graph_for(*inputs))\r\n+            except:\r\n+                fails += 1\r\n+        print(""failures"", fails)\r\n \r\n     @unittest.skipIf(IS_WINDOWS, ""NYI: fuser support for Windows"")\r\n     @unittest.skipIf(not RUN_CUDA, ""fuser requires CUDA"")\r\n```\r\nand run.\r\n\r\n## Expected behavior\r\n\r\nNot to produce pesky CI bugs on my pull requests.\r\n\r\n## Environment\r\n\r\nrecent master\r\n\r\n## Additional context\r\n\r\nSeen on CI at #13985 , fixed there by renaming the test to go after the flaky one.\r\n","python\r\ndiff --git a/test/test_jit.py b/test/test_jit.py\r\nindex a6fd537af..3ae4abe05 100644\r\n--- a/test/test_jit.py\r\n+++ b/test/test_jit.py\r\n@@ -688,9 +688,18 @@ class TestJit(JitTestCase):\r\n     @unittest.skipIf(not RUN_CUDA, ""fuser requires CUDA"")\r\n     @skipIfRocm\r\n     def test_lstm_fusion_concat_cuda(self):\r\n-        inputs = get_lstm_inputs('cuda')\r\n-        ge = self.checkTrace(LSTMCellC, inputs)\r\n-        self.assertExpectedGraph(ge.graph_for(*inputs))\r\n+        fails = 0\r\n+        for i in range(100):\r\n+            print ('\\r',i,'   ', end='')\r\n+            sys.stdout.flush()\r\n+            torch.manual_seed(i)\r\n+            try:\r\n+                inputs = get_lstm_inputs('cuda')\r\n+                ge = self.checkTrace(LSTMCellC, inputs)\r\n+                self.assertExpectedGraph(ge.graph_for(*inputs))\r\n+            except:\r\n+                fails += 1\r\n+        print(""failures"", fails)\r\n \r\n     @unittest.skipIf(IS_WINDOWS, ""NYI: fuser support for Windows"")\r\n     @unittest.skipIf(not RUN_CUDA, ""fuser requires CUDA"")\r\n"
13971,[JIT] fusing abs doesn't work on cuda## \U0001f41b Bug\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nWork\r\n\r\n## Environment\r\n\r\nTodayish master.\r\n\r\nI'll go fix it.,oncall: jit,t-vi,"## \U0001f41b Bug\r\n\r\n```python\r\n@torch.jit.script\r\ndef x(a):\r\n     return a.abs()*2\r\nx(torch.randn(5, device='cuda'))\r\n```\r\n\r\n## Expected behavior\r\n\r\nWork\r\n\r\n## Environment\r\n\r\nTodayish master.\r\n\r\nI'll go fix it.","python\r\n@torch.jit.script\r\ndef x(a):\r\n     return a.abs()*2\r\nx(torch.randn(5, device='cuda'))\r\n"
13638,"Variable/Tensor Merge Proposal## \U0001f680 High-level changes:\r\n1. **IMPORTANT**: Both `Variable` and `Variable::Impl` are removed, and `at::Tensor` is always the tensor that's passed around in PyTorch, and it can record autograd history when its autograd metadata (`AutogradMeta`) is not null.\r\n2. **IMPORTANT**: Autograd-related function implementations in Variable will be moved to VariableType.\r\n3. Autograd metadata now lives in an `AutogradMeta` struct that `TensorImpl` has a pointer to, and the `AutogradMeta` is *only* populated when the `at::Tensor` requires gradient.\r\n4. We decide whether to dispatch to VariableType / non-VariableType functions using the `at::AutoNonVariableTypeMode` in appropriate places internally. (We only dispatch to VariableType functions if we need profiling/JIT-tracing/autograd)\r\n5. Common Tensor functions (e.g. `numel()`\xa0/\xa0`sizes()`\xa0/\xa0`dim()`) are de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.\r\n6. `tensor.is_variable()` and `options.is_variable()` always return true, because every `at::Tensor` is a variable (and can record autograd history when its `AutogradMeta` is not null). (We keep `options.is_variable(...)` for backward compatibility, and raise warning if it's set to false.)\r\n7. API behavior change: changing shape/storage on `tensor.data` in Python or `tensor.data()` in C++ will no longer update `tensor`.\r\n\r\n## Pitch\r\n\r\nCurrently, the distinction between `at::Tensor` and `Variable` (subclass of `at::Tensor` that contains autograd metadata and functions) creates unnecessary cognitive overhead for PyTorch core development. We want to remove this distinction and make it possible to use `at::Tensor` everywhere in PyTorch. After merging `Variable` into `at::Tensor`, here are the common end-user APIs:\r\n\r\n- **When C++ user wants to create a non-history-recording `at::Tensor` from another `at::Tensor`:**\r\nCurrent API (unchanged):\r\n\r\nWhen the user calls `t.detach()`, we do the following under the hood:\r\n1. We do the shallow copy of `t`'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. `size` / `stride`).\r\n    - Note that subclasses of TensorImpl (e.g. `SparseTensorImpl`) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own `shallow_copy_and_detach()` function (by making the `shallow_copy_and_detach()` function virtual in TensorImpl and overriding it in TensorImpl subclasses).\r\n2. We set the `AutogradMeta` pointer to NULL, to indicate that it doesn't need to record history.\r\n3. We return an at::Tensor that wraps the new TensorImpl.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to enable/disable history-recording for an `at::Tensor`:**\r\nProposed API:\r\n\r\nWhen the user calls `t.requires_grad_(true)`, we do the following under the hood:\r\n1. We initialize a struct called `AutogradMeta`, which stores autograd-specific fields (such as `grad_`/`grad_fn_`/`grad_accumulator_`).\r\n2. We assign the struct to the `AutogradMeta` pointer in `t`'s TensorImpl.\r\n\r\nWhen the user calls `t.requires_grad_(false)`, we do the following under the hood:\r\n1. We set the `AutogradMeta` pointer in `t`'s TensorImpl to NULL.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to call non-Variable operations on an `at::Tensor` when dispatching through `type()`**\r\nProposed API:\r\n\r\nUnder the hood, `type()` checks whether the `at::AutoNonVariableTypeMode` thread-local guard is enabled when determining the type of the variable.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to change content of an `at::Tensor` that has AutogradMeta, without affecting the tensor's `grad_fn` or `version_counter_`**\r\nProposed behavior:\r\n\r\n\r\n\r\n## Motivation\r\n\r\n- **Overly Complex OOP design**: Currently the distinction between `Variable` and `Tensor` is hard to grasp: `Variable::Impl` is a subclass of TensorImpl, but it also has an `at::Tensor` data member which internally wraps another TensorImpl. This co-existence of ""is-a"" and ""has-a"" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in `Variable::Impl`, and which functions are applicable to Tensor vs. Variable (e.g. `is_wrapped_number()` is only valid on Tensor, not Variable) (for more context, also see note: [We regret making Variable hold a Tensor](https://github.com/pytorch/pytorch/blob/b6a8c45f57b65d11894c4a6e5a3267708ecec1c5/c10/core/TensorImpl.h#L470-L489)). Ideally, we want to use the same tensor type everywhere in PyTorch code.\r\n\r\n- **Unused data members in `Variable::Impl` take up cache/memory space**: Since `Variable::Impl` is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as `sizes_` / `strides_` / etc.). However, the `Variable::Impl` functions always call into the underlying `at::Tensor` and ignores the rest of the fields, which causes a lot of wasted cache/memory space.\r\n\r\n- **Virtual functions are slow**: We care about how much time it takes to execute common Tensor functions such as `numel()` / `sizes()` / `dim()`. Currently, these functions are `virtual` in TensorImpl, so that `Variable::Impl` (a subclass of TensorImpl) can override them and dispatch those calls to the `Variable::Impl`'s underlying `at::Tensor`. Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):\r\n\r\nBenchmark | Time (no flush) | Time (flush L1) | Time (flush L1+L2) | Time (flush L1+L2+L3)\r\n-- | -- | -- | -- | --\r\nTensor.dim() - non-virtual | 1.3 | 3.33 | 7.6 | 58\r\nVariable.dim() - virtual | 4.5 | 24.4 | 52 | 173.67\r\n**Runtime Savings** | **-71.11111%** | **-86.35246%** | **-85.38462%** | **-66.60333%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.numel() - non-virtual | 22.6 | 63.89 | 109.22 | 294.5\r\nVariable.numel() - virtual | 80.33 | 133.1 | 192 | 810.9\r\n**Runtime Savings** | **-71.86605%** | **-51.9985%** | **-43.11458%** | **-63.68233%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.size(0) - non-virtual | 30.4 | 60.1 | 100.44 | 384.3\r\nVariable.size(0) - virtual | 75.4 | 127.67 | 203.8 | 875.9\r\n**Runtime Savings** | **-59.6817%** | **-52.92551%** | **-50.71639%** | **-56.12513%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.sizes() - non-virtual | 2 | 4.25 | 13.25 | 67.6\r\nVariable.sizes() - virtual | 5.2 | 28.44 | 62.1 | 254.78\r\n**Runtime Savings** | **-61.53846%** | **-85.05626%** | **-78.66345%** | **-73.46731%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.resize_({0}) no-op - non-virtual | 23.11 | 86.44 | 105.44 | 332.33\r\nVariable.resize_({0}) no-op - virtual | 168.4 | 254.22 | 348.56 | 890.9\r\n**Runtime Savings** | **-86.27672%** | **-65.99795%** | **-69.74983%** | **-62.69727%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.resize_({64, 2048}) no-op - non-virtual | 33.4 | 102.56 | 129.56 | 407.22\r\nVariable.resize_({64, 2048}) no-op - virtual | 193 | 278.1 | 364.9 | 936.6\r\n**Runtime Savings** | **-82.6943%** | **-63.12118%** | **-64.49438%** | **-56.52146%**\r\n\r\n> Benchmarked commit: https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a\r\n> Benchmark script: https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\r\n> Non-virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:nonvirtual_tensorimpl\r\n> Virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:virtual_tensorimpl\r\n\r\nBased on our current implementation, the runtime difference for `dim()`, `numel()`, `size()`, `sizes()`, and no-op `resize()` comes from the virtual function call overhead and the `at::Tensor` data member indirection in `Variable::Impl`. If we de-virtualize those functions, we would be able to cut the runtime by **43%-86%** on the most common Tensor functions.\r\n\r\n\r\n\r\n## Breaking changes\r\n\r\nNote that this change will break the current API in the following way:\r\n\r\nIn the old world, whenever we want to create a `Variable` that shares the same data with another `Variable`, we simply do `auto var_new = make_variable(var.data())` or `auto var_new = var.detach()`, and any shape / data / storage pointer changes to `var_new` will be reflected in `var` automatically, because internally they share the same underlying `at::Tensor`.\r\n\r\nHowever, in the new world, there is no concept of the ""underlying `at::Tensor`"" of a Variable, since the Variable itself is the Tensor. When we want to create an `at::Tensor` that shares the same data with another `at::Tensor`, we can still call `auto t_new = t.detach()`, but in this case, only the tensor storage data is shared (via ref-counted pointer) between `t_new` and `t`, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Tensor (`t_new`) that are not bits inside tensor storage won't update the original Tensor (`t`), and we should no longer expect those data to be shared.\r\n\r\nThis has implications for Python call sites that do\r\n\r\nor\r\n\r\nIf `in_place_operation_()` only updates the data inside the tensor (such as `zeros_()`), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. `resize_` / `resize_as_` / `set_` / `transpose_`), such operation on `tensor.data` or `tensor_detached` will no longer update the `tensor`. We will address this inconsistency in the following ways:\r\n\r\n1. Add an `allow_tensor_metadata_change_` flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n\r\n\r\n\r\n## Finished changes\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/13827)\r\n1. Add a flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n3. Move `Variable::Impl` data members into TensorImpl as `AutogradMeta` struct\r\n4. Change `Variable::Impl` functions to use data members in `AutogradMeta` struct\r\n5. Add `shallow_copy()` function to each subclass of TensorImpl\r\n6. Do shallow copy when the user calls `make_variable(tensor)` / `variable.detach()` (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from `variable.detach()` we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)\r\n7. Add `AutogradMetaInterface` class, and make `AutogradMeta` a subclass of it, so that we can make `autograd_meta_` a unique_ptr in TensorImpl\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15487)\r\n1. Move `set_requires_grad()` / `requires_grad()` / `grad()` from `Variable::Impl` to `AutogradMeta`\r\n2. Move `Variable::Impl` functions such as `backward()` / `rebase_history()` / `grad_accumulator()` / `grad_fn()` out of `Variable::Impl` and into `AutogradMeta`.\r\n3. Note: we need to make these changes so that we can remove `Variable::Impl` class in the next PR.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15939)\r\n1. Add thread-local guard (`at::AutoNonVariableTypeMode`) to make sure that in VariableType.cpp the operations on baseType still dispatch to non-Variable type, even if the parameters are now Variables\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16305)\r\n1. Make `gesv_out` return the original input tensor instead of a new tensor (currently by copying the result tensor into the original input tensor, because a true in-place `gesv` is more difficult to implement. NOTE: also open an issue for this).\r\n2. In VariableType.cpp, after each in-place function on the ""unpacked"" tensor, check pointer address equality for storage in the original input variable's TensorImpl (check this for all arguments in `unpacked_args`)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16325)\r\n1. Remove `.type()` calls as much as possible, to reduce the need of using the `at::AutoNonVariableTypeMode` guard\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16596)\r\n1. Make JIT attributes `t_` and `ts_` store Variable instead of Tensor (and in `t_` and `ts_` use sites, don't wrap the tensor into Variable again) (global search `make_variable(` in jit/ to find places where we are doing double-wrapping for `t_` and `ts_` attributes)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/17031)\r\n1. `tril_` and `triu_` should not change the input tensor's TensorImpl pointer\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18225)\r\n1. Move `pyobj_` to TensorImpl itself, because we always need to be able to convert to and from the Python representation.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18223)\r\n1. Move `version_counter_` to storage or TensorImpl, because we may capture non-requires-grad variables inside an autograd function, and we need a working version counter in these cases.\r\n2. We should not share version counter in `shallow_copy_and_detach()`, because a pure Tensor doesn't have concept of version counter, and it's managed by autograd instead.\r\n3. We should preserve the API semantics of `tensor.data` in Python, and allow it as an escape route for in-place operations without bumping version counter.\r\n\r\n- [x] PR: https://github.com/pytorch/pytorch/pull/19139\r\n1. `tensor.is_variable()` should check whether the TensorImpl has AutogradMeta. `is_variable_` should be removed.\r\n\r\n- [x] PR: Fix version counter sharing in Variable.set_data(...) https://github.com/pytorch/pytorch/pull/20391\r\n\r\n- [x] PR: Move at::NonVariableTypeMode to TensorImpl, and check it in TensorImpl is_variable() https://github.com/pytorch/pytorch/pull/20392\r\n\r\n- [x] PR: Require passing version_counter and allow_tensor_metadata_change to shallow_copy_and_detach(): https://github.com/pytorch/pytorch/pull/20496\r\n\r\n- [x] PR: Shallow-copy `indices` and `values` in sparse tensor constructor https://github.com/pytorch/pytorch/pull/20330\r\n\r\n- [x] PR: Remove Variable::Impl (https://github.com/pytorch/pytorch/pull/17072)\r\n1. Remove the `at::Tensor` data member (`data_`) from `Variable::Impl`\r\n2. In Variable construction and in `Variable.set_data()`, copy all data from `data.impl` to the variable's TensorImpl.\r\n3. Make `Variable.data()` the same semantics as `tensor.data` in Python. Notice breakage in any `Variable.data()` call sites\r\n1. Remove the `Variable::Impl` class and the `DifferentiableViewImpl` class\r\n2. Remove mentions of `Variable::Impl` and `DifferentiableViewImpl`\r\n3. Fix comments in `[Tensor versus Variable in C++]`, `[We regret making Variable hold a Tensor]`, `[ Autograd View Variables ]`. Go through all comments in variable.h and variable.cpp and fix any inconsistency.\r\n4. **NOTE**: we don't need to add `SparseVariableImpl` that handles how to copy `SparseTensorImpl`, because `SparseTensorImpl` already implements the `shallow_copy_and_detach()` function that Variable factory functions can call.\r\n3. In places where we need to ensure the tensor is not requiring gradient, we should check `!requires_grad() || at::NonVariableTypeMode::is_enabled()`, instead of `!requires_grad() || !at::GradMode::is_enabled()`, because we don't want to move `at::GradMode` to ATen.\r\n\r\n## Changes remaining:\r\n\r\n- [x] Make AutogradMeta optional, so that Variable and Tensor become the same. (Tracking issue: https://github.com/pytorch/pytorch/issues/23032)\r\n\r\n- [ ] Miscellaneous cleanup\r\n1. Remove `unpack()` in VariableType*.cpp.\r\n2. Clean up the `unpack_args` logic in gen_variable_type.py, since we are not doing unpack anymore.\r\n3. Fix comments for `use_derived` in gen_variable_type.py\r\n4. Remove `requires_tensor: True` in native_functions.yaml. Figure out how to fix _dimV, _dimS case (`torch.randn(2, 3)._dimV()` shouldn't hit that error)\r\n\r\n- [ ] TensorImpl de-virtualization (tracking issue: https://github.com/pytorch/pytorch/issues/22815)\r\n\r\n- [ ] Sparse invariant fix (tracking issue: https://github.com/pytorch/pytorch/issues/22778)\r\n\r\n- [ ] Remove `tensor_data()` API (@yf225 is working on it)\r\n\r\n- [ ] Python / C++ Tensor API parity (@yf225 is working on it)\r\n1. Any Python Tensor API should also work on C++ Tensor, without explicit casting to Variable\r\n\r\n- [ ] C++ API doc fix: (@yf225 is working on it)\r\n1. Remove https://pytorch.org/cppdocs/#aten section, and replace all `at::Tensor` with `torch::Tensor`, and remove/fix all mentions of ATen in cpp docs and tutorials.",high priority|module: internals|module: cpp|feature|triaged,yf225,"## \U0001f680 High-level changes:\r\n1. **IMPORTANT**: Both `Variable` and `Variable::Impl` are removed, and `at::Tensor` is always the tensor that's passed around in PyTorch, and it can record autograd history when its autograd metadata (`AutogradMeta`) is not null.\r\n2. **IMPORTANT**: Autograd-related function implementations in Variable will be moved to VariableType.\r\n3. Autograd metadata now lives in an `AutogradMeta` struct that `TensorImpl` has a pointer to, and the `AutogradMeta` is *only* populated when the `at::Tensor` requires gradient.\r\n4. We decide whether to dispatch to VariableType / non-VariableType functions using the `at::AutoNonVariableTypeMode` in appropriate places internally. (We only dispatch to VariableType functions if we need profiling/JIT-tracing/autograd)\r\n5. Common Tensor functions (e.g. `numel()`\xa0/\xa0`sizes()`\xa0/\xa0`dim()`) are de-virtualized in TensorImpl and have their runtime reduced by 43%-86%.\r\n6. `tensor.is_variable()` and `options.is_variable()` always return true, because every `at::Tensor` is a variable (and can record autograd history when its `AutogradMeta` is not null). (We keep `options.is_variable(...)` for backward compatibility, and raise warning if it's set to false.)\r\n7. API behavior change: changing shape/storage on `tensor.data` in Python or `tensor.data()` in C++ will no longer update `tensor`.\r\n\r\n## Pitch\r\n\r\nCurrently, the distinction between `at::Tensor` and `Variable` (subclass of `at::Tensor` that contains autograd metadata and functions) creates unnecessary cognitive overhead for PyTorch core development. We want to remove this distinction and make it possible to use `at::Tensor` everywhere in PyTorch. After merging `Variable` into `at::Tensor`, here are the common end-user APIs:\r\n\r\n- **When C++ user wants to create a non-history-recording `at::Tensor` from another `at::Tensor`:**\r\nCurrent API (unchanged):\r\n```cpp\r\nauto t = torch::ones({2, 2}, torch::requires_grad()); // t is recording history\r\nauto t_detached = t.detach() // t_detached is the non-history-recording version of t\r\n```\r\nWhen the user calls `t.detach()`, we do the following under the hood:\r\n1. We do the shallow copy of `t`'s TensorImpl, which copies the storage pointer and all other TensorImpl fields (e.g. `size` / `stride`).\r\n    - Note that subclasses of TensorImpl (e.g. `SparseTensorImpl`) need to know how to make a shallow copy of themselves, and we dispatch this operation to each TensorImpl subclass' own `shallow_copy_and_detach()` function (by making the `shallow_copy_and_detach()` function virtual in TensorImpl and overriding it in TensorImpl subclasses).\r\n2. We set the `AutogradMeta` pointer to NULL, to indicate that it doesn't need to record history.\r\n3. We return an at::Tensor that wraps the new TensorImpl.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to enable/disable history-recording for an `at::Tensor`:**\r\nProposed API:\r\n```cpp\r\nauto t = torch::ones({2, 2});  // t is not recording history (this already works)\r\nt.requires_grad_(true);  // t is recording history now (new API)\r\nt.requires_grad_(false); // t is not recording history anymore (new API)\r\n```\r\nWhen the user calls `t.requires_grad_(true)`, we do the following under the hood:\r\n1. We initialize a struct called `AutogradMeta`, which stores autograd-specific fields (such as `grad_`/`grad_fn_`/`grad_accumulator_`).\r\n2. We assign the struct to the `AutogradMeta` pointer in `t`'s TensorImpl.\r\n\r\nWhen the user calls `t.requires_grad_(false)`, we do the following under the hood:\r\n1. We set the `AutogradMeta` pointer in `t`'s TensorImpl to NULL.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to call non-Variable operations on an `at::Tensor` when dispatching through `type()`**\r\nProposed API:\r\n```cpp\r\n{\r\n  auto t_type = t.type();  // `t_type` is a Variable type if `t` contains AutogradMeta\r\n}\r\n{\r\n  at::AutoNonVariableTypeMode grad_mode(false);  // thread-local guard (new API)\r\n  auto non_var_type = t.type();  // ""non_var_type"" is a non-Variable type\r\n}\r\n{\r\n  at::AutoNonVariableTypeMode grad_mode(true);  // thread-local guard (new API)\r\n  auto var_type = t.type();  // ""var_type"" is a Variable type\r\n}\r\n```\r\nUnder the hood, `type()` checks whether the `at::AutoNonVariableTypeMode` thread-local guard is enabled when determining the type of the variable.\r\n\r\n<br />\r\n\r\n- **When C++ user wants to change content of an `at::Tensor` that has AutogradMeta, without affecting the tensor's `grad_fn` or `version_counter_`**\r\nProposed behavior:\r\n```cpp\r\nauto t = torch::ones({2, 2});\r\nt.requires_grad_(true);\r\nAT_ASSERT(t.current_version() == 0);\r\nt.data().add_(1);  // This is consistent with Python `.data` behavior: changing `.data` of a tensor in Python doesn't affect the tensor's `grad_fn` or `version_counter_`\r\nAT_ASSERT(t.current_version() == 0);\r\n```\r\n\r\n\r\n## Motivation\r\n\r\n- **Overly Complex OOP design**: Currently the distinction between `Variable` and `Tensor` is hard to grasp: `Variable::Impl` is a subclass of TensorImpl, but it also has an `at::Tensor` data member which internally wraps another TensorImpl. This co-existence of ""is-a"" and ""has-a"" relationship makes the code complicated and adds cognitive overhead.  In particular, it's difficult to track which functions we have overridden in `Variable::Impl`, and which functions are applicable to Tensor vs. Variable (e.g. `is_wrapped_number()` is only valid on Tensor, not Variable) (for more context, also see note: [We regret making Variable hold a Tensor](https://github.com/pytorch/pytorch/blob/b6a8c45f57b65d11894c4a6e5a3267708ecec1c5/c10/core/TensorImpl.h#L470-L489)). Ideally, we want to use the same tensor type everywhere in PyTorch code.\r\n\r\n- **Unused data members in `Variable::Impl` take up cache/memory space**: Since `Variable::Impl` is a subclass of TensorImpl, it contains all of the data members that a normal TensorImpl would have (such as `sizes_` / `strides_` / etc.). However, the `Variable::Impl` functions always call into the underlying `at::Tensor` and ignores the rest of the fields, which causes a lot of wasted cache/memory space.\r\n\r\n- **Virtual functions are slow**: We care about how much time it takes to execute common Tensor functions such as `numel()` / `sizes()` / `dim()`. Currently, these functions are `virtual` in TensorImpl, so that `Variable::Impl` (a subclass of TensorImpl) can override them and dispatch those calls to the `Variable::Impl`'s underlying `at::Tensor`. Virtual function calls are slow because they involve an extra vtable lookup. Specifically, we did the following comparison on the most common Tensor functions (all timings are in ns):\r\n\r\nBenchmark | Time (no flush) | Time (flush L1) | Time (flush L1+L2) | Time (flush L1+L2+L3)\r\n-- | -- | -- | -- | --\r\nTensor.dim() - non-virtual | 1.3 | 3.33 | 7.6 | 58\r\nVariable.dim() - virtual | 4.5 | 24.4 | 52 | 173.67\r\n**Runtime Savings** | **-71.11111%** | **-86.35246%** | **-85.38462%** | **-66.60333%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.numel() - non-virtual | 22.6 | 63.89 | 109.22 | 294.5\r\nVariable.numel() - virtual | 80.33 | 133.1 | 192 | 810.9\r\n**Runtime Savings** | **-71.86605%** | **-51.9985%** | **-43.11458%** | **-63.68233%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.size(0) - non-virtual | 30.4 | 60.1 | 100.44 | 384.3\r\nVariable.size(0) - virtual | 75.4 | 127.67 | 203.8 | 875.9\r\n**Runtime Savings** | **-59.6817%** | **-52.92551%** | **-50.71639%** | **-56.12513%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.sizes() - non-virtual | 2 | 4.25 | 13.25 | 67.6\r\nVariable.sizes() - virtual | 5.2 | 28.44 | 62.1 | 254.78\r\n**Runtime Savings** | **-61.53846%** | **-85.05626%** | **-78.66345%** | **-73.46731%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.resize_({0}) no-op - non-virtual | 23.11 | 86.44 | 105.44 | 332.33\r\nVariable.resize_({0}) no-op - virtual | 168.4 | 254.22 | 348.56 | 890.9\r\n**Runtime Savings** | **-86.27672%** | **-65.99795%** | **-69.74983%** | **-62.69727%**\r\n\xa0 | \xa0 | \xa0 | \xa0 | \xa0\r\nTensor.resize_({64, 2048}) no-op - non-virtual | 33.4 | 102.56 | 129.56 | 407.22\r\nVariable.resize_({64, 2048}) no-op - virtual | 193 | 278.1 | 364.9 | 936.6\r\n**Runtime Savings** | **-82.6943%** | **-63.12118%** | **-64.49438%** | **-56.52146%**\r\n\r\n> Benchmarked commit: https://github.com/pytorch/pytorch/commit/f000101b8139378f342b175c11072a925c9d7c7a\r\n> Benchmark script: https://github.com/yf225/benchmark/blob/tensor_functions/timing/cpp2/benchmarks/aten_overheads.cpp\r\n> Non-virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:nonvirtual_tensorimpl\r\n> Virtual code: https://github.com/pytorch/pytorch/compare/master...yf225:virtual_tensorimpl\r\n\r\nBased on our current implementation, the runtime difference for `dim()`, `numel()`, `size()`, `sizes()`, and no-op `resize()` comes from the virtual function call overhead and the `at::Tensor` data member indirection in `Variable::Impl`. If we de-virtualize those functions, we would be able to cut the runtime by **43%-86%** on the most common Tensor functions.\r\n\r\n\r\n\r\n## Breaking changes\r\n\r\nNote that this change will break the current API in the following way:\r\n\r\nIn the old world, whenever we want to create a `Variable` that shares the same data with another `Variable`, we simply do `auto var_new = make_variable(var.data())` or `auto var_new = var.detach()`, and any shape / data / storage pointer changes to `var_new` will be reflected in `var` automatically, because internally they share the same underlying `at::Tensor`.\r\n\r\nHowever, in the new world, there is no concept of the ""underlying `at::Tensor`"" of a Variable, since the Variable itself is the Tensor. When we want to create an `at::Tensor` that shares the same data with another `at::Tensor`, we can still call `auto t_new = t.detach()`, but in this case, only the tensor storage data is shared (via ref-counted pointer) between `t_new` and `t`, but not the tensor size/stride information (they are copied by value). In other words, changing anything (e.g. size / stride / storage_ptr ) in the detached Tensor (`t_new`) that are not bits inside tensor storage won't update the original Tensor (`t`), and we should no longer expect those data to be shared.\r\n\r\nThis has implications for Python call sites that do\r\n```python\r\ntensor.data.in_place_operation_()\r\n```\r\nor\r\n```python\r\ntensor_detached = tensor.detach()\r\ntensor_detached.in_place_operation_()\r\n```\r\nIf `in_place_operation_()` only updates the data inside the tensor (such as `zeros_()`), such operation will still work properly; if the in-place operation changes the size, stride or the storage pointer inside the TensorImpl (e.g. `resize_` / `resize_as_` / `set_` / `transpose_`), such operation on `tensor.data` or `tensor_detached` will no longer update the `tensor`. We will address this inconsistency in the following ways:\r\n\r\n1. Add an `allow_tensor_metadata_change_` flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n\r\n\r\n\r\n## Finished changes\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/13827)\r\n1. Add a flag to `TensorImpl` to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.\r\n2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.\r\n3. Move `Variable::Impl` data members into TensorImpl as `AutogradMeta` struct\r\n4. Change `Variable::Impl` functions to use data members in `AutogradMeta` struct\r\n5. Add `shallow_copy()` function to each subclass of TensorImpl\r\n6. Do shallow copy when the user calls `make_variable(tensor)` / `variable.detach()` (Reason: now that autograd metadata lives in TensorImpl, in order to create a new history for for the Variable returned from `variable.detach()` we not only need to create a new AutogradMeta struct, but we also need to create a new TensorImpl object that stores pointer to the new AutogradMeta struct (which we obtain by shallow-copying the original TensorImpl). Otherwise, changing history of the detached Variable will also change the history of the original Variable, which is not the correct behavior.)\r\n7. Add `AutogradMetaInterface` class, and make `AutogradMeta` a subclass of it, so that we can make `autograd_meta_` a unique_ptr in TensorImpl\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15487)\r\n1. Move `set_requires_grad()` / `requires_grad()` / `grad()` from `Variable::Impl` to `AutogradMeta`\r\n2. Move `Variable::Impl` functions such as `backward()` / `rebase_history()` / `grad_accumulator()` / `grad_fn()` out of `Variable::Impl` and into `AutogradMeta`.\r\n3. Note: we need to make these changes so that we can remove `Variable::Impl` class in the next PR.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/15939)\r\n1. Add thread-local guard (`at::AutoNonVariableTypeMode`) to make sure that in VariableType.cpp the operations on baseType still dispatch to non-Variable type, even if the parameters are now Variables\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16305)\r\n1. Make `gesv_out` return the original input tensor instead of a new tensor (currently by copying the result tensor into the original input tensor, because a true in-place `gesv` is more difficult to implement. NOTE: also open an issue for this).\r\n2. In VariableType.cpp, after each in-place function on the ""unpacked"" tensor, check pointer address equality for storage in the original input variable's TensorImpl (check this for all arguments in `unpacked_args`)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16325)\r\n1. Remove `.type()` calls as much as possible, to reduce the need of using the `at::AutoNonVariableTypeMode` guard\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/16596)\r\n1. Make JIT attributes `t_` and `ts_` store Variable instead of Tensor (and in `t_` and `ts_` use sites, don't wrap the tensor into Variable again) (global search `make_variable(` in jit/ to find places where we are doing double-wrapping for `t_` and `ts_` attributes)\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/17031)\r\n1. `tril_` and `triu_` should not change the input tensor's TensorImpl pointer\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18225)\r\n1. Move `pyobj_` to TensorImpl itself, because we always need to be able to convert to and from the Python representation.\r\n\r\n- [x] PR: (https://github.com/pytorch/pytorch/pull/18223)\r\n1. Move `version_counter_` to storage or TensorImpl, because we may capture non-requires-grad variables inside an autograd function, and we need a working version counter in these cases.\r\n2. We should not share version counter in `shallow_copy_and_detach()`, because a pure Tensor doesn't have concept of version counter, and it's managed by autograd instead.\r\n3. We should preserve the API semantics of `tensor.data` in Python, and allow it as an escape route for in-place operations without bumping version counter.\r\n\r\n- [x] PR: https://github.com/pytorch/pytorch/pull/19139\r\n1. `tensor.is_variable()` should check whether the TensorImpl has AutogradMeta. `is_variable_` should be removed.\r\n\r\n- [x] PR: Fix version counter sharing in Variable.set_data(...) https://github.com/pytorch/pytorch/pull/20391\r\n\r\n- [x] PR: Move at::NonVariableTypeMode to TensorImpl, and check it in TensorImpl is_variable() https://github.com/pytorch/pytorch/pull/20392\r\n\r\n- [x] PR: Require passing version_counter and allow_tensor_metadata_change to shallow_copy_and_detach(): https://github.com/pytorch/pytorch/pull/20496\r\n\r\n- [x] PR: Shallow-copy `indices` and `values` in sparse tensor constructor https://github.com/pytorch/pytorch/pull/20330\r\n\r\n- [x] PR: Remove Variable::Impl (https://github.com/pytorch/pytorch/pull/17072)\r\n1. Remove the `at::Tensor` data member (`data_`) from `Variable::Impl`\r\n2. In Variable construction and in `Variable.set_data()`, copy all data from `data.impl` to the variable's TensorImpl.\r\n3. Make `Variable.data()` the same semantics as `tensor.data` in Python. Notice breakage in any `Variable.data()` call sites\r\n1. Remove the `Variable::Impl` class and the `DifferentiableViewImpl` class\r\n2. Remove mentions of `Variable::Impl` and `DifferentiableViewImpl`\r\n3. Fix comments in `[Tensor versus Variable in C++]`, `[We regret making Variable hold a Tensor]`, `[ Autograd View Variables ]`. Go through all comments in variable.h and variable.cpp and fix any inconsistency.\r\n4. **NOTE**: we don't need to add `SparseVariableImpl` that handles how to copy `SparseTensorImpl`, because `SparseTensorImpl` already implements the `shallow_copy_and_detach()` function that Variable factory functions can call.\r\n3. In places where we need to ensure the tensor is not requiring gradient, we should check `!requires_grad() || at::NonVariableTypeMode::is_enabled()`, instead of `!requires_grad() || !at::GradMode::is_enabled()`, because we don't want to move `at::GradMode` to ATen.\r\n\r\n## Changes remaining:\r\n\r\n- [x] Make AutogradMeta optional, so that Variable and Tensor become the same. (Tracking issue: https://github.com/pytorch/pytorch/issues/23032)\r\n\r\n- [ ] Miscellaneous cleanup\r\n1. Remove `unpack()` in VariableType*.cpp.\r\n2. Clean up the `unpack_args` logic in gen_variable_type.py, since we are not doing unpack anymore.\r\n3. Fix comments for `use_derived` in gen_variable_type.py\r\n4. Remove `requires_tensor: True` in native_functions.yaml. Figure out how to fix _dimV, _dimS case (`torch.randn(2, 3)._dimV()` shouldn't hit that error)\r\n\r\n- [ ] TensorImpl de-virtualization (tracking issue: https://github.com/pytorch/pytorch/issues/22815)\r\n\r\n- [ ] Sparse invariant fix (tracking issue: https://github.com/pytorch/pytorch/issues/22778)\r\n\r\n- [ ] Remove `tensor_data()` API (@yf225 is working on it)\r\n\r\n- [ ] Python / C++ Tensor API parity (@yf225 is working on it)\r\n1. Any Python Tensor API should also work on C++ Tensor, without explicit casting to Variable\r\n\r\n- [ ] C++ API doc fix: (@yf225 is working on it)\r\n1. Remove https://pytorch.org/cppdocs/#aten section, and replace all `at::Tensor` with `torch::Tensor`, and remove/fix all mentions of ATen in cpp docs and tutorials.","cpp\r\nauto t = torch::ones({2, 2}, torch::requires_grad()); // t is recording history\r\nauto t_detached = t.detach() // t_detached is the non-history-recording version of t\r\n"
13569,"Assertion fails when using DataParallel with nn.Embedding and max_norm != None## \U0001f41b Bug\r\n\r\nWhen using `DataParallel` with `nn.Embedding` with `max_norm` set to a non `None` value, the following assertion is triggered:\r\n\r\n`RuntimeError: output_nr_ == 0 ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.`\r\n\r\n## To Reproduce\r\n\r\nTo reproduce, use the following script:\r\n\r\n\r\n\r\nThis will output the following:\r\n\r\n    OK\r\n    Traceback (most recent call last):\r\n    File "".../lib/python3.7/runpy.py"", line 193, in _run_module_as_main\r\n        ""__main__"", mod_spec)\r\n    File "".../lib/python3.7/runpy.py"", line 85, in _run_code\r\n        exec(code, run_globals)\r\n    File "".../minimal.py"", line 20, in <module>\r\n        main()\r\n    File "".../minimal.py"", line 16, in main\r\n        parallel.forward(batch)\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward\r\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply\r\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply\r\n        raise output\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 59, in _worker\r\n        output = module(*input, **kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/modules/sparse.py"", line 113, in forward\r\n        self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n    File "".../lib/python3.7/site-packages/torch/nn/functional.py"", line 1233, in embedding\r\n        return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n    RuntimeError: output_nr_ == 0 ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.a\r\n\r\n## Expected behavior\r\n\r\nI expect the `forward` call to work the same whether I use `DataParallel` or not.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20181105\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 396.51\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181105)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181105 py3.7_cuda9.2.148_cudnn7.1.4_0  [cuda92]  pytorch\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n",high priority,nairbv,"## \U0001f41b Bug\r\n\r\nWhen using `DataParallel` with `nn.Embedding` with `max_norm` set to a non `None` value, the following assertion is triggered:\r\n\r\n`RuntimeError: output_nr_ == 0 ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.`\r\n\r\n## To Reproduce\r\n\r\nTo reproduce, use the following script:\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\ndef main():\r\n    device = ""cuda""\r\n    embedding = nn.Embedding(128, 16, max_norm=1).to(device)\r\n    batch = torch.randint(128, (32, ), device=device)\r\n\r\n    embedding.forward(batch)\r\n    print(""OK"")\r\n\r\n    parallel = nn.DataParallel(embedding)\r\n    parallel.forward(batch)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n\r\nThis will output the following:\r\n\r\n    OK\r\n    Traceback (most recent call last):\r\n    File "".../lib/python3.7/runpy.py"", line 193, in _run_module_as_main\r\n        ""__main__"", mod_spec)\r\n    File "".../lib/python3.7/runpy.py"", line 85, in _run_code\r\n        exec(code, run_globals)\r\n    File "".../minimal.py"", line 20, in <module>\r\n        main()\r\n    File "".../minimal.py"", line 16, in main\r\n        parallel.forward(batch)\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward\r\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply\r\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply\r\n        raise output\r\n    File "".../lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 59, in _worker\r\n        output = module(*input, **kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n    File "".../lib/python3.7/site-packages/torch/nn/modules/sparse.py"", line 113, in forward\r\n        self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n    File "".../lib/python3.7/site-packages/torch/nn/functional.py"", line 1233, in embedding\r\n        return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n    RuntimeError: output_nr_ == 0 ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.a\r\n\r\n## Expected behavior\r\n\r\nI expect the `forward` call to work the same whether I use `DataParallel` or not.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20181105\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 396.51\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181105)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181105 py3.7_cuda9.2.148_cudnn7.1.4_0  [cuda92]  pytorch\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n\r\n","python\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\ndef main():\r\n    device = ""cuda""\r\n    embedding = nn.Embedding(128, 16, max_norm=1).to(device)\r\n    batch = torch.randint(128, (32, ), device=device)\r\n\r\n    embedding.forward(batch)\r\n    print(""OK"")\r\n\r\n    parallel = nn.DataParallel(embedding)\r\n    parallel.forward(batch)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n"
13564,"[JIT] Cannot trace custom ops## \U0001f41b Bug\r\n\r\nCustom ops can be used in JIT script, but not in tracing.\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nEverything to work perfectly!\r\n\r\n## Environment\r\n\r\nToday's master\r\n\r\n## Context\r\n\r\nI noticed this with a formidable JIT challenge (@fmassa ):\r\nhttps://github.com/facebookresearch/maskrcnn-benchmark/issues/27#issuecomment-435789228\r\n\r\n@goldsborough : This is as mentioned on the slack.\r\n",oncall: jit,goldsborough,"## \U0001f41b Bug\r\n\r\nCustom ops can be used in JIT script, but not in tracing.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport os\r\nimport torch\r\nimport torch.jit\r\ncsrc = """"""\r\n#include <torch/extension.h>\r\n#include <torch/script.h>\r\n\r\nusing namespace at;\r\n\r\nTensor test(const Tensor& inp) {\r\n  return inp * 2;\r\n}\r\n\r\nstatic auto registry =\r\n  torch::jit::RegisterOperators()\r\n    .op(""mytest::test"", &test);\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(""test"", &test, ""super test!"");\r\n}\r\n""""""\r\n\r\nimport torch.utils.cpp_extension\r\n\r\next = torch.utils.cpp_extension.load_inline(""test"", [csrc], verbose=True,\r\n                                            extra_ldflags=['-ltorch','-lcaffe2',\r\n                                                           '-L'+os.path.join(os.path.dirname(torch._C.__file__), 'lib') ])\r\ntorch.ops.load_library(ext.__file__)\r\n\r\nt = torch.randn(5)\r\nprint(torch.ops.mytest.test(t)) # works\r\n\r\n@torch.jit.script\r\ndef test_wrapper(t):\r\n    return torch.ops.mytest.test(t)\r\n\r\nprint (torch.jit.trace(test_wrapper, (t,)))  # works, too\r\n\r\n\r\nprint (torch.jit.trace(torch.ops.mytest.test, (t,)))  # should work!\r\n```\r\n\r\n## Expected behavior\r\n\r\nEverything to work perfectly!\r\n\r\n## Environment\r\n\r\nToday's master\r\n\r\n## Context\r\n\r\nI noticed this with a formidable JIT challenge (@fmassa ):\r\nhttps://github.com/facebookresearch/maskrcnn-benchmark/issues/27#issuecomment-435789228\r\n\r\n@goldsborough : This is as mentioned on the slack.\r\n","python\r\nimport os\r\nimport torch\r\nimport torch.jit\r\ncsrc = """"""\r\n#include <torch/extension.h>\r\n#include <torch/script.h>\r\n\r\nusing namespace at;\r\n\r\nTensor test(const Tensor& inp) {\r\n  return inp * 2;\r\n}\r\n\r\nstatic auto registry =\r\n  torch::jit::RegisterOperators()\r\n    .op(""mytest::test"", &test);\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(""test"", &test, ""super test!"");\r\n}\r\n""""""\r\n\r\nimport torch.utils.cpp_extension\r\n\r\next = torch.utils.cpp_extension.load_inline(""test"", [csrc], verbose=True,\r\n                                            extra_ldflags=['-ltorch','-lcaffe2',\r\n                                                           '-L'+os.path.join(os.path.dirname(torch._C.__file__), 'lib') ])\r\ntorch.ops.load_library(ext.__file__)\r\n\r\nt = torch.randn(5)\r\nprint(torch.ops.mytest.test(t)) # works\r\n\r\n@torch.jit.script\r\ndef test_wrapper(t):\r\n    return torch.ops.mytest.test(t)\r\n\r\nprint (torch.jit.trace(test_wrapper, (t,)))  # works, too\r\n\r\n\r\nprint (torch.jit.trace(torch.ops.mytest.test, (t,)))  # should work!\r\n"
13386,"Incorrect output shape for max-pooling## \U0001f41b Bug\r\n\r\nFor some combinations of input size, pooling size and padding, the output size of `max_pool1d` is wrong. Furthermore, it is different between CPU and CUDA.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nOn GPU:\r\n\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199996])\r\n```\r\n\r\nOn CPU:\r\n\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199997])\r\n```\r\n\r\nFor an input length of `19200000`, I get an output length of `19200000` on GPU and `19200001` on CPU.\r\n\r\n## Expected behavior\r\n\r\nIn all cases, the output shape should match the input shape -- it's an uneven pooling window with stride 1 and matching symmetric padding. Also the output shape should be the same regardless of the device. Note that it works fine on both CUDA and CPU for an input length of `1920000 - 2` (one order of magnitude smaller).\r\n\r\n## Environment\r\n\r\nI can reproduce this both with a precompiled PyTorch 0.4.1 and a self-compiled PyTorch 1.0.\r\n\r\nPre-compiled:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nSelf-compiled:\r\n```\r\nPyTorch version: 1.0.0a0+710191e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 410.57\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nFor what it's worth, it works fine using Lasagne/Theano both on CPU and CUDA.",high priority,nairbv,"## \U0001f41b Bug\r\n\r\nFor some combinations of input size, pooling size and padding, the output size of `max_pool1d` is wrong. Furthermore, it is different between CPU and CUDA.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nOn GPU:\r\n```python\r\nimport torch\r\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cuda:0')\r\nwidth = 1921\r\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\r\nprint(x.shape)\r\nprint(y.shape)\r\n```\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199996])\r\n```\r\n\r\nOn CPU:\r\n```python\r\nimport torch\r\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cpu')\r\nwidth = 1921\r\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\r\nprint(x.shape)\r\nprint(y.shape)\r\n```\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199997])\r\n```\r\n\r\nFor an input length of `19200000`, I get an output length of `19200000` on GPU and `19200001` on CPU.\r\n\r\n## Expected behavior\r\n\r\nIn all cases, the output shape should match the input shape -- it's an uneven pooling window with stride 1 and matching symmetric padding. Also the output shape should be the same regardless of the device. Note that it works fine on both CUDA and CPU for an input length of `1920000 - 2` (one order of magnitude smaller).\r\n\r\n## Environment\r\n\r\nI can reproduce this both with a precompiled PyTorch 0.4.1 and a self-compiled PyTorch 1.0.\r\n\r\nPre-compiled:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nSelf-compiled:\r\n```\r\nPyTorch version: 1.0.0a0+710191e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 410.57\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nFor what it's worth, it works fine using Lasagne/Theano both on CPU and CUDA.","python\r\nimport torch\r\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cuda:0')\r\nwidth = 1921\r\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\r\nprint(x.shape)\r\nprint(y.shape)\r\n"
13324,"torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order## \U0001f41b Bug\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n\r\n\r\n## Expected behavior\r\n\r\nShould return an exception instead of a segmentation fault\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181024\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181024)\r\n[pip] torchaudio (0.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181024 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```\r\n",high priority|module: crash,nairbv,"## \U0001f41b Bug\r\n\r\nInstead of raising an exception, the function `torch.nn.utils.rnn.pack_padded_sequence` forces python environment to shut down due to a segmentation fault if the input is not in decreasing order.\r\n\r\n\r\n## To Reproduce\r\n\r\n```python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n```\r\n\r\n## Expected behavior\r\n\r\nShould return an exception instead of a segmentation fault\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181024\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\nGPU 2: GeForce GTX 1080\r\nGPU 3: GeForce GTX 1080\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.1/lib64/libcudnn.so\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181024)\r\n[pip] torchaudio (0.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_py35_py27__9.0.176_7.1.2_2    pytorch\r\n[conda] pytorch-nightly           1.0.0.dev20181024 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```\r\n","python \r\nimport torch\r\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\r\na = torch.ones(25, 300)\r\nb = torch.ones(22, 300)\r\nb_a = pad_sequence([b, a])\r\npack_padded_sequence(b_a, [22, 25])\r\n>>> 31906 abort (core dumped)  ipython\r\na_b = pad_sequence([a, b])\r\npack_padded_sequence(a_b, [25, 22]) # it works!\r\n"
13117,"Windows, CUDA : OS call failed or operation not supported on this OS## \U0001f41b Bug\r\n```\r\nOS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\n```\r\nwhen iterating over DataLoader loader with cuda tensors.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Defining the dataset class where data is a pandas dataframe and target_cols is a list of the columns who should be the targets.\r\n\r\n2. Creating an instance of the dataset and a DataLoader instance of it where X_train is my pandas dataframe and 'CONDOMINIUM_EXPENSES' is a column of the dataframe.\r\n\r\n3. Simply iterating over it throws the message above.\r\n\r\n\r\nComplete error message:\r\n```\r\nTHTTCTuHdCauCHCdhaeCheccudkaCHkC h FAIeckFLAu IdLaFC Af fhiIiLl eecle=k =FfiAIL lfe=ile=cc:\\progr:\\aprcm:dogatrc\\aa:\\p\\rpmmdaoirnogramtaid\\mingramidatcaota\\mia\\minincniconoda3\\ccodondan-bnda3da3\\ld\\conacod3a\\p\\-nbdac-olndd\\aybtldo\\py-pbrtlycdh\\tpoyrtcoorchr__11ch_h5245_514987721495052254459\\84wo7r7k9\\t9o287072r945c\\0h\\49877wo2\\csrr9c\\ge02wk\\ork\\torcne\\wtohork\\\\rrictccsh\\csr\\oSrtc\\cg\\oraegnrgerenerch\\cseicShrc\\ge\\iScne\\Storatroraiagng.cepShprgiac elr\\SSihneitaorrinn=ggag..ecpp licSharpningep.cpp=  lliinen=e=25232552 533 3 ee error=rerrrororr63 :r= 6orOS3= 6c= a63 :3 : OS calll :  faOS ciOlSe ad lolr  coaplefrall fa iilatfaleeiion lndd ot su orpoerported  on otpehisr atOSi\r\ndon no o or opept supporerrateatditi on noot suon tnppho nrtiost  OSe\r\nsupported on thd ois OnS \r\nthis OS\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-4:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt is supposed to iterate over the DataLoader object.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Famille\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 391.35\r\ncuDNN version: Probably one of the following:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda90                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda90_cudnn7he774522_1  [cuda90]  pytorch\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\nAlso I don't know why the script didn't find my version of pip but it is 18.1.\r\n\r\n## Additional context\r\n\r\nThe code was working last night and it suddenly started throwing this error for no apparent reason. I didn't change anything in the code and I didn't reboot the computer either. I am not aware than any update was done concerning the OS or drivers.\r\n\n\ncc @ngimel @SsnL @peterjc123 @nbcsm @guyang3532",module: windows|module: dataloader|module: cuda|triaged,cristianPanaite,"## \U0001f41b Bug\r\n```\r\nOS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\n```\r\nwhen iterating over DataLoader loader with cuda tensors.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Defining the dataset class where data is a pandas dataframe and target_cols is a list of the columns who should be the targets.\r\n```python\r\ndevice = torch.device('cuda:0')\r\n\r\n\r\nclass TrainDataset(Dataset):\r\n    def __init__(self, data, target_cols):\r\n        self.X = np.array(data.drop(target_cols, axis=1))\r\n        self.y = np.array(data[target_cols])\r\n\r\n    def __len__(self):\r\n        return self.X.shape[0]\r\n\r\n    def __getitem__(self, idx):\r\n        input = torch.from_numpy(self.X[idx]).float()\r\n        target = torch.from_numpy(self.y[idx]).float()\r\n        return input.to(device), target.to(device)\r\n```\r\n2. Creating an instance of the dataset and a DataLoader instance of it where X_train is my pandas dataframe and 'CONDOMINIUM_EXPENSES' is a column of the dataframe.\r\n```python\r\nX_train_set = TrainDataset(X_train, target_cols=['CONDOMINIUM_EXPENSES'])\r\nX_train_loader = DataLoader(X_train_set, batch_size=4, shuffle=True, num_workers=4)\r\n```\r\n3. Simply iterating over it throws the message above.\r\n```python\r\nfor data in X_train_loader:\r\n```\r\n\r\nComplete error message:\r\n```\r\nTHTTCTuHdCauCHCdhaeCheccudkaCHkC h FAIeckFLAu IdLaFC Af fhiIiLl eecle=k =FfiAIL lfe=ile=cc:\\progr:\\aprcm:dogatrc\\aa:\\p\\rpmmdaoirnogramtaid\\mingramidatcaota\\mia\\minincniconoda3\\ccodondan-bnda3da3\\ld\\conacod3a\\p\\-nbdac-olndd\\aybtldo\\py-pbrtlycdh\\tpoyrtcoorchr__11ch_h5245_514987721495052254459\\84wo7r7k9\\t9o287072r945c\\0h\\49877wo2\\csrr9c\\ge02wk\\ork\\torcne\\wtohork\\\\rrictccsh\\csr\\oSrtc\\cg\\oraegnrgerenerch\\cseicShrc\\ge\\iScne\\Storatroraiagng.cepShprgiac elr\\SSihneitaorrinn=ggag..ecpp licSharpningep.cpp=  lliinen=e=25232552 533 3 ee error=rerrrororr63 :r= 6orOS3= 6c= a63 :3 : OS calll :  faOS ciOlSe ad lolr  coaplefrall fa iilatfaleeiion lndd ot su orpoerported  on otpehisr atOSi\r\ndon no o or opept supporerrateatditi on noot suon tnppho nrtiost  OSe\r\nsupported on thd ois OnS \r\nthis OS\r\nProcess Process-2:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-4:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 258, in _bootstrap\r\n    self.run()\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\process.py"", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py"", line 61, in _worker_loop\r\n    data_queue.put((idx, samples))\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\queues.py"", line 341, in put\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File ""D:\\Anaconda3\\Lib\\multiprocessing\\reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File ""D:\\Anaconda3\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py"", line 108, in reduce_storage\r\n    metadata = storage._share_cuda_()\r\nRuntimeError: cuda runtime error (63) : OS call failed or operation not supported on this OS at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524549877902\\work\\torch\\csrc\\generic\\StorageSharing.cpp:253\r\n```\r\n\r\n## Expected behavior\r\n\r\nIt is supposed to iterate over the DataLoader object.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0\r\n\r\nOS: Microsoft Windows 10 Famille\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 391.35\r\ncuDNN version: Probably one of the following:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda90                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda90_cudnn7he774522_1  [cuda90]  pytorch\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\nAlso I don't know why the script didn't find my version of pip but it is 18.1.\r\n\r\n## Additional context\r\n\r\nThe code was working last night and it suddenly started throwing this error for no apparent reason. I didn't change anything in the code and I didn't reboot the computer either. I am not aware than any update was done concerning the OS or drivers.\r\n\n\ncc @ngimel @SsnL @peterjc123 @nbcsm @guyang3532","python\r\ndevice = torch.device('cuda:0')\r\n\r\n\r\nclass TrainDataset(Dataset):\r\n    def __init__(self, data, target_cols):\r\n        self.X = np.array(data.drop(target_cols, axis=1))\r\n        self.y = np.array(data[target_cols])\r\n\r\n    def __len__(self):\r\n        return self.X.shape[0]\r\n\r\n    def __getitem__(self, idx):\r\n        input = torch.from_numpy(self.X[idx]).float()\r\n        target = torch.from_numpy(self.y[idx]).float()\r\n        return input.to(device), target.to(device)\r\n"
12780,"[pytorch/jit] all lists get merged during tracing## \U0001f41b Bug\r\n\r\nIf you define a nn.Module that contains a forward func with multiple lists (That you stack together into a tensor at some point) that process works perfectly fine in the conventional workflow. \r\nHowever, if you attempt to convert your module into a ScriptModule that will be compiled into a graph representation, every list object defined in a `script_method` function seem to merge together somehow.\r\n\r\n## To Reproduce\r\n\r\nCreate a test bench with the source code below, it should be fully self contained\r\n\r\ndimensions of elements in `states`:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n[ Variable[CPUFloatType]{1,1,80} ],\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n\r\nand here is the error message that causes an overall failure of the trace operation:\r\n```\r\nRuntimeError: \r\ninvalid argument 0: Sizes of tensors must match except in dimension 1. Got 3 and 1 in dimension 0 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317:\r\noperation failed in interpreter:\r\n    halt_probability = self.halt_noise(halt_probability, training)\r\n    halt_probs.append(halt_probability)\r\n    print(halt_probs)\r\n    residual = torch.sum(torch.cat(halt_probs))\r\n    states_tensor = torch.stack(states, dim=1)\r\n                    ~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nfor the above code sample, \r\n`hiddens` should contain:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n```\r\n`states` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1,1,80} ]\r\n```\r\n`halt_probs` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n.\r\n## Environment\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20181004\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip install /tmp/torch_nightly-1.0.0.dev20181004-cp35-cp35m-linux_x86_64.whl`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.5.4\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\nAs I am doing some somewhat advanced pytorch module development, I could understand if my usage here is not supported - however there is currently no error message saying that I can't construct multiple tensors from elements captured in a forward pass.\r\n\r\nIt's certainly possible that some other part of my algorithm is failing, or maybe I'm using torch.jit.ScriptModule incorrectly? That would be great if that was the case,\r\n\r\nI could see this posing issues down the road with other types of dynamic process flow during forward pass loops (like ACT, but also NTMs, etc), which are probably mechanisms we want pytorch 1.0 to support properly.\r\n",oncall: jit,suo,"## \U0001f41b Bug\r\n\r\nIf you define a nn.Module that contains a forward func with multiple lists (That you stack together into a tensor at some point) that process works perfectly fine in the conventional workflow. \r\nHowever, if you attempt to convert your module into a ScriptModule that will be compiled into a graph representation, every list object defined in a `script_method` function seem to merge together somehow.\r\n\r\n## To Reproduce\r\n\r\nCreate a test bench with the source code below, it should be fully self contained\r\n```python\r\n\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.functional import F\r\n\r\nclass ACTNN(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['hidden_depth', 'hidden_width']\r\n    # hidden_width = 125\r\n   # hidden_depth = 3\r\n    # output_size = 80\r\n    # input_size = 125\r\n    def __init__(self, input_size, hidden_width, output_size, hidden_depth):\r\n        super(ACTNN, self).__init__()\r\n        self.hidden_depth = hidden_depth\r\n        self.hidden_width = hidden_width\r\n        self.rnn = torch.jit.trace(nn.RNN(input_size, hidden_width,\r\n                                          num_layers=self.hidden_depth, batch_first=True), example_inputs=(torch.rand(1, 1, input_size), torch.rand(self.hidden_depth, 1, hidden_width)))\r\n\r\n        self.proc = torch.jit.trace(nn.Linear(hidden_width, output_size), example_inputs=torch.rand(1, 1, hidden_width))\r\n\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor):\r\n\r\n        input = input.view(1, 1, -1)\r\n        hidden = hidden.view(self.hidden_depth, 1, self.hidden_width)\r\n        # Define the list objects\r\n        states = []\r\n        hiddens = []\r\n        halt_probs = []\r\n        n = 0\r\n        rnn_out, hidden = self.rnn(input, hidden)\r\n        state = self.proc(rnn_out)\r\n        hiddens.append(hidden)\r\n        states.append(state)\r\n        halt_probability = F.sigmoid(hiddens[n].sum())\r\n        halt_probs.append(halt_probability)\r\n       # At this point, both the states & halt_probs lists will contain:\r\n       # 1x 1,1,80 wide tensor\r\n       # 1x 3,1,125 wide tensor\r\n       # 1x 1 wide tensor\r\n\r\n        print(states)\r\n        print(halt_probs)\r\n        residual = torch.sum(torch.cat(halt_probs))\r\n\r\n        states_tensor = torch.stack(states, dim=1)\r\n        hiddens_tensor = torch.stack(hiddens, dim=1)\r\n        halt_subset = halt_probs[0:n-1]\r\n        halt_subset.append(residual)\r\n        halt_prob_tensor = torch.stack(halt_subset).view(-1)\r\n        output = torch.mv(states_tensor, halt_prob_tensor)\r\n        hidden = torch.mv(hiddens_tensor, halt_prob_tensor)\r\n        return output, hidden\r\n```\r\ndimensions of elements in `states`:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n[ Variable[CPUFloatType]{1,1,80} ],\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n\r\nand here is the error message that causes an overall failure of the trace operation:\r\n```\r\nRuntimeError: \r\ninvalid argument 0: Sizes of tensors must match except in dimension 1. Got 3 and 1 in dimension 0 at /pytorch/aten/src/TH/generic/THTensorMoreMath.cpp:1317:\r\noperation failed in interpreter:\r\n    halt_probability = self.halt_noise(halt_probability, training)\r\n    halt_probs.append(halt_probability)\r\n    print(halt_probs)\r\n    residual = torch.sum(torch.cat(halt_probs))\r\n    states_tensor = torch.stack(states, dim=1)\r\n                    ~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nfor the above code sample, \r\n`hiddens` should contain:\r\n```\r\n[ Variable[CPUFloatType]{3,1,125} ],\r\n```\r\n`states` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1,1,80} ]\r\n```\r\n`halt_probs` should contain:\r\n```\r\n[ Variable[CPUFloatType]{1} ]]\r\n```\r\n.\r\n## Environment\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20181004\r\n - OS (e.g., Linux): Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): `pip install /tmp/torch_nightly-1.0.0.dev20181004-cp35-cp35m-linux_x86_64.whl`\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.5.4\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\nAs I am doing some somewhat advanced pytorch module development, I could understand if my usage here is not supported - however there is currently no error message saying that I can't construct multiple tensors from elements captured in a forward pass.\r\n\r\nIt's certainly possible that some other part of my algorithm is failing, or maybe I'm using torch.jit.ScriptModule incorrectly? That would be great if that was the case,\r\n\r\nI could see this posing issues down the road with other types of dynamic process flow during forward pass loops (like ACT, but also NTMs, etc), which are probably mechanisms we want pytorch 1.0 to support properly.\r\n","python\r\n\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.functional import F\r\n\r\nclass ACTNN(torch.jit.ScriptModule):\r\n\r\n    __constants__ = ['hidden_depth', 'hidden_width']\r\n    # hidden_width = 125\r\n   # hidden_depth = 3\r\n    # output_size = 80\r\n    # input_size = 125\r\n    def __init__(self, input_size, hidden_width, output_size, hidden_depth):\r\n        super(ACTNN, self).__init__()\r\n        self.hidden_depth = hidden_depth\r\n        self.hidden_width = hidden_width\r\n        self.rnn = torch.jit.trace(nn.RNN(input_size, hidden_width,\r\n                                          num_layers=self.hidden_depth, batch_first=True), example_inputs=(torch.rand(1, 1, input_size), torch.rand(self.hidden_depth, 1, hidden_width)))\r\n\r\n        self.proc = torch.jit.trace(nn.Linear(hidden_width, output_size), example_inputs=torch.rand(1, 1, hidden_width))\r\n\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor):\r\n\r\n        input = input.view(1, 1, -1)\r\n        hidden = hidden.view(self.hidden_depth, 1, self.hidden_width)\r\n        # Define the list objects\r\n        states = []\r\n        hiddens = []\r\n        halt_probs = []\r\n        n = 0\r\n        rnn_out, hidden = self.rnn(input, hidden)\r\n        state = self.proc(rnn_out)\r\n        hiddens.append(hidden)\r\n        states.append(state)\r\n        halt_probability = F.sigmoid(hiddens[n].sum())\r\n        halt_probs.append(halt_probability)\r\n       # At this point, both the states & halt_probs lists will contain:\r\n       # 1x 1,1,80 wide tensor\r\n       # 1x 3,1,125 wide tensor\r\n       # 1x 1 wide tensor\r\n\r\n        print(states)\r\n        print(halt_probs)\r\n        residual = torch.sum(torch.cat(halt_probs))\r\n\r\n        states_tensor = torch.stack(states, dim=1)\r\n        hiddens_tensor = torch.stack(hiddens, dim=1)\r\n        halt_subset = halt_probs[0:n-1]\r\n        halt_subset.append(residual)\r\n        halt_prob_tensor = torch.stack(halt_subset).view(-1)\r\n        output = torch.mv(states_tensor, halt_prob_tensor)\r\n        hidden = torch.mv(hiddens_tensor, halt_prob_tensor)\r\n        return output, hidden\r\n"
12578,"Printing: mismatch result with numpy in pytorch 0.4I noticed a weird behavior when using torch.linspace.  Please take a look into the following code, where I compare the output of pytorch and numpy for different number of steps:\r\n\r\n\r\nIf I run the code above I have the following output:\r\n```\r\n11 steps\r\ntensor([-0.0001, -0.0001, -0.0001, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\r\n         0.0001,  0.0001,  0.0001])\r\n[-1.e-04 -8.e-05 -6.e-05 -4.e-05 -2.e-05  0.e+00  2.e-05  4.e-05  6.e-05\r\n  8.e-05  1.e-04]\r\n.....\r\n12 steps\r\ntensor([-1.0000e-04, -8.1818e-05, -6.3636e-05, -4.5455e-05, -2.7273e-05,\r\n        -9.0909e-06,  9.0909e-06,  2.7273e-05,  4.5455e-05,  6.3636e-05,\r\n         8.1818e-05,  1.0000e-04])\r\n[-1.00000000e-04 -8.18181818e-05 -6.36363636e-05 -4.54545455e-05\r\n -2.72727273e-05 -9.09090909e-06  9.09090909e-06  2.72727273e-05\r\n  4.54545455e-05  6.36363636e-05  8.18181818e-05  1.00000000e-04]\r\n.....\r\n13 steps\r\ntensor([-0.0001, -0.0001, -0.0001, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\r\n         0.0000,  0.0000,  0.0001,  0.0001,  0.0001])\r\n[-1.00000000e-04 -8.33333333e-05 -6.66666667e-05 -5.00000000e-05\r\n -3.33333333e-05 -1.66666667e-05  0.00000000e+00  1.66666667e-05\r\n  3.33333333e-05  5.00000000e-05  6.66666667e-05  8.33333333e-05\r\n  1.00000000e-04]\r\n.....\r\n14 steps\r\ntensor([-1.0000e-04, -8.4615e-05, -6.9231e-05, -5.3846e-05, -3.8462e-05,\r\n        -2.3077e-05, -7.6923e-06,  7.6923e-06,  2.3077e-05,  3.8462e-05,\r\n         5.3846e-05,  6.9231e-05,  8.4615e-05,  1.0000e-04])\r\n[-1.00000000e-04 -8.46153846e-05 -6.92307692e-05 -5.38461538e-05\r\n -3.84615385e-05 -2.30769231e-05 -7.69230769e-06  7.69230769e-06\r\n  2.30769231e-05  3.84615385e-05  5.38461538e-05  6.92307692e-05\r\n  8.46153846e-05  1.00000000e-04]\r\n```\r\nCuriously, when I set an even ""step"" everything seems to work, if I set an odd number of step it seems there is a lack of resolution.  Note that this doesn't happen in numpy. Moreover, I tried with pytorch 0.3 and the issue doesn't arise.  I  noticed the same behavior with torch.range and torch.arange. \r\n\r\nWhat could be the problem? ",high priority,ailzhang,"I noticed a weird behavior when using torch.linspace.  Please take a look into the following code, where I compare the output of pytorch and numpy for different number of steps:\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\nimport sys\r\n\r\nmult_factor=0.00002\r\n\r\n\r\n# Torch\r\nprint('11 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=11)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,11)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\nprint('12 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=12)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,12)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\nprint('13 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=13)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,13)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\n\r\nprint('14 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=14)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,14)* mult_factor\r\nprint(n_)\r\n```\r\nIf I run the code above I have the following output:\r\n```\r\n11 steps\r\ntensor([-0.0001, -0.0001, -0.0001, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\r\n         0.0001,  0.0001,  0.0001])\r\n[-1.e-04 -8.e-05 -6.e-05 -4.e-05 -2.e-05  0.e+00  2.e-05  4.e-05  6.e-05\r\n  8.e-05  1.e-04]\r\n.....\r\n12 steps\r\ntensor([-1.0000e-04, -8.1818e-05, -6.3636e-05, -4.5455e-05, -2.7273e-05,\r\n        -9.0909e-06,  9.0909e-06,  2.7273e-05,  4.5455e-05,  6.3636e-05,\r\n         8.1818e-05,  1.0000e-04])\r\n[-1.00000000e-04 -8.18181818e-05 -6.36363636e-05 -4.54545455e-05\r\n -2.72727273e-05 -9.09090909e-06  9.09090909e-06  2.72727273e-05\r\n  4.54545455e-05  6.36363636e-05  8.18181818e-05  1.00000000e-04]\r\n.....\r\n13 steps\r\ntensor([-0.0001, -0.0001, -0.0001, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\r\n         0.0000,  0.0000,  0.0001,  0.0001,  0.0001])\r\n[-1.00000000e-04 -8.33333333e-05 -6.66666667e-05 -5.00000000e-05\r\n -3.33333333e-05 -1.66666667e-05  0.00000000e+00  1.66666667e-05\r\n  3.33333333e-05  5.00000000e-05  6.66666667e-05  8.33333333e-05\r\n  1.00000000e-04]\r\n.....\r\n14 steps\r\ntensor([-1.0000e-04, -8.4615e-05, -6.9231e-05, -5.3846e-05, -3.8462e-05,\r\n        -2.3077e-05, -7.6923e-06,  7.6923e-06,  2.3077e-05,  3.8462e-05,\r\n         5.3846e-05,  6.9231e-05,  8.4615e-05,  1.0000e-04])\r\n[-1.00000000e-04 -8.46153846e-05 -6.92307692e-05 -5.38461538e-05\r\n -3.84615385e-05 -2.30769231e-05 -7.69230769e-06  7.69230769e-06\r\n  2.30769231e-05  3.84615385e-05  5.38461538e-05  6.92307692e-05\r\n  8.46153846e-05  1.00000000e-04]\r\n```\r\nCuriously, when I set an even ""step"" everything seems to work, if I set an odd number of step it seems there is a lack of resolution.  Note that this doesn't happen in numpy. Moreover, I tried with pytorch 0.3 and the issue doesn't arise.  I  noticed the same behavior with torch.range and torch.arange. \r\n\r\nWhat could be the problem? ","python\r\nimport torch\r\nimport numpy as np\r\nimport sys\r\n\r\nmult_factor=0.00002\r\n\r\n\r\n# Torch\r\nprint('11 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=11)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,11)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\nprint('12 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=12)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,12)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\nprint('13 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=13)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,13)* mult_factor\r\nprint(n_)\r\nprint('.....')\r\n\r\n\r\nprint('14 steps')\r\nn_ = torch.linspace(-5.0, 5.0,steps=14)* mult_factor\r\nprint(n_)\r\n\r\nn_ = np.linspace(-5.0, 5.0,14)* mult_factor\r\nprint(n_)\r\n"
12190,"Slowdown in distributions log_prob methods## \U0001f41b Bug\r\n\r\nOne of Pyro's branches (https://github.com/uber/pyro/pull/1431) is running against PyTorch's nightly build, and we noticed that the unit test stage in CI is almost twice as slow as compared to the 0.4.0 release. Many of the slow tests turn out to be HMC tests (https://github.com/uber/pyro/issues/1421), and the slowdown seems to mostly be in the distribution's `log_prob` methods. Pasting the results below for the normal distribution, but I am seeing this for other distributions too.\r\n\r\n## To Reproduce\r\n\r\n**version: 0.4.0**\r\n\r\n\r\n**master (1.0.0a0+ab9a597)**\r\n\r\n\r\nNote that the `log_prob` method hasn't changed, but the last line takes almost twice as long. The only thing I can think of is that the `broadcast_all` method in the constructor is now different, and the expanded instances are somehow slower. I am still investigating this.\r\n\r\n## Environment\r\n\r\n```\r\n  $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+ab9a597\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (1.0.0a0+ab9a597, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     1.0.0a0+ab9a597           <pip>\r\n[conda] torch                     0.5.0a0+2431eac           <pip>\r\n[conda] torch                     1.0.0a0+6ff568d           <pip>\r\n[conda] torch                     0.5.0a0+6660a12           <pip>\r\n[conda] torch                     0.5.0a0+35d52db           <pip>\r\n[conda] torch                     0.5.0a0+6c3792b           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\n## Additional context\r\n\r\nI also noticed that certain functions like `torch.randn` in the nightly build are almost 2X slower than compiling the source code on my system locally. That's the reason why I am benchmarking against the local build and not the pytorch nightly build.",module: performance,cpuhrsch|ailzhang,"## \U0001f41b Bug\r\n\r\nOne of Pyro's branches (https://github.com/uber/pyro/pull/1431) is running against PyTorch's nightly build, and we noticed that the unit test stage in CI is almost twice as slow as compared to the 0.4.0 release. Many of the slow tests turn out to be HMC tests (https://github.com/uber/pyro/issues/1421), and the slowdown seems to mostly be in the distribution's `log_prob` methods. Pasting the results below for the normal distribution, but I am seeing this for other distributions too.\r\n\r\n## To Reproduce\r\n\r\n**version: 0.4.0**\r\n```python\r\n>>> import torch\r\n>>> import torch.distributions as dist\r\n>>> torch.__version__\r\n '0.4.0'\r\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\r\n\r\n>>> %timeit torch.randn(1000, 2)\r\n16.2 \xb5s \xb1 342 ns per loop (mean \xb1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\r\n45.8 ms \xb1 1.6 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n\r\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.476309 s\r\nFile: /Users/npradhan/miniconda2/envs/pytorch-36/lib/python3.6/site-packages/torch/distributions/normal.py\r\nFunction: log_prob at line 62\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    62                                               def log_prob(self, value):\r\n    63     10000       5511.0      0.6      1.2          if self._validate_args:\r\n    64                                                       self._validate_sample(value)\r\n    65                                                   # compute the variance\r\n    66     10000      23427.0      2.3      4.9          var = (self.scale ** 2)\r\n    67     10000     180867.0     18.1     38.0          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\r\n    68     10000     266504.0     26.7     56.0          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\r\n```\r\n\r\n**master (1.0.0a0+ab9a597)**\r\n```python\r\n>>> import torch\r\n>>> import torch.distributions as dist\r\n>>> torch.__version__\r\n '1.0.0a0+ab9a597'\r\n\r\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\r\n\r\n>>> %timeit torch.randn(1000, 2)\r\n17.8 \xb5s \xb1 132 ns per loop (mean \xb1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\r\n72.1 ms \xb1 1.78 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n\r\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.782675 s\r\nFile: /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages/torch/distributions/normal.py\r\nFunction: log_prob at line 70\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    70                                               def log_prob(self, value):\r\n    71     10000       8708.0      0.9      1.1          if self._validate_args:\r\n    72                                                       self._validate_sample(value)\r\n    73                                                   # compute the variance\r\n    74     10000      46140.0      4.6      5.9          var = (self.scale ** 2)\r\n    75     10000     135462.0     13.5     17.3          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\r\n    76     10000     592365.0     59.2     75.7          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\r\n```\r\n\r\nNote that the `log_prob` method hasn't changed, but the last line takes almost twice as long. The only thing I can think of is that the `broadcast_all` method in the constructor is now different, and the expanded instances are somehow slower. I am still investigating this.\r\n\r\n## Environment\r\n\r\n```\r\n  $ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+ab9a597\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (1.0.0a0+ab9a597, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     1.0.0a0+ab9a597           <pip>\r\n[conda] torch                     0.5.0a0+2431eac           <pip>\r\n[conda] torch                     1.0.0a0+6ff568d           <pip>\r\n[conda] torch                     0.5.0a0+6660a12           <pip>\r\n[conda] torch                     0.5.0a0+35d52db           <pip>\r\n[conda] torch                     0.5.0a0+6c3792b           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\n## Additional context\r\n\r\nI also noticed that certain functions like `torch.randn` in the nightly build are almost 2X slower than compiling the source code on my system locally. That's the reason why I am benchmarking against the local build and not the pytorch nightly build.","python\r\n>>> import torch\r\n>>> import torch.distributions as dist\r\n>>> torch.__version__\r\n '0.4.0'\r\n>>> d = dist.Normal(torch.zeros(1000, 2), torch.ones(1000, 2))\r\n\r\n>>> %timeit torch.randn(1000, 2)\r\n16.2 \xb5s \xb1 342 ns per loop (mean \xb1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit [d.log_prob(torch.randn(1000, 2)) for _ in range(1000)]\r\n45.8 ms \xb1 1.6 ms per loop (mean \xb1 std. dev. of 7 runs, 10 loops each)\r\n\r\n>>> %lprun -f d.log_prob [d.log_prob(torch.randn(1000, 2)) for _ in range(10000)]\r\nTimer unit: 1e-06 s\r\n\r\nTotal time: 0.476309 s\r\nFile: /Users/npradhan/miniconda2/envs/pytorch-36/lib/python3.6/site-packages/torch/distributions/normal.py\r\nFunction: log_prob at line 62\r\n\r\nLine #      Hits         Time  Per Hit   % Time  Line Contents\r\n==============================================================\r\n    62                                               def log_prob(self, value):\r\n    63     10000       5511.0      0.6      1.2          if self._validate_args:\r\n    64                                                       self._validate_sample(value)\r\n    65                                                   # compute the variance\r\n    66     10000      23427.0      2.3      4.9          var = (self.scale ** 2)\r\n    67     10000     180867.0     18.1     38.0          log_scale = math.log(self.scale) if isinstance(self.scale, Number) else self.scale.log()\r\n    68     10000     266504.0     26.7     56.0          return -((value - self.loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))\r\n"
11206,"torch.tanh returns wrong result on sliced tensors## Issue description\r\n\r\n`torch.tanh` seems to yield the wrong result, if the input tensor is sliced.\r\nA `.contiguous` call seems to fix the behavior.\r\n[Link](https://discuss.pytorch.org/t/different-behaviors-on-slices/24365) to thread in the board.\r\n\r\n## Code example\r\n\r\n\r\n\r\n## System Info\r\n\r\n- OS: Ubuntu 16.04.5 LTS\r\n- PyTorch version: 0.5.0a0+6c7fb15 (installed from source)\r\n- Python version: 2.7\r\n- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CMake version: 3.11.1\r\n\r\nAlso reported on `0.4.1` in the thread.\r\n",high priority,cpuhrsch,"## Issue description\r\n\r\n`torch.tanh` seems to yield the wrong result, if the input tensor is sliced.\r\nA `.contiguous` call seems to fix the behavior.\r\n[Link](https://discuss.pytorch.org/t/different-behaviors-on-slices/24365) to thread in the board.\r\n\r\n## Code example\r\n\r\n```python\r\nx = torch.zeros(3, 3, 1, 2)\r\nx[:, 2, ...] = 1.\r\nprint(x[:, 2, ...])\r\nprint(torch.tanh(x[...]))\r\nprint(torch.tanh(x[:, 2, ...])) # wrong result\r\nprint(torch.tanh(x[:, 2, ...].contiguous())) # right result\r\n```\r\n\r\n## System Info\r\n\r\n- OS: Ubuntu 16.04.5 LTS\r\n- PyTorch version: 0.5.0a0+6c7fb15 (installed from source)\r\n- Python version: 2.7\r\n- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CMake version: 3.11.1\r\n\r\nAlso reported on `0.4.1` in the thread.\r\n","python\r\nx = torch.zeros(3, 3, 1, 2)\r\nx[:, 2, ...] = 1.\r\nprint(x[:, 2, ...])\r\nprint(torch.tanh(x[...]))\r\nprint(torch.tanh(x[:, 2, ...])) # wrong result\r\nprint(torch.tanh(x[:, 2, ...].contiguous())) # right result\r\n"
10978,"[feature request] padding for torch.cat Currently, `torch.cat` only allows to concatenate equal-dimensional Tensors (except in the dimension catting).\r\n\r\nFor example:\r\n\r\n\r\n\r\nThe proposal is to add a `padding` option to `torch.cat`, which will find the maximum size of Tensors in each dimension, and will place each Tensor within that larger padded Tensor.\r\n\r\n\r\n\r\nExample:\r\n\r\n\r\n\r\nIf the TensorList has size mismatches in multiple dimensions, the output size will be computed by finding the max of sizes in each dimension, except the dimension being concatenated.\r\n\r\nFor example:\r\n\r\n",triaged|enhancement|module: viewing and reshaping,soumith,"Currently, `torch.cat` only allows to concatenate equal-dimensional Tensors (except in the dimension catting).\r\n\r\nFor example:\r\n\r\n```python\r\n>>> x = torch.ones(2, 3)\r\n>>> y = torch.zeros(1, 3)\r\n>>> torch.cat([x, y], dim=0)\r\n\r\ntensor([[1., 1., 1.],\r\n              [1., 1., 1.],\r\n              [0., 0., 0.]])\r\n\r\n>>> x = torch.ones(2, 3)\r\n>>> y = torch.zeros(1, 4)\r\n>>> torch.cat([x, y], dim=0)\r\n\r\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 4 in dimension 1 at ../aten/src/TH/generic/THTensorMoreMath.cpp:1348\r\n```\r\n\r\nThe proposal is to add a `padding` option to `torch.cat`, which will find the maximum size of Tensors in each dimension, and will place each Tensor within that larger padded Tensor.\r\n\r\n```python\r\ntorch.cat(TensorList, dim, pad=True, pad_value=0.0)\r\n```\r\n\r\nExample:\r\n\r\n```python\r\n>>> x = torch.ones(2, 3)\r\n>>> y = torch.zeros(1, 4)\r\n>>> torch.cat([x, y], dim=0, pad=True, pad_value=0.5)\r\n\r\ntensor([[1., 1., 1., 0.5],\r\n              [1., 1., 1., 0.5],\r\n              [0., 0., 0., 0.]])\r\n```\r\n\r\nIf the TensorList has size mismatches in multiple dimensions, the output size will be computed by finding the max of sizes in each dimension, except the dimension being concatenated.\r\n\r\nFor example:\r\n\r\n```python\r\n>>> x = torch.ones(1, 2, 3, 4, 5)\r\n>>> y = torch.ones(3, 9, 2, 5, 1)\r\n>>> torch.cat([x, y], dim=0, pad=True).size()\r\ntorch.Size([4, 9, 3, 5, 5])\r\n```","python\r\n>>> x = torch.ones(2, 3)\r\n>>> y = torch.zeros(1, 3)\r\n>>> torch.cat([x, y], dim=0)\r\n\r\ntensor([[1., 1., 1.],\r\n              [1., 1., 1.],\r\n              [0., 0., 0.]])\r\n\r\n>>> x = torch.ones(2, 3)\r\n>>> y = torch.zeros(1, 4)\r\n>>> torch.cat([x, y], dim=0)\r\n\r\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 4 in dimension 1 at ../aten/src/TH/generic/THTensorMoreMath.cpp:1348\r\n"
10876,"[pytorch] requires_grad=False from torch.tensor ignored when the input tensor has requires_grad=True ## Issue description\r\n\r\nAs shown by the following example, the expected behavior (at least from me) is to copy only the data from `t1` to `t2`, however `torch.tensor()` seems to build up a computation graph from `t1` instead ...\r\n\r\n## Code example\r\n\r\n\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.6.3\r\n",high priority,weiyangfb,"## Issue description\r\n\r\nAs shown by the following example, the expected behavior (at least from me) is to copy only the data from `t1` to `t2`, however `torch.tensor()` seems to build up a computation graph from `t1` instead ...\r\n\r\n## Code example\r\n\r\n```python\r\n>>> t1 = torch.tensor([1,1,1], dtype=torch.float, requires_grad=True)\r\n>>> t2 = torch.tensor(t1, requires_grad=False)\r\n>>> t2.requires_grad\r\nTrue\r\n>>> t1\r\ntensor([1., 1., 1.], requires_grad=True)\r\n>>> t2\r\ntensor([1., 1., 1.], grad_fn=<CopyBackwards>)\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.6.3\r\n","python\r\n>>> t1 = torch.tensor([1,1,1], dtype=torch.float, requires_grad=True)\r\n>>> t2 = torch.tensor(t1, requires_grad=False)\r\n>>> t2.requires_grad\r\nTrue\r\n>>> t1\r\ntensor([1., 1., 1.], requires_grad=True)\r\n>>> t2\r\ntensor([1., 1., 1.], grad_fn=<CopyBackwards>)\r\n"
10654,"torch.jit.trace fails when the model has the function nn.functional.interpolate## Issue description\r\n\r\ntorch.jit.trace fails when the model contain nn.functional.interpolate, returning the error:\r\n\r\n## Code example\r\n\r\n## System Info\r\n\r\n``` bash\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+152762a\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Fedora release 28 (Twenty Eight)\r\nGCC version: (GCC) 7.3.0\r\nCMake version: version 3.11.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 396.45\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n```",oncall: jit,eellison,"## Issue description\r\n\r\ntorch.jit.trace fails when the model contain nn.functional.interpolate, returning the error:\r\n```bash\r\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\r\n```\r\n## Code example\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.jit import trace\r\nclass test(nn.Module):\r\n        def __init__(self):\r\n                super(test, self).__init__()\r\n                self.conv = nn.Conv2d(1, 32, kernel_size=3,padding=1)\r\n\r\n        def forward(self, x):\r\n                y = self.conv(x)\r\n                w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\r\n                return w\r\n\r\nf=test()\r\nx = torch.zeros(1,1,28,28)\r\nret = trace(x)(f)\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-65fe10b64581> in <module>()\r\n     14 f=test()\r\n     15 x = torch.zeros(1,1,28,28)\r\n---> 16 ret = trace(x)(f)\r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in wrapper(func)\r\n    288 \r\n    289         module = TopLevelTracedModule(func, **executor_options)\r\n--> 290         module._create_method_from_trace('forward', func, args)\r\n    291         return module\r\n    292 \r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    473             hook(self, input)\r\n    474         if torch._C._get_tracing_state():\r\n--> 475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n    477             result = self.forward(*input, **kwargs)\r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    463         tracing_state._traced_module_stack.append(self)\r\n    464         try:\r\n--> 465             result = self.forward(*input, **kwargs)\r\n    466         finally:\r\n    467             tracing_state.pop_scope()\r\n\r\n<ipython-input-1-65fe10b64581> in forward(self, x)\r\n      9         def forward(self, x):\r\n     10                 y = self.conv(x)\r\n---> 11                 w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\r\n     12                 return w\r\n     13 \r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/functional.py in interpolate(input, size, scale_factor, mode, align_corners)\r\n   2069         raise NotImplementedError(""Got 4D input, but linear mode needs 3D input"")\r\n   2070     elif input.dim() == 4 and mode == 'bilinear':\r\n-> 2071         return torch._C._nn.upsample_bilinear2d(input, _output_size(2), align_corners)\r\n   2072     elif input.dim() == 4 and mode == 'trilinear':\r\n   2073         raise NotImplementedError(""Got 4D input, but trilinear mode needs 5D input"")\r\n\r\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\r\n```\r\n## System Info\r\n\r\n``` bash\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+152762a\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Fedora release 28 (Twenty Eight)\r\nGCC version: (GCC) 7.3.0\r\nCMake version: version 3.11.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 396.45\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n```","bash\r\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\r\n"
10353,"ATen Core Headers Were Installed to ATen/core/core/*.h instead of ATen/core/*.h## Issue Description\r\n\r\nI git clone pytorch to `$PYTORCH_HOME`, and installed it using cmake to directory `$PYTORCH_HOME/build/install`. The ATen core headers were installed to `$PYTORCH_HOME/build/install/include/ATen/core/core/*.h` instead of `$PYTORCH_HOME/build/install/include/ATen/core/*.h` (**I think this is abnormal, because compiling custom operators requires `<ATen/core/*.h>`. See below.**).\r\n\r\nThe result of `tree $PYTORCH_HOME/build/install/include/ATen`:\r\n\r\n```\r\nATen\r\n`-- core\r\n    `-- core\r\n        |-- AlignOf.h\r\n        |-- ArrayRef.h\r\n        |-- ATenCoreTest.h\r\n        |-- Backtrace.h\r\n        |-- C++17.h\r\n        |-- CoreAPI.h\r\n        |-- DeviceType.h\r\n        |-- Error.h\r\n        |-- Half.h\r\n        |-- Half-inl.h\r\n        |-- IdWrapper.h\r\n        |-- intrusive_ptr.h\r\n        |-- optional.h\r\n        |-- SmallVector.h\r\n        `-- UniqueVoidPtr.h\r\n```\r\n\r\nIn the [DensePose](https://github.com/facebookresearch/DensePose) project which uses Caffe2, when I built custom operators:\r\n\r\n\r\nThe `make` command threw error of not finding `<ATen/core/Error.h>` for `caffe2/core/logging.h`:\r\n```\r\nScanning dependencies of target caffe2_detectron_custom_ops\r\n[ 12%] Building CXX object CMakeFiles/caffe2_detectron_custom_ops.dir/detectron/ops/pool_points_interp.cc.o\r\nIn file included from /home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/allocator.h:6:0,\r\n                 from /home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/context.h:9,\r\n                 from /home/users/houjing.huang/Project/DensePose/detectron/ops/pool_points_interp.h:13,\r\n                 from /home/users/houjing.huang/Project/DensePose/detectron/ops/pool_points_interp.cc:10:\r\n/home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/logging.h:10:29: fatal error: ATen/core/Error.h: No such file or directory\r\n #include <ATen/core/Error.h>\r\n                             ^\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/caffe2_detectron_custom_ops.dir/detectron/ops/pool_points_interp.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/caffe2_detectron_custom_ops.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```\r\n\r\nI found these lines in file `$PYTORCH_HOME/build/install_manifest.txt`:\r\n```\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/ATenCoreTest.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/AlignOf.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/ArrayRef.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Backtrace.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/C++17.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/CoreAPI.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/DeviceType.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Error.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Half-inl.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Half.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/IdWrapper.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/SmallVector.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/UniqueVoidPtr.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/intrusive_ptr.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/optional.h\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: Caffe2\r\n- How you installed PyTorch (conda, pip, source): source, referring to [caffe2.ai](https://caffe2.ai/docs/getting-started.html?platform=centos&configuration=cloud)\r\n- Build command to install Pytorch/Caffe2 (if compiling from source): \r\n\r\n    \r\n\r\n- OS: CentOS Linux release 7.4.1708\r\n- PyTorch version: master branch, commit f57e4ce1d59aab53e48b0ae93a7ef7e9a4dc1e2e\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: CUDA 8.0, cuDNN 7\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source): 4.8.5\r\n- CMake version: 3.12.0\r\n- Versions of any other relevant libraries: DensePose master branch, commit [e118a4](https://github.com/facebookresearch/DensePose/tree/e118a4c352cbaf849d70f47886ce3f6cba76116e)\r\n\r\n## Log Files When Installing Pytorch/Caffe2\r\n\r\n- The log of pytorch `cmake ..`: [log](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/cmake_output_houjing.huang.txt)\r\n- The log of pytorch `make -j12 install`: [log](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/make_install_output_houjing.huang.txt)\r\n- [`install_manifest.txt`](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/install_manifest.txt)\r\n\r\n## Is It A BUG?\r\n\r\nI manually copied `$PYTORCH_HOME/build/install/include/ATen/core/core/*.h` to `$PYTORCH_HOME/build/install/include/ATen/core/*.h` and then it's ok to build custom operators and run the tests.\r\n\r\nIt seems a bug of Pytorch/Caffe2 `make install`?",high priority|caffe2,ezyang,"## Issue Description\r\n\r\nI git clone pytorch to `$PYTORCH_HOME`, and installed it using cmake to directory `$PYTORCH_HOME/build/install`. The ATen core headers were installed to `$PYTORCH_HOME/build/install/include/ATen/core/core/*.h` instead of `$PYTORCH_HOME/build/install/include/ATen/core/*.h` (**I think this is abnormal, because compiling custom operators requires `<ATen/core/*.h>`. See below.**).\r\n\r\nThe result of `tree $PYTORCH_HOME/build/install/include/ATen`:\r\n\r\n```\r\nATen\r\n`-- core\r\n    `-- core\r\n        |-- AlignOf.h\r\n        |-- ArrayRef.h\r\n        |-- ATenCoreTest.h\r\n        |-- Backtrace.h\r\n        |-- C++17.h\r\n        |-- CoreAPI.h\r\n        |-- DeviceType.h\r\n        |-- Error.h\r\n        |-- Half.h\r\n        |-- Half-inl.h\r\n        |-- IdWrapper.h\r\n        |-- intrusive_ptr.h\r\n        |-- optional.h\r\n        |-- SmallVector.h\r\n        `-- UniqueVoidPtr.h\r\n```\r\n\r\nIn the [DensePose](https://github.com/facebookresearch/DensePose) project which uses Caffe2, when I built custom operators:\r\n```bash\r\n# In DensePose project\r\nmkdir build && cd build\r\ncmake .. -DCaffe2_DIR=$PYTORCH_HOME/build/install/share/cmake/Caffe2\r\nmake\r\n```\r\n\r\nThe `make` command threw error of not finding `<ATen/core/Error.h>` for `caffe2/core/logging.h`:\r\n```\r\nScanning dependencies of target caffe2_detectron_custom_ops\r\n[ 12%] Building CXX object CMakeFiles/caffe2_detectron_custom_ops.dir/detectron/ops/pool_points_interp.cc.o\r\nIn file included from /home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/allocator.h:6:0,\r\n                 from /home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/context.h:9,\r\n                 from /home/users/houjing.huang/Project/DensePose/detectron/ops/pool_points_interp.h:13,\r\n                 from /home/users/houjing.huang/Project/DensePose/detectron/ops/pool_points_interp.cc:10:\r\n/home/users/houjing.huang/Software/pytorch/build/install/include/caffe2/core/logging.h:10:29: fatal error: ATen/core/Error.h: No such file or directory\r\n #include <ATen/core/Error.h>\r\n                             ^\r\ncompilation terminated.\r\nmake[2]: *** [CMakeFiles/caffe2_detectron_custom_ops.dir/detectron/ops/pool_points_interp.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/caffe2_detectron_custom_ops.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```\r\n\r\nI found these lines in file `$PYTORCH_HOME/build/install_manifest.txt`:\r\n```\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/ATenCoreTest.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/AlignOf.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/ArrayRef.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Backtrace.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/C++17.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/CoreAPI.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/DeviceType.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Error.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Half-inl.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/Half.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/IdWrapper.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/SmallVector.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/UniqueVoidPtr.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/intrusive_ptr.h\r\n/mnt/data-1/data/houjing.huang/Software/pytorch/build/install/include/ATen/core/core/optional.h\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: Caffe2\r\n- How you installed PyTorch (conda, pip, source): source, referring to [caffe2.ai](https://caffe2.ai/docs/getting-started.html?platform=centos&configuration=cloud)\r\n- Build command to install Pytorch/Caffe2 (if compiling from source): \r\n\r\n    ```bash\r\n    git clone --recursive https://github.com/pytorch/pytorch\r\n    cd pytorch\r\n    mkdir build && cd build\r\n    cmake \\\r\n    -DCUDA_TOOLKIT_ROOT_DIR=$HOME/Software/cuda-8.0 \\\r\n    -DCMAKE_INSTALL_PREFIX=${PWD}/install ..\r\n    make -j12 install\r\n    ```\r\n\r\n- OS: CentOS Linux release 7.4.1708\r\n- PyTorch version: master branch, commit f57e4ce1d59aab53e48b0ae93a7ef7e9a4dc1e2e\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: CUDA 8.0, cuDNN 7\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source): 4.8.5\r\n- CMake version: 3.12.0\r\n- Versions of any other relevant libraries: DensePose master branch, commit [e118a4](https://github.com/facebookresearch/DensePose/tree/e118a4c352cbaf849d70f47886ce3f6cba76116e)\r\n\r\n## Log Files When Installing Pytorch/Caffe2\r\n\r\n- The log of pytorch `cmake ..`: [log](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/cmake_output_houjing.huang.txt)\r\n- The log of pytorch `make -j12 install`: [log](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/make_install_output_houjing.huang.txt)\r\n- [`install_manifest.txt`](https://github.com/huanghoujing/pytorch-caffe2-cmake-log/blob/master/install_manifest.txt)\r\n\r\n## Is It A BUG?\r\n\r\nI manually copied `$PYTORCH_HOME/build/install/include/ATen/core/core/*.h` to `$PYTORCH_HOME/build/install/include/ATen/core/*.h` and then it's ok to build custom operators and run the tests.\r\n\r\nIt seems a bug of Pytorch/Caffe2 `make install`?",bash\r\n# In DensePose project\r\nmkdir build && cd build\r\ncmake .. -DCaffe2_DIR=$PYTORCH_HOME/build/install/share/cmake/Caffe2\r\nmake\r\n
10345,"Potential Bug in torch.symeig()## Issue description\r\nThe eigenvectors produced by `torch.symeig()` are not always orthonormal.\r\n\r\n## Code example\r\n\r\n\r\nHere is the result:\r\n```plain\r\ntensor([[ 0.3573,  0.0288,  0.4334],\r\n        [ 3.1050,  6.5767, -3.2518],\r\n        [ 2.3730,  2.2232,  2.1961]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<SymeigBackward>)\r\ntensor([[ 0.3164, -0.1103,  1.8639],\r\n        [-0.1103, 63.4675, 14.8483],\r\n        [ 1.8639, 14.8483, 15.3969]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<MmBackward>)\r\ntensor([[ 15.3995,  25.7064,  -4.7303],\r\n        [ 25.7064,  48.1965, -16.4908],\r\n        [ -4.7303, -16.4908,  15.5848]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<MmBackward>)\r\ntensor([3.9242, 6.9424, 3.9478],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<NormBackward1>)\r\ntensor([0.5625, 7.9667, 3.9239],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<NormBackward1>)\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCMake version: version 3.11.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[conda] Could not collect",todo,ailzhang,"## Issue description\r\nThe eigenvectors produced by `torch.symeig()` are not always orthonormal.\r\n\r\n## Code example\r\n```python\r\nimport torch\r\n\r\n# Create a random symmetric matrix\r\np, q = 10, 3\r\ntorch.manual_seed(0)\r\nin_tensor = torch.rand(p, q, dtype=torch.float64, requires_grad=True).cuda()\r\ncov_in = torch.mm(in_tensor.t(), in_tensor)\r\n\r\n_, eig_vecs = torch.symeig(cov_in)\r\n\r\nprint(eig_vecs)\r\nprint(torch.mm(eig_vecs, eig_vecs.t()))\r\nprint(torch.mm(eig_vecs.t(), eig_vecs))\r\nprint(eig_vecs.norm(dim=0))\r\nprint(eig_vecs.norm(dim=1))\r\n```\r\n\r\nHere is the result:\r\n```plain\r\ntensor([[ 0.3573,  0.0288,  0.4334],\r\n        [ 3.1050,  6.5767, -3.2518],\r\n        [ 2.3730,  2.2232,  2.1961]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<SymeigBackward>)\r\ntensor([[ 0.3164, -0.1103,  1.8639],\r\n        [-0.1103, 63.4675, 14.8483],\r\n        [ 1.8639, 14.8483, 15.3969]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<MmBackward>)\r\ntensor([[ 15.3995,  25.7064,  -4.7303],\r\n        [ 25.7064,  48.1965, -16.4908],\r\n        [ -4.7303, -16.4908,  15.5848]],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<MmBackward>)\r\ntensor([3.9242, 6.9424, 3.9478],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<NormBackward1>)\r\ntensor([0.5625, 7.9667, 3.9239],\r\n       device='cuda:0', dtype=torch.float64, grad_fn=<NormBackward1>)\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4\r\nCMake version: version 3.11.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: Tesla K80\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.1.3\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[conda] Could not collect","python\r\nimport torch\r\n\r\n# Create a random symmetric matrix\r\np, q = 10, 3\r\ntorch.manual_seed(0)\r\nin_tensor = torch.rand(p, q, dtype=torch.float64, requires_grad=True).cuda()\r\ncov_in = torch.mm(in_tensor.t(), in_tensor)\r\n\r\n_, eig_vecs = torch.symeig(cov_in)\r\n\r\nprint(eig_vecs)\r\nprint(torch.mm(eig_vecs, eig_vecs.t()))\r\nprint(torch.mm(eig_vecs.t(), eig_vecs))\r\nprint(eig_vecs.norm(dim=0))\r\nprint(eig_vecs.norm(dim=1))\r\n"
9750,"Cannot build from source with anaconda python2.7I get a string problem when I try to build pytorch/caffe2 from source with anaconda python2.7.\r\nI create a virtual env with python2.7. And I activate the env. Then I try to build pytorch/caffe2. However, the result package is for python3.6. I think the problem is the same as [caffe2 issue 2360](https://github.com/caffe2/caffe2/issues/2360)\r\n\r\nIf I run \r\n```\r\necho $PATH\r\n```\r\n I get \r\n```\r\n/home/dell/anaconda3/envs/detectron/bin:/usr/local/cuda/bin:/home/dell/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\r\n```\r\nYou can see that the /home/dell/anaconda3/envs/detectron/bin is before /home/dell/anaconda3/bin. But I still get python3 package.\r\nThe output of \r\n\r\nis /home/dell/anaconda3/envs/detectron/bin/python.\r\nThe output of \r\n```\r\npython --version\r\n```\r\nis Python 2.7.15 :: Anaconda, Inc.\r\n\r\nWhen I build it, I get the the following output\r\n```\r\n++ python --version                                                               \r\n++ grep --only-matching '[0-9]\\.[0-9]\\.[0-9]*'                                                  \r\n+ PYTHON_VERSION=2.7.15\r\n```\r\nI think the program already get the right python version as I want. However, the result python version of caffe2 is still python2.",caffe2,pjh5,"I get a string problem when I try to build pytorch/caffe2 from source with anaconda python2.7.\r\nI create a virtual env with python2.7. And I activate the env. Then I try to build pytorch/caffe2. However, the result package is for python3.6. I think the problem is the same as [caffe2 issue 2360](https://github.com/caffe2/caffe2/issues/2360)\r\n\r\nIf I run \r\n```\r\necho $PATH\r\n```\r\n I get \r\n```\r\n/home/dell/anaconda3/envs/detectron/bin:/usr/local/cuda/bin:/home/dell/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\r\n```\r\nYou can see that the /home/dell/anaconda3/envs/detectron/bin is before /home/dell/anaconda3/bin. But I still get python3 package.\r\nThe output of \r\n```bash\r\nwhich python\r\n```\r\nis /home/dell/anaconda3/envs/detectron/bin/python.\r\nThe output of \r\n```\r\npython --version\r\n```\r\nis Python 2.7.15 :: Anaconda, Inc.\r\n\r\nWhen I build it, I get the the following output\r\n```\r\n++ python --version                                                               \r\n++ grep --only-matching '[0-9]\\.[0-9]\\.[0-9]*'                                                  \r\n+ PYTHON_VERSION=2.7.15\r\n```\r\nI think the program already get the right python version as I want. However, the result python version of caffe2 is still python2.",bash\r\nwhich python\r\n
9580,"[Caffe2] undefined reference to mkldnn_*## Issue description\r\nI'm trying to compile PyTorch, and it's failing at compiling Caffe2 when I use MKL-DNN with the following error:\r\n\r\n```\r\n[ 82%] Linking CXX executable ../bin/verify_api_visibility\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/op/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/op/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/verify_api_visibility] Error 1\r\n\r\ncaffe2/CMakeFiles/verify_api_visibility.dir/build.make:101: recipe for target 'bin/verify_api_visibility' failed\r\n\r\nCMakeFiles/Makefile2:923: recipe for target 'caffe2/CMakeFiles/verify_api_visibility.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/verify_api_visibility.dir/all] Error 2\r\n\r\nmake[1]: *** Waiting for unfinished jobs....\r\n\r\n[ 82%] Linking CXX executable ../bin/tbb_init_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/tbb_init_test] Error 1\r\n\r\ncaffe2/CMakeFiles/tbb_init_test.dir/build.make:101: recipe for target 'bin/tbb_init_test' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/tbb_init_test.dir/all] Error 2\r\n\r\nCMakeFiles/Makefile2:885: recipe for target 'caffe2/CMakeFiles/tbb_init_test.dir/all' failed\r\n\r\n[ 82%] Linking CXX executable ../bin/weakref_test\r\n\r\n[ 83%] Linking CXX executable ../bin/wrapdim_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/weakref_test] Error 1\r\n\r\ncaffe2/CMakeFiles/weakref_test.dir/build.make:101: recipe for target 'bin/weakref_test' failed\r\n\r\nCMakeFiles/Makefile2:847: recipe for target 'caffe2/CMakeFiles/weakref_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/weakref_test.dir/all] Error 2\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/wrapdim_test] Error 1\r\n\r\ncaffe2/CMakeFiles/wrapdim_test.dir/build.make:101: recipe for target 'bin/wrapdim_test' failed\r\n\r\nCMakeFiles/Makefile2:1075: recipe for target 'caffe2/CMakeFiles/wrapdim_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/wrapdim_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/undefined_tensor_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/undefined_tensor_test] Error 1\r\n\r\ncaffe2/CMakeFiles/undefined_tensor_test.dir/build.make:101: recipe for target 'bin/undefined_tensor_test' failed\r\n\r\nCMakeFiles/Makefile2:961: recipe for target 'caffe2/CMakeFiles/undefined_tensor_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/undefined_tensor_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/scalar_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\ncaffe2/CMakeFiles/scalar_test.dir/build.make:101: recipe for target 'bin/scalar_test' failed\r\n\r\nmake[2]: *** [bin/scalar_test] Error 1\r\n\r\nCMakeFiles/Makefile2:1182: recipe for target 'caffe2/CMakeFiles/scalar_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/scalar_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/scalar_tensor_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\ncaffe2/CMakeFiles/scalar_tensor_test.dir/build.make:101: recipe for target 'bin/scalar_tensor_test' failed\r\n\r\nmake[2]: *** [bin/scalar_tensor_test] Error 1\r\n\r\nCMakeFiles/Makefile2:999: recipe for target 'caffe2/CMakeFiles/scalar_tensor_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/scalar_tensor_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/native_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: Caffe2\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Build command you used (if compiling from source):\r\n\r\n- OS: Debian Stretch",caffe2,pjh5,"## Issue description\r\nI'm trying to compile PyTorch, and it's failing at compiling Caffe2 when I use MKL-DNN with the following error:\r\n\r\n```\r\n[ 82%] Linking CXX executable ../bin/verify_api_visibility\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/op/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/op/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/verify_api_visibility] Error 1\r\n\r\ncaffe2/CMakeFiles/verify_api_visibility.dir/build.make:101: recipe for target 'bin/verify_api_visibility' failed\r\n\r\nCMakeFiles/Makefile2:923: recipe for target 'caffe2/CMakeFiles/verify_api_visibility.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/verify_api_visibility.dir/all] Error 2\r\n\r\nmake[1]: *** Waiting for unfinished jobs....\r\n\r\n[ 82%] Linking CXX executable ../bin/tbb_init_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/tbb_init_test] Error 1\r\n\r\ncaffe2/CMakeFiles/tbb_init_test.dir/build.make:101: recipe for target 'bin/tbb_init_test' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/tbb_init_test.dir/all] Error 2\r\n\r\nCMakeFiles/Makefile2:885: recipe for target 'caffe2/CMakeFiles/tbb_init_test.dir/all' failed\r\n\r\n[ 82%] Linking CXX executable ../bin/weakref_test\r\n\r\n[ 83%] Linking CXX executable ../bin/wrapdim_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/weakref_test] Error 1\r\n\r\ncaffe2/CMakeFiles/weakref_test.dir/build.make:101: recipe for target 'bin/weakref_test' failed\r\n\r\nCMakeFiles/Makefile2:847: recipe for target 'caffe2/CMakeFiles/weakref_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/weakref_test.dir/all] Error 2\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/wrapdim_test] Error 1\r\n\r\ncaffe2/CMakeFiles/wrapdim_test.dir/build.make:101: recipe for target 'bin/wrapdim_test' failed\r\n\r\nCMakeFiles/Makefile2:1075: recipe for target 'caffe2/CMakeFiles/wrapdim_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/wrapdim_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/undefined_tensor_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nmake[2]: *** [bin/undefined_tensor_test] Error 1\r\n\r\ncaffe2/CMakeFiles/undefined_tensor_test.dir/build.make:101: recipe for target 'bin/undefined_tensor_test' failed\r\n\r\nCMakeFiles/Makefile2:961: recipe for target 'caffe2/CMakeFiles/undefined_tensor_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/undefined_tensor_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/scalar_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\ncaffe2/CMakeFiles/scalar_test.dir/build.make:101: recipe for target 'bin/scalar_test' failed\r\n\r\nmake[2]: *** [bin/scalar_test] Error 1\r\n\r\nCMakeFiles/Makefile2:1182: recipe for target 'caffe2/CMakeFiles/scalar_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/scalar_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/scalar_tensor_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n\r\ncaffe2/CMakeFiles/scalar_tensor_test.dir/build.make:101: recipe for target 'bin/scalar_tensor_test' failed\r\n\r\nmake[2]: *** [bin/scalar_tensor_test] Error 1\r\n\r\nCMakeFiles/Makefile2:999: recipe for target 'caffe2/CMakeFiles/scalar_tensor_test.dir/all' failed\r\n\r\nmake[1]: *** [caffe2/CMakeFiles/scalar_tensor_test.dir/all] Error 2\r\n\r\n[ 84%] Linking CXX executable ../bin/native_test\r\n\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_data_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_reorder_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_set_data_handle'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_forward_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_primitive_desc'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_at'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_engine_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_clone'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_destroy'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_get_output'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_convolution_backward_weights_desc_init'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_create'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_get_size'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_stream_submit'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_memory_primitive_desc_equal'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_s32'\r\n/opt/pytorch/build/lib/libcaffe2.so: undefined reference to `mkldnn_primitive_desc_query_pd'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: Caffe2\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Build command you used (if compiling from source):\r\n```cd /opt && git clone --recursive https://github.com/pytorch/pytorch \\\r\n    && cd pytorch && git submodule update --init && \\\r\n    MKLDNN_LIBRARY=/usr/local/lib \\\r\n    MKLDNN_INCLUDE_DIR=/usr/include \\\r\n    MKLDNN_LIB_DIR=/usr/local/lib \\\r\n    PYTHON_EXECUTABLE=/opt/conda/lib/libpython3.6m.so && \\\r\n    PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m && \\\r\n    export USE_OPENMP=1 && \\\r\n    export USE_MKLDNN=1 && \\\r\n    python setup.py install ```\r\n- OS: Debian Stretch",cd /opt && git clone --recursive https://github.com/pytorch/pytorch \\\r\n    && cd pytorch && git submodule update --init && \\\r\n    MKLDNN_LIBRARY=/usr/local/lib \\\r\n    MKLDNN_INCLUDE_DIR=/usr/include \\\r\n    MKLDNN_LIB_DIR=/usr/local/lib \\\r\n    PYTHON_EXECUTABLE=/opt/conda/lib/libpython3.6m.so && \\\r\n    PYTHON_INCLUDE_DIR=/opt/conda/include/python3.6m && \\\r\n    export USE_OPENMP=1 && \\\r\n    export USE_MKLDNN=1 && \\\r\n    python setup.py install 
9468,"[bug] Multiplication of tensor with numpy scalar does not always workThanks for the amazing software. I stumbled across (what I believe is a) bug when multiplying a torch tensor with a numpy scalar. \r\n\r\n## Issue description\r\n\r\nMultiplication of a torch tensor with numpy scalars exhibits unexpected behavior depending on the order of multiplication and datatypes. Specifically, multiplication of torch.FloatTensor with np.float32 does not work. Multiplication of torch.FloatTensor with np.float64 only works when written as `tensor * scalar` when tensor.requires_grad = True.\r\n\r\n## Code example\r\n\r\n### Trial 1: right multiplication with np.float64: the only setting that works:\r\n\r\nThe output is:\r\n```\r\ntensor([ 2.,  2.]) True torch.float32\r\n```\r\n\r\n### Trial 2: left multiplication with np.float64: does not work when tensor.requires_grad=True\r\n\r\nThe error message is:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-26-58039673906b> in <module>()\r\n      1 tensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\n      2 scalar = np.float64(2.0)\r\n----> 3 prod = scalar * tensor\r\n      4 print(prod, prod.requires_grad, prod.dtype)\r\n\r\n~/software/anaconda3/envs/pyt4/lib/python3.6/site-packages/torch/tensor.py in __array__(self, dtype)\r\n    374     def __array__(self, dtype=None):\r\n    375         if dtype is None:\r\n--> 376             return self.cpu().numpy()\r\n    377         else:\r\n    378             return self.cpu().numpy().astype(dtype, copy=False)\r\n\r\nRuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\r\n```\r\n### Trial 3: right multiplication with np.float32: does not work:\r\n\r\nThe error message is:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-1eb8188d0e28> in <module>()\r\n      1 tensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\n      2 scalar = np.float32(2.0)\r\n----> 3 prod = tensor * scalar\r\n      4 print(prod, prod.requires_grad, prod.dtype)\r\n\r\nTypeError: mul() received an invalid combination of arguments - got (numpy.float32), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (!numpy.float32!)\r\n * (float other)\r\n      didn't match because some of the arguments have invalid types: (!numpy.float32!)\r\n```\r\n\r\n### Trial 4: left multiplication with np.float32: does not work when tensor.requires_grad=True\r\n\r\nSame error message as Trial 2.\r\n\r\n\r\n## System Info\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 17.10\r\nGCC version: (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\r\nCMake version: version 3.9.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```\n\ncc @mruberry @rgommers @heitorschueroff",triaged|module: numpy,weiyangfb,"Thanks for the amazing software. I stumbled across (what I believe is a) bug when multiplying a torch tensor with a numpy scalar. \r\n\r\n## Issue description\r\n\r\nMultiplication of a torch tensor with numpy scalars exhibits unexpected behavior depending on the order of multiplication and datatypes. Specifically, multiplication of torch.FloatTensor with np.float32 does not work. Multiplication of torch.FloatTensor with np.float64 only works when written as `tensor * scalar` when tensor.requires_grad = True.\r\n\r\n## Code example\r\n\r\n### Trial 1: right multiplication with np.float64: the only setting that works:\r\n```python\r\ntensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\nscalar = np.float64(2.0)\r\nprod = tensor * scalar\r\nprint(prod, prod.requires_grad, prod.dtype)\r\n```\r\nThe output is:\r\n```\r\ntensor([ 2.,  2.]) True torch.float32\r\n```\r\n\r\n### Trial 2: left multiplication with np.float64: does not work when tensor.requires_grad=True\r\n```python\r\ntensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\nscalar = np.float64(2.0)\r\nprod = scalar * tensor\r\nprint(prod, prod.requires_grad, prod.dtype)\r\n```\r\nThe error message is:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-26-58039673906b> in <module>()\r\n      1 tensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\n      2 scalar = np.float64(2.0)\r\n----> 3 prod = scalar * tensor\r\n      4 print(prod, prod.requires_grad, prod.dtype)\r\n\r\n~/software/anaconda3/envs/pyt4/lib/python3.6/site-packages/torch/tensor.py in __array__(self, dtype)\r\n    374     def __array__(self, dtype=None):\r\n    375         if dtype is None:\r\n--> 376             return self.cpu().numpy()\r\n    377         else:\r\n    378             return self.cpu().numpy().astype(dtype, copy=False)\r\n\r\nRuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\r\n```\r\n### Trial 3: right multiplication with np.float32: does not work:\r\n```python\r\ntensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\nscalar = np.float32(2.0)\r\nprod = tensor * scalar\r\nprint(prod, prod.requires_grad, prod.dtype)\r\n```\r\nThe error message is:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-29-1eb8188d0e28> in <module>()\r\n      1 tensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\n      2 scalar = np.float32(2.0)\r\n----> 3 prod = tensor * scalar\r\n      4 print(prod, prod.requires_grad, prod.dtype)\r\n\r\nTypeError: mul() received an invalid combination of arguments - got (numpy.float32), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (!numpy.float32!)\r\n * (float other)\r\n      didn't match because some of the arguments have invalid types: (!numpy.float32!)\r\n```\r\n\r\n### Trial 4: left multiplication with np.float32: does not work when tensor.requires_grad=True\r\n```python\r\ntensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\nscalar = np.float32(2.0)\r\nprod = scalar * tensor\r\nprint(prod, prod.requires_grad, prod.dtype)\r\n```\r\nSame error message as Trial 2.\r\n\r\n\r\n## System Info\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 17.10\r\nGCC version: (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\r\nCMake version: version 3.9.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```\n\ncc @mruberry @rgommers @heitorschueroff","python\r\ntensor = torch.ones(2, requires_grad=True, dtype=torch.float32)\r\nscalar = np.float64(2.0)\r\nprod = tensor * scalar\r\nprint(prod, prod.requires_grad, prod.dtype)\r\n"
9383,"[docs] Make clear the format of torch.eig eigenvaluesThe second dimension must be for the complex imaginary part, but it is not clear in the docs.\r\n\r\n\r\nCurrently in https://pytorch.org/docs/master/torch.html?highlight=eig#torch.eig:\r\n`e (Tensor): the right eigenvalues of a` without specified shapes",todo|module: docs,ailzhang,"The second dimension must be for the complex imaginary part, but it is not clear in the docs.\r\n```python\r\na = torch.Tensor([[1, 0], [0, 1]])\r\nprint(a.eig()[0].shape) # torch.Size([2, 2])\r\nprint(np.linalg.eig(a)[0].shape) # (2,)\r\n```\r\n\r\nCurrently in https://pytorch.org/docs/master/torch.html?highlight=eig#torch.eig:\r\n`e (Tensor): the right eigenvalues of a` without specified shapes","python\r\na = torch.Tensor([[1, 0], [0, 1]])\r\nprint(a.eig()[0].shape) # torch.Size([2, 2])\r\nprint(np.linalg.eig(a)[0].shape) # (2,)\r\n"
9129,"[Bug]  Segmentation fault when importing fastText (with v0.4.0)Related #8358 ?\r\n\r\nI got segfault in importing torch (0.4.0) after importing `fastText`.\r\nI found it is caused by `from torch._C import *` (https://github.com/pytorch/pytorch/blob/master/torch/__init__.py#L80).\r\n\r\n\r\n\r\n## gdb\r\n\r\n```\r\n(gdb) run test.py\r\nStarting program: /home/cympfh/.pyenv/versions/anaconda3-5.1.0/bin/python test.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib64/libthread_db.so.1"".\r\nMissing separate debuginfo for /home/cympfh/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n[New Thread 0x7fffecfa8700 (LWP 10028)]\r\n[New Thread 0x7fffec7a7700 (LWP 10029)]\r\n[New Thread 0x7fffe9fa6700 (LWP 10030)]\r\n[New Thread 0x7fffe77a5700 (LWP 10031)]\r\n[New Thread 0x7fffe4fa4700 (LWP 10032)]\r\n[New Thread 0x7fffe27a3700 (LWP 10033)]\r\n[New Thread 0x7fffdffa2700 (LWP 10034)]\r\n[New Thread 0x7fffdd7a1700 (LWP 10035)]\r\n[New Thread 0x7fffdafa0700 (LWP 10036)]\r\n[New Thread 0x7fffd879f700 (LWP 10037)]\r\n[New Thread 0x7fffd5f9e700 (LWP 10038)]\r\n[New Thread 0x7fffd379d700 (LWP 10039)]\r\n[New Thread 0x7fffd0f9c700 (LWP 10040)]\r\n[New Thread 0x7fffce79b700 (LWP 10041)]\r\n[New Thread 0x7fffcbf9a700 (LWP 10042)]\r\n[Thread 0x7fffd0f9c700 (LWP 10040) exited]\r\n[Thread 0x7fffcbf9a700 (LWP 10042) exited]\r\n[Thread 0x7fffe9fa6700 (LWP 10030) exited]\r\n[Thread 0x7fffce79b700 (LWP 10041) exited]\r\n[Thread 0x7fffdafa0700 (LWP 10036) exited]\r\n[Thread 0x7fffd379d700 (LWP 10039) exited]\r\n[Thread 0x7fffdffa2700 (LWP 10034) exited]\r\n[Thread 0x7fffd5f9e700 (LWP 10038) exited]\r\n[Thread 0x7fffe4fa4700 (LWP 10032) exited]\r\n[Thread 0x7fffdd7a1700 (LWP 10035) exited]\r\n[Thread 0x7fffecfa8700 (LWP 10028) exited]\r\n[Thread 0x7fffd879f700 (LWP 10037) exited]\r\n[Thread 0x7fffe27a3700 (LWP 10033) exited]\r\n[Thread 0x7fffec7a7700 (LWP 10029) exited]\r\n[Thread 0x7fffe77a5700 (LWP 10031) exited]\r\nDetaching after fork from child process 10045.\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\npybind11::detail::make_new_python_type (rec=...) at /opt/conda/conda-bld/pytorch_1524586445097/work/third_party/pybind11/include/pybind11/detail/class.h:576\r\n576     /opt/conda/conda-bld/pytorch_1524586445097/work/third_party/pybind11/include/pybind11/detail/class.h: No such file or directory.\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-106.el7_2.8.x86_64\r\n\r\n```",high priority,weiyangfb,"Related #8358 ?\r\n\r\nI got segfault in importing torch (0.4.0) after importing `fastText`.\r\nI found it is caused by `from torch._C import *` (https://github.com/pytorch/pytorch/blob/master/torch/__init__.py#L80).\r\n\r\n```bash\r\n$ python --version\r\nPython 3.6.4 :: Anaconda, Inc.\r\n\r\n$ conda --version\r\nconda 4.4.10\r\n\r\n$ cat test.py\r\nimport fastText\r\nimport torch._C\r\n\r\n$ python test.py\r\nzsh: segmentation fault  python test.py\r\n```\r\n\r\n## gdb\r\n\r\n```\r\n(gdb) run test.py\r\nStarting program: /home/cympfh/.pyenv/versions/anaconda3-5.1.0/bin/python test.py\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library ""/lib64/libthread_db.so.1"".\r\nMissing separate debuginfo for /home/cympfh/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n[New Thread 0x7fffecfa8700 (LWP 10028)]\r\n[New Thread 0x7fffec7a7700 (LWP 10029)]\r\n[New Thread 0x7fffe9fa6700 (LWP 10030)]\r\n[New Thread 0x7fffe77a5700 (LWP 10031)]\r\n[New Thread 0x7fffe4fa4700 (LWP 10032)]\r\n[New Thread 0x7fffe27a3700 (LWP 10033)]\r\n[New Thread 0x7fffdffa2700 (LWP 10034)]\r\n[New Thread 0x7fffdd7a1700 (LWP 10035)]\r\n[New Thread 0x7fffdafa0700 (LWP 10036)]\r\n[New Thread 0x7fffd879f700 (LWP 10037)]\r\n[New Thread 0x7fffd5f9e700 (LWP 10038)]\r\n[New Thread 0x7fffd379d700 (LWP 10039)]\r\n[New Thread 0x7fffd0f9c700 (LWP 10040)]\r\n[New Thread 0x7fffce79b700 (LWP 10041)]\r\n[New Thread 0x7fffcbf9a700 (LWP 10042)]\r\n[Thread 0x7fffd0f9c700 (LWP 10040) exited]\r\n[Thread 0x7fffcbf9a700 (LWP 10042) exited]\r\n[Thread 0x7fffe9fa6700 (LWP 10030) exited]\r\n[Thread 0x7fffce79b700 (LWP 10041) exited]\r\n[Thread 0x7fffdafa0700 (LWP 10036) exited]\r\n[Thread 0x7fffd379d700 (LWP 10039) exited]\r\n[Thread 0x7fffdffa2700 (LWP 10034) exited]\r\n[Thread 0x7fffd5f9e700 (LWP 10038) exited]\r\n[Thread 0x7fffe4fa4700 (LWP 10032) exited]\r\n[Thread 0x7fffdd7a1700 (LWP 10035) exited]\r\n[Thread 0x7fffecfa8700 (LWP 10028) exited]\r\n[Thread 0x7fffd879f700 (LWP 10037) exited]\r\n[Thread 0x7fffe27a3700 (LWP 10033) exited]\r\n[Thread 0x7fffec7a7700 (LWP 10029) exited]\r\n[Thread 0x7fffe77a5700 (LWP 10031) exited]\r\nDetaching after fork from child process 10045.\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\npybind11::detail::make_new_python_type (rec=...) at /opt/conda/conda-bld/pytorch_1524586445097/work/third_party/pybind11/include/pybind11/detail/class.h:576\r\n576     /opt/conda/conda-bld/pytorch_1524586445097/work/third_party/pybind11/include/pybind11/detail/class.h: No such file or directory.\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-106.el7_2.8.x86_64\r\n\r\n```","bash\r\n$ python --version\r\nPython 3.6.4 :: Anaconda, Inc.\r\n\r\n$ conda --version\r\nconda 4.4.10\r\n\r\n$ cat test.py\r\nimport fastText\r\nimport torch._C\r\n\r\n$ python test.py\r\nzsh: segmentation fault  python test.py\r\n"
8820,"Pytorch 0.4.0: Model behavior changes heavily after save and load weights# Issue description\r\nDuring training I save model by using: torch.save(model.state_dict(), file). And I reach 99% accuracy on both test and train data set.\r\nThen, I load the model and test again using: model.load_state_dict(checkpoint, strict=True). The accuracy drops to 0.1%. An initial value!!\r\n\r\nMy net is quite simple, so I print all the weights in the checkpoint both before and after validation. They are exactly the same.\r\n\r\nI thought the drop_out layer causes this issue, but remove it doesn't work either.\r\n\r\n\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\nGPU 2: TITAN X (Pascal)\r\nGPU 3: TITAN X (Pascal)\r\n\r\nNvidia driver version: 381.09\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/libcudnn.so.7.0.5\r\n/usr/local/lib/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n",high priority,ssnl,"# Issue description\r\nDuring training I save model by using: torch.save(model.state_dict(), file). And I reach 99% accuracy on both test and train data set.\r\nThen, I load the model and test again using: model.load_state_dict(checkpoint, strict=True). The accuracy drops to 0.1%. An initial value!!\r\n\r\nMy net is quite simple, so I print all the weights in the checkpoint both before and after validation. They are exactly the same.\r\n\r\nI thought the drop_out layer causes this issue, but remove it doesn't work either.\r\n\r\n```python\r\n## Code example\r\nTo save:\r\n    model = Net()\r\n    model = Net().to(device)\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\r\n                                momentum=args.momentum,\r\n                                weight_decay=1e-5)\r\n    while acc < 0.98:\r\n        epoch += 1\r\n        adjust_learning_rate(optimizer, epoch)\r\n        acc1 = train(args, model, device, train_loader, optimizer, epoch)\r\n        acc2 = test(args, model, device, test_loader)\r\n        acc = (acc1+acc2)*0.5\r\n        torch.save(model.state_dict(), 'checkpoints/mnist_gpu_%d.pkl'%epoch)\r\n\r\nTo load:\r\n    model = Net()\r\n    model.eval()\r\n    model = Net().to(device)\r\n    checkpoint = torch.load('checkpoints/mnist_gpu_20.pkl')\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_pre'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n    model.load_state_dict(checkpoint, strict=True)\r\n    acc2 = test(args, model, device, test_loader)\r\n    checkpoint = model.state_dict()\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_after'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n\r\n#model is super simple:\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nclass Fire(torch.nn.Module):\r\n    def __init__(self,inchn,sqzout_chn,exp1x1out_chn,exp3x3out_chn):\r\n        super(Fire,self).__init__()\r\n        self.inchn = inchn\r\n        self.squeeze = torch.nn.Conv2d(inchn,sqzout_chn,kernel_size=1)\r\n        self.squeeze_act = torch.nn.ReLU(inplace=True)\r\n        self.expand1x1 = torch.nn.Conv2d(sqzout_chn,exp1x1out_chn,kernel_size=1)\r\n        self.expand1x1_act = torch.nn.ReLU(inplace=True)\r\n        self.expand3x3 = torch.nn.Conv2d(sqzout_chn,exp3x3out_chn,kernel_size=3, padding=1)\r\n        self.expand3x3_act = torch.nn.ReLU(inplace=True)\r\n    def forward(self, x):\r\n        x = self.squeeze_act(self.squeeze(x))\r\n        return torch.cat([\r\n                self.expand1x1_act(self.expand1x1(x)),\r\n                self.expand3x3_act(self.expand3x3(x))\r\n                ], 1)\r\nclass Net(nn.Module):\r\n    def __init__(self, num_class=10):\r\n        super(Net, self).__init__()\r\n        self.num_class = 10\r\n        self.features = torch.nn.Sequential(\r\n            torch.nn.Conv2d(3,96,kernel_size=3),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(96,16,64,64),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(128,32,128,128),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(256,64,256,256),\r\n        )\r\n        final_conv = torch.nn.Conv2d(512,self.num_class,kernel_size=1)\r\n        self.classifier = torch.nn.Sequential(\r\n            final_conv,\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.AdaptiveAvgPool2d(1)\r\n        )\r\n        for m in self.modules():\r\n            if isinstance(m, torch.nn.Conv2d):\r\n                if m is final_conv:\r\n                    torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)\r\n                else:\r\n                    torch.nn.init.kaiming_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        x = self.classifier(x)\r\n        x = x.view(-1, self.num_class)\r\n        return x\r\n```\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\nGPU 2: TITAN X (Pascal)\r\nGPU 3: TITAN X (Pascal)\r\n\r\nNvidia driver version: 381.09\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/libcudnn.so.7.0.5\r\n/usr/local/lib/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n","python\r\n## Code example\r\nTo save:\r\n    model = Net()\r\n    model = Net().to(device)\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\r\n                                momentum=args.momentum,\r\n                                weight_decay=1e-5)\r\n    while acc < 0.98:\r\n        epoch += 1\r\n        adjust_learning_rate(optimizer, epoch)\r\n        acc1 = train(args, model, device, train_loader, optimizer, epoch)\r\n        acc2 = test(args, model, device, test_loader)\r\n        acc = (acc1+acc2)*0.5\r\n        torch.save(model.state_dict(), 'checkpoints/mnist_gpu_%d.pkl'%epoch)\r\n\r\nTo load:\r\n    model = Net()\r\n    model.eval()\r\n    model = Net().to(device)\r\n    checkpoint = torch.load('checkpoints/mnist_gpu_20.pkl')\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_pre'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n    model.load_state_dict(checkpoint, strict=True)\r\n    acc2 = test(args, model, device, test_loader)\r\n    checkpoint = model.state_dict()\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_after'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n\r\n#model is super simple:\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nclass Fire(torch.nn.Module):\r\n    def __init__(self,inchn,sqzout_chn,exp1x1out_chn,exp3x3out_chn):\r\n        super(Fire,self).__init__()\r\n        self.inchn = inchn\r\n        self.squeeze = torch.nn.Conv2d(inchn,sqzout_chn,kernel_size=1)\r\n        self.squeeze_act = torch.nn.ReLU(inplace=True)\r\n        self.expand1x1 = torch.nn.Conv2d(sqzout_chn,exp1x1out_chn,kernel_size=1)\r\n        self.expand1x1_act = torch.nn.ReLU(inplace=True)\r\n        self.expand3x3 = torch.nn.Conv2d(sqzout_chn,exp3x3out_chn,kernel_size=3, padding=1)\r\n        self.expand3x3_act = torch.nn.ReLU(inplace=True)\r\n    def forward(self, x):\r\n        x = self.squeeze_act(self.squeeze(x))\r\n        return torch.cat([\r\n                self.expand1x1_act(self.expand1x1(x)),\r\n                self.expand3x3_act(self.expand3x3(x))\r\n                ], 1)\r\nclass Net(nn.Module):\r\n    def __init__(self, num_class=10):\r\n        super(Net, self).__init__()\r\n        self.num_class = 10\r\n        self.features = torch.nn.Sequential(\r\n            torch.nn.Conv2d(3,96,kernel_size=3),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(96,16,64,64),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(128,32,128,128),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(256,64,256,256),\r\n        )\r\n        final_conv = torch.nn.Conv2d(512,self.num_class,kernel_size=1)\r\n        self.classifier = torch.nn.Sequential(\r\n            final_conv,\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.AdaptiveAvgPool2d(1)\r\n        )\r\n        for m in self.modules():\r\n            if isinstance(m, torch.nn.Conv2d):\r\n                if m is final_conv:\r\n                    torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)\r\n                else:\r\n                    torch.nn.init.kaiming_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        x = self.classifier(x)\r\n        x = x.view(-1, self.num_class)\r\n        return x\r\n"
8741,"[Feature Request] Add to() method for optimizers/schedulersI think the optimizers and schedulers could use some extra work.  \r\nThe main thing I would love, is for optimizers and schedulers to have a `to()` method so we can send their parameters to a certain device. This would allow us to save the parameters and reload them at a later stage without any problems.\r\n\r\nRight now, the following script crashes with an error that the optimizer expects a `torch.FloatTensor` but got a `torch.cuda.FloatTensor`. I am not quite sure why, as I specifically load the data to CPU?\r\n\r\n\r\n\r\nThe same could be said about schedulers.",todo|module: optimizer|triaged,fmassa,"I think the optimizers and schedulers could use some extra work.  \r\nThe main thing I would love, is for optimizers and schedulers to have a `to()` method so we can send their parameters to a certain device. This would allow us to save the parameters and reload them at a later stage without any problems.\r\n\r\nRight now, the following script crashes with an error that the optimizer expects a `torch.FloatTensor` but got a `torch.cuda.FloatTensor`. I am not quite sure why, as I specifically load the data to CPU?\r\n\r\n```python\r\n# Note: This is not my actual script, but rather a representation of the steps and in what order I perform them\r\nnetwork = ... # Creating a network based of torch.nn.Module\r\noptim = torch.optim.SGD(network.parameters(), ...)\r\n\r\nstate = torch.load('file_path', lambda storage, loc: storage)  # Send all tensors to CPU\r\n\r\nnetwork.load_state_dict(state['network'])\r\noptim.load_state_dict(state['optim'])\r\n\r\nnetwork.to(torch.device('cuda'))\r\n#optim.to(torch.device('cuda'))   # Would this solve my problems? \r\n```\r\n\r\nThe same could be said about schedulers.","python\r\n# Note: This is not my actual script, but rather a representation of the steps and in what order I perform them\r\nnetwork = ... # Creating a network based of torch.nn.Module\r\noptim = torch.optim.SGD(network.parameters(), ...)\r\n\r\nstate = torch.load('file_path', lambda storage, loc: storage)  # Send all tensors to CPU\r\n\r\nnetwork.load_state_dict(state['network'])\r\noptim.load_state_dict(state['optim'])\r\n\r\nnetwork.to(torch.device('cuda'))\r\n#optim.to(torch.device('cuda'))   # Would this solve my problems? \r\n"
8481,"Retrocompatibility issue for batchnormModules created in pytorch v0.4 are not loadable in pytorch master. \r\n```\r\n        Missing key(s) in state_dict: ""batchnorm.num_batches_tracked"". \r\n```\r\n\r\n### Minimal code to reproduce:\r\n**Within pytorch 0.4**\r\n\r\nModule\r\n``` python\r\nclass MyModule(nn.Module):\r\n    """"""\r\n        Create a toy module with just one batch norm\r\n    """"""\r\n\r\n    def __init__(self):\r\n        super(MyModule, self).__init__()\r\n        self.batchnorm = nn.BatchNorm1d(10)\r\n\r\n    def forward(self, inputs):\r\n        return self.batchnorm(inputs)\r\n```\r\n\r\nSave the model\r\n\r\n\r\n**Within pytorch master**\r\n\r\n\r\n\r\nObserve:\r\n```\r\nTraceback (most recent call last):\r\n  File ""load.py"", line 10, in <module>\r\n    model.load_state_dict(state_dict)\r\n  File ""/private/home/samuelhumeau/.conda/envs/pytorch-sources/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 698, in load_state_dict\r\n    self.__class__.__name__, ""\\n\\t"".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for MyModule:\r\n        Missing key(s) in state_dict: ""batchnorm.num_batches_tracked"". \r\n```\r\n\r\n### Educated guess of what is going on\r\n\r\n```\r\n>>> print(state_dict._metadata)\r\nOrderedDict([('', {'version': 1}), ('batchnorm', {'version': 1})])\r\n```\r\n\r\n[This line](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L79 ) seems to only deal with the case where the version is None though.\r\n",todo,ssnl,"Modules created in pytorch v0.4 are not loadable in pytorch master. \r\n```\r\n        Missing key(s) in state_dict: ""batchnorm.num_batches_tracked"". \r\n```\r\n\r\n### Minimal code to reproduce:\r\n**Within pytorch 0.4**\r\n\r\nModule\r\n``` python\r\nclass MyModule(nn.Module):\r\n    """"""\r\n        Create a toy module with just one batch norm\r\n    """"""\r\n\r\n    def __init__(self):\r\n        super(MyModule, self).__init__()\r\n        self.batchnorm = nn.BatchNorm1d(10)\r\n\r\n    def forward(self, inputs):\r\n        return self.batchnorm(inputs)\r\n```\r\n\r\nSave the model\r\n```python\r\nmodel = MyModule()\r\nstate_dict = model.state_dict()\r\ntorch.save(state_dict, ""state_dict.mdl"")\r\n```\r\n\r\n**Within pytorch master**\r\n\r\n```python\r\nstate_dict = torch.load(""state_dict.mdl"")\r\nmodel = MyModule()\r\nmodel.load_state_dict(state_dict)\r\n```\r\n\r\nObserve:\r\n```\r\nTraceback (most recent call last):\r\n  File ""load.py"", line 10, in <module>\r\n    model.load_state_dict(state_dict)\r\n  File ""/private/home/samuelhumeau/.conda/envs/pytorch-sources/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 698, in load_state_dict\r\n    self.__class__.__name__, ""\\n\\t"".join(error_msgs)))\r\nRuntimeError: Error(s) in loading state_dict for MyModule:\r\n        Missing key(s) in state_dict: ""batchnorm.num_batches_tracked"". \r\n```\r\n\r\n### Educated guess of what is going on\r\n\r\n```\r\n>>> print(state_dict._metadata)\r\nOrderedDict([('', {'version': 1}), ('batchnorm', {'version': 1})])\r\n```\r\n\r\n[This line](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L79 ) seems to only deal with the case where the version is None though.\r\n","python\r\nmodel = MyModule()\r\nstate_dict = model.state_dict()\r\ntorch.save(state_dict, ""state_dict.mdl"")\r\n"
7999,"ReLU with NaN as input gives 0 as output## Issue description\r\n\r\nIt seems `torch.relu()` when given `nan` as input, it produces 0 instead of `nan`. I am wondering if this is the intended behavior. This behavior actually hides code bugs, making troubleshooting harder.\r\n\r\n## Code example\r\n\r\n\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.0)\r\n[conda] pytorch                   0.4.0           py35_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.0                    py35_0\r\n",todo,ssnl,"## Issue description\r\n\r\nIt seems `torch.relu()` when given `nan` as input, it produces 0 instead of `nan`. I am wondering if this is the intended behavior. This behavior actually hides code bugs, making troubleshooting harder.\r\n\r\n## Code example\r\n\r\n```python\r\na = torch.Tensor(1).fill_(float('nan'))\r\ntorch.relu(a)  # => 0\r\n```\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 14.04.5 LTS\r\nGCC version: (Ubuntu 4.8.5-4ubuntu8~14.04.2) 4.8.5\r\nCMake version: version 3.2.2\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 384.111\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.7.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.13.3)\r\n[pip] numpydoc (0.7.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.0)\r\n[conda] pytorch                   0.4.0           py35_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.0                    py35_0\r\n",python\r\na = torch.Tensor(1).fill_(float('nan'))\r\ntorch.relu(a)  # => 0\r\n
7849,"""RuntimeError: Expected tensor to have CUDA Backend, but got tensor with CUDA Backend""## Issue description\r\n\r\nThe error message [here](https://github.com/pytorch/pytorch/blob/aa214a8b8cc8bb7fa0a655499a709fc580e417db/aten/src/ATen/TensorUtils.cpp#L232) should say ""expected X, but got Y"" instead of ""expected X, but got X"". Currently `toString(t.type().backend())` is printed both times.\r\n\r\n## Code example\r\n\r\nTo reproduce:\r\n\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda90 1.0 h6433d27_0 pytorch\r\n[conda] pytorch 0.4.0 py36_cuda9.0.176_cudnn7.1.2_1 [cuda90] pytorch\r\n[conda] torchvision 0.2.1 py36_1 pytorch",todo,zou3519,"## Issue description\r\n\r\nThe error message [here](https://github.com/pytorch/pytorch/blob/aa214a8b8cc8bb7fa0a655499a709fc580e417db/aten/src/ATen/TensorUtils.cpp#L232) should say ""expected X, but got Y"" instead of ""expected X, but got X"". Currently `toString(t.type().backend())` is printed both times.\r\n\r\n## Code example\r\n\r\nTo reproduce:\r\n```python\r\nimport torch\r\ntorch.where(torch.tensor([1], dtype=torch.uint8).cuda(), torch.zeros(1).cpu(), torch.zeros(1).cpu())\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Quadro GP100\r\nGPU 1: Quadro GP100\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda90 1.0 h6433d27_0 pytorch\r\n[conda] pytorch 0.4.0 py36_cuda9.0.176_cudnn7.1.2_1 [cuda90] pytorch\r\n[conda] torchvision 0.2.1 py36_1 pytorch","python\r\nimport torch\r\ntorch.where(torch.tensor([1], dtype=torch.uint8).cuda(), torch.zeros(1).cpu(), torch.zeros(1).cpu())\r\n"
7844,"""Parameters of a model after .cuda() will be different objects with those before the call."" is wrong.Hi,\r\n\r\nIn the documentation, it is written:\r\n\r\n> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\r\n>\r\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\r\n\r\nHowever, doing .cuda() after intialiazing the optimizer still works. This is because the Module class applies the .cuda() in this way:\r\n\r\nThus, by modifying the `.data` attribute, it modifies the parameter tensors in-place. \r\n\r\nI then suggest to remove this ""warning"" from the documentation since I actually find this quite useful to be able to initialize the optimizer before doing .cuda().\r\n\r\nThank you.\r\n\r\nFr\xe9d\xe9rik\r\n",module: optimizer|triaged,yf225,"Hi,\r\n\r\nIn the documentation, it is written:\r\n\r\n> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\r\n>\r\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\r\n\r\nHowever, doing .cuda() after intialiazing the optimizer still works. This is because the Module class applies the .cuda() in this way:\r\n```python\r\nparam.data = fn(param.data)\r\nif param._grad is not None:\r\n    param._grad.data = fn(param._grad.data)\r\n```\r\nThus, by modifying the `.data` attribute, it modifies the parameter tensors in-place. \r\n\r\nI then suggest to remove this ""warning"" from the documentation since I actually find this quite useful to be able to initialize the optimizer before doing .cuda().\r\n\r\nThank you.\r\n\r\nFr\xe9d\xe9rik\r\n",python\r\nparam.data = fn(param.data)\r\nif param._grad is not None:\r\n    param._grad.data = fn(param._grad.data)\r\n
7743,"[pytorch] [feature request] Flatten convenience methodMinor suggestion (trivial to implement in user code, but having it in the library would improve code brevity). The purpose is to flatten specific trailing dimensions by passing negative dimension index.\r\n\r\nCan be useful for aggregating across multiple trailing dimensions, before mean/max etc get multiple dimensions support.\r\n\r\nExists in numpy/tensorflow/onnx, but semantics there doesn't allow flattening only specific dimensions.\r\n\r\n",todo,li-roy,"Minor suggestion (trivial to implement in user code, but having it in the library would improve code brevity). The purpose is to flatten specific trailing dimensions by passing negative dimension index.\r\n\r\nCan be useful for aggregating across multiple trailing dimensions, before mean/max etc get multiple dimensions support.\r\n\r\nExists in numpy/tensorflow/onnx, but semantics there doesn't allow flattening only specific dimensions.\r\n\r\n```python\r\ndef flatten(x, dim):\r\n    return x.view(x.size()[:dim] + (-1, ))\r\n\r\nflatten(torch.rand(2,3,4,5,6), dim = -2).shape\r\n# (2, 3, 4, 30)\r\n```","python\r\ndef flatten(x, dim):\r\n    return x.view(x.size()[:dim] + (-1, ))\r\n\r\nflatten(torch.rand(2,3,4,5,6), dim = -2).shape\r\n# (2, 3, 4, 30)\r\n"
7702,"[pytorch][bug] memory leaking for gloo backend when `all_reduce` CPU tensorThe `dist.all_reduce` caused memory leaking when using gloo backend for CPU memory.\r\nobservation:\r\n1. both `tcp` and `gloo` backends are tested, `tcp` is fine.\r\n2. if `index` is of fixed size, it's fine.\r\n3. other collective communication methods are not tested yet.\r\n4. `reduce` tensors on GPU memory seems fine.\r\n5. the memory usage increases linearly with the tensor size.\r\n\r\nI think the leaking may happen in the buffer management of gloo backend.\r\n\r\nreproducing snippets:\r\n\r\n\r\nhow to run:\r\n\r\n\r\n\r\nEnvs\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5\r\nCMake version: version 3.6.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 375.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.1)\r\n[pip3] torch (0.4.0)\r\n[conda] magma-cuda80              2.1.0                         5    soumith\r\n[conda] torch-0.1.12              2                         <pip>\r\n```",awaiting response (this tag is deprecated)|oncall: distributed,teng-li,"The `dist.all_reduce` caused memory leaking when using gloo backend for CPU memory.\r\nobservation:\r\n1. both `tcp` and `gloo` backends are tested, `tcp` is fine.\r\n2. if `index` is of fixed size, it's fine.\r\n3. other collective communication methods are not tested yet.\r\n4. `reduce` tensors on GPU memory seems fine.\r\n5. the memory usage increases linearly with the tensor size.\r\n\r\nI think the leaking may happen in the buffer management of gloo backend.\r\n\r\nreproducing snippets:\r\n```python\r\n# test.py\r\nimport time\r\nimport random\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\ndef print_mem(extra_str=''):\r\n    if dist.get_rank() == 0:\r\n        import os\r\n        import psutil\r\n        process = psutil.Process(os.getpid())\r\n        print(extra_str, process.memory_info().rss // 2**20, 'MB')\r\n\r\ndist.init_process_group('gloo', world_size=2, init_method='file:///tmp/shared_file')\r\nembedding = torch.randn(8000, 200)\r\nfor epo in range(1000):\r\n    index = torch.randint(embedding.size(0), (30000 - epo,)).long()\r\n    # ten = torch.randn(numel)\r\n    ten = embedding[index]\r\n    print_mem('before reducing')\r\n    dist.all_reduce(ten, op=dist.reduce_op.SUM)\r\n    print_mem('after reducing')\r\n```\r\n\r\nhow to run:\r\n```bash\r\npip install psutil\r\npython test.py & python test.py\r\n```\r\n\r\n\r\nEnvs\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5\r\nCMake version: version 3.6.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 375.26\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-7.5/lib64/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.1)\r\n[pip3] torch (0.4.0)\r\n[conda] magma-cuda80              2.1.0                         5    soumith\r\n[conda] torch-0.1.12              2                         <pip>\r\n```","python\r\n# test.py\r\nimport time\r\nimport random\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\ndef print_mem(extra_str=''):\r\n    if dist.get_rank() == 0:\r\n        import os\r\n        import psutil\r\n        process = psutil.Process(os.getpid())\r\n        print(extra_str, process.memory_info().rss // 2**20, 'MB')\r\n\r\ndist.init_process_group('gloo', world_size=2, init_method='file:///tmp/shared_file')\r\nembedding = torch.randn(8000, 200)\r\nfor epo in range(1000):\r\n    index = torch.randint(embedding.size(0), (30000 - epo,)).long()\r\n    # ten = torch.randn(numel)\r\n    ten = embedding[index]\r\n    print_mem('before reducing')\r\n    dist.all_reduce(ten, op=dist.reduce_op.SUM)\r\n    print_mem('after reducing')\r\n"
7284,"[feature request] Bucketization## Issue description\r\n\r\nBoth the 0.4.0 version and the `master` version are missing a bucketization operation, i.e. an operation which given a 1-D tensor of values, and another 1-D tensor of bucket boundaries, return a new tensor where each value is substituted for the index of the corresponding bucket.\r\n\r\nThe operation seems to be [available](https://github.com/pytorch/pytorch/blob/master/caffe2/operators/one_hot_ops.cc#L90) in Caffe2, but I'm not sure if the code can be reused in PyTorch.\r\n\r\nAlternatively, PyTorch could provide a variant of [`searchsorted`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html) so that the users can easily write O(log n) bucketization.\r\n\r\n## Code example\r\n\r\nA trivial implementation might be something like:\r\n\r\n\r\n\r\n\r\n\r\nN.B. the handling of bucket endpoints could be implemented differently.\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.8.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519",high priority|feature|triaged,glaringlee,"## Issue description\r\n\r\nBoth the 0.4.0 version and the `master` version are missing a bucketization operation, i.e. an operation which given a 1-D tensor of values, and another 1-D tensor of bucket boundaries, return a new tensor where each value is substituted for the index of the corresponding bucket.\r\n\r\nThe operation seems to be [available](https://github.com/pytorch/pytorch/blob/master/caffe2/operators/one_hot_ops.cc#L90) in Caffe2, but I'm not sure if the code can be reused in PyTorch.\r\n\r\nAlternatively, PyTorch could provide a variant of [`searchsorted`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.searchsorted.html) so that the users can easily write O(log n) bucketization.\r\n\r\n## Code example\r\n\r\nA trivial implementation might be something like:\r\n\r\n```python\r\ndef bucketize(tensor, bucket_boundaries):\r\n    result = torch.zeros_like(tensor, dtype=torch.int32)\r\n    for boundary in bucket_boundaries:\r\n        result += (tensor > boundary).int()\r\n    return result\r\n```\r\n\r\n```python\r\n>>> bucketize(torch.tensor([-3, 1, 2, 4]), bucket_boundaries=torch.tensor([2]))\r\ntensor([ 0,  0,  0,  1], dtype=torch.int32)\r\n```\r\n\r\nN.B. the handling of bucket endpoints could be implemented differently.\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.8.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519","python\r\ndef bucketize(tensor, bucket_boundaries):\r\n    result = torch.zeros_like(tensor, dtype=torch.int32)\r\n    for boundary in bucket_boundaries:\r\n        result += (tensor > boundary).int()\r\n    return result\r\n"
7038,"torch.stft output size is not right.I try replace librosa functions with pytorch here https://github.com/mozilla/TTS\r\n\r\nWhat I realized is, output sequence lengths are different for these two functions.\r\n\r\nBelow functions are used for testing. The point is given a audio signal in size 224960, pytorch's output is in size [815, 1025, 2] and librosa's [819, 1025].\r\n\r\nGiven the parameters ```n_fft, hop_length, win_length = [2048, 275, 1102]``` 819 seems right. \r\n\r\n",todo,ssnl,"I try replace librosa functions with pytorch here https://github.com/mozilla/TTS\r\n\r\nWhat I realized is, output sequence lengths are different for these two functions.\r\n\r\nBelow functions are used for testing. The point is given a audio signal in size 224960, pytorch's output is in size [815, 1025, 2] and librosa's [819, 1025].\r\n\r\nGiven the parameters ```n_fft, hop_length, win_length = [2048, 275, 1102]``` 819 seems right. \r\n\r\n```python\r\nwin = torch.hann_window(win_length).cuda()\r\ndef spectrogram_torch(y):\r\n    n_fft, hop_length, win_length = [2048, 275, 1102]\r\n    y[1:] = y[1:] - 0.097 * y[:-1]\r\n    print(y.shape)\r\n    start = time.time()\r\n    D = torch.stft(y, frame_length=win_length, hop=hop_length, fft_size=n_fft,\r\n                   onesided=True, window=win)[:, :, 1]\r\n    print(time.time()-start)\r\n    D = torch.abs(D)\r\n    D[D==0] = 1e-5\r\n    D = 20 * torch.log10(D)\r\n    D -= ap.ref_level_db\r\n    D = torch.clamp((D - ap.min_level_db) / -ap.min_level_db, 1e-8, 1)\r\n    return D\r\n\r\n\r\ndef spectrogram(y):\r\n    start = time.time()\r\n    D = ap.apply_preemphasis(y)\r\n    n_fft, hop_length, win_length = [2048, 275, 1102]\r\n    D = librosa.stft(y=D, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\r\n    print(time.time()-start)\r\n    S = ap._amp_to_db(np.abs(D)) - ap.ref_level_db\r\n    S = ap._normalize(S)\r\n    S = S.astype(np.float32)\r\n    return S\r\n```","python\r\nwin = torch.hann_window(win_length).cuda()\r\ndef spectrogram_torch(y):\r\n    n_fft, hop_length, win_length = [2048, 275, 1102]\r\n    y[1:] = y[1:] - 0.097 * y[:-1]\r\n    print(y.shape)\r\n    start = time.time()\r\n    D = torch.stft(y, frame_length=win_length, hop=hop_length, fft_size=n_fft,\r\n                   onesided=True, window=win)[:, :, 1]\r\n    print(time.time()-start)\r\n    D = torch.abs(D)\r\n    D[D==0] = 1e-5\r\n    D = 20 * torch.log10(D)\r\n    D -= ap.ref_level_db\r\n    D = torch.clamp((D - ap.min_level_db) / -ap.min_level_db, 1e-8, 1)\r\n    return D\r\n\r\n\r\ndef spectrogram(y):\r\n    start = time.time()\r\n    D = ap.apply_preemphasis(y)\r\n    n_fft, hop_length, win_length = [2048, 275, 1102]\r\n    D = librosa.stft(y=D, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\r\n    print(time.time()-start)\r\n    S = ap._amp_to_db(np.abs(D)) - ap.ref_level_db\r\n    S = ap._normalize(S)\r\n    S = S.astype(np.float32)\r\n    return S\r\n"
6988,"The loss computation with `size_average` should average over batch example or batch element ?Traditionally, when we have a batched data with shape `(N, D)` where `N` is batch size and `D` is data dimension. The losses are often calculated for each training example say\r\n```\r\nL_i = loss(X_i), i = 1, ..., N\r\n```\r\nAnd then total loss is averaged over the batch size\r\n```\r\nL = (1/N)*sum(L_i)\r\n```\r\n\r\nHowever, it seems it is not what `nn.*Loss` is doing for the flag `size_average=True/False`\r\n\r\ne.g. \r\n\r\n\r\nit seems the `size_average` does not properly average the loss over the batch of examples, but averaged over all dimensions. ",todo|module: docs,li-roy,"Traditionally, when we have a batched data with shape `(N, D)` where `N` is batch size and `D` is data dimension. The losses are often calculated for each training example say\r\n```\r\nL_i = loss(X_i), i = 1, ..., N\r\n```\r\nAnd then total loss is averaged over the batch size\r\n```\r\nL = (1/N)*sum(L_i)\r\n```\r\n\r\nHowever, it seems it is not what `nn.*Loss` is doing for the flag `size_average=True/False`\r\n\r\ne.g. \r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ninput = torch.randn(3, 2)\r\ntarget = torch.rand(3, 2)\r\n\r\nprint(input)\r\nprint(target)\r\n\r\nfull_loss = F.mse_loss(input, target, reduce=False)\r\nprint(full_loss)\r\nloss_sum = F.mse_loss(input, target, size_average=False)\r\nloss_mean = F.mse_loss(input, target, size_average=True)\r\nprint(loss_sum)\r\nprint(loss_mean)\r\n\r\nbatch_loss = full_loss.sum(dim=-1)\r\ncorrect_loss_mean = batch_loss.mean()\r\ncorrect_loss_sum = batch_loss.sum()\r\nprint(correct_loss_mean)\r\nprint(correct_loss_sum)\r\n\r\nassert correct_loss_sum == loss_sum\r\nassert correct_loss_mean == loss_mean\r\n```\r\nit seems the `size_average` does not properly average the loss over the batch of examples, but averaged over all dimensions. ","python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ninput = torch.randn(3, 2)\r\ntarget = torch.rand(3, 2)\r\n\r\nprint(input)\r\nprint(target)\r\n\r\nfull_loss = F.mse_loss(input, target, reduce=False)\r\nprint(full_loss)\r\nloss_sum = F.mse_loss(input, target, size_average=False)\r\nloss_mean = F.mse_loss(input, target, size_average=True)\r\nprint(loss_sum)\r\nprint(loss_mean)\r\n\r\nbatch_loss = full_loss.sum(dim=-1)\r\ncorrect_loss_mean = batch_loss.mean()\r\ncorrect_loss_sum = batch_loss.sum()\r\nprint(correct_loss_mean)\r\nprint(correct_loss_sum)\r\n\r\nassert correct_loss_sum == loss_sum\r\nassert correct_loss_mean == loss_mean\r\n"
6918,"repr of tensors seems to have extra space- PyTorch or Caffe2: Pytorch\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- OS: macos 10.13\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.6.5\r\n\r\nIf I run this code in the interpreter,\r\n\r\n\r\n\r\nI get this output:\r\n\r\n\r\n\r\nI put a `~` instead of a space for clarity. As you can see, extra spaces are printed. This happens even if I create a singleton 1D tensor.",todo|module: printing,li-roy,"- PyTorch or Caffe2: Pytorch\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- OS: macos 10.13\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.6.5\r\n\r\nIf I run this code in the interpreter,\r\n\r\n```python\r\nimport torch\r\nt = torch.tensor([1, 2])\r\nrepr(t)\r\n```\r\n\r\nI get this output:\r\n\r\n```python\r\ntensor([~1,~~2])\r\n```\r\n\r\nI put a `~` instead of a space for clarity. As you can see, extra spaces are printed. This happens even if I create a singleton 1D tensor.","python\r\nimport torch\r\nt = torch.tensor([1, 2])\r\nrepr(t)\r\n"
6912,Ugly and poorer tensor visualisation with v0.4.0`numpy` deals well with printing arrays.\r\n\r\n\r\n\r\n`torch` no longer does.\r\n\r\n\r\n\r\n,module: printing,li-roy,"`numpy` deals well with printing arrays.\r\n\r\n```python\r\nIn [1]: np.array([1, 2, 3])       \r\nOut[1]: array([1, 2, 3])          \r\n                                   \r\nIn [2]: print(np.array([1, 2, 3]))\r\n[1 2 3]\r\n```\r\n\r\n`torch` no longer does.\r\n\r\n```python\r\nIn [3]: torch.tensor([1, 2, 3])       \r\nOut[3]: tensor([ 1,  2,  3])\r\n\r\nIn [4]: print(torch.tensor([1, 2, 3]))\r\ntensor([ 1,  2,  3])\r\n```\r\n\r\n```python\r\n 1  2  3\r\n 4  5  6\r\n[torch.FloatTensor of size 2x3]\r\n\r\n# was much cleaner (no brackets and commas) and informative (`dtype` and `size`) than\r\n\r\ntensor([[ 1,  2,  3], \r\n        [ 4,  5,  6]])\r\n```","python\r\nIn [1]: np.array([1, 2, 3])       \r\nOut[1]: array([1, 2, 3])          \r\n                                   \r\nIn [2]: print(np.array([1, 2, 3]))\r\n[1 2 3]\r\n"
6885,"[PyTorch] Add torch.astensor and deprecate torch.from_numpyI believe it would be a good addition to add a new factory function, `torch.astensor`, which is equivalent to `torch.tensor`, but which doesn't perform a copy if possible. This means that passing a `torch.Tensor` returns a view of the same tensor, and passing a numpy array would have a behavior similar to `torch.from_numpy`.\r\n\r\nThis means that we could potentially have a call on `torch.astensor` in the beginning of every function, as a way of supporting other data types than torch tensors for torch operations.\r\n\r\nFor example\r\n\r\n\r\nGiven that `torch.tensor` infers the type of the tensor from the content of the data we pass to it, we could then probably deprecate `torch.from_numpy` in favor if this unified constructor.\r\n\r\nWhat do you think?",triage review,gchanan,"I believe it would be a good addition to add a new factory function, `torch.astensor`, which is equivalent to `torch.tensor`, but which doesn't perform a copy if possible. This means that passing a `torch.Tensor` returns a view of the same tensor, and passing a numpy array would have a behavior similar to `torch.from_numpy`.\r\n\r\nThis means that we could potentially have a call on `torch.astensor` in the beginning of every function, as a way of supporting other data types than torch tensors for torch operations.\r\n\r\nFor example\r\n```python\r\n# in torch namespace\r\ndef exp(x):\r\n    x = torch.astensor(x)\r\n    return x.exp()\r\n\r\n# can now call `exp` on floats, lists, arrays\r\ntorch.exp(1.0)\r\ntorch.exp([1.0, 2.0])\r\ntorch.exp(np.array([1.0, 2.0]))\r\n```\r\n\r\nGiven that `torch.tensor` infers the type of the tensor from the content of the data we pass to it, we could then probably deprecate `torch.from_numpy` in favor if this unified constructor.\r\n\r\nWhat do you think?","python\r\n# in torch namespace\r\ndef exp(x):\r\n    x = torch.astensor(x)\r\n    return x.exp()\r\n\r\n# can now call `exp` on floats, lists, arrays\r\ntorch.exp(1.0)\r\ntorch.exp([1.0, 2.0])\r\ntorch.exp(np.array([1.0, 2.0]))\r\n"
6865,[PyTorch] Don't use scientific notation for printing integer tensorsWe currently use scientific notation (E-notation) to print large integer tensors. We shouldn't do this because it hides meaningful digits and because E-notation corresponds to floating point data types in Python.\r\n\r\n,high priority|module: printing,li-roy,"We currently use scientific notation (E-notation) to print large integer tensors. We shouldn't do this because it hides meaningful digits and because E-notation corresponds to floating point data types in Python.\r\n\r\n```python\r\n>>> x = torch.ones(525076, dtype=torch.int)\r\n>>> x.sum()\r\ntensor(5.2508e+05)\r\n>>> x.sum().item()\r\n525076\r\n```","python\r\n>>> x = torch.ones(525076, dtype=torch.int)\r\n>>> x.sum()\r\ntensor(5.2508e+05)\r\n>>> x.sum().item()\r\n525076\r\n"
6863,"[PyTorch] Printing large tensors is slowPrinting large tensors is slow in master.\r\n\r\n\r\n\r\nThis is important because we often deal with very large tensors.\r\n\r\nNote that printing large tensors is much faster in NumPy (10,000x in this case):\r\n\r\n",high priority,fmassa,"Printing large tensors is slow in master.\r\n\r\n```python\r\n>>> x = torch.randn(1000, 1000, 1000)\r\n>>> %timeit repr(x)\r\n1 loop, best of 3: 21.8 s per loop\r\n```\r\n\r\nThis is important because we often deal with very large tensors.\r\n\r\nNote that printing large tensors is much faster in NumPy (10,000x in this case):\r\n\r\n```python\r\n>>> %timeit repr(x.numpy())\r\n100 loops, best of 3: 2.18 ms per loop\r\n```","python\r\n>>> x = torch.randn(1000, 1000, 1000)\r\n>>> %timeit repr(x)\r\n1 loop, best of 3: 21.8 s per loop\r\n"
6511,"torch.irfft produces ""cuFFT error: CUFFT_ALLOC_FAILED"" when called after torch.rfft@SsnL \r\n\r\nThe code below produces:\r\n```\r\n(25088, 4001, 2)\r\n\r\n  File ""bug.py"", line 28, in <module>\r\n    layer(bottom1, bottom2)\r\n  File ""/home/cvlab/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 371, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""bug.py"", line 20, in forward\r\n    cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\nRuntimeError: cuFFT error: CUFFT_ALLOC_FAILED\r\n```\r\n\r\n\r\n\r\nWorks without error, but there seems a shape misalignment:\r\n\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: master\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 2.7",todo,ssnl,"@SsnL \r\n\r\nThe code below produces:\r\n```\r\n(25088, 4001, 2)\r\n\r\n  File ""bug.py"", line 28, in <module>\r\n    layer(bottom1, bottom2)\r\n  File ""/home/cvlab/vadim/.wigwam/prefix/python/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 371, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""bug.py"", line 20, in forward\r\n    cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\nRuntimeError: cuFFT error: CUFFT_ALLOC_FAILED\r\n```\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass CompactBilinearPooling(nn.Module):\r\n    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool = True):\r\n        super(CompactBilinearPooling, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.sum_pool = sum_pool\r\n\r\n        generate_sketch_matrix = lambda rand_h, rand_s, input_dim, output_dim: torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, out = torch.LongTensor()), rand_h.long()]), rand_s.float(), torch.Size([input_dim, output_dim])).to_dense()\r\n        self.sparse_sketch_matrix1 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim1,)), 2 * torch.randint(2, size = (input_dim1,)) - 1, input_dim1, output_dim))\r\n        self.sparse_sketch_matrix2 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim2,)), 2 * torch.randint(2, size = (input_dim2,)) - 1, input_dim2, output_dim))\r\n\r\n    def forward(self, bottom1, bottom2):\r\n        sketch_1 = bottom1.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix1).view(-1, self.output_dim)\r\n        sketch_2 = bottom2.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix2).view(-1, self.output_dim)\r\n        fft1_real, fft1_imag = torch.rfft(sketch_1, 1).permute(2, 0, 1)\r\n        fft2_real, fft2_imag = torch.rfft(sketch_2, 1).permute(2, 0, 1)\r\n        fft_product = torch.stack([fft1_real * fft2_real - fft1_imag * fft2_imag, fft1_real * fft2_imag - fft1_imag * fft2_real], dim = -1)\r\n        print(fft_product.size())\r\n        cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\n        return cbp.sum(dim = 1).sum(dim = 1) if self.sum_pool else cbp.permute(0, 3, 1, 2)\r\n\r\nbottom1 = torch.randn(128, 512, 14, 14).cuda()\r\nbottom2 = torch.randn(128, 512, 14, 14).cuda()\r\nlayer = CompactBilinearPooling(512, 512, 8000)\r\nlayer.cuda()\r\nlayer.train()\r\nlayer(bottom1, bottom2)\r\n```\r\n\r\nWorks without error, but there seems a shape misalignment:\r\n```python\r\nimport torch\r\nfft_product = torch.rfft(torch.cuda.FloatTensor(25088, 8000), 1)\r\nprint(fft_product.size())\r\n# (25088, 4001, 2)\r\nprint(torch.irfft(fft_product, 1).size())\r\n# (25088, 8001)\r\n```\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: master\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 2.7","python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass CompactBilinearPooling(nn.Module):\r\n    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool = True):\r\n        super(CompactBilinearPooling, self).__init__()\r\n        self.output_dim = output_dim\r\n        self.sum_pool = sum_pool\r\n\r\n        generate_sketch_matrix = lambda rand_h, rand_s, input_dim, output_dim: torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, out = torch.LongTensor()), rand_h.long()]), rand_s.float(), torch.Size([input_dim, output_dim])).to_dense()\r\n        self.sparse_sketch_matrix1 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim1,)), 2 * torch.randint(2, size = (input_dim1,)) - 1, input_dim1, output_dim))\r\n        self.sparse_sketch_matrix2 = nn.Parameter(generate_sketch_matrix(torch.randint(output_dim, size = (input_dim2,)), 2 * torch.randint(2, size = (input_dim2,)) - 1, input_dim2, output_dim))\r\n\r\n    def forward(self, bottom1, bottom2):\r\n        sketch_1 = bottom1.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix1).view(-1, self.output_dim)\r\n        sketch_2 = bottom2.permute(0, 2, 3, 1).contiguous().matmul(self.sparse_sketch_matrix2).view(-1, self.output_dim)\r\n        fft1_real, fft1_imag = torch.rfft(sketch_1, 1).permute(2, 0, 1)\r\n        fft2_real, fft2_imag = torch.rfft(sketch_2, 1).permute(2, 0, 1)\r\n        fft_product = torch.stack([fft1_real * fft2_real - fft1_imag * fft2_imag, fft1_real * fft2_imag - fft1_imag * fft2_real], dim = -1)\r\n        print(fft_product.size())\r\n        cbp = torch.irfft(fft_product, 1).view(len(bottom1), bottom1.size(-2), bottom1.size(-1), self.output_dim) * self.output_dim\r\n        return cbp.sum(dim = 1).sum(dim = 1) if self.sum_pool else cbp.permute(0, 3, 1, 2)\r\n\r\nbottom1 = torch.randn(128, 512, 14, 14).cuda()\r\nbottom2 = torch.randn(128, 512, 14, 14).cuda()\r\nlayer = CompactBilinearPooling(512, 512, 8000)\r\nlayer.cuda()\r\nlayer.train()\r\nlayer(bottom1, bottom2)\r\n"
6313,"[utils.bottleneck] Bottleneck crashes with multi-threaded data loader`torch.utils.bottleneck` doesn't work properly when the code contains a data loader that uses more than 0 threads.\r\n\r\nMinimum reproducible example (`mwe.py`):\r\n\r\n\r\nRunning the script via:\r\n```\r\npython -m torch.utils.bottleneck -- mwe.py --num-workers 0\r\n```\r\nworks fine, while\r\n```\r\npython -m torch.utils.bottleneck -- mwe2.py --num-workers 1\r\n```\r\ncrashes with the following stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 280, in <module>\r\n    main()\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 261, in main\r\n    autograd_prof_cpu, autograd_prof_cuda = run_autograd_prof(code, globs)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 155, in run_autograd_prof\r\n    result.append(run_prof(use_cuda=True))\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 149, in run_prof\r\n    exec(code, globs, None)\r\n  File ""mwe2.py"", line 15, in <module>\r\n    for i, batch in enumerate(data_loader):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 285, in __next__\r\n    return self._process_next_batch(batch)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 306, in _process_next_batch\r\n    raise batch.exc_type(batch.exc_msg)\r\nRuntimeError: Traceback (most recent call last):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 57, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 57, in <listcomp>\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in __getitem__\r\n    return tuple(tensor[index] for tensor in self.tensors)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in <genexpr>\r\n    return tuple(tensor[index] for tensor in self.tensors)\r\nRuntimeError: /private/home/fmassa/github/pytorch/torch/csrc/autograd/profiler.h:52: initialization error\r\n```\r\n\r\nassigning this to @zou3519 , even thought I'm not sure if it's a problem in the profiler or in the `bottleneck` tool.\r\n\r\npytorch version `'0.4.0a0+b21e135'`\n\ncc @ezyang @gchanan @zou3519 @SsnL",module: dataloader|triaged|module: bottleneck|quansight-nack,zou3519,"`torch.utils.bottleneck` doesn't work properly when the code contains a data loader that uses more than 0 threads.\r\n\r\nMinimum reproducible example (`mwe.py`):\r\n```python\r\nimport argparse\r\nimport torch\r\nimport torch.utils.data\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='mwe')\r\n    parser.add_argument('--num-workers', default=0, type=int)\r\n    args = parser.parse_args()\r\n\r\n    data = torch.rand(10, 1000)\r\n    target = torch.rand(10)\r\n    dataset = torch.utils.data.TensorDataset(data, target)\r\n    data_loader = torch.utils.data.DataLoader(dataset,\r\n        batch_size=2, num_workers=args.num_workers)\r\n    for i, batch in enumerate(data_loader):\r\n        pass\r\n```\r\n\r\nRunning the script via:\r\n```\r\npython -m torch.utils.bottleneck -- mwe.py --num-workers 0\r\n```\r\nworks fine, while\r\n```\r\npython -m torch.utils.bottleneck -- mwe2.py --num-workers 1\r\n```\r\ncrashes with the following stack trace:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/runpy.py"", line 193, in _run_module_as_main\r\n    ""__main__"", mod_spec)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/runpy.py"", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 280, in <module>\r\n    main()\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 261, in main\r\n    autograd_prof_cpu, autograd_prof_cuda = run_autograd_prof(code, globs)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 155, in run_autograd_prof\r\n    result.append(run_prof(use_cuda=True))\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/bottleneck/__main__.py"", line 149, in run_prof\r\n    exec(code, globs, None)\r\n  File ""mwe2.py"", line 15, in <module>\r\n    for i, batch in enumerate(data_loader):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 285, in __next__\r\n    return self._process_next_batch(batch)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 306, in _process_next_batch\r\n    raise batch.exc_type(batch.exc_msg)\r\nRuntimeError: Traceback (most recent call last):\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 57, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 57, in <listcomp>\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in __getitem__\r\n    return tuple(tensor[index] for tensor in self.tensors)\r\n  File ""/private/home/fmassa/.conda/envs/detectron_v2/lib/python3.6/site-packages/torch/utils/data/dataset.py"", line 40, in <genexpr>\r\n    return tuple(tensor[index] for tensor in self.tensors)\r\nRuntimeError: /private/home/fmassa/github/pytorch/torch/csrc/autograd/profiler.h:52: initialization error\r\n```\r\n\r\nassigning this to @zou3519 , even thought I'm not sure if it's a problem in the profiler or in the `bottleneck` tool.\r\n\r\npytorch version `'0.4.0a0+b21e135'`\n\ncc @ezyang @gchanan @zou3519 @SsnL","python\r\nimport argparse\r\nimport torch\r\nimport torch.utils.data\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='mwe')\r\n    parser.add_argument('--num-workers', default=0, type=int)\r\n    args = parser.parse_args()\r\n\r\n    data = torch.rand(10, 1000)\r\n    target = torch.rand(10)\r\n    dataset = torch.utils.data.TensorDataset(data, target)\r\n    data_loader = torch.utils.data.DataLoader(dataset,\r\n        batch_size=2, num_workers=args.num_workers)\r\n    for i, batch in enumerate(data_loader):\r\n        pass\r\n"
6286,"Doing an operation on tensors of a different deviceIn ATen, doing operation on tensors of a different device seems unspecified. \r\n\r\n\r\n\r\nCurrently, according to @ezyang there's an error. However, writing models, I think it's preferred it be one of the latter options, since my model will work but maybe just a bit slower. I think @gchanan just suggested that keeping z on GPU 0 is the way PyTorch goes, and maybe ATen should also do this\r\n",module: internals,goldsborough,"In ATen, doing operation on tensors of a different device seems unspecified. \r\n\r\n```c++\r\nat::Tensor x, y;\r\n{\r\n  AutoGPU autogpu(0);\r\n  x = at::CUDA(at::kFloat).randn({100});\r\n  y = at::CUDA(at::kFloat).randn({100});\r\n}\r\n{\r\n  AutoGPU autogpu(1);\r\n  auto z = x * y; // What happens here? Error | Copy z to GPU 1 | keep z on GPU 0\r\n}\r\n```\r\n\r\nCurrently, according to @ezyang there's an error. However, writing models, I think it's preferred it be one of the latter options, since my model will work but maybe just a bit slower. I think @gchanan just suggested that keeping z on GPU 0 is the way PyTorch goes, and maybe ATen should also do this\r\n","c++\r\nat::Tensor x, y;\r\n{\r\n  AutoGPU autogpu(0);\r\n  x = at::CUDA(at::kFloat).randn({100});\r\n  y = at::CUDA(at::kFloat).randn({100});\r\n}\r\n{\r\n  AutoGPU autogpu(1);\r\n  auto z = x * y; // What happens here? Error | Copy z to GPU 1 | keep z on GPU 0\r\n}\r\n"
6219,"[BUG] Incorrect behavior of sparse matrix-matrix multiplicationThis PyTorch bug was introduced by https://github.com/pytorch/pytorch/pull/4707. \r\nUnlike claimed there, `indices` should not only be unique, but also be sorted in coalesced matrix for correct behavior of matrix multiplication. This raises a major issue when performing matrix multiplication with coalesced then transposed matrix. \r\n\r\nThe following is tested on 2e156f3, but any version after https://github.com/pytorch/pytorch/commit/5e72d7af136eb5a9105b501923f68d1246fa90b1 will have the same behavior. \r\n\r\n\r\n\r\n    \r\n     1\r\n     1\r\n     1\r\n    [torch.FloatTensor of size (3,1)]\r\n\r\n    \r\n     3\r\n     0\r\n     0\r\n    [torch.FloatTensor of size (3,1)]\r\n\r\n\r\nI will create a pull request regarding this and https://github.com/pytorch/pytorch/issues/6171 later today. \r\n\r\n\r\n\r\n",high priority,weiyangfb,"This PyTorch bug was introduced by https://github.com/pytorch/pytorch/pull/4707. \r\nUnlike claimed there, `indices` should not only be unique, but also be sorted in coalesced matrix for correct behavior of matrix multiplication. This raises a major issue when performing matrix multiplication with coalesced then transposed matrix. \r\n\r\nThe following is tested on 2e156f3, but any version after https://github.com/pytorch/pytorch/commit/5e72d7af136eb5a9105b501923f68d1246fa90b1 will have the same behavior. \r\n\r\n```python\r\nimport torch\r\nidx = torch.LongTensor([[0,1,2], [2,1,0]])\r\nval = torch.ones(3)\r\nD = torch.sparse.FloatTensor(idx,val,torch.Size([3,3]))\r\nDc = D.coalesce()\r\nx = torch.ones((3,1))\r\n```\r\n\r\n```python\r\ntorch.mm(D.t(), x)\r\n```    \r\n     1\r\n     1\r\n     1\r\n    [torch.FloatTensor of size (3,1)]\r\n\r\n```python\r\ntorch.mm(Dc.t(), x)\r\n```    \r\n     3\r\n     0\r\n     0\r\n    [torch.FloatTensor of size (3,1)]\r\n\r\n\r\nI will create a pull request regarding this and https://github.com/pytorch/pytorch/issues/6171 later today. \r\n\r\n\r\n\r\n","python\r\nimport torch\r\nidx = torch.LongTensor([[0,1,2], [2,1,0]])\r\nval = torch.ones(3)\r\nD = torch.sparse.FloatTensor(idx,val,torch.Size([3,3]))\r\nDc = D.coalesce()\r\nx = torch.ones((3,1))\r\n"
6217,"[PyTorch] indexing by a zero-dim Tensor should slice (not copy)PyTorch master\r\n\r\nCurrently, indexing a tensor by a zero-dim tensor creates a copy. We should make it return an alias.\r\n\r\n\r\n\r\n```\r\n 0  1  2\r\n 3  4  5\r\n 6  7  8\r\n[torch.FloatTensor of size (3,3)]\r\n```\r\n\r\n\r\nReported by lvdmaaten",high priority,colesbury,"PyTorch master\r\n\r\nCurrently, indexing a tensor by a zero-dim tensor creates a copy. We should make it return an alias.\r\n\r\n```python\r\nidx = torch.tensor(0, dtype=torch.int64)\r\nx = torch.arange(9).reshape(3, 3)\r\nx[idx].zero_()   # no effect since x[idx] copies; it should slice instead\r\nprint(x)\r\n```\r\n\r\n```\r\n 0  1  2\r\n 3  4  5\r\n 6  7  8\r\n[torch.FloatTensor of size (3,3)]\r\n```\r\n\r\n\r\nReported by lvdmaaten","python\r\nidx = torch.tensor(0, dtype=torch.int64)\r\nx = torch.arange(9).reshape(3, 3)\r\nx[idx].zero_()   # no effect since x[idx] copies; it should slice instead\r\nprint(x)\r\n"
5811,"Distributed communication never frees target memoryWhile working with the distributed package (TCP backend), I noticed that the memory used to store the target tensors in is never released by the garbage collector. Consider the following minimal example that produces a memory leak (and ultimately a crash):\r\n\r\n\r\nObviously, this example is pretty extreme, but the issue is also reproducible for smaller data sizes and long-running processes. When `all_gather` is removed, the memory is freed as expected.\r\n\r\nI generally would suppose that the garbage collector should release the memory after each iteration, or at least some time after each. However, I'm relatively new to Python and PyTorch, so this may be conceptually necessary or intended?\r\n\r\nThe easiest workaround is obviously to define the target tensors outside the loop.\r\n\r\nAdding \r\n\r\ndoes not solve the problem.\r\n\r\nSystem information:\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6\r\n\r\nThanks for developing PyTorch, keep up the good work!",high priority,ailzhang,"While working with the distributed package (TCP backend), I noticed that the memory used to store the target tensors in is never released by the garbage collector. Consider the following minimal example that produces a memory leak (and ultimately a crash):\r\n\r\n```python\r\ndef run(size):\r\n    while True:\r\n        values = torch.zeros(100, 100000)\r\n        target = [torch.zeros(100, 100000)] * size\r\n        dist.all_gather(tensor_list=target, tensor=values)\r\n        time.sleep(1)\r\n```\r\nObviously, this example is pretty extreme, but the issue is also reproducible for smaller data sizes and long-running processes. When `all_gather` is removed, the memory is freed as expected.\r\n\r\nI generally would suppose that the garbage collector should release the memory after each iteration, or at least some time after each. However, I'm relatively new to Python and PyTorch, so this may be conceptually necessary or intended?\r\n\r\nThe easiest workaround is obviously to define the target tensors outside the loop.\r\n\r\nAdding \r\n```python\r\ndel target\r\ngc.collect()\r\n```\r\ndoes not solve the problem.\r\n\r\nSystem information:\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6\r\n\r\nThanks for developing PyTorch, keep up the good work!","python\r\ndef run(size):\r\n    while True:\r\n        values = torch.zeros(100, 100000)\r\n        target = [torch.zeros(100, 100000)] * size\r\n        dist.all_gather(tensor_list=target, tensor=values)\r\n        time.sleep(1)\r\n"
5405,scatter_add_ should support scalar source (including Python scalar)I was looking to use `scatter_add_` to do `bincount`.\r\n\r\nat `0.4.0a0+1fdb392`\r\n\n\ncc @mikaylagawarecki,triaged|module: scatter & gather ops,mikaylagawarecki,"I was looking to use `scatter_add_` to do `bincount`.\r\n```python\r\nimport torch\r\na = torch.LongTensor([2, 0, 3, 3])\r\nr = torch.LongTensor(5)\r\n\r\n# works\r\nr.zero_().scatter_(0, a, 1)\r\n# 1\r\n# 0\r\n# 1\r\n# 1\r\n# 0\r\n# [torch.LongTensor of size 5]\r\n\r\n# scalar source doesn't work\r\nr.zero_().scatter_add_(0, a, 1)\r\n#Traceback (most recent call last):\r\n#  File ""<stdin>"", line 1, in <module>\r\n#TypeError: scatter_add_ received an invalid combination of arguments - got (int, torch.LongTensor, int), #but expected (int dim, torch.LongTensor index, torch.LongTensor src)\r\n\r\n# no broadcasting? but no checking for memory bounds either?\r\nr.zero_().scatter_add_(0, a, torch.LongTensor([1]))\r\n#1.4033e+14\r\n# 0.0000e+00\r\n# 1.0000e+00\r\n# 5.4931e+18\r\n# 0.0000e+00\r\n# [torch.LongTensor of size 5]\r\n\r\n# works ok\r\nr.zero_().scatter_add_(0, a, torch.LongTensor([1]).expand_as(a))\r\n# 1\r\n# 0\r\n# 1\r\n# 2\r\n# 0\r\n# [torch.LongTensor of size 5]\r\n\r\n```\r\nat `0.4.0a0+1fdb392`\r\n\n\ncc @mikaylagawarecki","python\r\nimport torch\r\na = torch.LongTensor([2, 0, 3, 3])\r\nr = torch.LongTensor(5)\r\n\r\n# works\r\nr.zero_().scatter_(0, a, 1)\r\n# 1\r\n# 0\r\n# 1\r\n# 1\r\n# 0\r\n# [torch.LongTensor of size 5]\r\n\r\n# scalar source doesn't work\r\nr.zero_().scatter_add_(0, a, 1)\r\n#Traceback (most recent call last):\r\n#  File ""<stdin>"", line 1, in <module>\r\n#TypeError: scatter_add_ received an invalid combination of arguments - got (int, torch.LongTensor, int), #but expected (int dim, torch.LongTensor index, torch.LongTensor src)\r\n\r\n# no broadcasting? but no checking for memory bounds either?\r\nr.zero_().scatter_add_(0, a, torch.LongTensor([1]))\r\n#1.4033e+14\r\n# 0.0000e+00\r\n# 1.0000e+00\r\n# 5.4931e+18\r\n# 0.0000e+00\r\n# [torch.LongTensor of size 5]\r\n\r\n# works ok\r\nr.zero_().scatter_add_(0, a, torch.LongTensor([1]).expand_as(a))\r\n# 1\r\n# 0\r\n# 1\r\n# 2\r\n# 0\r\n# [torch.LongTensor of size 5]\r\n\r\n"
4164,[Proposal] Named Axes/Dimensions or Tensor Shape AnnotationsIt would be great if there was a way to name tensor dimensions and use the names in all of the tensor methods that take dim as an argument. \r\n\r\n\r\n\r\nThis could obviously be expanded to support axis objects that could support a basic algebra to allow using dimensions in functions like `view()`.\r\n\r\nAnother somewhat related feature would be a way to add type annotations as proposed in [this google doc](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl):\r\n\r\n\r\n\r\n- [Datarray: numpy with axis names](https://github.com/BIDS/datarray)\r\n- [python mailing list discussion](https://mail.python.org/pipermail/numpy-discussion/2017-November/077429.html)\r\n- [Google doc proposing array shape annotations](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl)\r\n- [Python PEP 472 proposing named indexing]( https://www.python.org/dev/peps/pep-0472/)\n\ncc @ezyang @gchanan @zou3519,feature|module: molly-guard|triaged|module: named tensor,zou3519,"It would be great if there was a way to name tensor dimensions and use the names in all of the tensor methods that take dim as an argument. \r\n\r\n```python\r\nimages = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\nmore_images = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\n\r\ntorch.cat((images, more_images), dim='batch')\r\n```\r\n\r\nThis could obviously be expanded to support axis objects that could support a basic algebra to allow using dimensions in functions like `view()`.\r\n\r\nAnother somewhat related feature would be a way to add type annotations as proposed in [this google doc](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl):\r\n\r\n```python\r\nBatch = DimensionVar('Batch')\r\nRow = DimensionVar('Row')\r\nColumn = DimensionVar('Column')\r\nChannel = DimensionVar('Channel', size=3)\r\n\r\nRGBImage = NDArray[Batch, Row, Column, Channel]\r\nGrayscaleImage = NDArray[Batch, Row, Column]\r\n\r\ndef rgb_to_grayscale(array: RGBImage) -> GrayscaleImage:\r\n  return numpy.mean(array, axis=RGBImage.axis(Channel))\r\n```\r\n\r\n- [Datarray: numpy with axis names](https://github.com/BIDS/datarray)\r\n- [python mailing list discussion](https://mail.python.org/pipermail/numpy-discussion/2017-November/077429.html)\r\n- [Google doc proposing array shape annotations](https://docs.google.com/document/d/1vpMse4c6DrWH5rq2tQSx3qwP_m_0lyn-Ij4WHqQqRHY/edit#heading=h.rkj7d39awayl)\r\n- [Python PEP 472 proposing named indexing]( https://www.python.org/dev/peps/pep-0472/)\n\ncc @ezyang @gchanan @zou3519","python\r\nimages = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\nmore_images = torch.Tensor(data, dims=('batch', 'channel', 'height', 'width'))\r\n\r\ntorch.cat((images, more_images), dim='batch')\r\n"
3789,"broadcasting behaves differently on CPU and GPUGRUCell and LSTMCell on GPU do not support broadcasting, while RNNCell supports broadcasting on both CPU and GPU. This inconsistency is not expected and not documented.\r\n\r\nHere's the example code:\r\n\r\nThe output of this example is:\r\n```\r\ngru error\r\ninvalid argument 3: Input and Hidden tensor sizes should be the same. at /pytorch/torch/lib/THCUNN/generic/FusedRNNKernel.cu:19\r\n```\r\n\r\nIs this a known issue or not expected?",high priority,ssnl,"GRUCell and LSTMCell on GPU do not support broadcasting, while RNNCell supports broadcasting on both CPU and GPU. This inconsistency is not expected and not documented.\r\n\r\nHere's the example code:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nrnn = nn.RNNCell(10, 20)\r\ngru = nn.GRUCell(10, 20)\r\ninput = Variable(torch.randn(3, 10))\r\nh0 = Variable(torch.randn(1, 20)) # this should be (3, 20)\r\n# both cells support broadcasting on CPU\r\nhx_rnn = rnn(input, h0)\r\nhx_gru = gru(input, h0)\r\nassert hx_rnn.size() == hx_gru.size()\r\n\r\n# send models and variables to GPU\r\nrnn.cuda()\r\ngru.cuda()\r\ninput = input.cuda()\r\nh0 = h0.cuda()\r\n\r\n# RNNCell is OK\r\ntry:\r\n    hx_rnn_cuda = rnn(input, h0)\r\nexcept RuntimeError as e:\r\n    print('rnn error\\n', e)\r\n\r\n# GRUCell on GPU does not support broadcasting\r\ntry:\r\n    hx_gru_cuda = gru(input, h0)\r\nexcept RuntimeError as e:\r\n    print('gru error\\n', e)\r\n```\r\nThe output of this example is:\r\n```\r\ngru error\r\ninvalid argument 3: Input and Hidden tensor sizes should be the same. at /pytorch/torch/lib/THCUNN/generic/FusedRNNKernel.cu:19\r\n```\r\n\r\nIs this a known issue or not expected?","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nrnn = nn.RNNCell(10, 20)\r\ngru = nn.GRUCell(10, 20)\r\ninput = Variable(torch.randn(3, 10))\r\nh0 = Variable(torch.randn(1, 20)) # this should be (3, 20)\r\n# both cells support broadcasting on CPU\r\nhx_rnn = rnn(input, h0)\r\nhx_gru = gru(input, h0)\r\nassert hx_rnn.size() == hx_gru.size()\r\n\r\n# send models and variables to GPU\r\nrnn.cuda()\r\ngru.cuda()\r\ninput = input.cuda()\r\nh0 = h0.cuda()\r\n\r\n# RNNCell is OK\r\ntry:\r\n    hx_rnn_cuda = rnn(input, h0)\r\nexcept RuntimeError as e:\r\n    print('rnn error\\n', e)\r\n\r\n# GRUCell on GPU does not support broadcasting\r\ntry:\r\n    hx_gru_cuda = gru(input, h0)\r\nexcept RuntimeError as e:\r\n    print('gru error\\n', e)\r\n"
3558,"Numerical issue in torch.gesv(b, A) and torch.inverse()I found gesv would get completely wrong results once the dimension of matrix A is large, say 200. (The same problem also occurs in torch.inverse()). But in numpy, linalg.solve works very well in much larger A and b.\r\n\r\n\r\n\r\nget rec:\r\n   12.5733     0.7703     3.3207  ...    -71.7724   -10.3178    -4.7247\r\n   28.7078     2.0905    17.2917  ...   -184.5936   -28.1566   -21.9251\r\n   55.6995     3.9526    29.5577  ...   -357.7238   -53.0334   -37.8292\r\n              ...                  \u22f1                 ...               \r\n  -29.2093    -2.0292   -15.7188  ...    188.9897    27.8947    19.6602\r\n  -57.7021    -3.9133   -21.4146  ...    349.9576    51.1917    30.0501\r\n   -5.3475    -0.3623    -1.7454  ...     31.8616     5.1076     3.6032\r\n",needs reproduction,ssnl,"I found gesv would get completely wrong results once the dimension of matrix A is large, say 200. (The same problem also occurs in torch.inverse()). But in numpy, linalg.solve works very well in much larger A and b.\r\n\r\n```python\r\nA = torch.randn([200, 200])\r\nb = torch.eye(200)\r\nx, _ = torch.gesv(b, A)\r\nrec = A.mm(x)\r\n```\r\n\r\nget rec:\r\n   12.5733     0.7703     3.3207  ...    -71.7724   -10.3178    -4.7247\r\n   28.7078     2.0905    17.2917  ...   -184.5936   -28.1566   -21.9251\r\n   55.6995     3.9526    29.5577  ...   -357.7238   -53.0334   -37.8292\r\n              ...                  \u22f1                 ...               \r\n  -29.2093    -2.0292   -15.7188  ...    188.9897    27.8947    19.6602\r\n  -57.7021    -3.9133   -21.4146  ...    349.9576    51.1917    30.0501\r\n   -5.3475    -0.3623    -1.7454  ...     31.8616     5.1076     3.6032\r\n","python\r\nA = torch.randn([200, 200])\r\nb = torch.eye(200)\r\nx, _ = torch.gesv(b, A)\r\nrec = A.mm(x)\r\n"
3475,"CUDA tensor allows negative values in torch.multinomial\r\n\r\nLooking at the values returned from tn.multinomial(1) and counting them, it seems like it returns the values of the multinomial distribution taken from t, i.e. that it has ignored the signs of the values. However, it seem like `tn.multinomial(n where n > 1)` always returns a tensor filled with zeros.",high priority,ssnl,"```python\r\nimport torch\r\nt = torch.rand(10) #create CPU tensor\r\nt.multinomial(1) #returns a result, as expected\r\ntn = -t\r\ntn.multinomial(1) # throws an exception, as per the documentation\r\ntn = tn.cuda()\r\ntn.multinomial(1) # returns a value, don't know from what distribution\r\ntn.multinomial(1000, True) # returns a tensor of 1000 zeros\r\n```\r\n\r\nLooking at the values returned from tn.multinomial(1) and counting them, it seems like it returns the values of the multinomial distribution taken from t, i.e. that it has ignored the signs of the values. However, it seem like `tn.multinomial(n where n > 1)` always returns a tensor filled with zeros.","python\r\nimport torch\r\nt = torch.rand(10) #create CPU tensor\r\nt.multinomial(1) #returns a result, as expected\r\ntn = -t\r\ntn.multinomial(1) # throws an exception, as per the documentation\r\ntn = tn.cuda()\r\ntn.multinomial(1) # returns a value, don't know from what distribution\r\ntn.multinomial(1000, True) # returns a tensor of 1000 zeros\r\n"
3076,"BN slows down double-backprop enormouslyWhen using a ConvNet without batchnorm layers, the optimization with a gradient penalty takes approximately 5 times longer than without the gradient penalty. This ratio 1:5 corresponds exactly to the ratio expected by the double-backpropagation algorithm. But when the ConvNet has batchnorm layers, this ratio goes up to approximately 1:30.\r\n\r\nNote that these ratios are independent of the number of layers.\r\nFor instance, the output of the following code with 3 conv layers (n_layers=3) gives:\r\n\r\n> No BN and no lambda\r\n> epoch  0 loss 2.30 time 9\r\n> No BN and with lambda\r\n> epoch  0 loss 2.30 time 50\r\n> With BN and no lambda\r\n> epoch  0 loss 2.30 time 12\r\n> With BN and with lambda\r\n> epoch  0 loss 2.30 time 305\r\n\r\n\r\n\n\ncc @VitalyFedyunin @ngimel",module: performance|triaged,ssnl,"When using a ConvNet without batchnorm layers, the optimization with a gradient penalty takes approximately 5 times longer than without the gradient penalty. This ratio 1:5 corresponds exactly to the ratio expected by the double-backpropagation algorithm. But when the ConvNet has batchnorm layers, this ratio goes up to approximately 1:30.\r\n\r\nNote that these ratios are independent of the number of layers.\r\nFor instance, the output of the following code with 3 conv layers (n_layers=3) gives:\r\n\r\n> No BN and no lambda\r\n> epoch  0 loss 2.30 time 9\r\n> No BN and with lambda\r\n> epoch  0 loss 2.30 time 50\r\n> With BN and no lambda\r\n> epoch  0 loss 2.30 time 12\r\n> With BN and with lambda\r\n> epoch  0 loss 2.30 time 305\r\n\r\n\r\n```python\r\nimport torch                                                                                                                                                                                                                                                                                                                                                                \r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\n\r\nclass Identity(nn.Module):\r\n    def __init__(self, *args):\r\n        super(Identity, self).__init__()\r\n\r\n    def forward(self, x): \r\n        return x\r\n\r\nclass Layer(nn.Module):\r\n    def __init__(self, in_planes, planes, BN):\r\n        super(Layer, self).__init__()\r\n        self.bn = BN(planes)\r\n        self.conv = nn.Conv2d(in_planes, planes, 3, padding=1, bias=False)\r\n        self.act = nn.ReLU()\r\n\r\n    def forward(self, x): \r\n        return self.act(self.bn(self.conv(x)))\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self, BN, n_layers=3):\r\n        super(Net, self).__init__()\r\n        self.in_planes = 3 \r\n        self.layers = self._make_layers(Layer,  64, n_layers, BN) \r\n        self.linear = nn.Linear(64, 10) \r\n\r\n    def _make_layers(self, block, planes, num_blocks, BN):\r\n        strides = [1] + [1] * (num_blocks - 1)\r\n        layers = []\r\n        for stride in strides:\r\n            layers.append(block(self.in_planes, planes, BN))\r\n            self.in_planes = planes\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x): \r\n        out = self.layers(x)\r\n        out = F.avg_pool2d(out, out.size(2))\r\n        out = out.view(out.size(0), -1) \r\n        return self.linear(out)\r\n\r\ndef do_epoch(net, criterion, optimizer, lam):\r\n    net.train()\r\n    aggr_loss = count = 0 \r\n    for _ in range(1000):\r\n        inputs = Variable(torch.cuda.FloatTensor(128,3,32,32).normal_(), requires_grad=True)\r\n        targets = Variable(torch.LongTensor(128).random_(0, 10).cuda())\r\n        outputs = net(inputs)\r\n\r\n        optimizer.zero_grad()\r\n        loss = criterion(outputs, targets)\r\n        loss.backward(create_graph=(lam > 0)) \r\n\r\n        # gradient penalty\r\n        if lam > 0:\r\n            gpenalty = inputs.grad.view(inputs.size(0), -1).add(1e-5).norm(1, 1).mean()\r\n            (lam * gpenalty).backward()\r\n\r\n        optimizer.step()\r\n\r\n        count += 1\r\n        aggr_loss += loss.data[0]\r\n\r\n    return aggr_loss / count\r\n\r\ndef main(net, lam):\r\n    net.cuda()\r\n    cudnn.benchmark = True\r\n\r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    criterion.cuda()\r\n\r\n    optimizer = torch.optim.SGD(net.parameters(), lr=.001, momentum=0.9)\r\n\r\n    for epoch in range(1):\r\n        time_start = time.time()\r\n\r\n        loss = do_epoch(net, criterion, optimizer, lam)\r\n\r\n        print(""epoch %2d loss %.2f time %d"" % (epoch, loss, time.time()-time_start))\r\n\r\nn_layers = 3 \r\nBN = Identity\r\nprint('No BN and no lambda')\r\nmain(Net(BN, n_layers), 0.) \r\n\r\nprint('No BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n\r\nBN = nn.BatchNorm2d\r\nprint('With BN and no lambda')\r\nmain(Net(BN, n_layers), 0.)\r\n\r\nprint('With BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n```\n\ncc @VitalyFedyunin @ngimel","python\r\nimport torch                                                                                                                                                                                                                                                                                                                                                                \r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\n\r\nclass Identity(nn.Module):\r\n    def __init__(self, *args):\r\n        super(Identity, self).__init__()\r\n\r\n    def forward(self, x): \r\n        return x\r\n\r\nclass Layer(nn.Module):\r\n    def __init__(self, in_planes, planes, BN):\r\n        super(Layer, self).__init__()\r\n        self.bn = BN(planes)\r\n        self.conv = nn.Conv2d(in_planes, planes, 3, padding=1, bias=False)\r\n        self.act = nn.ReLU()\r\n\r\n    def forward(self, x): \r\n        return self.act(self.bn(self.conv(x)))\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self, BN, n_layers=3):\r\n        super(Net, self).__init__()\r\n        self.in_planes = 3 \r\n        self.layers = self._make_layers(Layer,  64, n_layers, BN) \r\n        self.linear = nn.Linear(64, 10) \r\n\r\n    def _make_layers(self, block, planes, num_blocks, BN):\r\n        strides = [1] + [1] * (num_blocks - 1)\r\n        layers = []\r\n        for stride in strides:\r\n            layers.append(block(self.in_planes, planes, BN))\r\n            self.in_planes = planes\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x): \r\n        out = self.layers(x)\r\n        out = F.avg_pool2d(out, out.size(2))\r\n        out = out.view(out.size(0), -1) \r\n        return self.linear(out)\r\n\r\ndef do_epoch(net, criterion, optimizer, lam):\r\n    net.train()\r\n    aggr_loss = count = 0 \r\n    for _ in range(1000):\r\n        inputs = Variable(torch.cuda.FloatTensor(128,3,32,32).normal_(), requires_grad=True)\r\n        targets = Variable(torch.LongTensor(128).random_(0, 10).cuda())\r\n        outputs = net(inputs)\r\n\r\n        optimizer.zero_grad()\r\n        loss = criterion(outputs, targets)\r\n        loss.backward(create_graph=(lam > 0)) \r\n\r\n        # gradient penalty\r\n        if lam > 0:\r\n            gpenalty = inputs.grad.view(inputs.size(0), -1).add(1e-5).norm(1, 1).mean()\r\n            (lam * gpenalty).backward()\r\n\r\n        optimizer.step()\r\n\r\n        count += 1\r\n        aggr_loss += loss.data[0]\r\n\r\n    return aggr_loss / count\r\n\r\ndef main(net, lam):\r\n    net.cuda()\r\n    cudnn.benchmark = True\r\n\r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    criterion.cuda()\r\n\r\n    optimizer = torch.optim.SGD(net.parameters(), lr=.001, momentum=0.9)\r\n\r\n    for epoch in range(1):\r\n        time_start = time.time()\r\n\r\n        loss = do_epoch(net, criterion, optimizer, lam)\r\n\r\n        print(""epoch %2d loss %.2f time %d"" % (epoch, loss, time.time()-time_start))\r\n\r\nn_layers = 3 \r\nBN = Identity\r\nprint('No BN and no lambda')\r\nmain(Net(BN, n_layers), 0.) \r\n\r\nprint('No BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n\r\nBN = nn.BatchNorm2d\r\nprint('With BN and no lambda')\r\nmain(Net(BN, n_layers), 0.)\r\n\r\nprint('With BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n"
2917,"CuDNN batchnorm has batch size limit\r\n\r\nThis will give CUDNN_STATUS_NOT_SUPPORTED error, as reported here: https://discuss.pytorch.org/t/cudnn-status-not-supported-with-large-bach-size-when-using-batchnorm/7014/4",high priority|module: dependency bug,soumith,"```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.backends.cudnn.enabled=True\r\nx = Variable( torch.rand(140000,1).contiguous()).cuda()\r\n\r\nprint (torch.backends.cudnn.version())\r\n\r\nbn = nn.BatchNorm1d(1)\r\nbn.cuda()\r\n\r\nxbn = bn(x)\r\nxbn.size()\r\n```\r\n\r\nThis will give CUDNN_STATUS_NOT_SUPPORTED error, as reported here: https://discuss.pytorch.org/t/cudnn-status-not-supported-with-large-bach-size-when-using-batchnorm/7014/4","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ntorch.backends.cudnn.enabled=True\r\nx = Variable( torch.rand(140000,1).contiguous()).cuda()\r\n\r\nprint (torch.backends.cudnn.version())\r\n\r\nbn = nn.BatchNorm1d(1)\r\nbn.cuda()\r\n\r\nxbn = bn(x)\r\nxbn.size()\r\n"
2838,torch.cuda.Tensor.bernoulli() missing\r\n\r\nThis used to work before I updated to the master (my previous version was before Variable / ATen big change).,high priority,killeent,"```python\r\nimport torch\r\ntorch.Tensor([0.5]).bernoulli() # works\r\ntorch.Tensor([0.5]).cuda().bernoulli() # fails:\r\n# Traceback (most recent call last):\r\n#   File ""<stdin>"", line 1, in <module>\r\n# AttributeError: 'torch.cuda.FloatTensor' object has no attribute 'bernoulli'\r\n```\r\n\r\nThis used to work before I updated to the master (my previous version was before Variable / ATen big change).","python\r\nimport torch\r\ntorch.Tensor([0.5]).bernoulli() # works\r\ntorch.Tensor([0.5]).cuda().bernoulli() # fails:\r\n# Traceback (most recent call last):\r\n#   File ""<stdin>"", line 1, in <module>\r\n# AttributeError: 'torch.cuda.FloatTensor' object has no attribute 'bernoulli'\r\n"
2816,"Certain point-wise GPU operations with broadcasting are up to 12x slower going from v0.2.0 to masterSome point-wise tensor operations are up to 12 times slower on master (https://github.com/pytorch/pytorch/commit/2b9765ad02b666d623566935b385fc3d058ad33d) compared to v0.2.0 when one of the operands is broadcasted.\r\n\r\nWe are able to consistently reproduce this issue with the following code (note that the same issue appears also for point-wise sum and division):\r\n\r\n\r\nA typical output is as follows:\r\n```\r\n(pytorch_master) $ python timeit.py \r\n0.0461528801918\r\n\r\n(pytorch_v0.2) $ python timeit.py \r\n0.00369760036469\r\n```\r\n\r\nThe slow-down seems to be related to the axis that `x` and `w` have in common, in fact, when using\r\n\r\nor\r\n\r\nthere is no noticeable slow-down, while\r\n\r\nproduces the same behavior as the script above.\r\n\r\nThis has been tested on different systems using CUDA 8.0 and a variety of GPUs, including: TITAN X, TITAN Xp and GTX 1050.",high priority|module: performance|module: cuda|triaged,gchanan,"Some point-wise tensor operations are up to 12 times slower on master (https://github.com/pytorch/pytorch/commit/2b9765ad02b666d623566935b385fc3d058ad33d) compared to v0.2.0 when one of the operands is broadcasted.\r\n\r\nWe are able to consistently reproduce this issue with the following code (note that the same issue appears also for point-wise sum and division):\r\n```python\r\nimport torch\r\nimport time\r\n\r\nN, C, H, W = 64, 256, 64, 64\r\nrepetitions = 100\r\n\r\nx = torch.randn(N, C, H, W).cuda()\r\nw = torch.randn(C).cuda()\r\n\r\nt0 = time.time()\r\nfor _ in range(repetitions):\r\n    y = x * w.view(1, C, 1, 1)\r\n    torch.cuda.synchronize()\r\nprint (time.time() - t0) / repetitions\r\n```\r\n\r\nA typical output is as follows:\r\n```\r\n(pytorch_master) $ python timeit.py \r\n0.0461528801918\r\n\r\n(pytorch_v0.2) $ python timeit.py \r\n0.00369760036469\r\n```\r\n\r\nThe slow-down seems to be related to the axis that `x` and `w` have in common, in fact, when using\r\n```python\r\nw = torch.randn(N).cuda()\r\n...\r\ny = x * w.view(N, 1, 1, 1)\r\n```\r\nor\r\n```python\r\nw = torch.randn(W).cuda()\r\n...\r\ny = x * w.view(1, 1, 1, W)\r\n```\r\nthere is no noticeable slow-down, while\r\n```python\r\nw = torch.randn(H).cuda()\r\n...\r\ny = x * w.view(1, 1, H, 1)\r\n```\r\nproduces the same behavior as the script above.\r\n\r\nThis has been tested on different systems using CUDA 8.0 and a variety of GPUs, including: TITAN X, TITAN Xp and GTX 1050.","python\r\nimport torch\r\nimport time\r\n\r\nN, C, H, W = 64, 256, 64, 64\r\nrepetitions = 100\r\n\r\nx = torch.randn(N, C, H, W).cuda()\r\nw = torch.randn(C).cuda()\r\n\r\nt0 = time.time()\r\nfor _ in range(repetitions):\r\n    y = x * w.view(1, C, 1, 1)\r\n    torch.cuda.synchronize()\r\nprint (time.time() - t0) / repetitions\r\n"
2741,"tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)\r\nreturns:\r\n```\r\n(2, 2, 1)\r\n```\r\n\r\nHowever:\r\n\r\n\r\nreturns:\r\n```\r\n(2, 1, 2)\r\n```\r\n\r\nThis is inconsistent with numpy, and not the expected behavior. It should be of shape (2,2,1).",high priority,killeent,"```python\r\na=np.array([[1,2],[3,4]])\r\na[...,None].shape\r\n```\r\nreturns:\r\n```\r\n(2, 2, 1)\r\n```\r\n\r\nHowever:\r\n\r\n```python\r\nta=torch.from_numpy(a)\r\nta[...,None].size()\r\n```\r\nreturns:\r\n```\r\n(2, 1, 2)\r\n```\r\n\r\nThis is inconsistent with numpy, and not the expected behavior. It should be of shape (2,2,1).","python\r\na=np.array([[1,2],[3,4]])\r\na[...,None].shape\r\n"
2125,"RuntimeError: error executing torch_shm_manager\r\n```\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-73-cfff7ccbdfa2> in <module>()\r\n     15 # get some random training images\r\n     16 dataiter = iter(trainloader)\r\n---> 17 images, labels = dataiter.next()\r\n     18 \r\n     19 # show images\r\n\r\n//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    210                 self.reorder_dict[idx] = batch\r\n    211                 continue\r\n--> 212             return self._process_next_batch(batch)\r\n    213 \r\n    214     next = __next__  # Python 2 compatibility\r\n\r\n//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py in _process_next_batch(self, batch)\r\n    237         self._put_indices()\r\n    238         if isinstance(batch, ExceptionWrapper):\r\n--> 239             raise batch.exc_type(batch.exc_msg)\r\n    240         return batch\r\n    241 \r\n\r\nRuntimeError: Traceback (most recent call last):\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 41, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 110, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 110, in <listcomp>\r\n    return [default_collate(samples) for samples in transposed]\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 90, in default_collate\r\n    storage = batch[0].storage()._new_shared(numel)\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/storage.py"", line 111, in _new_shared\r\n    return cls._new_using_filename(size)\r\nRuntimeError: error executing torch_shm_manager at ""//anaconda/lib/python3.5/site-packages/torch/lib/torch_shm_manager"" at /Users/soumith/miniconda2/conda-bld/pytorch_1493757035034/work/torch/lib/libshm/core.cpp:125\r\n\r\n```\r\n\u200b\n\ncc @ezyang @gchanan @zou3519",high priority|module: binaries|triaged,soumith,"```python\r\n#Let us show some of the training images, for fun.\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# functions to show an image\r\n\r\n\r\ndef imshow(img):\r\n    img = img / 2 + 0.5     # unnormalize\r\n    npimg = img.numpy()\r\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n\r\n\r\n# get some random training images\r\ndataiter = iter(trainloader)\r\nimages, labels = dataiter.next()\r\n\r\n# show images\r\nimshow(torchvision.utils.make_grid(images))\r\n# print labels\r\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))\r\n\r\n\r\n\r\n\r\n```\r\n```\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-73-cfff7ccbdfa2> in <module>()\r\n     15 # get some random training images\r\n     16 dataiter = iter(trainloader)\r\n---> 17 images, labels = dataiter.next()\r\n     18 \r\n     19 # show images\r\n\r\n//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    210                 self.reorder_dict[idx] = batch\r\n    211                 continue\r\n--> 212             return self._process_next_batch(batch)\r\n    213 \r\n    214     next = __next__  # Python 2 compatibility\r\n\r\n//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py in _process_next_batch(self, batch)\r\n    237         self._put_indices()\r\n    238         if isinstance(batch, ExceptionWrapper):\r\n--> 239             raise batch.exc_type(batch.exc_msg)\r\n    240         return batch\r\n    241 \r\n\r\nRuntimeError: Traceback (most recent call last):\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 41, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 110, in default_collate\r\n    return [default_collate(samples) for samples in transposed]\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 110, in <listcomp>\r\n    return [default_collate(samples) for samples in transposed]\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 90, in default_collate\r\n    storage = batch[0].storage()._new_shared(numel)\r\n  File ""//anaconda/lib/python3.5/site-packages/torch/storage.py"", line 111, in _new_shared\r\n    return cls._new_using_filename(size)\r\nRuntimeError: error executing torch_shm_manager at ""//anaconda/lib/python3.5/site-packages/torch/lib/torch_shm_manager"" at /Users/soumith/miniconda2/conda-bld/pytorch_1493757035034/work/torch/lib/libshm/core.cpp:125\r\n\r\n```\r\n\u200b\n\ncc @ezyang @gchanan @zou3519","python\r\n#Let us show some of the training images, for fun.\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# functions to show an image\r\n\r\n\r\ndef imshow(img):\r\n    img = img / 2 + 0.5     # unnormalize\r\n    npimg = img.numpy()\r\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\r\n\r\n\r\n# get some random training images\r\ndataiter = iter(trainloader)\r\nimages, labels = dataiter.next()\r\n\r\n# show images\r\nimshow(torchvision.utils.make_grid(images))\r\n# print labels\r\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))\r\n\r\n\r\n\r\n\r\n"
1927,"[feature request] time-distributed layers for application of normal layers to sequence dataAs we move from images to videos, it seems imperative to feed sequential data into common image layers. However, the problem of dealing sequential data with such layers is not clears on Pytorch. I suggest here to devise something like -time-distributed layers for wrapping normal layers and aligning them for sequential inputs and outputs. I think in the course of time, it is going to be a more important pattern to have in Pytorch. Hope, I explain the problem clearly?\r\n\r\nLately, I solved that problem for 1D feat vectors as follows; I wait some comments to generalize it if necessary. \r\n",module: nn|triaged,t-vi,"As we move from images to videos, it seems imperative to feed sequential data into common image layers. However, the problem of dealing sequential data with such layers is not clears on Pytorch. I suggest here to devise something like -time-distributed layers for wrapping normal layers and aligning them for sequential inputs and outputs. I think in the course of time, it is going to be a more important pattern to have in Pytorch. Hope, I explain the problem clearly?\r\n\r\nLately, I solved that problem for 1D feat vectors as follows; I wait some comments to generalize it if necessary. \r\n```python\r\nclass TimeDistributed(nn.Module):\r\n    def __init__(self, module):\r\n        super(TimeDistributed, self).__init__()\r\n        self.module = module\r\n\r\n    def forward(self, x):\r\n        if len(x.size()) <= 2:\r\n            return self.module(x)\r\n        t, n = x.size(0), x.size(1) \r\n        # merge batch and seq dimensions\r\n        x_reshape = x.contiguous().view(t * n, x.size(2))\r\n        y = self.module(x_reshape)\r\n        # We have to reshape Y\r\n        y = y.contiguous().view(t, n, y.size()[1])\r\n        return y\r\n```","python\r\nclass TimeDistributed(nn.Module):\r\n    def __init__(self, module):\r\n        super(TimeDistributed, self).__init__()\r\n        self.module = module\r\n\r\n    def forward(self, x):\r\n        if len(x.size()) <= 2:\r\n            return self.module(x)\r\n        t, n = x.size(0), x.size(1) \r\n        # merge batch and seq dimensions\r\n        x_reshape = x.contiguous().view(t * n, x.size(2))\r\n        y = self.module(x_reshape)\r\n        # We have to reshape Y\r\n        y = y.contiguous().view(t, n, y.size()[1])\r\n        return y\r\n"
1791,"Improve the performance of btriunpackThe part of `btriunpack` that extract pivots has been causing some unexpected performance bottlenecks in qpth. Here's a newer version I've tried that uses gather/scatter operations across a batched vector instead of row interchanges on a batched matrix. I think it's a step towards a better method but the current form is just as slow. I want to do what the LAPACK LASWP function provides but with a batch so maybe we could use some knowledge from those implementations, like [this one in OpenBLAS](https://github.com/xianyi/OpenBLAS/blob/develop/lapack/laswp/generic/laswp_k_1.c).\r\n\r\n## Slightly improved pivot matrix extraction but still slow version using gather/scatter\r\n\r\n",todo|feature,bamos,"The part of `btriunpack` that extract pivots has been causing some unexpected performance bottlenecks in qpth. Here's a newer version I've tried that uses gather/scatter operations across a batched vector instead of row interchanges on a batched matrix. I think it's a step towards a better method but the current form is just as slow. I want to do what the LAPACK LASWP function provides but with a batch so maybe we could use some knowledge from those implementations, like [this one in OpenBLAS](https://github.com/xianyi/OpenBLAS/blob/develop/lapack/laswp/generic/laswp_k_1.c).\r\n\r\n## Slightly improved pivot matrix extraction but still slow version using gather/scatter\r\n\r\n```Python\r\nPidx = type(LU_data)(range(sz)).repeat(nBatch, 1).long()\r\n\r\nfor i in range(sz):\r\n    k = LU_pivots[:, i] - 1\r\n    t = Pidx[:, i].clone()\r\n    Pidx[:, i] = torch.gather(Pidx, 1, k.unsqueeze(1).long())\r\n    Pidx.scatter_(1, k.unsqueeze(1).long(), t.unsqueeze(1))\r\n\r\nP = type(LU_data)(nBatch, sz, sz).zero_()\r\nfor i in range(nBatch):\r\n    P[i].scatter_(0, Pidx[i].unsqueeze(0), 1.0)\r\n```","Python\r\nPidx = type(LU_data)(range(sz)).repeat(nBatch, 1).long()\r\n\r\nfor i in range(sz):\r\n    k = LU_pivots[:, i] - 1\r\n    t = Pidx[:, i].clone()\r\n    Pidx[:, i] = torch.gather(Pidx, 1, k.unsqueeze(1).long())\r\n    Pidx.scatter_(1, k.unsqueeze(1).long(), t.unsqueeze(1))\r\n\r\nP = type(LU_data)(nBatch, sz, sz).zero_()\r\nfor i in range(nBatch):\r\n    P[i].scatter_(0, Pidx[i].unsqueeze(0), 1.0)\r\n"
1631,"Gather backward is incorrect with repeated indicesThe gather function gives incorrect gradients on both CPU and GPU when using repeated indices; no warnings or errors are raised, and the documentation doesn't say anything about this. I've seen this discussed a bit on slack, but couldn't find any issues for it on GitHub.\r\n\r\nHere's a small test case (using PyTorch 0.1.12_2 on Ubuntu 16.04):\r\n\r\n\r\n\r\nThis gives the output:\r\n\r\n```\r\ndtype:  <class 'torch.FloatTensor'>\r\nx:  [ 0.1]\r\ni:  [0 0]\r\ny:  [ 0.1  0.1]\r\ndy:  [ 0.2         0.30000001]\r\ndx:  [ 0.30000001]\r\ndtype:  <class 'torch.cuda.FloatTensor'>\r\nx:  [ 0.1]\r\ni:  [0 0]\r\ny:  [ 0.1  0.1]\r\ndy:  [ 0.2         0.30000001]\r\ndx:  [ 0.30000001]\r\n```\r\n\r\nFor this example I expect dx to be [0.5] since the first element of x is duplicated to both elements of y; however instead of summing the elements of dy, the gather backward pass simply chooses one of the elements of dy corresponding to the element of x.\r\n\r\nIn this particular example we could use `index_select` instead of `gather`, but there are a lot of situations where `index_select` cannot substitute for `gather`; it would thus be extremely useful if `gather` could support repeated indices. ",high priority,apaszke,"The gather function gives incorrect gradients on both CPU and GPU when using repeated indices; no warnings or errors are raised, and the documentation doesn't say anything about this. I've seen this discussed a bit on slack, but couldn't find any issues for it on GitHub.\r\n\r\nHere's a small test case (using PyTorch 0.1.12_2 on Ubuntu 16.04):\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ndef gather_test(dtype):\r\n  x = torch.FloatTensor([0.1])\r\n  i = torch.LongTensor([0, 0])\r\n  x = Variable(x.type(dtype), requires_grad=True)\r\n  i = Variable(i.type(dtype).long())\r\n  y = x.gather(0, i)\r\n  dy = torch.FloatTensor([0.2, 0.3]).type(dtype)\r\n  y.backward(dy)\r\n  print('dtype: ', dtype)\r\n  print('x: ', x.data.cpu().numpy())\r\n  print('i: ', i.data.cpu().numpy())\r\n  print('y: ', y.data.cpu().numpy())\r\n  print('dy: ', dy.cpu().numpy())\r\n  print('dx: ', x.grad.data.cpu().numpy())\r\n\r\ngather_test(torch.FloatTensor)\r\ngather_test(torch.cuda.FloatTensor)\r\n```\r\n\r\nThis gives the output:\r\n\r\n```\r\ndtype:  <class 'torch.FloatTensor'>\r\nx:  [ 0.1]\r\ni:  [0 0]\r\ny:  [ 0.1  0.1]\r\ndy:  [ 0.2         0.30000001]\r\ndx:  [ 0.30000001]\r\ndtype:  <class 'torch.cuda.FloatTensor'>\r\nx:  [ 0.1]\r\ni:  [0 0]\r\ny:  [ 0.1  0.1]\r\ndy:  [ 0.2         0.30000001]\r\ndx:  [ 0.30000001]\r\n```\r\n\r\nFor this example I expect dx to be [0.5] since the first element of x is duplicated to both elements of y; however instead of summing the elements of dy, the gather backward pass simply chooses one of the elements of dy corresponding to the element of x.\r\n\r\nIn this particular example we could use `index_select` instead of `gather`, but there are a lot of situations where `index_select` cannot substitute for `gather`; it would thus be extremely useful if `gather` could support repeated indices. ","python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ndef gather_test(dtype):\r\n  x = torch.FloatTensor([0.1])\r\n  i = torch.LongTensor([0, 0])\r\n  x = Variable(x.type(dtype), requires_grad=True)\r\n  i = Variable(i.type(dtype).long())\r\n  y = x.gather(0, i)\r\n  dy = torch.FloatTensor([0.2, 0.3]).type(dtype)\r\n  y.backward(dy)\r\n  print('dtype: ', dtype)\r\n  print('x: ', x.data.cpu().numpy())\r\n  print('i: ', i.data.cpu().numpy())\r\n  print('y: ', y.data.cpu().numpy())\r\n  print('dy: ', dy.cpu().numpy())\r\n  print('dx: ', x.grad.data.cpu().numpy())\r\n\r\ngather_test(torch.FloatTensor)\r\ngather_test(torch.cuda.FloatTensor)\r\n"
1605,"torch.autograd.backward does not handle dependent Variables correctlyHere's a failure case on master:\r\n\r\n\r\n\r\nand here's the stack-trace:\r\n```\r\nTraceback (most recent call last):\r\n  File ""foo.py"", line 10, in <module>\r\n    torch.autograd.backward([y, z], [torch.randn(10), torch.randn(10)])\r\n  File ""/home/soumith/code/pytorch/torch/autograd/__init__.py"", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File ""/home/soumith/code/pytorch/torch/autograd/function.py"", line 90, in apply\r\n    return self._forward_cls.backward(self, *args)\r\n  File ""/home/soumith/code/pytorch/torch/autograd/_functions/basic_ops.py"", line 209, in backward\r\n    var, = ctx.saved_variables\r\nRuntimeError: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.\r\n```",high priority,apaszke,"Here's a failure case on master:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.randn(10), requires_grad=True)\r\ny = x ** 2\r\nz = y ** 3\r\n\r\ntorch.autograd.backward([y, z], [torch.randn(10), torch.randn(10)])\r\n```\r\n\r\nand here's the stack-trace:\r\n```\r\nTraceback (most recent call last):\r\n  File ""foo.py"", line 10, in <module>\r\n    torch.autograd.backward([y, z], [torch.randn(10), torch.randn(10)])\r\n  File ""/home/soumith/code/pytorch/torch/autograd/__init__.py"", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File ""/home/soumith/code/pytorch/torch/autograd/function.py"", line 90, in apply\r\n    return self._forward_cls.backward(self, *args)\r\n  File ""/home/soumith/code/pytorch/torch/autograd/_functions/basic_ops.py"", line 209, in backward\r\n    var, = ctx.saved_variables\r\nRuntimeError: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.\r\n```","python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.randn(10), requires_grad=True)\r\ny = x ** 2\r\nz = y ** 3\r\n\r\ntorch.autograd.backward([y, z], [torch.randn(10), torch.randn(10)])\r\n"
1591,"Pad PackedSequences to original batch lengthThe current flow for handling variable length sequences in RNNs is:\r\n\r\n\r\n\r\nThe output size of the Variable returned by pad_packed_sequence is determined by the max length in lengths: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106.\r\n\r\nHowever, this only works in recovering the original size of the input if the max length sequence has no padding (max length == length dim of batched input). For normal, sensible batching this makes sense and should be true.\r\n\r\nBut if a model is using, say, DataParallel, the batch might be split such that there __is__ extra padding. And the output size from the RNN will be truncated (which might break other things down-stream).\r\n\r\nTo fix this potentially unexpected behavior, I propose two possible simple patches.\r\n\r\n1) A `max_batch_size` field is calculated from the original input and added to the PackedSequence namedtuple: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6 and used instead of https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\r\n\r\n2) An optional `max_batch_size` parameter is added to pad_packed_sequence which would be used as an override.\r\n\r\nI prefer 1), but I suppose 2) has the advantage of being fully backwards compatible (even though it is discouraged to be directly creating or meddling with PackedSequence tuples, it is possible).\n\ncc @cpuhrsch",hackamonth|triaged|module: nestedtensor,cpuhrsch,"The current flow for handling variable length sequences in RNNs is:\r\n\r\n```python\r\npacked_input = torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\r\npacked_output = rnn(packed)[0]\r\noutput = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\r\n```\r\n\r\nThe output size of the Variable returned by pad_packed_sequence is determined by the max length in lengths: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106.\r\n\r\nHowever, this only works in recovering the original size of the input if the max length sequence has no padding (max length == length dim of batched input). For normal, sensible batching this makes sense and should be true.\r\n\r\nBut if a model is using, say, DataParallel, the batch might be split such that there __is__ extra padding. And the output size from the RNN will be truncated (which might break other things down-stream).\r\n\r\nTo fix this potentially unexpected behavior, I propose two possible simple patches.\r\n\r\n1) A `max_batch_size` field is calculated from the original input and added to the PackedSequence namedtuple: https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L6 and used instead of https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L106\r\n\r\n2) An optional `max_batch_size` parameter is added to pad_packed_sequence which would be used as an override.\r\n\r\nI prefer 1), but I suppose 2) has the advantage of being fully backwards compatible (even though it is discouraged to be directly creating or meddling with PackedSequence tuples, it is possible).\n\ncc @cpuhrsch","python\r\npacked_input = torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\r\npacked_output = rnn(packed)[0]\r\noutput = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\r\n"
1528,"LongTensor indexing with duplication not propagating gradientsIt seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:\r\n\r\n\r\n\r\nHowever:\r\n\r\n\r\nThis is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.\r\n",high priority,apaszke,"It seems like if we use LongTensor indexing to duplicate entries of a Variable, this breaks autograd. Specifically, if any entry in a vector is duplicated, backward() only maintains gradients for the last copy of the element. Simple example:\r\n\r\n```python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[1].backward() # backprop on second element\r\nprint(y.grad)\r\n""""""\r\nVariable containing:\r\n 1\r\n[torch.FloatTensor of size 1]""""""\r\n```\r\n\r\nHowever:\r\n```python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[0].backward()  # backprop on first element, same exact value as second\r\nprint(y.grad)\r\n""""""\r\nVariable containing:\r\n 0\r\n[torch.FloatTensor of size 1]""""""\r\n```\r\n\r\nThis is version under version 0.1.11+fc48d2c, tested on 0.1.11+9f3119a as well.  This happens for both cuda and cpu tensors.\r\n","python\r\ny = Variable(torch.ones(1), requires_grad=True)\r\ny_dup = y[torch.LongTensor([0,0])]\r\ny_dup[1].backward() # backprop on second element\r\nprint(y.grad)\r\n""""""\r\nVariable containing:\r\n 1\r\n[torch.FloatTensor of size 1]""""""\r\n"
1450,"Bug in new autograd backward (with LSTM Cell)This is how I implement the decoder of a sequence to sequence model\r\n\r\nI'm not sure about the new autograd mechanics but this worked in the previous version.\r\nIf I didn't make the unrolling codes a function it will work. It will also work with CPU.\r\nI compiled from source [(699755e)](https://github.com/pytorch/pytorch/commit/699755e04f8bbb4378a70f03dfe0849094fd0255) with Python 2.7, CUDA 8.0 and Cudnn 6",high priority,apaszke,"This is how I implement the decoder of a sequence to sequence model\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\n\r\ndef decoder(input_, embedding, lstm, projection, states):\r\n    """""" unroll the LSTM Cell, returns the flattened logits""""""\r\n    emb = embedding(input_.t())\r\n    hs = []\r\n    for i in range(input_.size(1)):\r\n        h, c = lstm(emb[i], states)\r\n        hs.append(h)\r\n        states = (h, c)\r\n    lstm_out = torch.stack(hs, dim=0)\r\n    logit = projection(lstm_out.contiguous().view(-1, lstm.hidden_size))\r\n    return logit\r\n\r\nembedding = nn.Embedding(4, 64, padding_idx=0).cuda()\r\nlstm = nn.LSTMCell(64, 64).cuda()\r\nprojection = nn.Linear(64, 4).cuda()\r\n\r\ninput_ = Variable(torch.LongTensor([[1, 2, 3], [3, 2, 1]])).cuda()\r\nstates = (Variable(torch.zeros(2, 64)).cuda(), Variable(torch.zeros(2, 64)).cuda())\r\ntarget = Variable(torch.LongTensor([[3, 2, 1], [2, 3, 1]])).cuda()\r\n\r\nlogit = decoder(input_, embedding, lstm, projection, states)\r\nloss = F.cross_entropy(logit, target.t().contiguous().view(-1))\r\nloss.backward()  #  RuntimeError: No grad accumulator for a saved leaf!\r\n\r\n```\r\nI'm not sure about the new autograd mechanics but this worked in the previous version.\r\nIf I didn't make the unrolling codes a function it will work. It will also work with CPU.\r\nI compiled from source [(699755e)](https://github.com/pytorch/pytorch/commit/699755e04f8bbb4378a70f03dfe0849094fd0255) with Python 2.7, CUDA 8.0 and Cudnn 6","python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.nn import functional as F\r\n\r\ndef decoder(input_, embedding, lstm, projection, states):\r\n    """""" unroll the LSTM Cell, returns the flattened logits""""""\r\n    emb = embedding(input_.t())\r\n    hs = []\r\n    for i in range(input_.size(1)):\r\n        h, c = lstm(emb[i], states)\r\n        hs.append(h)\r\n        states = (h, c)\r\n    lstm_out = torch.stack(hs, dim=0)\r\n    logit = projection(lstm_out.contiguous().view(-1, lstm.hidden_size))\r\n    return logit\r\n\r\nembedding = nn.Embedding(4, 64, padding_idx=0).cuda()\r\nlstm = nn.LSTMCell(64, 64).cuda()\r\nprojection = nn.Linear(64, 4).cuda()\r\n\r\ninput_ = Variable(torch.LongTensor([[1, 2, 3], [3, 2, 1]])).cuda()\r\nstates = (Variable(torch.zeros(2, 64)).cuda(), Variable(torch.zeros(2, 64)).cuda())\r\ntarget = Variable(torch.LongTensor([[3, 2, 1], [2, 3, 1]])).cuda()\r\n\r\nlogit = decoder(input_, embedding, lstm, projection, states)\r\nloss = F.cross_entropy(logit, target.t().contiguous().view(-1))\r\nloss.backward()  #  RuntimeError: No grad accumulator for a saved leaf!\r\n\r\n"
1288,"Forgetting `retain_variables=True` causes segfault in conv2dUsually calling backwards twice gives this message:\r\n\r\n```\r\nRuntimeError: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.\r\n```\r\n\r\nBut conv2d just segfaults.\r\n\r\n\r\n\r\n$ pt test.py\r\nA\r\nB\r\nSegmentation fault (core dumped)",high priority,soumith,"Usually calling backwards twice gives this message:\r\n\r\n```\r\nRuntimeError: Trying to backward through the graph second time, but the buffers have already been freed. Please specify retain_variables=True when calling backward for the first time.\r\n```\r\n\r\nBut conv2d just segfaults.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nl1 = nn.Conv2d(3, 10, 4, 2, 1, bias=False).cuda()\r\ninput = Variable(torch.cuda.FloatTensor(5, 3, 8, 8))\r\n\r\no1 = l1(input)\r\n\r\nprint(""A"")\r\no1.sum().backward()\r\nprint(""B"")\r\no1.sum().backward()\r\nprint(""C"")\r\n```\r\n\r\n$ pt test.py\r\nA\r\nB\r\nSegmentation fault (core dumped)","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nl1 = nn.Conv2d(3, 10, 4, 2, 1, bias=False).cuda()\r\ninput = Variable(torch.cuda.FloatTensor(5, 3, 8, 8))\r\n\r\no1 = l1(input)\r\n\r\nprint(""A"")\r\no1.sum().backward()\r\nprint(""B"")\r\no1.sum().backward()\r\nprint(""C"")\r\n"
1254,"Function request: np.corrcoefUpdate: the scipy.stats.pearsonr part of this issue is now tracked in https://github.com/pytorch/pytorch/issues/50333. This issue is for implementing np.corrcoef.\r\n\r\n**ORIGINAL ISSUE BELOW**\r\n\r\nHey all,\r\n\r\nI implemented bare bones versions of `np.corrcoef` and `scipy.stats.pearsonr`. These are two functions I use all the time, so I often have to convert back and forth to numpy for this. It'd be nice to have these incorporated, although I only see the need for the forward pass.\r\n\r\nClearly, they need to be in a more suitable format (inheriting from Function?) and need some tests, so any advice & code review there is appreciated. The functions are found below and in this [gist](https://gist.github.com/ncullen93/58e71c4303b89e420bd8e0b0aa54bf48)\r\n\r\n\r\n\r\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @mruberry @rgommers @heitorschueroff",high priority|triaged|module: numpy|function request,heitorschueroff,"Update: the scipy.stats.pearsonr part of this issue is now tracked in https://github.com/pytorch/pytorch/issues/50333. This issue is for implementing np.corrcoef.\r\n\r\n**ORIGINAL ISSUE BELOW**\r\n\r\nHey all,\r\n\r\nI implemented bare bones versions of `np.corrcoef` and `scipy.stats.pearsonr`. These are two functions I use all the time, so I often have to convert back and forth to numpy for this. It'd be nice to have these incorporated, although I only see the need for the forward pass.\r\n\r\nClearly, they need to be in a more suitable format (inheriting from Function?) and need some tests, so any advice & code review there is appreciated. The functions are found below and in this [gist](https://gist.github.com/ncullen93/58e71c4303b89e420bd8e0b0aa54bf48)\r\n\r\n```python\r\ndef pearsonr(x, y):\r\n    """"""\r\n    Mimics `scipy.stats.pearsonr`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 1D torch.Tensor\r\n    y : 1D torch.Tensor\r\n\r\n    Returns\r\n    -------\r\n    r_val : float\r\n        pearsonr correlation coefficient between x and y\r\n    \r\n    Scipy docs ref:\r\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\r\n    \r\n    Scipy code ref:\r\n        https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py#L2975-L3033\r\n    Example:\r\n        >>> x = np.random.randn(100)\r\n        >>> y = np.random.randn(100)\r\n        >>> sp_corr = scipy.stats.pearsonr(x, y)[0]\r\n        >>> th_corr = pearsonr(torch.from_numpy(x), torch.from_numpy(y))\r\n        >>> np.allclose(sp_corr, th_corr)\r\n    """"""\r\n    mean_x = torch.mean(x)\r\n    mean_y = torch.mean(y)\r\n    xm = x.sub(mean_x)\r\n    ym = y.sub(mean_y)\r\n    r_num = xm.dot(ym)\r\n    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\r\n    r_val = r_num / r_den\r\n    return r_val\r\n\r\ndef corrcoef(x):\r\n    """"""\r\n    Mimics `np.corrcoef`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 2D torch.Tensor\r\n    \r\n    Returns\r\n    -------\r\n    c : torch.Tensor\r\n        if x.size() = (5, 100), then return val will be of size (5,5)\r\n\r\n    Numpy docs ref:\r\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\r\n    Numpy code ref: \r\n        https://github.com/numpy/numpy/blob/v1.12.0/numpy/lib/function_base.py#L2933-L3013\r\n\r\n    Example:\r\n        >>> x = np.random.randn(5,120)\r\n        # result is a (5,5) matrix of correlations between rows\r\n        >>> np_corr = np.corrcoef(x)\r\n        >>> th_corr = corrcoef(torch.from_numpy(x))\r\n        >>> np.allclose(np_corr, th_corr.numpy())\r\n        # [out]: True\r\n    """"""\r\n    # calculate covariance matrix of rows\r\n    mean_x = torch.mean(x, 1)\r\n    xm = x.sub(mean_x.expand_as(x))\r\n    c = xm.mm(xm.t())\r\n    c = c / (x.size(1) - 1)\r\n\r\n    # normalize covariance matrix\r\n    d = torch.diag(c)\r\n    stddev = torch.pow(d, 0.5)\r\n    c = c.div(stddev.expand_as(c))\r\n    c = c.div(stddev.expand_as(c).t())\r\n\r\n    # clamp between -1 and 1\r\n    # probably not necessary but numpy does it\r\n    c = torch.clamp(c, -1.0, 1.0)\r\n\r\n    return c\r\n```\r\n\r\ncc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @mruberry @rgommers @heitorschueroff","python\r\ndef pearsonr(x, y):\r\n    """"""\r\n    Mimics `scipy.stats.pearsonr`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 1D torch.Tensor\r\n    y : 1D torch.Tensor\r\n\r\n    Returns\r\n    -------\r\n    r_val : float\r\n        pearsonr correlation coefficient between x and y\r\n    \r\n    Scipy docs ref:\r\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\r\n    \r\n    Scipy code ref:\r\n        https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py#L2975-L3033\r\n    Example:\r\n        >>> x = np.random.randn(100)\r\n        >>> y = np.random.randn(100)\r\n        >>> sp_corr = scipy.stats.pearsonr(x, y)[0]\r\n        >>> th_corr = pearsonr(torch.from_numpy(x), torch.from_numpy(y))\r\n        >>> np.allclose(sp_corr, th_corr)\r\n    """"""\r\n    mean_x = torch.mean(x)\r\n    mean_y = torch.mean(y)\r\n    xm = x.sub(mean_x)\r\n    ym = y.sub(mean_y)\r\n    r_num = xm.dot(ym)\r\n    r_den = torch.norm(xm, 2) * torch.norm(ym, 2)\r\n    r_val = r_num / r_den\r\n    return r_val\r\n\r\ndef corrcoef(x):\r\n    """"""\r\n    Mimics `np.corrcoef`\r\n\r\n    Arguments\r\n    ---------\r\n    x : 2D torch.Tensor\r\n    \r\n    Returns\r\n    -------\r\n    c : torch.Tensor\r\n        if x.size() = (5, 100), then return val will be of size (5,5)\r\n\r\n    Numpy docs ref:\r\n        https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\r\n    Numpy code ref: \r\n        https://github.com/numpy/numpy/blob/v1.12.0/numpy/lib/function_base.py#L2933-L3013\r\n\r\n    Example:\r\n        >>> x = np.random.randn(5,120)\r\n        # result is a (5,5) matrix of correlations between rows\r\n        >>> np_corr = np.corrcoef(x)\r\n        >>> th_corr = corrcoef(torch.from_numpy(x))\r\n        >>> np.allclose(np_corr, th_corr.numpy())\r\n        # [out]: True\r\n    """"""\r\n    # calculate covariance matrix of rows\r\n    mean_x = torch.mean(x, 1)\r\n    xm = x.sub(mean_x.expand_as(x))\r\n    c = xm.mm(xm.t())\r\n    c = c / (x.size(1) - 1)\r\n\r\n    # normalize covariance matrix\r\n    d = torch.diag(c)\r\n    stddev = torch.pow(d, 0.5)\r\n    c = c.div(stddev.expand_as(c))\r\n    c = c.div(stddev.expand_as(c).t())\r\n\r\n    # clamp between -1 and 1\r\n    # probably not necessary but numpy does it\r\n    c = torch.clamp(c, -1.0, 1.0)\r\n\r\n    return c\r\n"
1224,"'torch.nn' has no attribute 'TripletMarginLoss'Calling\r\n\r\nresults in:\r\n```\r\nAttributeError: module 'torch.nn' has no attribute 'TripletMarginLoss'\r\n```\r\neven with\r\n\r\nRelated PR that added introduced TripletMarginLoss: #1165 \r\n\r\nWork-around for now is to use `F.triplet_margin_loss` directly, where `F` is `import torch.nn.functional as F`.",high priority,apaszke,"Calling\r\n```python\r\ntriplet_loss = nn.TripletMarginLoss()\r\n```\r\nresults in:\r\n```\r\nAttributeError: module 'torch.nn' has no attribute 'TripletMarginLoss'\r\n```\r\neven with\r\n```python\r\nimport torch.nn as nn\r\n```\r\nRelated PR that added introduced TripletMarginLoss: #1165 \r\n\r\nWork-around for now is to use `F.triplet_margin_loss` directly, where `F` is `import torch.nn.functional as F`.",python\r\ntriplet_loss = nn.TripletMarginLoss()\r\n
1184,"[memory regression] autograd still holding on to some buffers even if requires_grad=False\r\n\r\nThis is a snippet that has ~2.5GB of memory usage in `v0.1.10`, but 12GB+ in `v0.1.11` (and goes OOM)\r\nThanks to @yunjey for reporting this.",high priority,apaszke,"```python\r\nimport torchvision\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nresnet = torchvision.models.resnet152(pretrained=True)\r\nresnet = resnet.cuda()\r\nfor param in resnet.parameters():\r\n    param.requires_grad = False\r\nimages = torch.randn(128, 3, 224, 224)\r\nimages = images.cuda()\r\noutputs = resnet(Variable(images))\r\n```\r\n\r\nThis is a snippet that has ~2.5GB of memory usage in `v0.1.10`, but 12GB+ in `v0.1.11` (and goes OOM)\r\nThanks to @yunjey for reporting this.","python\r\nimport torchvision\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nresnet = torchvision.models.resnet152(pretrained=True)\r\nresnet = resnet.cuda()\r\nfor param in resnet.parameters():\r\n    param.requires_grad = False\r\nimages = torch.randn(128, 3, 224, 224)\r\nimages = images.cuda()\r\noutputs = resnet(Variable(images))\r\n"
1164,"Incorrect output for remainder function on integer type tensorsReproduction code:\r\n\r\n\r\n\r\nGenerated output:\r\n\r\n```\r\n$ python bug.py \r\nTo diagonal:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nRemainder - Floating point:\r\n\r\n 0  2  1\r\n 1  0  2\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nFmod - Floating point:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nRemainder - Long:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.LongTensor of size 3x3]\r\n\r\nFmod - Long:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.LongTensor of size 3x3]\r\n```\r\n\r\nThe interesting section to look at is **Remainder - Long**. You observe that it gets the same output as with `fmod`, while for a floating point tensor, you get different results.\r\n\r\nAccording to the docs, _The remainder has the same sign as the divisor._, which is not the case here.\r\n\r\nThe weekend is coming up, so I can look at fixing + adding test to avoid regression if you want.\r\nI imagine that looking around the neighbourhood of f16a624b35dd28fbd4cdcd3bd08dfc2421c3e2b0 would be the place to start.",high priority|module: dependency bug,bunelr,"Reproduction code:\r\n\r\n```python\r\nimport torch\r\n\r\nnb_elts = 3\r\n\r\ncoords = torch.range(0, nb_elts-1, 1)\r\nx_coord = coords.unsqueeze(1).expand(nb_elts, nb_elts)\r\ny_coord = coords.unsqueeze(0).expand(nb_elts, nb_elts)\r\n\r\nto_diagonal = (x_coord - y_coord)\r\n\r\nprint(""To diagonal:"")\r\nprint(to_diagonal)\r\n\r\nprint(""Remainder - Floating point:"")\r\nprint(to_diagonal.remainder(nb_elts))\r\nprint(""Fmod - Floating point:"")\r\nprint(to_diagonal.fmod(nb_elts))\r\n\r\n\r\nto_diagonal = to_diagonal.long()\r\nprint(""Remainder - Long:"")\r\nprint(to_diagonal.remainder(nb_elts))\r\nprint(""Fmod - Long:"")\r\nprint(to_diagonal.fmod(nb_elts))\r\n```\r\n\r\nGenerated output:\r\n\r\n```\r\n$ python bug.py \r\nTo diagonal:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nRemainder - Floating point:\r\n\r\n 0  2  1\r\n 1  0  2\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nFmod - Floating point:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.FloatTensor of size 3x3]\r\n\r\nRemainder - Long:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.LongTensor of size 3x3]\r\n\r\nFmod - Long:\r\n\r\n 0 -1 -2\r\n 1  0 -1\r\n 2  1  0\r\n[torch.LongTensor of size 3x3]\r\n```\r\n\r\nThe interesting section to look at is **Remainder - Long**. You observe that it gets the same output as with `fmod`, while for a floating point tensor, you get different results.\r\n\r\nAccording to the docs, _The remainder has the same sign as the divisor._, which is not the case here.\r\n\r\nThe weekend is coming up, so I can look at fixing + adding test to avoid regression if you want.\r\nI imagine that looking around the neighbourhood of f16a624b35dd28fbd4cdcd3bd08dfc2421c3e2b0 would be the place to start.","python\r\nimport torch\r\n\r\nnb_elts = 3\r\n\r\ncoords = torch.range(0, nb_elts-1, 1)\r\nx_coord = coords.unsqueeze(1).expand(nb_elts, nb_elts)\r\ny_coord = coords.unsqueeze(0).expand(nb_elts, nb_elts)\r\n\r\nto_diagonal = (x_coord - y_coord)\r\n\r\nprint(""To diagonal:"")\r\nprint(to_diagonal)\r\n\r\nprint(""Remainder - Floating point:"")\r\nprint(to_diagonal.remainder(nb_elts))\r\nprint(""Fmod - Floating point:"")\r\nprint(to_diagonal.fmod(nb_elts))\r\n\r\n\r\nto_diagonal = to_diagonal.long()\r\nprint(""Remainder - Long:"")\r\nprint(to_diagonal.remainder(nb_elts))\r\nprint(""Fmod - Long:"")\r\nprint(to_diagonal.fmod(nb_elts))\r\n"
1155,"gradoutput shapes not being resized for some functions in backwardAs Timothee Lacroix reports:\r\n\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""a.py"", line 21, in <module>\r\n    pred.backward()\r\n  File ""/Users/chinso/anaconda/lib/python2.7/site-packages/torch/autograd/variable.py"", line 146, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File ""/Users/chinso/anaconda/lib/python2.7/site-packages/torch/autograd/_functions/blas.py"", line 40, in backward\r\n    grad_matrix1 = torch.mm(grad_output, matrix2.t())\r\nRuntimeError: matrices expected, got 1D, 2D tensors at /Users/soumith/miniconda2/conda-bld/pytorch_1490902430884/work/torch/lib/TH/generic/THTensorMath.c:1224\r\n```",high priority,apaszke,"As Timothee Lacroix reports:\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\ndtype=torch.FloatTensor\r\nclass Test(nn.Module):\r\n    def __init__(self, N, R, A, P, k):\r\n        super(Test, self).__init__()\r\n\r\n        self.k = k             # maximum subgraph size\r\n        self.at = nn.Parameter(torch.randn(R, k, k).type(dtype), requires_grad=True)\r\n        self.perm = nn.Parameter(torch.randn(N, k).type(dtype), requires_grad=True)\r\n\r\n    def forward(self, (s, o, r)):\r\n        res = self.perm[s, :].dot(self.at[r, :, :].mm(self.perm[o, :].view(self.k, 1)))\r\n        return res\r\n\r\n\r\nmodel = Test(5, 3, 1, 1, 3)\r\n\r\npred = model.forward((0,0,0))\r\npred.backward()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""a.py"", line 21, in <module>\r\n    pred.backward()\r\n  File ""/Users/chinso/anaconda/lib/python2.7/site-packages/torch/autograd/variable.py"", line 146, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n  File ""/Users/chinso/anaconda/lib/python2.7/site-packages/torch/autograd/_functions/blas.py"", line 40, in backward\r\n    grad_matrix1 = torch.mm(grad_output, matrix2.t())\r\nRuntimeError: matrices expected, got 1D, 2D tensors at /Users/soumith/miniconda2/conda-bld/pytorch_1490902430884/work/torch/lib/TH/generic/THTensorMath.c:1224\r\n```","python\r\nimport torch\r\nfrom torch import nn\r\n\r\ndtype=torch.FloatTensor\r\nclass Test(nn.Module):\r\n    def __init__(self, N, R, A, P, k):\r\n        super(Test, self).__init__()\r\n\r\n        self.k = k             # maximum subgraph size\r\n        self.at = nn.Parameter(torch.randn(R, k, k).type(dtype), requires_grad=True)\r\n        self.perm = nn.Parameter(torch.randn(N, k).type(dtype), requires_grad=True)\r\n\r\n    def forward(self, (s, o, r)):\r\n        res = self.perm[s, :].dot(self.at[r, :, :].mm(self.perm[o, :].view(self.k, 1)))\r\n        return res\r\n\r\n\r\nmodel = Test(5, 3, 1, 1, 3)\r\n\r\npred = model.forward((0,0,0))\r\npred.backward()\r\n"
1058,"CosineEmbeddingLoss tensor sizes not matching when grad_output != 1The following code produces an error for me (torch version string 0.1.10_2):\r\n\r\n\r\nError is:\r\nRuntimeError: inconsistent tensor size at /data/users/soumith/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:842\r\n\r\nI don't think this should produce an error. Can you tell me if this is already known/being fixed or how we should work around it ? I'm assuming that in nn/_functions/loss - `CosineEmbeddingLoss(Function)` - `backward()`, you should be doing something like `gw1.mul_(grad_output[0])` at the end instead of `gw1.mul_(grad_output)`",high priority,apaszke,"The following code produces an error for me (torch version string 0.1.10_2):\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ncos_loss = nn.CosineEmbeddingLoss()\r\nt1 = torch.randn(10, 5)\r\nt2 = torch.randn(10, 5)\r\nlab = t1.sum(1).ge(0).long() * 2 - 1\r\nloss1 = cos_loss(Variable(t1, requires_grad=True), Variable(t2), Variable(lab))\r\nloss2 = loss1 / 10\r\nloss2.backward()\r\n```\r\nError is:\r\nRuntimeError: inconsistent tensor size at /data/users/soumith/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:842\r\n\r\nI don't think this should produce an error. Can you tell me if this is already known/being fixed or how we should work around it ? I'm assuming that in nn/_functions/loss - `CosineEmbeddingLoss(Function)` - `backward()`, you should be doing something like `gw1.mul_(grad_output[0])` at the end instead of `gw1.mul_(grad_output)`","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ncos_loss = nn.CosineEmbeddingLoss()\r\nt1 = torch.randn(10, 5)\r\nt2 = torch.randn(10, 5)\r\nlab = t1.sum(1).ge(0).long() * 2 - 1\r\nloss1 = cos_loss(Variable(t1, requires_grad=True), Variable(t2), Variable(lab))\r\nloss2 = loss1 / 10\r\nloss2.backward()\r\n"
1031,"serializing large tensors results in system error or zero\r\n\r\nRunning this on OSX:\r\n```\r\n0\r\nTraceback (most recent call last):\r\n  File ""a.py"", line 5, in <module>\r\n    torch.save(a, 'a.pth')\r\n  File ""/Users/soumith/code/pytorch/torch/serialization.py"", line 120, in save\r\n    return _save(obj, f, pickle_module, pickle_protocol)\r\n  File ""/Users/soumith/code/pytorch/torch/serialization.py"", line 192, in _save\r\n    serialized_storages[key]._write_file(f)\r\nRuntimeError: Unknown error: -1\r\n```\r\n\r\nMore details of failure in Ubuntu 14.04.1 in https://discuss.pytorch.org/t/value-changed-after-loading-a-saved-tensor/1133",high priority,soumith,"```python\r\nimport torch\r\n\r\na = torch.ones(800*3*480*640)\r\nprint(a.eq(0).sum())\r\ntorch.save(a, 'a.pth')\r\nb = torch.load('a.pth')\r\n\r\nprint(b.eq(0).sum())\r\n```\r\n\r\nRunning this on OSX:\r\n```\r\n0\r\nTraceback (most recent call last):\r\n  File ""a.py"", line 5, in <module>\r\n    torch.save(a, 'a.pth')\r\n  File ""/Users/soumith/code/pytorch/torch/serialization.py"", line 120, in save\r\n    return _save(obj, f, pickle_module, pickle_protocol)\r\n  File ""/Users/soumith/code/pytorch/torch/serialization.py"", line 192, in _save\r\n    serialized_storages[key]._write_file(f)\r\nRuntimeError: Unknown error: -1\r\n```\r\n\r\nMore details of failure in Ubuntu 14.04.1 in https://discuss.pytorch.org/t/value-changed-after-loading-a-saved-tensor/1133","python\r\nimport torch\r\n\r\na = torch.ones(800*3*480*640)\r\nprint(a.eq(0).sum())\r\ntorch.save(a, 'a.pth')\r\nb = torch.load('a.pth')\r\n\r\nprint(b.eq(0).sum())\r\n"
1015,"off-by-one in torch.cat on Variables with a reversed() iteratorThe following two calls to `cat` on Variables are not the same:\r\n\r\nThe former misses an element of `reversed(l)`. This doesn't apply to `torch.cat` on tensors, which doesn't accept `list_reversediterator` as a valid input.",high priority,apaszke,"The following two calls to `cat` on Variables are not the same:\r\n```python\r\nl = [Variable(torch.ones(1,3)*i) for i in range(3)]\r\ntorch.cat(reversed(l), 0)        # row of 1's then row of 0's\r\ntorch.cat(list(reversed(l)), 0)  # row of 2's, 1's, then 0's\r\n```\r\nThe former misses an element of `reversed(l)`. This doesn't apply to `torch.cat` on tensors, which doesn't accept `list_reversediterator` as a valid input.","python\r\nl = [Variable(torch.ones(1,3)*i) for i in range(3)]\r\ntorch.cat(reversed(l), 0)        # row of 1's then row of 0's\r\ntorch.cat(list(reversed(l)), 0)  # row of 2's, 1's, then 0's\r\n"
1014,inconsistent behavior of torch.cat() for 1-dimensional variables and tensorsI think I found an unexpected behavior of `torch.cat()`. It behaves differently for 1 dimensional variables and tensors:\r\n\r\n\r\nbut everything works for 2 dimensional tensors:\r\n\r\n\r\nThanks for all your work!,high priority,apaszke,"I think I found an unexpected behavior of `torch.cat()`. It behaves differently for 1 dimensional variables and tensors:\r\n```Python\r\nx = torch.rand(1)\r\nX = Variable(x)\r\n\r\nx.size() == X.size()            # true\r\n\r\ntorch.cat((x,x),0)              # ok\r\ntorch.cat((x,x),1)              # ok\r\n\r\ntorch.cat((X,X),0)              # ok\r\ntorch.cat((X,X),1)              # -> fails\r\n```\r\n\r\nbut everything works for 2 dimensional tensors:\r\n```Python\r\ny = torch.rand(2,1)\r\nY = Variable(y)\r\n\r\ny.size() == Y.size()            # true\r\n\r\ntorch.cat((y,y),0)              # ok\r\ntorch.cat((y,y),1)              # ok\r\n\r\ntorch.cat((Y,Y),0)              # ok\r\ntorch.cat((Y,Y),1)              # ok\r\n```\r\n\r\nThanks for all your work!","Python\r\nx = torch.rand(1)\r\nX = Variable(x)\r\n\r\nx.size() == X.size()            # true\r\n\r\ntorch.cat((x,x),0)              # ok\r\ntorch.cat((x,x),1)              # ok\r\n\r\ntorch.cat((X,X),0)              # ok\r\ntorch.cat((X,X),1)              # -> fails\r\n"
972,MarginRankingLoss should accept 2D inputsIt expects 1D inputs and target right now.\r\n,high priority,apaszke,"It expects 1D inputs and target right now.\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable as V\r\n\r\nx1 = torch.ones(64, 128)\r\nx2 = torch.ones(64, 128)\r\ntarget = torch.ones(64)\r\n\r\nloss = nn.MarginRankingLoss(margin=1.0).forward(V(x1), V(x2), V(target))\r\nprint loss\r\n```","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable as V\r\n\r\nx1 = torch.ones(64, 128)\r\nx2 = torch.ones(64, 128)\r\ntarget = torch.ones(64)\r\n\r\nloss = nn.MarginRankingLoss(margin=1.0).forward(V(x1), V(x2), V(target))\r\nprint loss\r\n"
943,"nn.Embedding with max_norm acting strange?Hi,\r\n\r\nI was trying to use the max-norm feature of  `nn.Embedding` and noticed that it seems to change the way indexing works.\r\n\r\nSo, this is correct:\r\n\r\n\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n\r\n(1 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nand this happens when using `max_norm=1`:\r\n\r\n\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.7178 -0.1059  0.4266 -0.2857  0.4581\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n\r\n(1 ,.,.) = \r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nAs seen, the `[1, 0]`th index seems to load embedding corresponding to 2 instead of 1 as requested.\r\n\r\n Am I misunderstanding something?\r\n",high priority|module: dependency bug,apaszke,"Hi,\r\n\r\nI was trying to use the max-norm feature of  `nn.Embedding` and noticed that it seems to change the way indexing works.\r\n\r\nSo, this is correct:\r\n\r\n```python\r\nA = nn.Embedding(20, 5)\r\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\r\n```\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n\r\n(1 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nand this happens when using `max_norm=1`:\r\n\r\n```python\r\nA = nn.Embedding(20, 5, max_norm=1)\r\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\r\n```\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.7178 -0.1059  0.4266 -0.2857  0.4581\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n\r\n(1 ,.,.) = \r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nAs seen, the `[1, 0]`th index seems to load embedding corresponding to 2 instead of 1 as requested.\r\n\r\n Am I misunderstanding something?\r\n","python\r\nA = nn.Embedding(20, 5)\r\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\r\n"
839,In-place ops on leaf Variables create reference cyclesThis script runs out of memory:\r\n\r\n\r\n,high priority,apaszke,"This script runs out of memory:\r\n\r\n```python\r\nfor _ in range(100000):\r\n    init = Variable(torch.cuda.FloatTensor(2, 32, 512).zero_())\r\n    init[0] = Variable(torch.cuda.FloatTensor(32, 512).zero_())\r\n```\r\n","python\r\nfor _ in range(100000):\r\n    init = Variable(torch.cuda.FloatTensor(2, 32, 512).zero_())\r\n    init[0] = Variable(torch.cuda.FloatTensor(32, 512).zero_())\r\n"
828,"backward() in Autograd Index Function is broken when indexing with LongTensorIn `torch/autograd/_functions/tensor.py:20`:\r\n\r\n\r\nI think the index-copy statement doesn't work as intended.\r\n```\r\nIn [15]: a = torch.zeros((3, 5))\r\n\r\nIn [16]: a\r\nOut[16]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n\r\nIn [17]: b = torch.ones((2, 5))\r\n\r\nIn [18]: a.index(torch.LongTensor([0, 2])).copy_(b)\r\nOut[18]: \r\n\r\n 1  1  1  1  1\r\n 1  1  1  1  1\r\n[torch.FloatTensor of size 2x5]\r\n\r\nIn [19]: a\r\nOut[19]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n```\r\n\r\nI guess the statement should be something like\r\n",high priority,apaszke,"In `torch/autograd/_functions/tensor.py:20`:\r\n```python\r\n    def backward(self, grad_output):\r\n        # TODO: this won't have to be zeroed\r\n        grad_input = grad_output.new(self.input_size).zero_()\r\n        grad_input.index(self.index).copy_(grad_output)\r\n        return grad_input\r\n```\r\n\r\nI think the index-copy statement doesn't work as intended.\r\n```\r\nIn [15]: a = torch.zeros((3, 5))\r\n\r\nIn [16]: a\r\nOut[16]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n\r\nIn [17]: b = torch.ones((2, 5))\r\n\r\nIn [18]: a.index(torch.LongTensor([0, 2])).copy_(b)\r\nOut[18]: \r\n\r\n 1  1  1  1  1\r\n 1  1  1  1  1\r\n[torch.FloatTensor of size 2x5]\r\n\r\nIn [19]: a\r\nOut[19]: \r\n\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n 0  0  0  0  0\r\n[torch.FloatTensor of size 3x5]\r\n```\r\n\r\nI guess the statement should be something like\r\n```python\r\n        grad_input.index_copy_(0, self.index, grad_output)\r\n```","python\r\n    def backward(self, grad_output):\r\n        # TODO: this won't have to be zeroed\r\n        grad_input = grad_output.new(self.input_size).zero_()\r\n        grad_input.index(self.index).copy_(grad_output)\r\n        return grad_input\r\n"
776,"Failed when RNN run with cudnn with `requires_grad` set to `False`\r\nWhen run on cpu or `requires_grad` set to `True`, it does succeed.\r\nOtherwise, it fails with error message:\r\n```sh\r\nTraceback (most recent call last):\r\n  File ""test_lstm.py"", line 17, in <module>\r\n    output.backward(torch.ones(output.size()).cuda())\r\n  File ""/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/variable.py"", line 158, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\nRuntimeError: CudnnRNN returned an invalid number of gradient tensors (expected 11, but got 4)\r\n```\r\nIt seems like the check done by the framework has a bug.",high priority,apaszke,"```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nlstm = nn.LSTM(2, 3, 1, bidirectional=True)\r\nlstm.cuda()\r\nfor param in lstm.parameters():\r\n    param.requires_grad = False  # NOTE: Failed when 'False' but succeed when 'True'\r\n\r\ninput = torch.ones(4, 5, 2)  # [T, b, i]\r\ninput = input.cuda()\r\ninput = Variable(input, requires_grad=True)\r\noutput, _ = lstm(input)\r\noutput.backward(torch.ones(output.size()).cuda())\r\n```\r\nWhen run on cpu or `requires_grad` set to `True`, it does succeed.\r\nOtherwise, it fails with error message:\r\n```sh\r\nTraceback (most recent call last):\r\n  File ""test_lstm.py"", line 17, in <module>\r\n    output.backward(torch.ones(output.size()).cuda())\r\n  File ""/home/jrmei/.local/lib/python2.7/site-packages/torch/autograd/variable.py"", line 158, in backward\r\n    self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\nRuntimeError: CudnnRNN returned an invalid number of gradient tensors (expected 11, but got 4)\r\n```\r\nIt seems like the check done by the framework has a bug.","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nlstm = nn.LSTM(2, 3, 1, bidirectional=True)\r\nlstm.cuda()\r\nfor param in lstm.parameters():\r\n    param.requires_grad = False  # NOTE: Failed when 'False' but succeed when 'True'\r\n\r\ninput = torch.ones(4, 5, 2)  # [T, b, i]\r\ninput = input.cuda()\r\ninput = Variable(input, requires_grad=True)\r\noutput, _ = lstm(input)\r\noutput.backward(torch.ones(output.size()).cuda())\r\n"
719,Module.zero_grad fails for Parameters with requires_grad=FalseReported [on the forums](https://discuss.pytorch.org/t/module-zero-grad-with-requires-grad-false-for-some-parameter/431).\r\n\r\n,high priority,apaszke,"Reported [on the forums](https://discuss.pytorch.org/t/module-zero-grad-with-requires-grad-false-for-some-parameter/431).\r\n\r\n```python\r\nimport torch\r\n\r\nfrom torch import Tensor\r\nfrom torch.nn.parameter import Parameter\r\nfrom torch.nn import Module\r\n\r\nclass Blah(Module):\r\n    def __init__(self, dim):\r\n        super(Blah, self).__init__()\r\n        self.s = Parameter(torch.rand(1, dim), requires_grad = False)\r\n        self.t = Parameter(torch.rand(1, dim))\r\n\r\nblah = Blah(10)\r\n\r\nblah.zero_grad()\r\n```","python\r\nimport torch\r\n\r\nfrom torch import Tensor\r\nfrom torch.nn.parameter import Parameter\r\nfrom torch.nn import Module\r\n\r\nclass Blah(Module):\r\n    def __init__(self, dim):\r\n        super(Blah, self).__init__()\r\n        self.s = Parameter(torch.rand(1, dim), requires_grad = False)\r\n        self.t = Parameter(torch.rand(1, dim))\r\n\r\nblah = Blah(10)\r\n\r\nblah.zero_grad()\r\n"
689,"torch.cat puts result on current GPU rather than GPU of inputs\r\nThis is a problem for `nn.DataParallel` and similar multi-gpu things. Paszke says it might be a bug in [the wrapper](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/Tensor.cwrap#L576), because this is what `THCPAutoGPU` is supposed to deal with.",high priority,apaszke,"```python\r\n>>> import torch\r\n>>> a = torch.rand(5).cuda(1)\r\n>>> b = torch.rand(5).cuda(1)\r\n>>> c = torch.cat([a,b], 0)\r\n>>> c.get_device()\r\n0\r\n>>> torch.cuda.set_device(1)\r\n>>> c = torch.cat([a,b], 0)\r\n>>> c.get_device()\r\n1\r\n```\r\nThis is a problem for `nn.DataParallel` and similar multi-gpu things. Paszke says it might be a bug in [the wrapper](https://github.com/pytorch/pytorch/blob/master/torch/csrc/generic/methods/Tensor.cwrap#L576), because this is what `THCPAutoGPU` is supposed to deal with.","python\r\n>>> import torch\r\n>>> a = torch.rand(5).cuda(1)\r\n>>> b = torch.rand(5).cuda(1)\r\n>>> c = torch.cat([a,b], 0)\r\n>>> c.get_device()\r\n0\r\n>>> torch.cuda.set_device(1)\r\n>>> c = torch.cat([a,b], 0)\r\n>>> c.get_device()\r\n1\r\n"
674,"transpose() followed by conv breaks backward passThe following snippet demonstrates the problem:\r\n\r\n\r\n\r\nRunning this code and you will get the following error:\r\n\r\n```\r\n  File "".../lib/python3.5/site-packages/torch/tensor.py"", line 214, in view\r\n    raise ValueError(""input should be contiguous"")\r\nValueError: input should be contiguous\r\n```\r\n\r\nThe fix is to change [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/conv.py#L179) to `output += [t.contiguous().view(*size)]`",high priority,apaszke,"The following snippet demonstrates the problem:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\nx = Variable(torch.from_numpy(np.random.randn(4, 6, 5)).float(), requires_grad=True)\r\nx = torch.transpose(x, 1, 2)\r\nconv = torch.nn.Conv1d(in_channels=5, out_channels=10, kernel_size=2)\r\nresult = torch.sum(conv(x))\r\nresult.backward()\r\n```\r\n\r\nRunning this code and you will get the following error:\r\n\r\n```\r\n  File "".../lib/python3.5/site-packages/torch/tensor.py"", line 214, in view\r\n    raise ValueError(""input should be contiguous"")\r\nValueError: input should be contiguous\r\n```\r\n\r\nThe fix is to change [here](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/conv.py#L179) to `output += [t.contiguous().view(*size)]`","python\r\nimport torch\r\nimport torch.nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\nx = Variable(torch.from_numpy(np.random.randn(4, 6, 5)).float(), requires_grad=True)\r\nx = torch.transpose(x, 1, 2)\r\nconv = torch.nn.Conv1d(in_channels=5, out_channels=10, kernel_size=2)\r\nresult = torch.sum(conv(x))\r\nresult.backward()\r\n"
627,torch.cat bug/unexpected behaviorI would expect `x` and `y` are the same in the following:\r\n\r\n``\r\n\r\nBut they're not:\r\n````\r\nx\r\n  0   6  12  18\r\n  1   7  13  19\r\n  2   8  14  20\r\n  3   9  15  21\r\n  4  10  16  22\r\n  5  11  17  23\r\n[torch.LongTensor of size 6x4]\r\n\r\ny\r\n  0   1   2   3\r\n  4   5   6   7\r\n  8   9  10  11\r\n 12  13  14  15\r\n 16  17  18  19\r\n 20  21  22  23\r\n[torch.LongTensor of size 6x4]\r\n````,high priority|module: dependency bug,apaszke,"I would expect `x` and `y` are the same in the following:\r\n\r\n````python\r\nimport numpy as np\r\nimport torch\r\n\r\na0 = np.reshape(np.arange(24), (4,6))\r\na1 = np.transpose(a0)\r\n\r\nt1 = torch.from_numpy(a1)\r\nx = t1.unsqueeze(0)[0]\r\ny = torch.cat([t1.unsqueeze(0)])[0]\r\n\r\nprint(x)\r\nprint(y)\r\n````\r\n\r\nBut they're not:\r\n````\r\nx\r\n  0   6  12  18\r\n  1   7  13  19\r\n  2   8  14  20\r\n  3   9  15  21\r\n  4  10  16  22\r\n  5  11  17  23\r\n[torch.LongTensor of size 6x4]\r\n\r\ny\r\n  0   1   2   3\r\n  4   5   6   7\r\n  8   9  10  11\r\n 12  13  14  15\r\n 16  17  18  19\r\n 20  21  22  23\r\n[torch.LongTensor of size 6x4]\r\n````","python\r\nimport numpy as np\r\nimport torch\r\n\r\na0 = np.reshape(np.arange(24), (4,6))\r\na1 = np.transpose(a0)\r\n\r\nt1 = torch.from_numpy(a1)\r\nx = t1.unsqueeze(0)[0]\r\ny = torch.cat([t1.unsqueeze(0)])[0]\r\n\r\nprint(x)\r\nprint(y)\r\n"
582,"nn.ELU(inplace=True) errors in backward\r\n\r\n```\r\n/Users/soumith/code/pytorch/torch/autograd/variable.pyc in backward(self, gradient, retain_variables)\r\n    154                 raise RuntimeError('backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable')\r\n    155             gradient = self.data.new().resize_as_(self.data).fill_(1)\r\n--> 156         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n    157\r\n    158     def register_hook(self, hook):\r\n\r\n/Users/soumith/code/pytorch/torch/nn/_functions/thnn/auto.pyc in backward(self, grad_output)\r\n    150         t = self.saved_tensors\r\n    151         if save_output:\r\n--> 152             input, output, params = t[0], t[1], t[2:]\r\n    153         else:\r\n    154             input, params = t[0], t[1:]\r\n\r\nIndexError: tuple index out of range\r\n```",high priority,apaszke,"```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ninput = Variable(torch.randn(10), requires_grad=True)\r\n\r\n# relu passes\r\nrelu = nn.ReLU(inplace=True)\r\nh1 = input * 2\r\nout = relu(h1)\r\nout.backward(torch.randn(10))\r\n\r\n# elu fails\r\nelu = nn.ELU(inplace=True)\r\nh1 = input * 2\r\nout = elu(h1)\r\nout.backward(torch.randn(10))\r\n```\r\n\r\n```\r\n/Users/soumith/code/pytorch/torch/autograd/variable.pyc in backward(self, gradient, retain_variables)\r\n    154                 raise RuntimeError('backward should be called only on a scalar (i.e. 1-element tensor) or with gradient w.r.t. the variable')\r\n    155             gradient = self.data.new().resize_as_(self.data).fill_(1)\r\n--> 156         self._execution_engine.run_backward((self,), (gradient,), retain_variables)\r\n    157\r\n    158     def register_hook(self, hook):\r\n\r\n/Users/soumith/code/pytorch/torch/nn/_functions/thnn/auto.pyc in backward(self, grad_output)\r\n    150         t = self.saved_tensors\r\n    151         if save_output:\r\n--> 152             input, output, params = t[0], t[1], t[2:]\r\n    153         else:\r\n    154             input, params = t[0], t[1:]\r\n\r\nIndexError: tuple index out of range\r\n```","python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\ninput = Variable(torch.randn(10), requires_grad=True)\r\n\r\n# relu passes\r\nrelu = nn.ReLU(inplace=True)\r\nh1 = input * 2\r\nout = relu(h1)\r\nout.backward(torch.randn(10))\r\n\r\n# elu fails\r\nelu = nn.ELU(inplace=True)\r\nh1 = input * 2\r\nout = elu(h1)\r\nout.backward(torch.randn(10))\r\n"
573,"cuDNN detection failure at build timeWe had a request from our users to install pytorch on our GPU cluster so I gave it a try and just compiled pytorch with CUDA 8.0.44 and cuDNN 5.1 within anaconda and during ./run_test.sh I can see the following:\r\n\r\n```\r\n~/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.py:60: UserWarning: cuDNN library has been detected, but your pytorch installation was compiled without support for it. You might want to rebuild pytorch, making sure the library is visible to the build system.\r\n  warnings.warn(""cuDNN library has been detected, but your pytorch ""\r\n```\r\n\r\nI didn't change the environment between the build process and the tests, meaning the build scripts missed cuDNN detection. It is available in `$LD_LIBRARY_PATH` though and properly detected by other software installation on our cluster (we use lmod to manage the software environment).\r\n\r\nSo it looks like pytorch's cuDNN detection script `tools/setup_helpers/cudnn.py` doesn't care about the standard environment variables like `$LD_LIBRARY_PATH` or `$CPATH`, but simply uses the variables `$CUDNN_LIB_DIR` and `$CUDNN_INCLUDE_DIR` (or `$CUDA_HOME` but we don't install cuDNN in `CUDA_HOME`):\r\n\r\ntools/setup_helpers/cudnn.py:\r\n\r\n\r\n...\r\n\r\nAs a workaround, I will define these non-standard `$CUDNN_*` before building pytorch, but I wanted to let you know of the build issue.",todo,apaszke,"We had a request from our users to install pytorch on our GPU cluster so I gave it a try and just compiled pytorch with CUDA 8.0.44 and cuDNN 5.1 within anaconda and during ./run_test.sh I can see the following:\r\n\r\n```\r\n~/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/__init__.py:60: UserWarning: cuDNN library has been detected, but your pytorch installation was compiled without support for it. You might want to rebuild pytorch, making sure the library is visible to the build system.\r\n  warnings.warn(""cuDNN library has been detected, but your pytorch ""\r\n```\r\n\r\nI didn't change the environment between the build process and the tests, meaning the build scripts missed cuDNN detection. It is available in `$LD_LIBRARY_PATH` though and properly detected by other software installation on our cluster (we use lmod to manage the software environment).\r\n\r\nSo it looks like pytorch's cuDNN detection script `tools/setup_helpers/cudnn.py` doesn't care about the standard environment variables like `$LD_LIBRARY_PATH` or `$CPATH`, but simply uses the variables `$CUDNN_LIB_DIR` and `$CUDNN_INCLUDE_DIR` (or `$CUDA_HOME` but we don't install cuDNN in `CUDA_HOME`):\r\n\r\ntools/setup_helpers/cudnn.py:\r\n```python\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR'),\r\n        os.path.join(CUDA_HOME, 'lib'),\r\n        os.path.join(CUDA_HOME, 'lib64'),\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n    ]))\r\n```\r\n\r\n...\r\n\r\nAs a workaround, I will define these non-standard `$CUDNN_*` before building pytorch, but I wanted to let you know of the build issue.","python\r\n    lib_paths = list(filter(bool, [\r\n        os.getenv('CUDNN_LIB_DIR'),\r\n        os.path.join(CUDA_HOME, 'lib'),\r\n        os.path.join(CUDA_HOME, 'lib64'),\r\n        '/usr/lib/x86_64-linux-gnu/',\r\n    ]))\r\n"
544,"CUDA error for some Conv2d dimensionsI've run into some specific sizes of convolutions that work on CPU but crash on GPU:\r\n\r\n\r\n\r\nAny of the bad sizes results in:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""foo.py"", line 19, in <module>\r\n    y = conv(x_var)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 210, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 235, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/functional.py"", line 37, in conv2d\r\n    return f(input, weight, bias) if bias is not None else f(input, weight)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/_functions/conv.py"", line 33, in forward\r\n    output = self._update_output(input, weight, bias)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/_functions/conv.py"", line 84, in _update_output\r\n    self.groups, cudnn.benchmark)\r\nRuntimeError: CUDA error\r\n```\r\n\r\nI'm using PyTorch installed from source at https://github.com/pytorch/pytorch/commit/f8e89fbe1123f6788992b70361f13ad498665327 on Ubuntu 16.04 with CUDA 8.0.44, CUDA driver 367.57, and cuDNN 5.1.5.\r\n\r\nI get the same error on a Pascal Titan X and a Maxwell Titan X, and on both Python 2.7.12 and Python 3.5.2.",high priority,colesbury,"I've run into some specific sizes of convolutions that work on CPU but crash on GPU:\r\n\r\n```python\r\nimport torch\r\n\r\ndtype = torch.cuda.FloatTensor\r\n\r\nbad_sizes = [\r\n  (1, 256, 109, 175),\r\n  (1, 256, 80, 128),\r\n  (1, 256, 120, 192),\r\n]\r\n\r\nx = torch.randn(*bad_sizes[2]).type(dtype)\r\nx_var = torch.autograd.Variable(x)\r\nprint('x.size() = ', x.size())\r\n\r\nconv = torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\r\nconv.type(dtype)\r\n\r\ny = conv(x_var)\r\nprint(y.size())\r\n```\r\n\r\nAny of the bad sizes results in:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""foo.py"", line 19, in <module>\r\n    y = conv(x_var)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 210, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/modules/conv.py"", line 235, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/functional.py"", line 37, in conv2d\r\n    return f(input, weight, bias) if bias is not None else f(input, weight)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/_functions/conv.py"", line 33, in forward\r\n    output = self._update_output(input, weight, bias)\r\n  File ""/home/justin/code/env3/lib/python3.5/site-packages/torch/nn/_functions/conv.py"", line 84, in _update_output\r\n    self.groups, cudnn.benchmark)\r\nRuntimeError: CUDA error\r\n```\r\n\r\nI'm using PyTorch installed from source at https://github.com/pytorch/pytorch/commit/f8e89fbe1123f6788992b70361f13ad498665327 on Ubuntu 16.04 with CUDA 8.0.44, CUDA driver 367.57, and cuDNN 5.1.5.\r\n\r\nI get the same error on a Pascal Titan X and a Maxwell Titan X, and on both Python 2.7.12 and Python 3.5.2.","python\r\nimport torch\r\n\r\ndtype = torch.cuda.FloatTensor\r\n\r\nbad_sizes = [\r\n  (1, 256, 109, 175),\r\n  (1, 256, 80, 128),\r\n  (1, 256, 120, 192),\r\n]\r\n\r\nx = torch.randn(*bad_sizes[2]).type(dtype)\r\nx_var = torch.autograd.Variable(x)\r\nprint('x.size() = ', x.size())\r\n\r\nconv = torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\r\nconv.type(dtype)\r\n\r\ny = conv(x_var)\r\nprint(y.size())\r\n"
543,"Error when backprop Conv1dHello, Thank you for building this great Library.\r\nI was going through the [tutorial](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)'s nn example (CNN), and when I changed `Conv2d()` to `Conv1d()`, an error was emitted when the `loss.backward()` was called. The code below runs successfully up to right before `loss.backward()` (last line).\r\n\r\n\r\n\r\nThe error is `AttributeError: 'NoneType' object has no attribute 'dim'`, and is from `_view3d() `function in `nn/_functions/conv.py`. If I set `requires_grad=True` for the input `Variable`, this error disappears, but this doesn't seem like a right solution.\r\n\r\nThank you.",high priority,apaszke,"Hello, Thank you for building this great Library.\r\nI was going through the [tutorial](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)'s nn example (CNN), and when I changed `Conv2d()` to `Conv1d()`, an error was emitted when the `loss.backward()` was called. The code below runs successfully up to right before `loss.backward()` (last line).\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()                                                                                                                     \r\n         self.conv1 = nn.Conv1d(1, 6, 5)                                                                                                                                                                                                                                                                      \r\n         self.conv2 = nn.Conv1d(6, 16, 5)\r\n         self.fc1   = nn.Linear(384, 120) # an affine operation: y = Wx + b\r\n         self.fc2   = nn.Linear(120, 84)\r\n         self.fc3   = nn.Linear(84, 10)\r\n\r\n     def forward(self, x):\r\n         x = F.relu(self.conv1(x)) # Max pooling over a (2, 2) window\r\n\r\n         x = F.relu(self.conv2(x)) # If the size is a square you can only specify a single number\r\n\r\n         x = x.view(-1, self.num_flat_features(x))\r\n         x = F.relu(self.fc1(x))\r\n         x = F.relu(self.fc2(x))\r\n         x = self.fc3(x)\r\n         return x\r\n\r\n     def num_flat_features(self, x):\r\n         size = x.size()[1:] # all dimensions except the batch dimension\r\n         num_features = 1\r\n         for s in size:\r\n             num_features *= s\r\n         return num_features\r\n\r\nnet = Net()\r\n\r\ninput = Variable(torch.randn(1, 1, 32))\r\nout = net(input)\r\n\r\nnet.zero_grad()\r\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\r\ncriterion = nn.MSELoss()\r\nloss = criterion(out, target)\r\n\r\nloss.backward()\r\n```\r\n\r\nThe error is `AttributeError: 'NoneType' object has no attribute 'dim'`, and is from `_view3d() `function in `nn/_functions/conv.py`. If I set `requires_grad=True` for the input `Variable`, this error disappears, but this doesn't seem like a right solution.\r\n\r\nThank you.","python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()                                                                                                                     \r\n         self.conv1 = nn.Conv1d(1, 6, 5)                                                                                                                                                                                                                                                                      \r\n         self.conv2 = nn.Conv1d(6, 16, 5)\r\n         self.fc1   = nn.Linear(384, 120) # an affine operation: y = Wx + b\r\n         self.fc2   = nn.Linear(120, 84)\r\n         self.fc3   = nn.Linear(84, 10)\r\n\r\n     def forward(self, x):\r\n         x = F.relu(self.conv1(x)) # Max pooling over a (2, 2) window\r\n\r\n         x = F.relu(self.conv2(x)) # If the size is a square you can only specify a single number\r\n\r\n         x = x.view(-1, self.num_flat_features(x))\r\n         x = F.relu(self.fc1(x))\r\n         x = F.relu(self.fc2(x))\r\n         x = self.fc3(x)\r\n         return x\r\n\r\n     def num_flat_features(self, x):\r\n         size = x.size()[1:] # all dimensions except the batch dimension\r\n         num_features = 1\r\n         for s in size:\r\n             num_features *= s\r\n         return num_features\r\n\r\nnet = Net()\r\n\r\ninput = Variable(torch.randn(1, 1, 32))\r\nout = net(input)\r\n\r\nnet.zero_grad()\r\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\r\ncriterion = nn.MSELoss()\r\nloss = criterion(out, target)\r\n\r\nloss.backward()\r\n"
538,"wrong comparison of Variable scalar with python objectsThis code returns False, and I don't think it should:\r\n\r\n",high priority,apaszke,"This code returns False, and I don't think it should:\r\n\r\n```python\r\na = Variable(torch.FloatTensor(3, 4).fill_(3))\r\nprint(a[1, 0] == 3)\r\n```","python\r\na = Variable(torch.FloatTensor(3, 4).fill_(3))\r\nprint(a[1, 0] == 3)\r\n"
438,Crash in backward on variable which does not require_grad\r\n\r\nThe case in which the creator is NULL and `requires_grad` is `false` is not handled:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L146\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L153,high priority,colesbury,"```python\r\nimport torch\r\nx = torch.autograd.Variable(torch.randn(5,5))\r\nx.backward(torch.randn(5, 5))\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nThe case in which the creator is NULL and `requires_grad` is `false` is not handled:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L146\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L153","python\r\nimport torch\r\nx = torch.autograd.Variable(torch.randn(5,5))\r\nx.backward(torch.randn(5, 5))\r\nSegmentation fault (core dumped)\r\n"
415,"add copy.deepcopy support to nn.Container modelsRight now, one has to do:\r\n\r\n\r\n\r\nReported by @bharath272 ",high priority,apaszke,"Right now, one has to do:\r\n\r\n```python\r\nnet = ...\r\nstate = net.state_dict()\r\nstate_clone = copy.deepcopy(state)\r\nnet2 = ...\r\nnet2.load_state_dict(state_clone)\r\n```\r\n\r\nReported by @bharath272 ",python\r\nnet = ...\r\nstate = net.state_dict()\r\nstate_clone = copy.deepcopy(state)\r\nnet2 = ...\r\nnet2.load_state_dict(state_clone)\r\n
394,Return namedtuples from torch.* function with multiple return argumentsBecause this:\r\n\r\nlooks much better than this:\r\n,feature|triaged|module: ux|Stale,zasdfgbnm,Because this:\r\n```python\r\nactions = model(input).max(1).indices\r\n```\r\nlooks much better than this:\r\n```python\r\nactions = model(input).max(1)[1]\r\n```,python\r\nactions = model(input).max(1).indices\r\n
327,"Segmentation fault when dividing by zero with integer tensorsThis seems to be an issue with TH and not pytorch, the same thing happens in lua torch.\r\n\r\n\r\n\r\nAnother unrelated weirdness happens with float tensors when the answer is `inf`\r\n\r\n\r\nwhich works on lua torch, but gives\r\n```\r\n<repr(<torch.FloatTensor at 0x7f8a012eee18>) failed: RuntimeError: inconsistent tensor size at /tmp/pip-zh0a53-build/torch/lib/TH/generic/THTensorMath.c:90>\r\n```\r\nin pytorch.\n\ncc @ezyang @gchanan @zou3519",high priority|module: crash|triaged|module: NaNs and Infs,Baranowski,"This seems to be an issue with TH and not pytorch, the same thing happens in lua torch.\r\n\r\n```python\r\nimport torch\r\na = torch.IntTensor([0,1])\r\nb = torch.IntTensor([0,1])\r\nprint(a.div(b)) # Floating point exception (core dumped)\r\n```\r\n\r\nAnother unrelated weirdness happens with float tensors when the answer is `inf`\r\n\r\n```python\r\nimport torch\r\na = torch.FloatTensor([1,1])\r\nb = torch.FloatTensor([0,0])\r\na.div(b)\r\n```\r\nwhich works on lua torch, but gives\r\n```\r\n<repr(<torch.FloatTensor at 0x7f8a012eee18>) failed: RuntimeError: inconsistent tensor size at /tmp/pip-zh0a53-build/torch/lib/TH/generic/THTensorMath.c:90>\r\n```\r\nin pytorch.\n\ncc @ezyang @gchanan @zou3519","python\r\nimport torch\r\na = torch.IntTensor([0,1])\r\nb = torch.IntTensor([0,1])\r\nprint(a.div(b)) # Floating point exception (core dumped)\r\n"
312,memory leak in embedding layerThere seems to be a memory leak (in the backward only) of the embedding layer:\r\n\r\n,high priority,apaszke,"There seems to be a memory leak (in the backward only) of the embedding layer:\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nn_words = 1000\r\ndim = 128\r\nemb = nn.Embedding(1000, 128).cuda()\r\n\r\nx = Variable(torch.LongTensor(100).random_(n_words)).cuda()\r\n\r\nwhile True:\r\n    emb(x).sum().backward()\r\n```","python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nn_words = 1000\r\ndim = 128\r\nemb = nn.Embedding(1000, 128).cuda()\r\n\r\nx = Variable(torch.LongTensor(100).random_(n_words)).cuda()\r\n\r\nwhile True:\r\n    emb(x).sum().backward()\r\n"
299,Problem with bmm`bmm` doesn't seem to work with pytorch autograd at the moment. Here is a simple code\r\n\r\nand the error\r\n\r\n```RuntimeError: output tensor of incorrect size at /Users/soumith/anaconda/conda-bld/pytorch-0.1.6_1480182266313/work/torch/lib/TH/generic/THTensorMath.c:993```,high priority,apaszke,"`bmm` doesn't seem to work with pytorch autograd at the moment. Here is a simple code\r\n```python\r\nx = Variable(torch.FloatTensor(1, 2, 3).fill_(0.1))\r\ny = Variable(torch.FloatTensor(1, 3, 4).fill_(0.3))\r\n\r\nz = x.bmm(y)\r\n```\r\nand the error\r\n\r\n```RuntimeError: output tensor of incorrect size at /Users/soumith/anaconda/conda-bld/pytorch-0.1.6_1480182266313/work/torch/lib/TH/generic/THTensorMath.c:993```","python\r\nx = Variable(torch.FloatTensor(1, 2, 3).fill_(0.1))\r\ny = Variable(torch.FloatTensor(1, 3, 4).fill_(0.3))\r\n\r\nz = x.bmm(y)\r\n"
296,"Problem with comparison overload operatorsInstead of raising an error or behaving like their equivalent `gt`/`eq`/etc functions, it simply returns a boolean.\r\n\r\n",high priority,apaszke,"Instead of raising an error or behaving like their equivalent `gt`/`eq`/etc functions, it simply returns a boolean.\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.rand(5)\r\n>>> a\r\n\r\n 0.8091\r\n 0.1895\r\n 0.0382\r\n 0.9516\r\n 0.6266\r\n[torch.FloatTensor of size 5]\r\n\r\n>>> a > 0.2\r\nTrue\r\n>>> a == 1\r\nFalse\r\n\r\n```",python\r\n>>> import torch\r\n>>> a = torch.rand(5)\r\n>>> a\r\n\r\n 0.8091\r\n 0.1895\r\n 0.0382\r\n 0.9516\r\n 0.6266\r\n[torch.FloatTensor of size 5]\r\n\r\n>>> a > 0.2\r\nTrue\r\n>>> a == 1\r\nFalse\r\n\r\n
292,Add functional interface to nne.g.\r\n\r\n,high priority,szagoruyko,"e.g.\r\n\r\n```python\r\nimport torch.nn.functional as F\r\nF.relu(tensor)\r\nF.conv2d(input, weight, bias)\r\n```","python\r\nimport torch.nn.functional as F\r\nF.relu(tensor)\r\nF.conv2d(input, weight, bias)\r\n"
289,"squeeze dimension after mean / sumCurrently,\r\n\r\n\r\n\r\nreturns a vector of shape (1, 4), but shouldn't the dimension over which we sum / mean / etc disappear?",todo,gchanan,"Currently,\r\n\r\n```python\r\ntorch.FloatTensor(3, 4).normal_().sum(0)\r\n```\r\n\r\nreturns a vector of shape (1, 4), but shouldn't the dimension over which we sum / mean / etc disappear?","python\r\ntorch.FloatTensor(3, 4).normal_().sum(0)\r\n"
279,Support graphs with stochastic nodes in torch.autogradWe\u2019ve had some discussions on the stochastic nodes (see e.g. [this paper for discussion](https://arxiv.org/abs/1506.05254)) this week and this is the API we concluded on.\r\n\r\n**EDIT: WE'VE CHANGED THE API**\r\n\r\n\r\n\r\ncc: @ludc @yuandong-tian,high priority,apaszke,"We\u2019ve had some discussions on the stochastic nodes (see e.g. [this paper for discussion](https://arxiv.org/abs/1506.05254)) this week and this is the API we concluded on.\r\n\r\n**EDIT: WE'VE CHANGED THE API**\r\n\r\n```python\r\ny1 = f(input1, weights)\r\nz1 = sample(y1)\r\nr1 = get_reward(z1)\r\n\r\ny2 = f(input2, weights)\r\nz2 = sample(y2)\r\nr2 = get_reward(z2)\r\n\r\n# r1 and r2 is are vectors of scalar rewards\r\n\r\nz1.reinforce(r1 + gamma*r2)\r\nz2.reinforce(r2)\r\n(r1 + r2).backward()\r\n```\r\n\r\ncc: @ludc @yuandong-tian","python\r\ny1 = f(input1, weights)\r\nz1 = sample(y1)\r\nr1 = get_reward(z1)\r\n\r\ny2 = f(input2, weights)\r\nz2 = sample(y2)\r\nr2 = get_reward(z2)\r\n\r\n# r1 and r2 is are vectors of scalar rewards\r\n\r\nz1.reinforce(r1 + gamma*r2)\r\nz2.reinforce(r2)\r\n(r1 + r2).backward()\r\n"
262,"allow forward / backward hooks to rewrite outputs and gradientsRight now, hooks registered need to be read-only, and cannot be used to modify the grad_input / output and still get correct results in the whole graph.\r\nThis is because we make some read-only assumptions and optimize buffer reuse.\r\n\r\nAllow writeable hooks with an interface like this:\r\n\r\n\r\n\r\nthat would allow arbitrary changing of grad_input and output with some user-defined stuff. This for example will be useful for metalearning and in RL.\r\n\r\nHere's an example put together by @apaszke showcasing the bugs if we try to modify the gradients in the current implementation of hooks:\r\n\r\n\r\n\r\ncc: @ludc ",high priority,apaszke,"Right now, hooks registered need to be read-only, and cannot be used to modify the grad_input / output and still get correct results in the whole graph.\r\nThis is because we make some read-only assumptions and optimize buffer reuse.\r\n\r\nAllow writeable hooks with an interface like this:\r\n\r\n```python\r\nregister_backward_hook('name', hook, write=True)\r\n```\r\n\r\nthat would allow arbitrary changing of grad_input and output with some user-defined stuff. This for example will be useful for metalearning and in RL.\r\n\r\nHere's an example put together by @apaszke showcasing the bugs if we try to modify the gradients in the current implementation of hooks:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.randn(5, 5), requires_grad=True)\r\ny = Variable(torch.randn(5, 5), requires_grad=True)\r\n\r\na = x * 2\r\nb = y * 3\r\n\r\ndef hook_a(grad_output):\r\n    grad_output.mul_(2)\r\n\r\na.register_hook('test', hook_a)\r\n\r\nc = a + b\r\nc.sum().backward()\r\n\r\nprint(x.grad) # should be 2, is 2\r\nprint(y.grad) # should be 3, is 6\r\n```\r\n\r\ncc: @ludc ","python\r\nregister_backward_hook('name', hook, write=True)\r\n"
256,Segmentation fault when indexing with float slicesI get a segfault when running the following script:\r\n\r\n,high priority,soumith,"I get a segfault when running the following script:\r\n\r\n```python\r\nimport torch\r\nt = torch.rand(3,3)\r\nt[0.0, 0.0:1.0] # segfault\r\n```","python\r\nimport torch\r\nt = torch.rand(3,3)\r\nt[0.0, 0.0:1.0] # segfault\r\n"
249,backward failing when using expandThe following code will raise an issue:\r\n\r\n\r\n\r\n`RuntimeError: inconsistent tensor size at /home/guismay/pytorch-master/torch/lib/TH/generic/THTensorMath.c:576`,high priority,apaszke,"The following code will raise an issue:\r\n\r\n```python\r\nx = Variable(torch.FloatTensor(10, 7).normal_(), requires_grad=True)\r\ny = Variable(torch.FloatTensor(10, 7).normal_())\r\nx -= x.mean(1).expand(x.size())\r\n\r\nnn.MSELoss()(x, y).backward()\r\n```\r\n\r\n`RuntimeError: inconsistent tensor size at /home/guismay/pytorch-master/torch/lib/TH/generic/THTensorMath.c:576`","python\r\nx = Variable(torch.FloatTensor(10, 7).normal_(), requires_grad=True)\r\ny = Variable(torch.FloatTensor(10, 7).normal_())\r\nx -= x.mean(1).expand(x.size())\r\n\r\nnn.MSELoss()(x, y).backward()\r\n"
241,"PyTorch goes distributedTogether with @0mp, @VirrageS andy @jytug we're developing a `torch.distributed` package for PyTorch. All work is done [in a fork](https://github.com/apaszke/pytorch-dist/tree/thd) on a `thd` branch (we didn't want to make a lot of unnecessary noise in the main repo). We're creating this issue, so we can gather feedback on our API designs from all you guys.\r\n\r\nWe plan to make the package have two modes. The user has to choose one of them as part of the initialisation.\r\n\r\n### Process group mode\r\n\r\nThis is very similar to the API defined in MPI. We assume all processes are equal, assign them ranks and later on, allow them to use a well known set of communication collectives like `reduce`, `broadcast`, `allReduce`, `gather`, `scatter`, etc.\r\n\r\n**Example:**\r\n\r\n\r\n\r\n### Master-worker mode\r\n\r\nThis would provide a very similar API to the `torch.cuda` package. At the beginning of your script you would have to call `torch.distributed.init_master_worker(backend='mpi')`\r\n\r\nOperation execution is asynchronous w.r.t. to the master process, we'll implement a CUDA-like concurrency model (streams + events). Until then, the only sync points are copies between master and workers.\r\n\r\n**Example:**\r\n\r\n\r\n\r\n### How to launch the jobs\r\n\r\nWe'll provide a `pytorch_exec` utility that will spawn the process groups in a similar fashion that `mpiexec` does.\r\n\r\n### Decoupling data backends from other logic\r\n\r\nYou might have noticed that both `init_process_group` and `init_master_worker` accept a `backend` argument. We're aware that the best strategy for sending the data might be different for every user, and it will be crucial to pick a good one to limit communication overhead. This was the reason why we decided to introduce a `DataChannel` interface, so users will be able to pick from one of the provided implementations (initially MPI and raw TCP sockets, later RDMA etc.), or add custom ones, so they can easily achieve the lowest overhead possible in their setup.\r\n\r\n**Please let us know what you think! Thanks!**",triage review,apaszke,"Together with @0mp, @VirrageS andy @jytug we're developing a `torch.distributed` package for PyTorch. All work is done [in a fork](https://github.com/apaszke/pytorch-dist/tree/thd) on a `thd` branch (we didn't want to make a lot of unnecessary noise in the main repo). We're creating this issue, so we can gather feedback on our API designs from all you guys.\r\n\r\nWe plan to make the package have two modes. The user has to choose one of them as part of the initialisation.\r\n\r\n### Process group mode\r\n\r\nThis is very similar to the API defined in MPI. We assume all processes are equal, assign them ranks and later on, allow them to use a well known set of communication collectives like `reduce`, `broadcast`, `allReduce`, `gather`, `scatter`, etc.\r\n\r\n**Example:**\r\n\r\n```python\r\nimport torch.distributed\r\ntorch.distributed.init_process_group(backend='tcp')\r\nmy_rank = torch.distributed.get_rank()\r\nnum_processes = torch.distributed.get_num_processes()\r\n\r\n...\r\n\r\nif my_rank == 0:\r\n    torch.distributed.send(tensor, 1)\r\nelse:\r\n    tensor = torch.distributed.recv(0)\r\n\r\n...\r\n\r\nresult = torch.distributed.all_reduce(tensor)\r\n```\r\n\r\n### Master-worker mode\r\n\r\nThis would provide a very similar API to the `torch.cuda` package. At the beginning of your script you would have to call `torch.distributed.init_master_worker(backend='mpi')`\r\n\r\nOperation execution is asynchronous w.r.t. to the master process, we'll implement a CUDA-like concurrency model (streams + events). Until then, the only sync points are copies between master and workers.\r\n\r\n**Example:**\r\n\r\n```python\r\nimport torch.distributed\r\ntorch.distributed.init_master_worker(backend='tcp')\r\n\r\nx = torch.distributed.FloatTensor(20, 20).fill_(4)\r\ny = torch.randn(20, 20).dist_send()\r\nz = x + y\r\n# z.get_node(), z.get_device() == 0, -1 (i.e. CPU)\r\ncuda_x = x.cuda()\r\n# cuda_x.get_node(), cuda_x.get_device() == 0, 0\r\nwith torch.distributed.node(1):\r\n    a = torch.distributed.FloatTensor(10, device=1)\r\n    # a.get_node(), a.get_device() == 1, 1\r\n    cuda_y = y.cuda()\r\n    # cuda_y.get_node(), cuda_y.get_device() == 0, 0\r\n    q = cuda_x + cuda_y\r\n    # q.get_node(), q.get_device() == 0, 0\r\n```\r\n\r\n### How to launch the jobs\r\n\r\nWe'll provide a `pytorch_exec` utility that will spawn the process groups in a similar fashion that `mpiexec` does.\r\n\r\n### Decoupling data backends from other logic\r\n\r\nYou might have noticed that both `init_process_group` and `init_master_worker` accept a `backend` argument. We're aware that the best strategy for sending the data might be different for every user, and it will be crucial to pick a good one to limit communication overhead. This was the reason why we decided to introduce a `DataChannel` interface, so users will be able to pick from one of the provided implementations (initially MPI and raw TCP sockets, later RDMA etc.), or add custom ones, so they can easily achieve the lowest overhead possible in their setup.\r\n\r\n**Please let us know what you think! Thanks!**","python\r\nimport torch.distributed\r\ntorch.distributed.init_process_group(backend='tcp')\r\nmy_rank = torch.distributed.get_rank()\r\nnum_processes = torch.distributed.get_num_processes()\r\n\r\n...\r\n\r\nif my_rank == 0:\r\n    torch.distributed.send(tensor, 1)\r\nelse:\r\n    tensor = torch.distributed.recv(0)\r\n\r\n...\r\n\r\nresult = torch.distributed.all_reduce(tensor)\r\n"
238,"CrossEntropyLoss on CPU is leakingrunning the imagenet example will slow leak memory\r\n\r\n- gc.collect() does not help\r\n- dataparallel is not the issue, even single-GPU job has the leak\r\n\r\nThis is the snippet that will leak:\r\n\r\n\r\nThis wont leak:\r\n```\r\nfor epoch in range(args.nEpochs):\r\n    for i, data in enumerate(train_loader, 0):\r\n        input, label = data\r\n        input = Variable(input.cuda())\r\n        label = Variable(label.cuda())\r\n        def closure():\r\n            output = model(input)\r\n            loss = criterion(output, label)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n        optimizer.step(closure)\r\n        print(i)\r\n```",high priority,apaszke,"running the imagenet example will slow leak memory\r\n\r\n- gc.collect() does not help\r\n- dataparallel is not the issue, even single-GPU job has the leak\r\n\r\nThis is the snippet that will leak:\r\n```python\r\nfor epoch in range(args.nEpochs):\r\n    for i, data in enumerate(train_loader, 0):\r\n        input, label = data\r\n        input = Variable(input)\r\n        label = Variable(label)\r\n        def closure():\r\n            output = model(input)\r\n            loss = criterion(output, label)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n        optimizer.step(closure)\r\n        print(i)\r\n```\r\n\r\nThis wont leak:\r\n```\r\nfor epoch in range(args.nEpochs):\r\n    for i, data in enumerate(train_loader, 0):\r\n        input, label = data\r\n        input = Variable(input.cuda())\r\n        label = Variable(label.cuda())\r\n        def closure():\r\n            output = model(input)\r\n            loss = criterion(output, label)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n        optimizer.step(closure)\r\n        print(i)\r\n```","python\r\nfor epoch in range(args.nEpochs):\r\n    for i, data in enumerate(train_loader, 0):\r\n        input, label = data\r\n        input = Variable(input)\r\n        label = Variable(label)\r\n        def closure():\r\n            output = model(input)\r\n            loss = criterion(output, label)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n        optimizer.step(closure)\r\n        print(i)\r\n"
230,invalid memory sizeRunning this code:\r\n\r\n\r\n\r\n\r\nreturns the following error:\r\n```RuntimeError: $ Torch: invalid memory size -- maybe an overflow? at /home/guismay/pytorch-master/torch/lib/TH/THGeneral.c:257```\r\n\r\nSo I have to run `torch.LongTensor(x.numpy()[::-1].copy())`instead.,high priority,apaszke,"Running this code:\r\n\r\n\r\n```python\r\nx = torch.LongTensor(3, 4).random_(10)\r\ntorch.LongTensor(x.numpy()[::-1])\r\n```\r\n\r\nreturns the following error:\r\n```RuntimeError: $ Torch: invalid memory size -- maybe an overflow? at /home/guismay/pytorch-master/torch/lib/TH/THGeneral.c:257```\r\n\r\nSo I have to run `torch.LongTensor(x.numpy()[::-1].copy())`instead.","python\r\nx = torch.LongTensor(3, 4).random_(10)\r\ntorch.LongTensor(x.numpy()[::-1])\r\n"
186,max of cuda variable not working\r\n\r\nThis code returns an error:\r\n`AttributeError: 'ByteTensor' object has no attribute 'nonzero'``\r\nbut is working fine on CPU,high priority,soumith,```python\r\nx = Variable(torch.FloatTensor(5).normal_()).cuda() * 10\r\nprint(x.max())\r\n```\r\n\r\nThis code returns an error:\r\n`AttributeError: 'ByteTensor' object has no attribute 'nonzero'``\r\nbut is working fine on CPU,python\r\nx = Variable(torch.FloatTensor(5).normal_()).cuda() * 10\r\nprint(x.max())\r\n
