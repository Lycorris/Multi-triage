17240,"PHP CLI fork bug<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n`php`\r\n```\r\ngrpc support => enabled\r\ngrpc module version => 1.15.0\r\n ```\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n` MAC OS`\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n ```\r\nphp -v:\r\nPHP 7.2.10 (cli) (built: Sep 14 2018 07:07:08) ( NTS )\r\nCopyright (c) 1997-2018 The PHP Group\r\nZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies\r\n    with Zend OPcache v7.2.10, Copyright (c) 1999-2018, by Zend Technologies\r\n```\r\n \r\n### What did you do?\r\nI want to fork 3 process on cli model, trigger three orphan process, but child process not quit\r\n\r\n\r\n```\r\n\u279c  Test ps -l\r\n  UID   PID  PPID        F CPU PRI NI       SZ    RSS WCHAN     S             ADDR TTY           TIME CMD\r\n  501   852   833     4006   0  31  0  4297248   1280 -      Ss+                 0 ttys001    0:00.25 /bin/zsh --login -i\r\n  501   696   695     4006   0  31  0  4298336   4940 -      S                   0 ttys002    0:00.83 -zsh\r\n  501  1480     1        6   0  31  0  4356908   2212 -      S                   0 ttys002    0:00.00 php Test.php\r\n  501  1481     1        6   0  31  0  4356908   2216 -      S                   0 ttys002    0:00.00 php Test.php\r\n  501  1482     1        6   0  31  0  4356908   2228 -      S                   0 ttys002    0:00.00 php Test.php\r\n```\r\n \r\n### What did you expect to see?\r\nin cli model fork multi process, even don't use grpc, only install it\r\n \r\n### What did you see instead?\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\nSee https://github.com/grpc/grpc/blob/master/TROUBLESHOOTING.md for how to diagnose problems better.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",lang/php,stanley-cheung,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n`php`\r\n```\r\ngrpc support => enabled\r\ngrpc module version => 1.15.0\r\n ```\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n` MAC OS`\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n ```\r\nphp -v:\r\nPHP 7.2.10 (cli) (built: Sep 14 2018 07:07:08) ( NTS )\r\nCopyright (c) 1997-2018 The PHP Group\r\nZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies\r\n    with Zend OPcache v7.2.10, Copyright (c) 1999-2018, by Zend Technologies\r\n```\r\n \r\n### What did you do?\r\nI want to fork 3 process on cli model, trigger three orphan process, but child process not quit\r\n```php\r\n$user = 3;\r\nwhile ($user >= 1) {\r\n    $pid = pcntl_fork();\r\n    if (-1 == $pid) {\r\n        die(""fork\u8fdb\u7a0b\u5931\u8d25\\n"");\r\n    } else if (0 == $pid) {\r\n        break;\r\n    } else {\r\n        $user--;\r\n        if ($user == 0) {\r\n            exit;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n```\r\n\u279c  Test ps -l\r\n  UID   PID  PPID        F CPU PRI NI       SZ    RSS WCHAN     S             ADDR TTY           TIME CMD\r\n  501   852   833     4006   0  31  0  4297248   1280 -      Ss+                 0 ttys001    0:00.25 /bin/zsh --login -i\r\n  501   696   695     4006   0  31  0  4298336   4940 -      S                   0 ttys002    0:00.83 -zsh\r\n  501  1480     1        6   0  31  0  4356908   2212 -      S                   0 ttys002    0:00.00 php Test.php\r\n  501  1481     1        6   0  31  0  4356908   2216 -      S                   0 ttys002    0:00.00 php Test.php\r\n  501  1482     1        6   0  31  0  4356908   2228 -      S                   0 ttys002    0:00.00 php Test.php\r\n```\r\n \r\n### What did you expect to see?\r\nin cli model fork multi process, even don't use grpc, only install it\r\n \r\n### What did you see instead?\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\nSee https://github.com/grpc/grpc/blob/master/TROUBLESHOOTING.md for how to diagnose problems better.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","php\r\n$user = 3;\r\nwhile ($user >= 1) {\r\n    $pid = pcntl_fork();\r\n    if (-1 == $pid) {\r\n        die(""fork\u8fdb\u7a0b\u5931\u8d25\\n"");\r\n    } else if (0 == $pid) {\r\n        break;\r\n    } else {\r\n        $user--;\r\n        if ($user == 0) {\r\n            exit;\r\n        }\r\n    }\r\n}\r\n"
17076,"gpr_mu_lock() aborted when calling grpc_completion_queue_pluck()<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n gRPC version: v1.16.0\r\nLanguage: C\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n 3.16.53-OpenNetworkLinux (base on Debian Linux)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n gcc 5.4.0\r\n \r\n### What did you do?\r\nI'm using [grpc-c](https://github.com/Juniper/grpc-c) which is a C implementation of gRPC layered on top of core library.\r\n \r\n### What did you expect to see?\r\n Don't abort.\r\n \r\n### What did you see instead?\r\nCoredump happened due to assertion failed. The backtrace is:\r\n```\r\n#0  0x00007fa3e98f8067 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\r\n#1  0x00007fa3e98f9448 in __GI_abort () at abort.c:89\r\n#2  0x00007fa3e9d6a843 in gpr_mu_lock (mu=mu@entry=0x7fa3d4026e60) at src/core/lib/gpr/sync_posix.cc:65\r\n#3  0x00007fa3e9ccf671 in pollset_work (ps=0x7fa3d4026e60, worker_hdl=<optimized out>, deadline=<optimized out>) at src/core/lib/iomgr/ev_epoll1_linux.cc:1037\r\n#4  0x00007fa3e9cf7a69 in cq_pluck (cq=0x7fa3d4026d60, tag=0x7fa3d4026ad0, deadline=..., reserved=<optimized out>) at src/core/lib/surface/completion_queue.cc:1275\r\n#5  0x00007fa3e9cf7e1b in grpc_completion_queue_pluck (cq=<optimized out>, tag=tag@entry=0x7fa3d4026ad0, deadline=..., reserved=reserved@entry=0x0) at src/core/lib/surface/completion_queue.cc:1302\r\n#6  0x00007fa3ea1e975e in grpc_c_client_request_unary (client=0x4d46700, mdarray=<optimized out>, flags=<optimized out>, method=<optimized out>, input=<optimized out>, output=<optimized out>, status=0x7fa3e2c5dcd0, client_streaming=0, server_streaming=0,  input_packer=0x7fa3ea3f5288 <generic.generic_request_packer>, input_unpacker=0x7fa3ea3f5343 <generic.generic_request_unpacker>, input_free=0x7fa3ea3f554f <generic.generic_request_free>, output_packer=0x7fa3ea3f55b3 <generic.generic_reply_packer>, output_unpacker=0x7fa3ea3f566e <generic.generic_reply_unpacker>, output_free=0x7fa3ea3f587a <generic.generic_reply_free>, timeout=10000) at ../../lib/client.c:1170\r\n```\r\nIt's abort because of the code below:\r\n\r\n\r\n### Anything else we should know about your project / environment?\r\nNo\r\n",kind/bug|lang/core|priority/P2|disposition/stale,guantaol,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n gRPC version: v1.16.0\r\nLanguage: C\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n 3.16.53-OpenNetworkLinux (base on Debian Linux)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n gcc 5.4.0\r\n \r\n### What did you do?\r\nI'm using [grpc-c](https://github.com/Juniper/grpc-c) which is a C implementation of gRPC layered on top of core library.\r\n \r\n### What did you expect to see?\r\n Don't abort.\r\n \r\n### What did you see instead?\r\nCoredump happened due to assertion failed. The backtrace is:\r\n```\r\n#0  0x00007fa3e98f8067 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\r\n#1  0x00007fa3e98f9448 in __GI_abort () at abort.c:89\r\n#2  0x00007fa3e9d6a843 in gpr_mu_lock (mu=mu@entry=0x7fa3d4026e60) at src/core/lib/gpr/sync_posix.cc:65\r\n#3  0x00007fa3e9ccf671 in pollset_work (ps=0x7fa3d4026e60, worker_hdl=<optimized out>, deadline=<optimized out>) at src/core/lib/iomgr/ev_epoll1_linux.cc:1037\r\n#4  0x00007fa3e9cf7a69 in cq_pluck (cq=0x7fa3d4026d60, tag=0x7fa3d4026ad0, deadline=..., reserved=<optimized out>) at src/core/lib/surface/completion_queue.cc:1275\r\n#5  0x00007fa3e9cf7e1b in grpc_completion_queue_pluck (cq=<optimized out>, tag=tag@entry=0x7fa3d4026ad0, deadline=..., reserved=reserved@entry=0x0) at src/core/lib/surface/completion_queue.cc:1302\r\n#6  0x00007fa3ea1e975e in grpc_c_client_request_unary (client=0x4d46700, mdarray=<optimized out>, flags=<optimized out>, method=<optimized out>, input=<optimized out>, output=<optimized out>, status=0x7fa3e2c5dcd0, client_streaming=0, server_streaming=0,  input_packer=0x7fa3ea3f5288 <generic.generic_request_packer>, input_unpacker=0x7fa3ea3f5343 <generic.generic_request_unpacker>, input_free=0x7fa3ea3f554f <generic.generic_request_free>, output_packer=0x7fa3ea3f55b3 <generic.generic_reply_packer>, output_unpacker=0x7fa3ea3f566e <generic.generic_reply_unpacker>, output_free=0x7fa3ea3f587a <generic.generic_reply_free>, timeout=10000) at ../../lib/client.c:1170\r\n```\r\nIt's abort because of the code below:\r\n```c\r\nvoid gpr_mu_lock(gpr_mu* mu) {\r\n#ifdef GPR_LOW_LEVEL_COUNTERS\r\n  GPR_ATM_INC_COUNTER(gpr_mu_locks);\r\n#endif\r\n  GPR_TIMER_SCOPE(""gpr_mu_lock"", 0);\r\n  GPR_ASSERT(pthread_mutex_lock(mu) == 0);\r\n}\r\n```\r\n\r\n### Anything else we should know about your project / environment?\r\nNo\r\n","c\r\nvoid gpr_mu_lock(gpr_mu* mu) {\r\n#ifdef GPR_LOW_LEVEL_COUNTERS\r\n  GPR_ATM_INC_COUNTER(gpr_mu_locks);\r\n#endif\r\n  GPR_TIMER_SCOPE(""gpr_mu_lock"", 0);\r\n  GPR_ASSERT(pthread_mutex_lock(mu) == 0);\r\n}\r\n"
17004,"Channel destructor failure during teardown### What version of gRPC and what language are you using?\r\n \r\ngrpcio 1.16.0, Python2\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nUbuntu 16.04\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nPython 2.7.12\r\n \r\n### What did you do?\r\n\r\nCommand line script that opens a TLS channel with interceptors, creates a client, makes some calls, closes the channel, and terminates.\r\n \r\n### What did you expect to see?\r\n \r\nThe script terminate cleanly\r\n \r\n### What did you see instead?\r\n \r\nA spurious failure in a Python destructor. The grpc._channel.Channel.__del__ method is attempting to call module global functions, but they have already been destroyed. `cygrpc.fork_unregister_channel(self)` fails, because cygrpc is None. Similarly, _moot is already gone.\r\n\r\n```\r\nI1025 17:06:40.485048414   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3d4eb40)\r\nI1025 17:06:40.485095120   25945 completion_queue.cc:1388]   grpc_completion_queue_destroy(cq=0x3d4eb40)\r\nI1025 17:06:40.485114258   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3d4eb40)\r\nI1025 17:06:40.485136530   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3a03660)\r\nI1025 17:06:40.485155296   25945 completion_queue.cc:1388]   grpc_completion_queue_destroy(cq=0x3a03660)\r\nI1025 17:06:40.485224591   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3a03660)\r\nI1025 17:06:40.485249325   25945 channel.cc:455]             grpc_channel_destroy(channel=0x3a56d80)\r\nI1025 17:06:40.485638087   25945 init.cc:158]                grpc_shutdown(void)\r\nException AttributeError: ""'NoneType' object has no attribute 'fork_unregister_channel'"" in <bound method Channel.__del__ of <grpc._channel.Channel object at 0x7f272ad99510>\r\n ```\r\n### Anything else we should know about your project / environment?\r\n\r\nI have not been able to put together a reproducible test case, as it seems hard to get the stars to align just right. The script that does trigger it is a Django command line application with a mass of dependencies, but I don't think anything bad is being done to mess with Python garbage collection. I think it is just bad luck that my client and the channel it contains is being garbage collected too late. The failure is documented Python behavior when the reference to the object is at module scope (in my case, a simple cache):\r\n\r\n> when __del__() is invoked is response to a module being deleted (e.g., when execution of the program is done), other globals referenced by the __del__() method may already have been deleted\r\n\r\nWork around is simple:\r\n\r\n",kind/bug|lang/Python,srini100,"### What version of gRPC and what language are you using?\r\n \r\ngrpcio 1.16.0, Python2\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nUbuntu 16.04\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nPython 2.7.12\r\n \r\n### What did you do?\r\n\r\nCommand line script that opens a TLS channel with interceptors, creates a client, makes some calls, closes the channel, and terminates.\r\n \r\n### What did you expect to see?\r\n \r\nThe script terminate cleanly\r\n \r\n### What did you see instead?\r\n \r\nA spurious failure in a Python destructor. The grpc._channel.Channel.__del__ method is attempting to call module global functions, but they have already been destroyed. `cygrpc.fork_unregister_channel(self)` fails, because cygrpc is None. Similarly, _moot is already gone.\r\n\r\n```\r\nI1025 17:06:40.485048414   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3d4eb40)\r\nI1025 17:06:40.485095120   25945 completion_queue.cc:1388]   grpc_completion_queue_destroy(cq=0x3d4eb40)\r\nI1025 17:06:40.485114258   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3d4eb40)\r\nI1025 17:06:40.485136530   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3a03660)\r\nI1025 17:06:40.485155296   25945 completion_queue.cc:1388]   grpc_completion_queue_destroy(cq=0x3a03660)\r\nI1025 17:06:40.485224591   25945 completion_queue.cc:1382]   grpc_completion_queue_shutdown(cq=0x3a03660)\r\nI1025 17:06:40.485249325   25945 channel.cc:455]             grpc_channel_destroy(channel=0x3a56d80)\r\nI1025 17:06:40.485638087   25945 init.cc:158]                grpc_shutdown(void)\r\nException AttributeError: ""'NoneType' object has no attribute 'fork_unregister_channel'"" in <bound method Channel.__del__ of <grpc._channel.Channel object at 0x7f272ad99510>\r\n ```\r\n### Anything else we should know about your project / environment?\r\n\r\nI have not been able to put together a reproducible test case, as it seems hard to get the stars to align just right. The script that does trigger it is a Django command line application with a mass of dependencies, but I don't think anything bad is being done to mess with Python garbage collection. I think it is just bad luck that my client and the channel it contains is being garbage collected too late. The failure is documented Python behavior when the reference to the object is at module scope (in my case, a simple cache):\r\n\r\n> when __del__() is invoked is response to a module being deleted (e.g., when execution of the program is done), other globals referenced by the __del__() method may already have been deleted\r\n\r\nWork around is simple:\r\n\r\n```python\r\n    def __del__(self):\r\n        # TODO(https://github.com/grpc/grpc/issues/12531): Several releases\r\n        # after 1.12 (1.16 or thereabouts?) add a ""self._channel.close"" call\r\n        # here (or more likely, call self._close() here). We don't do this today\r\n        # because many valid use cases today allow the channel to be deleted\r\n        # immediately after stubs are created. After a sufficient period of time\r\n        # has passed for all users to be trusted to hang out to their channels\r\n        # for as long as they are in use and to close them after using them,\r\n        # then deletion of this grpc._channel.Channel instance can be made to\r\n        # effect closure of the underlying cygrpc.Channel instance.\r\n        if cygrpc is not None:\r\n            cygrpc.fork_unregister_channel(self)\r\n        if _moot is not None:\r\n            _moot(self._connectivity_state)\r\n```","python\r\n    def __del__(self):\r\n        # TODO(https://github.com/grpc/grpc/issues/12531): Several releases\r\n        # after 1.12 (1.16 or thereabouts?) add a ""self._channel.close"" call\r\n        # here (or more likely, call self._close() here). We don't do this today\r\n        # because many valid use cases today allow the channel to be deleted\r\n        # immediately after stubs are created. After a sufficient period of time\r\n        # has passed for all users to be trusted to hang out to their channels\r\n        # for as long as they are in use and to close them after using them,\r\n        # then deletion of this grpc._channel.Channel instance can be made to\r\n        # effect closure of the underlying cygrpc.Channel instance.\r\n        if cygrpc is not None:\r\n            cygrpc.fork_unregister_channel(self)\r\n        if _moot is not None:\r\n            _moot(self._connectivity_state)\r\n"
17001,"Deadlock closing channel object from Python server<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\nVersion 1.15.0 with Python 2.7 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux, Debian 9.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Python 2.7.13\r\n gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n\r\n### What did you do?\r\nWe were running speech to text calls in a Python Tornado server.  To avoid memory leaks with channels, we explicitly create the grpc channel to pass to the SpeechClient.  Example:\r\n\r\n`\r\n    \r\n### What did you expect to see?\r\nSuccessful closing of the channel.\r\n \r\n### What did you see instead?\r\nOur server works fine in most cases but we had two instances where the process deadlocked.  What appeared to happen was that when the `with` statement triggered the channel close, it initiated a grpc shutdown which triggered the timer thread(s) to exit.  When one of the timer threads was closing it initiated a destruction of the Credentials object which then required access to the Python GIL.  It appears that closing the channel is not releasing the GIL so depending on the timing and reference counting, our Python code can request a shutdown of the channel while still holding the GIL but then be blocked waiting for the timer thread to exit which is waiting on the GIL.  In looking though the Cython files, I didn't see any references to `with nogil` before the `grpc_shutdown` call in the channel _close method (https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi#L395)\r\n\r\n```\r\n(gdb) thread apply all py-list\r\n\r\nThread 7 (Thread 0x7f21b7813700 (LWP 12493)):\r\n 263                            if wacquire is None:\r\n 264                                send(obj)\r\n 265                            else:\r\n 266                                wacquire()\r\n 267                                try:\r\n>268                                    send(obj)\r\n 269                                finally:\r\n 270                                    wrelease()\r\n 271                    except IndexError:\r\n 272                        pass\r\n 273            except Exception, e:\r\n\r\nThread 6 (Thread 0x7f21b8815700 (LWP 12489)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 5 (Thread 0x7f21bb016700 (LWP 12488)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 4 (Thread 0x7f21b8014700 (LWP 12487)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 3 (Thread 0x7f21b7012700 (LWP 12486)):\r\n 949            return _StreamStreamMultiCallable(\r\n 950                self._channel, _channel_managed_call_management(self._call_state),\r\n 951                _common.encode(method), request_serializer, response_deserializer)\r\n 952    \r\n 953        def _close(self):\r\n>954            self._channel.close(cygrpc.StatusCode.cancelled, 'Channel closed!')\r\n 955            _moot(self._connectivity_state)\r\n 956    \r\n 957        def _close_on_fork(self):\r\n 958            self._channel.close_on_fork(cygrpc.StatusCode.cancelled,\r\n 959                                        'Channel closed due to fork')\r\n\r\nThread 2 (Thread 0x7f21ae80d700 (LWP 3484)):\r\n 335            waiter.acquire()\r\n 336            self.__waiters.append(waiter)\r\n 337            saved_state = self._release_save()\r\n 338            try:    # restore state no matter what (e.g., KeyboardInterrupt)\r\n 339                if timeout is None:\r\n>340                    waiter.acquire()\r\n 341                    if __debug__:\r\n 342                        self._note(""%s.wait(): got it"", self)\r\n 343                else:\r\n 344                    # Balancing act:  We can't afford a pure busy loop, so we\r\n 345                    # have to sleep; but if we sleep the whole timeout time,\r\n\r\nThread 1 (Thread 0x7f21cc6fb700 (LWP 2729)):\r\n 858                        # clear alarm so it doesn't fire while poll is waiting for\r\n 859                        # events.\r\n 860                        signal.setitimer(signal.ITIMER_REAL, 0, 0)\r\n 861    \r\n 862                    try:\r\n---Type <return> to continue, or q <return> to quit---\r\n>863                        event_pairs = self._impl.poll(poll_timeout)\r\n 864                    except Exception as e:\r\n 865                        # Depending on python version and IOLoop implementation,\r\n 866                        # different exception types may be thrown and there are\r\n 867                        # two ways EINTR might be signaled:\r\n 868                        # * e.errno == errno.EINTR\r\n(gdb) thread apply all bt\r\n\r\nThread 7 (Thread 0x7f21b7813700 (LWP 12493)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x00007f21c8a9c0ed in conn_send_string.isra.2 (string=<optimized out>, length=25) at ./Modules/_multiprocessing/socket_connection.c:122\r\n#6  0x00007f21c8a9caed in connection_send_obj (self=0x56049bd52a40, obj=<optimized out>) at ./Modules/_multiprocessing/connection.h:277\r\n#7  0x000056049a12e510 in call_function (oparg=<optimized out>, pp_stack=0x7f21b7812538) at ../Python/ceval.c:4340\r\n#8  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#9  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#10 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#11 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#12 0x000056049a12f3a0 in ext_do_call (nk=0, na=0, flags=<optimized out>, pp_stack=0x7f21b78127f0, func=<function at remote 0x7f21c5058938>) at ../Python/ceval.c:4666\r\n#13 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#14 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7812938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#15 call_function (oparg=<optimized out>, pp_stack=0x7f21b7812938) at ../Python/ceval.c:4372\r\n#16 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#17 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7812a88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#18 call_function (oparg=<optimized out>, pp_stack=0x7f21b7812a88) at ../Python/ceval.c:4372\r\n#19 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#20 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#21 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#22 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#23 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#24 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#25 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#26 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#27 0x00007f21cc2d8494 in start_thread (arg=0x7f21b7813700) at pthread_create.c:333\r\n#28 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 6 (Thread 0x7f21b8815700 (LWP 12489)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x000056049a207f2f in PyGILState_Ensure () at ../Python/pystate.c:611\r\n#6  0x00007f219d89022e in __pyx_f_4grpc_7_cython_6cygrpc__destroy (__pyx_v_state=0x7f21c3b15250) at src/python/grpcio/grpc/_cython/cygrpc.cpp:18766\r\n#7  0x00007f219d981ca6 in grpc_call_credentials_unref (creds=0x7f2184028aa0) at src/core/lib/security/credentials/credentials.cc:91\r\n#8  0x00007f219d98c126 in ssl_channel_destroy (sc=0x7f2178010ee0) at src/core/lib/security/security_connector/security_connector.cc:654\r\n#9  0x00007f219d914560 in grpc_channel_args_destroy (a=0x7f21780cad70) at src/core/lib/channel/channel_args.cc:205\r\n#10 0x00007f219d9b5ab5 in grpc_subchannel_key_destroy (k=0x7f217800b2f0) at src/core/ext/filters/client_channel/subchannel_index.cc:89\r\n#11 0x00007f219d9b3a88 in subchannel_destroy (arg=0x7f217800ac70, error=<optimized out>) at src/core/ext/filters/client_channel/subchannel.cc:192\r\n#12 0x00007f219d92bb5c in exec_ctx_run (error=0x0, closure=<optimized out>) at src/core/lib/iomgr/exec_ctx.cc:40\r\n#13 grpc_core::ExecCtx::Flush (this=0x7f21b8814ea0) at src/core/lib/iomgr/exec_ctx.cc:134\r\n#14 0x00007f219d93db2a in run_some_timers () at src/core/lib/iomgr/timer_manager.cc:121\r\n#15 timer_main_loop () at src/core/lib/iomgr/timer_manager.cc:222\r\n#16 timer_thread (completed_thread_ptr=0x7f218400d300) at src/core/lib/iomgr/timer_manager.cc:269\r\n#17 0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#18 grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#19 0x00007f21cc2d8494 in start_thread (arg=0x7f21b8815700) at pthread_create.c:333\r\n#20 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\n---Type <return> to continue, or q <return> to quit---\r\nThread 5 (Thread 0x7f21bb016700 (LWP 12488)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f21840964c8, mu=0x7f2184096490, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d92c711 in GrpcExecutor::ThreadMain (arg=0x7f2184096490) at src/core/lib/iomgr/executor.cc:174\r\n#3  0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#4  grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#5  0x00007f21cc2d8494 in start_thread (arg=0x7f21bb016700) at pthread_create.c:333\r\n#6  0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 4 (Thread 0x7f21b8014700 (LWP 12487)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f21840a5b38, mu=0x7f21840a5b00, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d92c711 in GrpcExecutor::ThreadMain (arg=0x7f21840a5b00) at src/core/lib/iomgr/executor.cc:174\r\n#3  0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#4  grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#5  0x00007f21cc2d8494 in start_thread (arg=0x7f21b8014700) at pthread_create.c:333\r\n#6  0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 3 (Thread 0x7f21b7012700 (LWP 12486)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f219ddf28c0 <g_cv_shutdown>, mu=0x7f219ddf2940 <_ZL4g_mu>, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d93d737 in stop_threads () at src/core/lib/iomgr/timer_manager.cc:311\r\n#3  0x00007f219d911a4b in grpc_shutdown () at src/core/lib/surface/init.cc:165\r\n#4  0x00007f219d9056cf in __pyx_f_4grpc_7_cython_6cygrpc__close (__pyx_v_channel=__pyx_v_channel@entry=0x7f21c3af8cb0, __pyx_v_code=__pyx_v_code@entry=GRPC_STATUS_CANCELLED, __pyx_v_details=__pyx_v_details@entry='Channel closed!', __pyx_v_drain_calls=False)\r\n    at src/python/grpcio/grpc/_cython/cygrpc.cpp:15637\r\n#5  0x00007f219d906291 in __pyx_pf_4grpc_7_cython_6cygrpc_7Channel_14close (__pyx_v_details='Channel closed!', __pyx_v_code=<optimized out>, __pyx_v_self=0x7f21c3af8cb0) at src/python/grpcio/grpc/_cython/cygrpc.cpp:17646\r\n#6  __pyx_pw_4grpc_7_cython_6cygrpc_7Channel_15close (__pyx_v_self=<grpc._cython.cygrpc.Channel at remote 0x7f21c3af8cb0>, __pyx_args=<optimized out>, __pyx_kwds=<optimized out>) at src/python/grpcio/grpc/_cython/cygrpc.cpp:17624\r\n#7  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7f21b7010db8) at ../Python/ceval.c:4352\r\n#8  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#9  0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7010f08, func=<function at remote 0x7f21c4f97050>) at ../Python/ceval.c:4437\r\n#10 call_function (oparg=<optimized out>, pp_stack=0x7f21b7010f08) at ../Python/ceval.c:4372\r\n#11 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#12 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#13 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#14 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#15 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#16 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#17 0x000056049a11d799 in PyObject_CallFunctionObjArgs () at ../Objects/abstract.c:2774\r\n#18 0x000056049a12b207 in PyEval_EvalFrameEx () at ../Python/ceval.c:2950\r\n#19 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#20 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#21 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#22 0x000056049a12f3a0 in ext_do_call (nk=0, na=1, flags=<optimized out>, pp_stack=0x7f21b70117f0, func=<function at remote 0x7f21c50eb8c0>) at ../Python/ceval.c:4666\r\n#23 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#24 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7011938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#25 call_function (oparg=<optimized out>, pp_stack=0x7f21b7011938) at ../Python/ceval.c:4372\r\n#26 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#27 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7011a88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#28 call_function (oparg=<optimized out>, pp_stack=0x7f21b7011a88) at ../Python/ceval.c:4372\r\n#29 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#30 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#31 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#32 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#33 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#34 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n---Type <return> to continue, or q <return> to quit---\r\n#35 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#36 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#37 0x00007f21cc2d8494 in start_thread (arg=0x7f21b7012700) at pthread_create.c:333\r\n#38 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 2 (Thread 0x7f21ae80d700 (LWP 3484)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x7f2198000b10) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x7f2198000b10, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x7f2198000b10, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a1c8742 in lock_PyThread_acquire_lock.lto_priv.2499 () at ../Modules/threadmodule.c:52\r\n#5  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c118) at ../Python/ceval.c:4352\r\n#6  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#7  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#8  0x000056049a12e448 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c328, func=<function at remote 0x7f21c96ce320>) at ../Python/ceval.c:4447\r\n#9  call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c328) at ../Python/ceval.c:4372\r\n#10 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#11 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#12 0x000056049a12e448 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c538, func=<function at remote 0x7f21c9581d70>) at ../Python/ceval.c:4447\r\n#13 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c538) at ../Python/ceval.c:4372\r\n#14 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#15 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#16 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#17 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#18 0x000056049a12f3a0 in ext_do_call (nk=0, na=0, flags=<optimized out>, pp_stack=0x7f21ae80c7f0, func=<function at remote 0x7f21c50d2c08>) at ../Python/ceval.c:4666\r\n#19 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#20 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#21 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c938) at ../Python/ceval.c:4372\r\n#22 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#23 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80ca88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#24 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80ca88) at ../Python/ceval.c:4372\r\n#25 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#26 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#27 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#28 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#29 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#30 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#31 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#32 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#33 0x00007f21cc2d8494 in start_thread (arg=0x7f21ae80d700) at pthread_create.c:333\r\n#34 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 1 (Thread 0x7f21cc6fb700 (LWP 2729)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x000056049a22007d in pyepoll_poll.lto_priv () at ../Modules/selectmodule.c:1035\r\n#6  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7ffe06529fb8) at ../Python/ceval.c:4352\r\n#7  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#8  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#9  0x000056049a12eb78 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe0652a1c8, func=<function at remote 0x7f21c8cb5e60>) at ../Python/ceval.c:4447\r\n#10 call_function (oparg=<optimized out>, pp_stack=0x7ffe0652a1c8) at ../Python/ceval.c:4372\r\n#11 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n---Type <return> to continue, or q <return> to quit---\r\n#12 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#13 0x000056049a1267b9 in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:669\r\n#14 0x000056049a156bff in run_mod.lto_priv () at ../Python/pythonrun.c:1376\r\n#15 0x000056049a151b52 in PyRun_FileExFlags () at ../Python/pythonrun.c:1362\r\n#16 0x000056049a15169e in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:948\r\n#17 0x000056049a102771 in Py_Main () at ../Modules/main.c:640\r\n#18 0x00007f21cb62d2e1 in __libc_start_main (main=0x56049a1020a0 <main>, argc=6, argv=0x7ffe0652a608, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffe0652a5f8) at ../csu/libc-start.c:291\r\n#19 0x000056049a101f9a in _start ()\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/bug|lang/Python,ericgribkoff,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\nVersion 1.15.0 with Python 2.7 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux, Debian 9.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Python 2.7.13\r\n gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\n\r\n### What did you do?\r\nWe were running speech to text calls in a Python Tornado server.  To avoid memory leaks with channels, we explicitly create the grpc channel to pass to the SpeechClient.  Example:\r\n\r\n```python\r\ncreds = service_account.Credentials.from_service_account_file(self.application_credentials_file)\r\nwith speech_grpc_transport.SpeechGrpcTransport.create_channel(credentials=creds) as channel:\r\n    try:\r\n        transport = speech_grpc_transport.SpeechGrpcTransport(\r\n            address=speech.SpeechClient.SERVICE_ADDRESS,\r\n            channel=channel,\r\n            credentials=None,\r\n        )\r\n                    \r\n        google_client = speech.SpeechClient(transport=transport)\r\n    except Exception:\r\n        return\r\n````\r\n    \r\n### What did you expect to see?\r\nSuccessful closing of the channel.\r\n \r\n### What did you see instead?\r\nOur server works fine in most cases but we had two instances where the process deadlocked.  What appeared to happen was that when the `with` statement triggered the channel close, it initiated a grpc shutdown which triggered the timer thread(s) to exit.  When one of the timer threads was closing it initiated a destruction of the Credentials object which then required access to the Python GIL.  It appears that closing the channel is not releasing the GIL so depending on the timing and reference counting, our Python code can request a shutdown of the channel while still holding the GIL but then be blocked waiting for the timer thread to exit which is waiting on the GIL.  In looking though the Cython files, I didn't see any references to `with nogil` before the `grpc_shutdown` call in the channel _close method (https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi#L395)\r\n\r\n```\r\n(gdb) thread apply all py-list\r\n\r\nThread 7 (Thread 0x7f21b7813700 (LWP 12493)):\r\n 263                            if wacquire is None:\r\n 264                                send(obj)\r\n 265                            else:\r\n 266                                wacquire()\r\n 267                                try:\r\n>268                                    send(obj)\r\n 269                                finally:\r\n 270                                    wrelease()\r\n 271                    except IndexError:\r\n 272                        pass\r\n 273            except Exception, e:\r\n\r\nThread 6 (Thread 0x7f21b8815700 (LWP 12489)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 5 (Thread 0x7f21bb016700 (LWP 12488)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 4 (Thread 0x7f21b8014700 (LWP 12487)):\r\nUnable to locate gdb frame for python bytecode interpreter\r\n\r\nThread 3 (Thread 0x7f21b7012700 (LWP 12486)):\r\n 949            return _StreamStreamMultiCallable(\r\n 950                self._channel, _channel_managed_call_management(self._call_state),\r\n 951                _common.encode(method), request_serializer, response_deserializer)\r\n 952    \r\n 953        def _close(self):\r\n>954            self._channel.close(cygrpc.StatusCode.cancelled, 'Channel closed!')\r\n 955            _moot(self._connectivity_state)\r\n 956    \r\n 957        def _close_on_fork(self):\r\n 958            self._channel.close_on_fork(cygrpc.StatusCode.cancelled,\r\n 959                                        'Channel closed due to fork')\r\n\r\nThread 2 (Thread 0x7f21ae80d700 (LWP 3484)):\r\n 335            waiter.acquire()\r\n 336            self.__waiters.append(waiter)\r\n 337            saved_state = self._release_save()\r\n 338            try:    # restore state no matter what (e.g., KeyboardInterrupt)\r\n 339                if timeout is None:\r\n>340                    waiter.acquire()\r\n 341                    if __debug__:\r\n 342                        self._note(""%s.wait(): got it"", self)\r\n 343                else:\r\n 344                    # Balancing act:  We can't afford a pure busy loop, so we\r\n 345                    # have to sleep; but if we sleep the whole timeout time,\r\n\r\nThread 1 (Thread 0x7f21cc6fb700 (LWP 2729)):\r\n 858                        # clear alarm so it doesn't fire while poll is waiting for\r\n 859                        # events.\r\n 860                        signal.setitimer(signal.ITIMER_REAL, 0, 0)\r\n 861    \r\n 862                    try:\r\n---Type <return> to continue, or q <return> to quit---\r\n>863                        event_pairs = self._impl.poll(poll_timeout)\r\n 864                    except Exception as e:\r\n 865                        # Depending on python version and IOLoop implementation,\r\n 866                        # different exception types may be thrown and there are\r\n 867                        # two ways EINTR might be signaled:\r\n 868                        # * e.errno == errno.EINTR\r\n(gdb) thread apply all bt\r\n\r\nThread 7 (Thread 0x7f21b7813700 (LWP 12493)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x00007f21c8a9c0ed in conn_send_string.isra.2 (string=<optimized out>, length=25) at ./Modules/_multiprocessing/socket_connection.c:122\r\n#6  0x00007f21c8a9caed in connection_send_obj (self=0x56049bd52a40, obj=<optimized out>) at ./Modules/_multiprocessing/connection.h:277\r\n#7  0x000056049a12e510 in call_function (oparg=<optimized out>, pp_stack=0x7f21b7812538) at ../Python/ceval.c:4340\r\n#8  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#9  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#10 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#11 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#12 0x000056049a12f3a0 in ext_do_call (nk=0, na=0, flags=<optimized out>, pp_stack=0x7f21b78127f0, func=<function at remote 0x7f21c5058938>) at ../Python/ceval.c:4666\r\n#13 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#14 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7812938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#15 call_function (oparg=<optimized out>, pp_stack=0x7f21b7812938) at ../Python/ceval.c:4372\r\n#16 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#17 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7812a88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#18 call_function (oparg=<optimized out>, pp_stack=0x7f21b7812a88) at ../Python/ceval.c:4372\r\n#19 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#20 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#21 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#22 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#23 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#24 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#25 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#26 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#27 0x00007f21cc2d8494 in start_thread (arg=0x7f21b7813700) at pthread_create.c:333\r\n#28 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 6 (Thread 0x7f21b8815700 (LWP 12489)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x000056049a207f2f in PyGILState_Ensure () at ../Python/pystate.c:611\r\n#6  0x00007f219d89022e in __pyx_f_4grpc_7_cython_6cygrpc__destroy (__pyx_v_state=0x7f21c3b15250) at src/python/grpcio/grpc/_cython/cygrpc.cpp:18766\r\n#7  0x00007f219d981ca6 in grpc_call_credentials_unref (creds=0x7f2184028aa0) at src/core/lib/security/credentials/credentials.cc:91\r\n#8  0x00007f219d98c126 in ssl_channel_destroy (sc=0x7f2178010ee0) at src/core/lib/security/security_connector/security_connector.cc:654\r\n#9  0x00007f219d914560 in grpc_channel_args_destroy (a=0x7f21780cad70) at src/core/lib/channel/channel_args.cc:205\r\n#10 0x00007f219d9b5ab5 in grpc_subchannel_key_destroy (k=0x7f217800b2f0) at src/core/ext/filters/client_channel/subchannel_index.cc:89\r\n#11 0x00007f219d9b3a88 in subchannel_destroy (arg=0x7f217800ac70, error=<optimized out>) at src/core/ext/filters/client_channel/subchannel.cc:192\r\n#12 0x00007f219d92bb5c in exec_ctx_run (error=0x0, closure=<optimized out>) at src/core/lib/iomgr/exec_ctx.cc:40\r\n#13 grpc_core::ExecCtx::Flush (this=0x7f21b8814ea0) at src/core/lib/iomgr/exec_ctx.cc:134\r\n#14 0x00007f219d93db2a in run_some_timers () at src/core/lib/iomgr/timer_manager.cc:121\r\n#15 timer_main_loop () at src/core/lib/iomgr/timer_manager.cc:222\r\n#16 timer_thread (completed_thread_ptr=0x7f218400d300) at src/core/lib/iomgr/timer_manager.cc:269\r\n#17 0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#18 grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#19 0x00007f21cc2d8494 in start_thread (arg=0x7f21b8815700) at pthread_create.c:333\r\n#20 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\n---Type <return> to continue, or q <return> to quit---\r\nThread 5 (Thread 0x7f21bb016700 (LWP 12488)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f21840964c8, mu=0x7f2184096490, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d92c711 in GrpcExecutor::ThreadMain (arg=0x7f2184096490) at src/core/lib/iomgr/executor.cc:174\r\n#3  0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#4  grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#5  0x00007f21cc2d8494 in start_thread (arg=0x7f21bb016700) at pthread_create.c:333\r\n#6  0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 4 (Thread 0x7f21b8014700 (LWP 12487)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f21840a5b38, mu=0x7f21840a5b00, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d92c711 in GrpcExecutor::ThreadMain (arg=0x7f21840a5b00) at src/core/lib/iomgr/executor.cc:174\r\n#3  0x00007f219d9113bc in grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::operator() (__closure=0x0, v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:100\r\n#4  grpc_core::(anonymous namespace)::ThreadInternalsPosix::__lambda0::_FUN (v=<optimized out>) at src/core/lib/gprpp/thd_posix.cc:103\r\n#5  0x00007f21cc2d8494 in start_thread (arg=0x7f21b8014700) at pthread_create.c:333\r\n#6  0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 3 (Thread 0x7f21b7012700 (LWP 12486)):\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007f219d90f952 in gpr_cv_wait (cv=0x7f219ddf28c0 <g_cv_shutdown>, mu=0x7f219ddf2940 <_ZL4g_mu>, abs_deadline=...) at src/core/lib/gpr/sync_posix.cc:79\r\n#2  0x00007f219d93d737 in stop_threads () at src/core/lib/iomgr/timer_manager.cc:311\r\n#3  0x00007f219d911a4b in grpc_shutdown () at src/core/lib/surface/init.cc:165\r\n#4  0x00007f219d9056cf in __pyx_f_4grpc_7_cython_6cygrpc__close (__pyx_v_channel=__pyx_v_channel@entry=0x7f21c3af8cb0, __pyx_v_code=__pyx_v_code@entry=GRPC_STATUS_CANCELLED, __pyx_v_details=__pyx_v_details@entry='Channel closed!', __pyx_v_drain_calls=False)\r\n    at src/python/grpcio/grpc/_cython/cygrpc.cpp:15637\r\n#5  0x00007f219d906291 in __pyx_pf_4grpc_7_cython_6cygrpc_7Channel_14close (__pyx_v_details='Channel closed!', __pyx_v_code=<optimized out>, __pyx_v_self=0x7f21c3af8cb0) at src/python/grpcio/grpc/_cython/cygrpc.cpp:17646\r\n#6  __pyx_pw_4grpc_7_cython_6cygrpc_7Channel_15close (__pyx_v_self=<grpc._cython.cygrpc.Channel at remote 0x7f21c3af8cb0>, __pyx_args=<optimized out>, __pyx_kwds=<optimized out>) at src/python/grpcio/grpc/_cython/cygrpc.cpp:17624\r\n#7  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7f21b7010db8) at ../Python/ceval.c:4352\r\n#8  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#9  0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7010f08, func=<function at remote 0x7f21c4f97050>) at ../Python/ceval.c:4437\r\n#10 call_function (oparg=<optimized out>, pp_stack=0x7f21b7010f08) at ../Python/ceval.c:4372\r\n#11 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#12 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#13 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#14 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#15 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#16 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#17 0x000056049a11d799 in PyObject_CallFunctionObjArgs () at ../Objects/abstract.c:2774\r\n#18 0x000056049a12b207 in PyEval_EvalFrameEx () at ../Python/ceval.c:2950\r\n#19 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#20 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#21 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#22 0x000056049a12f3a0 in ext_do_call (nk=0, na=1, flags=<optimized out>, pp_stack=0x7f21b70117f0, func=<function at remote 0x7f21c50eb8c0>) at ../Python/ceval.c:4666\r\n#23 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#24 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7011938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#25 call_function (oparg=<optimized out>, pp_stack=0x7f21b7011938) at ../Python/ceval.c:4372\r\n#26 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#27 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21b7011a88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#28 call_function (oparg=<optimized out>, pp_stack=0x7f21b7011a88) at ../Python/ceval.c:4372\r\n#29 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#30 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#31 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#32 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#33 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#34 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n---Type <return> to continue, or q <return> to quit---\r\n#35 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#36 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#37 0x00007f21cc2d8494 in start_thread (arg=0x7f21b7012700) at pthread_create.c:333\r\n#38 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 2 (Thread 0x7f21ae80d700 (LWP 3484)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x7f2198000b10) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x7f2198000b10, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x7f2198000b10, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a1c8742 in lock_PyThread_acquire_lock.lto_priv.2499 () at ../Modules/threadmodule.c:52\r\n#5  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c118) at ../Python/ceval.c:4352\r\n#6  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#7  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#8  0x000056049a12e448 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c328, func=<function at remote 0x7f21c96ce320>) at ../Python/ceval.c:4447\r\n#9  call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c328) at ../Python/ceval.c:4372\r\n#10 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#11 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#12 0x000056049a12e448 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c538, func=<function at remote 0x7f21c9581d70>) at ../Python/ceval.c:4447\r\n#13 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c538) at ../Python/ceval.c:4372\r\n#14 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#15 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#16 0x000056049a143778 in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#17 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#18 0x000056049a12f3a0 in ext_do_call (nk=0, na=0, flags=<optimized out>, pp_stack=0x7f21ae80c7f0, func=<function at remote 0x7f21c50d2c08>) at ../Python/ceval.c:4666\r\n#19 PyEval_EvalFrameEx () at ../Python/ceval.c:3028\r\n#20 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80c938, func=<function at remote 0x7f21c96ceed8>) at ../Python/ceval.c:4437\r\n#21 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80c938) at ../Python/ceval.c:4372\r\n#22 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#23 0x000056049a12e14f in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7f21ae80ca88, func=<function at remote 0x7f21c964f0c8>) at ../Python/ceval.c:4437\r\n#24 call_function (oparg=<optimized out>, pp_stack=0x7f21ae80ca88) at ../Python/ceval.c:4372\r\n#25 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#26 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#27 0x000056049a1435be in function_call.lto_priv () at ../Objects/funcobject.c:523\r\n#28 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#29 0x000056049a159e1e in instancemethod_call.lto_priv () at ../Objects/classobject.c:2602\r\n#30 0x000056049a1150c3 in PyObject_Call () at ../Objects/abstract.c:2547\r\n#31 0x000056049a132a10 in PyEval_CallObjectWithKeywords () at ../Python/ceval.c:4221\r\n#32 0x000056049a1fb722 in t_bootstrap () at ../Modules/threadmodule.c:620\r\n#33 0x00007f21cc2d8494 in start_thread (arg=0x7f21ae80d700) at pthread_create.c:333\r\n#34 0x00007f21cb6f5acf in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:97\r\n\r\nThread 1 (Thread 0x7f21cc6fb700 (LWP 2729)):\r\n#0  0x00007f21cc2e0536 in futex_abstimed_wait_cancelable (private=0, abstime=0x0, expected=0, futex_word=0x56049af80310) at ../sysdeps/unix/sysv/linux/futex-internal.h:205\r\n#1  do_futex_wait (sem=sem@entry=0x56049af80310, abstime=0x0) at sem_waitcommon.c:111\r\n#2  0x00007f21cc2e05e4 in __new_sem_wait_slow (sem=0x56049af80310, abstime=0x0) at sem_waitcommon.c:181\r\n#3  0x000056049a103a04 in PyThread_acquire_lock () at ../Python/thread_pthread.h:324\r\n#4  0x000056049a14f83a in PyEval_RestoreThread () at ../Python/ceval.c:359\r\n#5  0x000056049a22007d in pyepoll_poll.lto_priv () at ../Modules/selectmodule.c:1035\r\n#6  0x000056049a12884a in call_function (oparg=<optimized out>, pp_stack=0x7ffe06529fb8) at ../Python/ceval.c:4352\r\n#7  PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n#8  0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#9  0x000056049a12eb78 in fast_function (nk=0, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe0652a1c8, func=<function at remote 0x7f21c8cb5e60>) at ../Python/ceval.c:4447\r\n#10 call_function (oparg=<optimized out>, pp_stack=0x7ffe0652a1c8) at ../Python/ceval.c:4372\r\n#11 PyEval_EvalFrameEx () at ../Python/ceval.c:2989\r\n---Type <return> to continue, or q <return> to quit---\r\n#12 0x000056049a1269f5 in PyEval_EvalCodeEx () at ../Python/ceval.c:3584\r\n#13 0x000056049a1267b9 in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:669\r\n#14 0x000056049a156bff in run_mod.lto_priv () at ../Python/pythonrun.c:1376\r\n#15 0x000056049a151b52 in PyRun_FileExFlags () at ../Python/pythonrun.c:1362\r\n#16 0x000056049a15169e in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:948\r\n#17 0x000056049a102771 in Py_Main () at ../Modules/main.c:640\r\n#18 0x00007f21cb62d2e1 in __libc_start_main (main=0x56049a1020a0 <main>, argc=6, argv=0x7ffe0652a608, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffe0652a5f8) at ../csu/libc-start.c:291\r\n#19 0x000056049a101f9a in _start ()\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","python\r\ncreds = service_account.Credentials.from_service_account_file(self.application_credentials_file)\r\nwith speech_grpc_transport.SpeechGrpcTransport.create_channel(credentials=creds) as channel:\r\n    try:\r\n        transport = speech_grpc_transport.SpeechGrpcTransport(\r\n            address=speech.SpeechClient.SERVICE_ADDRESS,\r\n            channel=channel,\r\n            credentials=None,\r\n        )\r\n                    \r\n        google_client = speech.SpeechClient(transport=transport)\r\n    except Exception:\r\n        return\r\n"
16939,"Objective-C: Cancelling a GRPCCall crashes the app on applicationDidEnterBackground<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nUsing gRPC 1.14.1 in Objective-C\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\niOS 11.4, 11.3, 11.2, 12.1, 10.3\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nApple LLVM version 10.0.0 (clang-1000.11.45.5)\r\nTarget: x86_64-apple-darwin18.0.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n### What did you do?\r\n\r\nWe basically start a streaming call when the app enters foreground and cancel it when it enters background.\r\n\r\nA lot of our users experience a crash when `applicationDidEnterBackground` gets called.\r\n\r\nIn `AppDelegate.swift`\r\n\r\n\r\nIn `StreamingController.swift`\r\n\r\n\r\n### What did you expect to see?\r\n \r\n I expect the stream to get cancelled.\r\n\r\n### What did you see instead?\r\n \r\nThe app crashes.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope\r\n",kind/bug|lang/ObjC,muxi,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nUsing gRPC 1.14.1 in Objective-C\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\niOS 11.4, 11.3, 11.2, 12.1, 10.3\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nApple LLVM version 10.0.0 (clang-1000.11.45.5)\r\nTarget: x86_64-apple-darwin18.0.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n### What did you do?\r\n\r\nWe basically start a streaming call when the app enters foreground and cancel it when it enters background.\r\n\r\nA lot of our users experience a crash when `applicationDidEnterBackground` gets called.\r\n\r\nIn `AppDelegate.swift`\r\n```swift\r\n@objc private func applicationDidEnterBackground() {\r\n    streamingController.cancel()\r\n}\r\n\r\n@objc private func applicationWillEnterForeground() {\r\n    streamingController.stream()\r\n}\r\n```\r\n\r\nIn `StreamingController.swift`\r\n```swift\r\npublic func stream() {\r\n    let writer = GRXBufferedPipe()\r\n    let call = client.rpcToStream(withWriter: writer) { [weak self] (done, response, error) in\r\n        // Omited code\r\n    }\r\n    streamingCall = call\r\n}\r\n\r\npublic func cancel() {\r\n    streamingCall?.cancel()\r\n    streamingCall = nil\r\n}\r\n```\r\n\r\n### What did you expect to see?\r\n \r\n I expect the stream to get cancelled.\r\n\r\n### What did you see instead?\r\n \r\nThe app crashes.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope\r\n",swift\r\n@objc private func applicationDidEnterBackground() {\r\n    streamingController.cancel()\r\n}\r\n\r\n@objc private func applicationWillEnterForeground() {\r\n    streamingController.stream()\r\n}\r\n
16895,"c# cancellationToken ignored on a simple call<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n 1.15.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Windows 10 (Server)\r\nAndroid (Xamarin Forms) (Client)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Visual Studio 2017\r\n \r\n### What did you do?\r\nI try to cancel a simple rpc call from the client side, when the server disconnected/is unavailable, to avoid long timeout.\r\n \r\n### What did you expect to see?\r\n When the cancellationtoken is cancelled i expect it to return instantly (with an error).\r\n \r\n### What did you see instead?\r\n The call times out, it seems the token was ignored. I verified, that the token was set to cancel.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nA few code extracts:\r\n\r\n\r\n\r\n\r\n```proto\r\nservice GrpcMultilinkGateway\r\n{\r\n\trpc GrpcGetSim1Config(Empty) returns (GrpcSimConfig){}\r\n}\r\n```\r\n\r\nEdit:\r\nThe rpc exception contains a cancellationerror (which is correct) but simply takes to long to throw. To me it looks like a normal timeout occurs, and after that instead of a timeout error a cancellation error was thrown.\r\n```\r\n[0:] Grpc.Core.RpcException: Status(StatusCode=Cancelled, Detail=""Cancelled"")\r\n  at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Threading.Tasks.Task task) [0x0003e] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Threading.Tasks.Task task) [0x00028] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter.ValidateEnd (System.Threading.Tasks.Task task) [0x00008] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter`1[TResult].GetResult () [0x00000] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at Amc.ViewModels.AtosMissionCube.MultiLinkGateway.Sim.Sim1ScopeViewModel+<LoadItems>d__2.MoveNext () [0x00043] in C:\\git\\Atos.Set.Tec.Dfs.Mgmt\\src\\frontend\\Amc\\Amc\\ViewModels\\AtosMissionCube\\MultiLinkGateway\\Sim\\Sim1ScopeViewModel.cs:28 \r\n```",kind/question|lang/C#|priority/P2|disposition/stale,jtattermusch,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n 1.15.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Windows 10 (Server)\r\nAndroid (Xamarin Forms) (Client)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Visual Studio 2017\r\n \r\n### What did you do?\r\nI try to cancel a simple rpc call from the client side, when the server disconnected/is unavailable, to avoid long timeout.\r\n \r\n### What did you expect to see?\r\n When the cancellationtoken is cancelled i expect it to return instantly (with an error).\r\n \r\n### What did you see instead?\r\n The call times out, it seems the token was ignored. I verified, that the token was set to cancel.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nA few code extracts:\r\n```csharp\r\n            GrpcSimConfig config;\r\n            try\r\n            {\r\n                // TODO add option to cancel this\r\n                config = await _client.GrpcGetSim1ConfigAsync(new Empty(), cancellationToken: token);\r\n                if (token.IsCancellationRequested)\r\n                {\r\n                    return false;\r\n                }\r\n            }\r\n            catch (RpcException)\r\n            {\r\n                return false;\r\n            }\r\n```\r\n\r\n```csharp\r\n            // Show a confirmation popup and quit the config view.\r\n            var result = await DisplayAlert(""Unsaved changes!"", ""Do you really want to cancel?"", ""Yes"", ""No"");\r\n            if (result)\r\n            {\r\n                _cancellationTokenSource.Cancel();\r\n                await Navigation.PopModalAsync();\r\n            }\r\n```\r\n\r\n```proto\r\nservice GrpcMultilinkGateway\r\n{\r\n\trpc GrpcGetSim1Config(Empty) returns (GrpcSimConfig){}\r\n}\r\n```\r\n\r\nEdit:\r\nThe rpc exception contains a cancellationerror (which is correct) but simply takes to long to throw. To me it looks like a normal timeout occurs, and after that instead of a timeout error a cancellation error was thrown.\r\n```\r\n[0:] Grpc.Core.RpcException: Status(StatusCode=Cancelled, Detail=""Cancelled"")\r\n  at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess (System.Threading.Tasks.Task task) [0x0003e] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification (System.Threading.Tasks.Task task) [0x00028] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter.ValidateEnd (System.Threading.Tasks.Task task) [0x00008] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at System.Runtime.CompilerServices.TaskAwaiter`1[TResult].GetResult () [0x00000] in <43dbbdc147f2482093d8409abb04c233>:0 \r\n  at Amc.ViewModels.AtosMissionCube.MultiLinkGateway.Sim.Sim1ScopeViewModel+<LoadItems>d__2.MoveNext () [0x00043] in C:\\git\\Atos.Set.Tec.Dfs.Mgmt\\src\\frontend\\Amc\\Amc\\ViewModels\\AtosMissionCube\\MultiLinkGateway\\Sim\\Sim1ScopeViewModel.cs:28 \r\n```","csharp\r\n            GrpcSimConfig config;\r\n            try\r\n            {\r\n                // TODO add option to cancel this\r\n                config = await _client.GrpcGetSim1ConfigAsync(new Empty(), cancellationToken: token);\r\n                if (token.IsCancellationRequested)\r\n                {\r\n                    return false;\r\n                }\r\n            }\r\n            catch (RpcException)\r\n            {\r\n                return false;\r\n            }\r\n"
16877,"C++: Asan reports ""heap use after free"" when a `ClientAsyncReaderWriter` is destroyed after `ClientContext`### What version of gRPC and what language are you using?\r\n \r\n 1.14.2, C++.\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nRunning iPhone 8 emulator from MacOS 10.13.6.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nXCode Version 9.4.1\r\n\r\n### Summary\r\n\r\n(using `GenericClientAsyncReaderWriter` for the sake of demonstration, but it's likely that any of the related classes will trigger the issue)\r\n\r\nWhen an `ClientAsyncReaderWriter` is being destroyed after the corresponding `ClientContext`, its the reader-writer's memory is already released by `ClientContext`'s destructor, resulting in an access to a dangling pointer. This is flagged by Address Sanitizer.\r\n\r\nMinimal repro:\r\n\r\n\r\n### Details\r\n\r\nAs far as I understand the issue (see full Asan report [here](https://gist.github.com/var-const/a80bb5863ca73f37d7ceede8eaa61cf6)), the memory used by the underlying `grpc_call` is allocated in an arena (`gpr_arena_create`); the creation of the arena is triggered by `GenericStub::PrepareCall`. `grpc_call` is reference-counted. However, the `ClientContext` does hold a reference to the call, while the `GenericClientAsyncReaderWriter` does not -- in fact, its `operator delete` is explicitly a [no-op](https://github.com/grpc/grpc/blob/master/include/grpcpp/impl/codegen/async_stream.h#L517-L520) (this is a little surprising given that the factory function returns a `std::unique_ptr` to the `GenericClientAsyncReaderWriter` -- I'd expect a raw pointer). Consequently, when `ClientContext` is being destroyed, it releases the last known ref to the `grpc_call`, triggering invocation of `gpr_arena_destroy`. Then, when `GenericClientAsyncReaderWriter` is being destroyed, the expression `delete ptr`, where `ptr` is the pointer to the `GenericClientAsyncReaderWriter` object already deallocated alongside the whole arena, triggers the Address Sanitizer. Note that Asan doesn't take into account the fact that `operator delete` is essentially a no-op in this case.\r\n\r\nThis makes it impossible to get a clean run under ASan in a program where the destruction order happens to be ""wrong"".",lang/c++,vjpai,"### What version of gRPC and what language are you using?\r\n \r\n 1.14.2, C++.\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nRunning iPhone 8 emulator from MacOS 10.13.6.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nXCode Version 9.4.1\r\n\r\n### Summary\r\n\r\n(using `GenericClientAsyncReaderWriter` for the sake of demonstration, but it's likely that any of the related classes will trigger the issue)\r\n\r\nWhen an `ClientAsyncReaderWriter` is being destroyed after the corresponding `ClientContext`, its the reader-writer's memory is already released by `ClientContext`'s destructor, resulting in an access to a dangling pointer. This is flagged by Address Sanitizer.\r\n\r\nMinimal repro:\r\n```cpp\r\n// Necessary setup\r\nstd::shared_ptr<grpc::Channel> channel =\r\n    grpc::CreateChannel("""", grpc::InsecureChannelCredentials());\r\ngrpc::GenericStub stub{channel};\r\ngrpc::CompletionQueue queue;\r\n\r\nstd::unique_ptr<grpc::ClientContext> context{new grpc::ClientContext{}};\r\nstd::unique_ptr<grpc::GenericClientAsyncReaderWriter> call =\r\n    stub.PrepareCall(context.get(), """", &queue);\r\n\r\ncontext.reset();\r\ncall.reset(); // Will trigger Asan\r\n```\r\n\r\n### Details\r\n\r\nAs far as I understand the issue (see full Asan report [here](https://gist.github.com/var-const/a80bb5863ca73f37d7ceede8eaa61cf6)), the memory used by the underlying `grpc_call` is allocated in an arena (`gpr_arena_create`); the creation of the arena is triggered by `GenericStub::PrepareCall`. `grpc_call` is reference-counted. However, the `ClientContext` does hold a reference to the call, while the `GenericClientAsyncReaderWriter` does not -- in fact, its `operator delete` is explicitly a [no-op](https://github.com/grpc/grpc/blob/master/include/grpcpp/impl/codegen/async_stream.h#L517-L520) (this is a little surprising given that the factory function returns a `std::unique_ptr` to the `GenericClientAsyncReaderWriter` -- I'd expect a raw pointer). Consequently, when `ClientContext` is being destroyed, it releases the last known ref to the `grpc_call`, triggering invocation of `gpr_arena_destroy`. Then, when `GenericClientAsyncReaderWriter` is being destroyed, the expression `delete ptr`, where `ptr` is the pointer to the `GenericClientAsyncReaderWriter` object already deallocated alongside the whole arena, triggers the Address Sanitizer. Note that Asan doesn't take into account the fact that `operator delete` is essentially a no-op in this case.\r\n\r\nThis makes it impossible to get a clean run under ASan in a program where the destruction order happens to be ""wrong"".","cpp\r\n// Necessary setup\r\nstd::shared_ptr<grpc::Channel> channel =\r\n    grpc::CreateChannel("""", grpc::InsecureChannelCredentials());\r\ngrpc::GenericStub stub{channel};\r\ngrpc::CompletionQueue queue;\r\n\r\nstd::unique_ptr<grpc::ClientContext> context{new grpc::ClientContext{}};\r\nstd::unique_ptr<grpc::GenericClientAsyncReaderWriter> call =\r\n    stub.PrepareCall(context.get(), """", &queue);\r\n\r\ncontext.reset();\r\ncall.reset(); // Will trigger Asan\r\n"
16875,"C++: assertion failure when destroying a `ByteBuffer` after `CompletionQueue`### What version of gRPC and what language are you using?\r\n \r\n 1.14.2, C++.\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nRunning iPhone 8 emulator from MacOS 10.13.6.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nXCode Version 9.4.1\r\n\r\n### Additional details\r\n\r\nIn the configuration I am running, pthreads are used (the implementation of `gpr_tls_set` is the one from `tls_pthread.cc`).\r\n \r\n### Summary\r\n\r\n(I'm using `grpc::CompletionQueue` for the sake of demonstration, but any gRPC object inheriting from `GrpcLibraryCodegen` should do).\r\n\r\nShortest repro:\r\n\r\n\r\nIn short, there is a surprising (to me, at least) implicit lifetime dependency between `grpc::ByteBuffer` and objects inheriting from `GrpcLibraryCodegen`. If a `grpc::ByteBuffer` happens to be destroyed after the last object inheriting from `GrpcLibraryCodegen`, the buffer's destructor will trigger an assertion and crash.\r\n\r\n### Details\r\n\r\nObjects inheriting from `GrpcLibraryCodegen` essentially make C core reference-counted; each constructor increments, and each destructor decrements, the number of references to C core, and once the last reference is destroyed, the C core is shut down. The global shutdown, among other things, [shuts down](https://github.com/grpc/grpc/blob/master/src/core/lib/surface/init.cc#L183) [`ExecCtx`](https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/exec_ctx.h#L196). However, the destructor of `ByteBuffer` [creates an `ExecCtx`](https://github.com/grpc/grpc/blob/master/src/core/lib/surface/byte_buffer.cc#L77). Consequently, if the destructor of `ByteBuffer` happens to be called after gRPC core has been shut down, this will fail because global shutdown has already been called on `ExecCtx`, leading to an [assertion failure](https://github.com/grpc/grpc/blob/master/src/core/lib/gpr/tls_pthread.cc#L26) and a crash.\r\n\r\nThis issue currently may trigger a crash in Firestore iOS client.\r\n",kind/bug|lang/c++|priority/P2|disposition/stale,vjpai,"### What version of gRPC and what language are you using?\r\n \r\n 1.14.2, C++.\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nRunning iPhone 8 emulator from MacOS 10.13.6.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nXCode Version 9.4.1\r\n\r\n### Additional details\r\n\r\nIn the configuration I am running, pthreads are used (the implementation of `gpr_tls_set` is the one from `tls_pthread.cc`).\r\n \r\n### Summary\r\n\r\n(I'm using `grpc::CompletionQueue` for the sake of demonstration, but any gRPC object inheriting from `GrpcLibraryCodegen` should do).\r\n\r\nShortest repro:\r\n```cpp\r\n{\r\n  grpc::Slice slice{""foo""};\r\n  grpc::ByteBuffer b{&slice, 1}; // Buffer must be non-empty for the repro's purposes\r\n  grpc::CompletionQueue cq; // Assuming it's the only GrpcLibraryCodegen-inheriting object around\r\n} // Once the scope ends, [tls_pthread.cc:27] assertion failed: 0 == result\r\n```\r\n\r\nIn short, there is a surprising (to me, at least) implicit lifetime dependency between `grpc::ByteBuffer` and objects inheriting from `GrpcLibraryCodegen`. If a `grpc::ByteBuffer` happens to be destroyed after the last object inheriting from `GrpcLibraryCodegen`, the buffer's destructor will trigger an assertion and crash.\r\n\r\n### Details\r\n\r\nObjects inheriting from `GrpcLibraryCodegen` essentially make C core reference-counted; each constructor increments, and each destructor decrements, the number of references to C core, and once the last reference is destroyed, the C core is shut down. The global shutdown, among other things, [shuts down](https://github.com/grpc/grpc/blob/master/src/core/lib/surface/init.cc#L183) [`ExecCtx`](https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/exec_ctx.h#L196). However, the destructor of `ByteBuffer` [creates an `ExecCtx`](https://github.com/grpc/grpc/blob/master/src/core/lib/surface/byte_buffer.cc#L77). Consequently, if the destructor of `ByteBuffer` happens to be called after gRPC core has been shut down, this will fail because global shutdown has already been called on `ExecCtx`, leading to an [assertion failure](https://github.com/grpc/grpc/blob/master/src/core/lib/gpr/tls_pthread.cc#L26) and a crash.\r\n\r\nThis issue currently may trigger a crash in Firestore iOS client.\r\n","cpp\r\n{\r\n  grpc::Slice slice{""foo""};\r\n  grpc::ByteBuffer b{&slice, 1}; // Buffer must be non-empty for the repro's purposes\r\n  grpc::CompletionQueue cq; // Assuming it's the only GrpcLibraryCodegen-inheriting object around\r\n} // Once the scope ends, [tls_pthread.cc:27] assertion failed: 0 == result\r\n"
16786,"Get response message for a rpc call that ends in a non-OK status code using c# client.<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\npython server [v1.9.1](https://github.com/grpc/grpc/tree/v1.9.1) \r\nc++ client [v1.2.5](https://github.com/grpc/grpc/tree/v1.2.5)\r\nc# client [v1.5.0](https://github.com/grpc/grpc/tree/v1.15.0) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\npython server: aws ec2 Ubuntu 16.04.2 LTS\r\nc++ client: Windows 7-10\r\nc# client: Windows 10\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\npython server: 3.6.1\r\nc++ client: VC++ 2013\r\nc# client: .NET Framework 4.7.2 \r\n \r\n### What did you do?\r\nI have service definition like to following:\r\n```protobuf\r\nsyntax = ""proto3"";\r\n\r\n// Object that is saved as row in database.\r\nmessage Foo {\r\n    // various members that need validation before saved to database by server.\r\n}\r\n\r\n// Generic message that holds errors that occured during a request.\r\nmessage Errors {\r\n\r\n}\r\n\r\nmessage CreateFooResponse {\r\n    oneof response {\r\n        // The object as it now exists in the database.\r\n        Foo foo = 1;\r\n\r\n        // Validation/other errors that occured during the attempt to save.\r\n        Errors errors = 2;\r\n    }\r\n}\r\n\r\nservice FooService {\r\n    // Create new Foo object.\r\n    rpc Create(Foo) returns (CreateFooResponse) {}\r\n\r\n    // Get/List/Update/Delete rpc methods ommited\r\n}\r\n```\r\n\r\nThe service is implemented in python like:\r\n\r\n \r\n### What did you expect to see?\r\nWhen a call is made using the c++ client and server side validation fails resulting in a status code of something other than OK, I am able to access both the [::grpc::Status](https://github.com/grpc/grpc/blob/v1.2.5/include/grpc%2B%2B/impl/codegen/status.h) object and the response message that contains the details of exactly what went wrong.  This is great and working as expected.\r\n \r\n### What did you see instead?\r\nWhen a call is made using the c# client and server side validation fails resulting in a status code of something other than OK, a [Grpc.Core.RpcException](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/RpcException.cs) is thrown and I can access the [Grpc.Core.Status](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/Status.cs) for the request but NOT the response message that contains the details of exactly what went wrong.\r\n \r\n### Anything else we should know about your project / environment?\r\nThis has come up because the company I work for is starting to expose this service to a third party.  We have been using it internally for quite a while with the c++ client and have had no issues.  However this third party is working in c# and as you can see they can't access the response message when something goes wrong.  Now I have not worked with c# before, but I have replicated the issue and after digging through docs/source am not seeing an obvious way around the issue.  Is there a way to get the response message from a request that ends in a non-OK status code that I missed?\r\n\r\nFrom the c# source I'm seeing the method HandleUnaryResponse in the [Grpc.Core.Internal.AsyncCall](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/Internal/AsyncCall.cs).  At the start of the method the response message is deserialized and if something went wrong it is handled.  But then at the end of the method I see:\r\n\r\n\r\nIt seem that the response message (which has been deserialized and would be valid as far as I can tell) is just discarded if the status code is anything but OK.  I understand throwing when something goes wrong but might it be possible to attach the response message to the thrown [Grpc.Core.RpcException](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/RpcException.cs)?  I mean at that point we have already incurred the overhead of deserializing the response message and as far as I know it wouldn't cost much to keep it around for a little longer.  Then the above code would be something like:\r\n\r\n\r\nA client could then have something like:\r\n\r\n\r\nWhile reworking the python server and c++ client used by my company is possible it would be quite a bit of work.  So I would like to hold off on it until:\r\n\r\n* I know if there is an existing way for a c# client to get the response message for a request that ends in a non-OK status code.\r\n* If it would be possible to make a change that would allow this.\r\n\r\nAs a sanity check for me, I'm wondering if it common practice to discard the response message for a request that ends in a non-OK status code for implementations in other languages?  Am I just implementing the service in an unintended way?  If so I would appreciate feedback on how it should be done so as not to run into issues like this in the future.",kind/enhancement|lang/C#|priority/P2,jtattermusch,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\npython server [v1.9.1](https://github.com/grpc/grpc/tree/v1.9.1) \r\nc++ client [v1.2.5](https://github.com/grpc/grpc/tree/v1.2.5)\r\nc# client [v1.5.0](https://github.com/grpc/grpc/tree/v1.15.0) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\npython server: aws ec2 Ubuntu 16.04.2 LTS\r\nc++ client: Windows 7-10\r\nc# client: Windows 10\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\npython server: 3.6.1\r\nc++ client: VC++ 2013\r\nc# client: .NET Framework 4.7.2 \r\n \r\n### What did you do?\r\nI have service definition like to following:\r\n```protobuf\r\nsyntax = ""proto3"";\r\n\r\n// Object that is saved as row in database.\r\nmessage Foo {\r\n    // various members that need validation before saved to database by server.\r\n}\r\n\r\n// Generic message that holds errors that occured during a request.\r\nmessage Errors {\r\n\r\n}\r\n\r\nmessage CreateFooResponse {\r\n    oneof response {\r\n        // The object as it now exists in the database.\r\n        Foo foo = 1;\r\n\r\n        // Validation/other errors that occured during the attempt to save.\r\n        Errors errors = 2;\r\n    }\r\n}\r\n\r\nservice FooService {\r\n    // Create new Foo object.\r\n    rpc Create(Foo) returns (CreateFooResponse) {}\r\n\r\n    // Get/List/Update/Delete rpc methods ommited\r\n}\r\n```\r\n\r\nThe service is implemented in python like:\r\n```python\r\nclass FooService(FooServiceServicer):\r\n\r\n    def Create(self, request, context):\r\n        response = CreateFooResponse()\r\n        try:\r\n            model = message_to_model(request) # convert foo object to model\r\n            model.validate() # exception thrown if validation fails\r\n            model.save() # exception thrown if save fails\r\n            response.foo.CopyFrom(model_to_message(model))\r\n            return response\r\n        except ValidationError as e:\r\n            context.set_code(StatusCode.INVALID_ARGUMENT)\r\n            context.set_details('The provided foo is not valid.  See errors for details.')\r\n            response.errors.CopyFrom(exception_to_proto(e))\r\n            return response\r\n        # other exception handling omitted for brevity\r\n```\r\n \r\n### What did you expect to see?\r\nWhen a call is made using the c++ client and server side validation fails resulting in a status code of something other than OK, I am able to access both the [::grpc::Status](https://github.com/grpc/grpc/blob/v1.2.5/include/grpc%2B%2B/impl/codegen/status.h) object and the response message that contains the details of exactly what went wrong.  This is great and working as expected.\r\n \r\n### What did you see instead?\r\nWhen a call is made using the c# client and server side validation fails resulting in a status code of something other than OK, a [Grpc.Core.RpcException](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/RpcException.cs) is thrown and I can access the [Grpc.Core.Status](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/Status.cs) for the request but NOT the response message that contains the details of exactly what went wrong.\r\n \r\n### Anything else we should know about your project / environment?\r\nThis has come up because the company I work for is starting to expose this service to a third party.  We have been using it internally for quite a while with the c++ client and have had no issues.  However this third party is working in c# and as you can see they can't access the response message when something goes wrong.  Now I have not worked with c# before, but I have replicated the issue and after digging through docs/source am not seeing an obvious way around the issue.  Is there a way to get the response message from a request that ends in a non-OK status code that I missed?\r\n\r\nFrom the c# source I'm seeing the method HandleUnaryResponse in the [Grpc.Core.Internal.AsyncCall](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/Internal/AsyncCall.cs).  At the start of the method the response message is deserialized and if something went wrong it is handled.  But then at the end of the method I see:\r\n```csharp\r\nvar status = receivedStatus.Status;\r\nif (status.StatusCode != StatusCode.OK)\r\n{\r\n    unaryResponseTcs.SetException(new RpcException(status, receivedStatus.Trailers));\r\n    return;\r\n}\r\n\r\nunaryResponseTcs.SetResult(msg);\r\n```\r\n\r\nIt seem that the response message (which has been deserialized and would be valid as far as I can tell) is just discarded if the status code is anything but OK.  I understand throwing when something goes wrong but might it be possible to attach the response message to the thrown [Grpc.Core.RpcException](https://github.com/grpc/grpc/blob/v1.15.0/src/csharp/Grpc.Core/RpcException.cs)?  I mean at that point we have already incurred the overhead of deserializing the response message and as far as I know it wouldn't cost much to keep it around for a little longer.  Then the above code would be something like:\r\n```csharp\r\nvar status = receivedStatus.Status;\r\nif (status.StatusCode != StatusCode.OK)\r\n{\r\n    // add the response message a third parameter to RpcException constructor.\r\n    unaryResponseTcs.SetException(new RpcException(status, receivedStatus.Trailers, msg));\r\n    return;\r\n}\r\n\r\nunaryResponseTcs.SetResult(msg);\r\n```\r\n\r\nA client could then have something like:\r\n```csharp\r\ntry\r\n{\r\n    // make rpc call\r\n}\r\ncatch (RpcException e)\r\n{\r\n    // e.Response would be of type System.Object to accommodate any type without having\r\n    // to create something like 'class RpcException<TResponse> : RpcException {}'\r\n    var response = e.Response as ResponseTypeExpectedForRpcCall; // cast response to expected type.\r\n    if (response != null)\r\n    {\r\n        // do something with response message.\r\n    }\r\n    else if (e.Response != null)\r\n    {\r\n        // something was set on e.Response but it was not the type the programmer expected it to be.\r\n    }\r\n    else\r\n    {\r\n        // exception was thrown when response was not available.\r\n    }\r\n}\r\n```\r\n\r\nWhile reworking the python server and c++ client used by my company is possible it would be quite a bit of work.  So I would like to hold off on it until:\r\n\r\n* I know if there is an existing way for a c# client to get the response message for a request that ends in a non-OK status code.\r\n* If it would be possible to make a change that would allow this.\r\n\r\nAs a sanity check for me, I'm wondering if it common practice to discard the response message for a request that ends in a non-OK status code for implementations in other languages?  Am I just implementing the service in an unintended way?  If so I would appreciate feedback on how it should be done so as not to run into issues like this in the future.","python\r\nclass FooService(FooServiceServicer):\r\n\r\n    def Create(self, request, context):\r\n        response = CreateFooResponse()\r\n        try:\r\n            model = message_to_model(request) # convert foo object to model\r\n            model.validate() # exception thrown if validation fails\r\n            model.save() # exception thrown if save fails\r\n            response.foo.CopyFrom(model_to_message(model))\r\n            return response\r\n        except ValidationError as e:\r\n            context.set_code(StatusCode.INVALID_ARGUMENT)\r\n            context.set_details('The provided foo is not valid.  See errors for details.')\r\n            response.errors.CopyFrom(exception_to_proto(e))\r\n            return response\r\n        # other exception handling omitted for brevity\r\n"
16749,"Running a simple client-server in the same process triggers an error with TSAN<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\ngrpc 1.10.0 / C++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\ng++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n \r\n### What did you do?\r\n\r\nRunning a client-server in the same processes triggers a TSAN error.\r\n\r\nI attached a minimal project with a Docker file that can be used to reproduce the error. To reproduce run,\r\n\r\n\r\n [grpc-tsan.zip](https://github.com/grpc/grpc/files/2435349/grpc-tsan.zip)\r\n\r\n### What did you expect to see?\r\n \r\nNo tsan error\r\n \r\n### What did you see instead?\r\n\r\n```\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Atomic read of size 1 at 0x7b3800007000 by main thread:\r\n    #0 pthread_mutex_lock <null> (libtsan.so.0+0x3faeb)\r\n    #1 gpr_mu_lock <null> (libgrpc.so.6+0xe0e68)\r\n\r\n  Previous write of size 8 at 0x7b3800007000 by thread T1 (mutexes: write M286, write M284):\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Location is heap block of size 216 at 0x7b3800007000 allocated by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Mutex M286 (0x7b4c00010018) created at:\r\n    #0 pthread_mutex_init <null> (libtsan.so.0+0x2c5bd)\r\n    #1 gpr_mu_init <null> (libgrpc.so.6+0xe0dea)\r\n\r\n  Mutex M284 (0x7b28000051e8) created at:\r\n    #0 pthread_mutex_init <null> (libtsan.so.0+0x2c5bd)\r\n    #1 gpr_mu_init <null> (libgrpc.so.6+0xe0dea)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x3faeb) in __interceptor_pthread_mutex_lock\r\n==================\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Write of size 8 at 0x7b0800001600 by main thread:\r\n    #0 free <null> (libtsan.so.0+0x2b146)\r\n    #1 grpc_connectivity_state_set(grpc_connectivity_state_tracker*, grpc_connectivity_state, grpc_error*, char const*) <null> (libgrpc.so.6+0x8db6a)\r\n\r\n  Previous write of size 8 at 0x7b0800001600 by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x2b146) in free\r\n==================\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Write of size 8 at 0x7b10000024c0 by main thread:\r\n    #0 free <null> (libtsan.so.0+0x2b146)\r\n    #1 grpc_combiner_continue_exec_ctx() <null> (libgrpc.so.6+0x5fc6b)\r\n\r\n  Previous write of size 8 at 0x7b10000024c0 by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x2b146) in free\r\n==================\r\n```\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\n\r\n",kind/bug|lang/c++|priority/P2|disposition/stale,yashykt,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\ngrpc 1.10.0 / C++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\ng++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n \r\n### What did you do?\r\n\r\nRunning a client-server in the same processes triggers a TSAN error.\r\n\r\nI attached a minimal project with a Docker file that can be used to reproduce the error. To reproduce run,\r\n\r\n```bash\r\ndocker build -t grpc-tsan .\r\ndocker run -v $PWD:/src -w /src -it grpc-tsan /bin/bash -c ""make;./clientserver""\r\n```\r\n [grpc-tsan.zip](https://github.com/grpc/grpc/files/2435349/grpc-tsan.zip)\r\n\r\n### What did you expect to see?\r\n \r\nNo tsan error\r\n \r\n### What did you see instead?\r\n\r\n```\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Atomic read of size 1 at 0x7b3800007000 by main thread:\r\n    #0 pthread_mutex_lock <null> (libtsan.so.0+0x3faeb)\r\n    #1 gpr_mu_lock <null> (libgrpc.so.6+0xe0e68)\r\n\r\n  Previous write of size 8 at 0x7b3800007000 by thread T1 (mutexes: write M286, write M284):\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Location is heap block of size 216 at 0x7b3800007000 allocated by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Mutex M286 (0x7b4c00010018) created at:\r\n    #0 pthread_mutex_init <null> (libtsan.so.0+0x2c5bd)\r\n    #1 gpr_mu_init <null> (libgrpc.so.6+0xe0dea)\r\n\r\n  Mutex M284 (0x7b28000051e8) created at:\r\n    #0 pthread_mutex_init <null> (libtsan.so.0+0x2c5bd)\r\n    #1 gpr_mu_init <null> (libgrpc.so.6+0xe0dea)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x3faeb) in __interceptor_pthread_mutex_lock\r\n==================\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Write of size 8 at 0x7b0800001600 by main thread:\r\n    #0 free <null> (libtsan.so.0+0x2b146)\r\n    #1 grpc_connectivity_state_set(grpc_connectivity_state_tracker*, grpc_connectivity_state, grpc_error*, char const*) <null> (libgrpc.so.6+0x8db6a)\r\n\r\n  Previous write of size 8 at 0x7b0800001600 by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x2b146) in free\r\n==================\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=46)\r\n  Write of size 8 at 0x7b10000024c0 by main thread:\r\n    #0 free <null> (libtsan.so.0+0x2b146)\r\n    #1 grpc_combiner_continue_exec_ctx() <null> (libgrpc.so.6+0x5fc6b)\r\n\r\n  Previous write of size 8 at 0x7b10000024c0 by thread T1:\r\n    #0 malloc <null> (libtsan.so.0+0x2ae13)\r\n    #1 gpr_malloc <null> (libgrpc.so.6+0xdf04e)\r\n\r\n  Thread T1 'grpc_executor' (tid=48, running) created by main thread at:\r\n    #0 pthread_create <null> (libtsan.so.0+0x2bcfe)\r\n    #1 gpr_thd_new(unsigned long*, char const*, void (*)(void*), void*, gpr_thd_options const*) <null> (libgrpc.so.6+0xe1452)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/lib/x86_64-linux-gnu/libtsan.so.0+0x2b146) in free\r\n==================\r\n```\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\n\r\n","bash\r\ndocker build -t grpc-tsan .\r\ndocker run -v $PWD:/src -w /src -it grpc-tsan /bin/bash -c ""make;./clientserver""\r\n"
16718,"TypeError: expected bytes, str found<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n grpcio, and grcpio-tools version: 1.15.0\r\npython\r\n\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nProblem encountered both in Centos 7.5 & Python 3.6.5 (Docker setup) as well as; MacOS High Sierra 10.13.6 & python: 3.6.2\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n python 3.6\r\n \r\n### What did you do?\r\nProgram: A grpc ssl helloworld greeter; server and client python scripts\r\n\r\nHow i generate my proto:\r\n\r\n\r\nHow i generate self signed certs:\r\n```openssl req -x509 -newkey rsa:4096 -keyout self.key -out self.crt -days 365  -nodes -subj '/CN=localhost'```\r\n\r\nClient Code:\r\n```\r\ndef run_secure_crt():\r\n    print('Secure Client call (crt)')\r\n    private_crt_file = 'self.crt'\r\n    trusted_certs = open(private_crt_file).read()\r\n    credentials = grpc.ssl_channel_credentials(root_certificates=trusted_certs)\r\n    channel = grpc.secure_channel('localhost:16506', credentials)\r\n\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n    print(""Greeter client received (crt): "" + response.message)\r\n```\r\n\r\nServer Code:\r\n```\r\ndef serve_ssl():\r\n    print('Starting up grpc server (ssl)...', os.getpid())\r\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)\r\n\r\n    private_key_file = 'self.key'\r\n    with open(private_key_file, 'rb') as f:\r\n        private_key = f.read()\r\n\r\n    cert_file = 'self.crt'\r\n    with open(cert_file, 'rb') as f:\r\n        cert_chain = f.read()\r\n\r\n    server_cred = grpc.ssl_server_credentials(((private_key, cert_chain,),))\r\n    server.add_secure_port('[::]:16506', server_cred)\r\n\r\n    server.start()\r\n    print('Started at', 'localhost:16506 (ssl)', os.getpid())\r\n    try:\r\n        while True:\r\n            time.sleep(_ONE_DAY_IN_SECONDS)\r\n    except KeyboardInterrupt:\r\n        print('Stoping grpc server ', os.getpid(), 'localhost:16506')\r\n        server.stop(0)\r\n```\r\n\r\nhelloworld.proto\r\n```\r\nsyntax = ""proto3"";\r\n\r\n//option java_multiple_files = true;\r\n//option java_package = ""io.grpc.examples.helloworld"";\r\n//option java_outer_classname = ""HelloWorldProto"";\r\n//option objc_class_prefix = ""HLW"";\r\n\r\npackage helloworld;\r\n\r\n// The greeting service definition.\r\nservice Greeter {\r\n  // Sends a greeting\r\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\r\n}\r\n\r\n// The request message containing the user's name.\r\nmessage HelloRequest {\r\n  string name = 1;\r\n}\r\n\r\n// The response message containing the greetings\r\nmessage HelloReply {\r\n  string message = 1;\r\n}\r\n```\r\n\r\n \r\n### What did you expect to see?\r\n A successful grpc call\r\n ```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nGreeter client received (crt): Hello, you!\r\n```\r\n\r\n### What did you see instead?\r\n`TypeError: expected bytes, str found`\r\n ```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nTypeError: expected bytes, str found\r\nException ignored in: 'grpc._cython.cygrpc.SSLChannelCredentials.c'\r\nTypeError: expected bytes, str found\r\nTraceback (most recent call last):\r\n  File ""greeter_client.py"", line 61, in <module>\r\n    run_secure_crt()\r\n  File ""greeter_client.py"", line 41, in run_secure_crt\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n  File ""/Users/ginaulak/anaconda/envs/py35/lib/python3.6/site-packages/grpc/_channel.py"", line 532, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File ""/Users/ginaulak/anaconda/envs/py35/lib/python3.6/site-packages/grpc/_channel.py"", line 466, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.UNKNOWN\r\n\tdetails = ""lame client channel""\r\n\tdebug_error_string = ""{""created"":""@1538040595.935106000"",""description"":""lame client channel"",""file"":""src/core/lib/surface/lame_client.cc"",""file_line"":90}""\r\n>\r\n```\r\n\r\n\r\n### Anything else we should know about your project / environment?\r\nI tried with the dev build mentioned in : https://github.com/grpc/grpc/issues/16623 Unfortunately, it does not work for me. I also tried with grpcio 1.8.6, did not work.\r\n\r\nIt works with grpcio 1.7.0 though. However, I also want to have Tensorflow 1.10 in my project which require grpcio>=1.8.6 \r\nIf i try with grpcio==1.7.0, it is working fine.\r\n```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nGreeter client received (crt): Hello, you!\r\n```",kind/question|lang/Python|priority/P2,lidizheng,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n grpcio, and grcpio-tools version: 1.15.0\r\npython\r\n\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nProblem encountered both in Centos 7.5 & Python 3.6.5 (Docker setup) as well as; MacOS High Sierra 10.13.6 & python: 3.6.2\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n python 3.6\r\n \r\n### What did you do?\r\nProgram: A grpc ssl helloworld greeter; server and client python scripts\r\n\r\nHow i generate my proto:\r\n```python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. helloworld.proto```\r\n\r\nHow i generate self signed certs:\r\n```openssl req -x509 -newkey rsa:4096 -keyout self.key -out self.crt -days 365  -nodes -subj '/CN=localhost'```\r\n\r\nClient Code:\r\n```\r\ndef run_secure_crt():\r\n    print('Secure Client call (crt)')\r\n    private_crt_file = 'self.crt'\r\n    trusted_certs = open(private_crt_file).read()\r\n    credentials = grpc.ssl_channel_credentials(root_certificates=trusted_certs)\r\n    channel = grpc.secure_channel('localhost:16506', credentials)\r\n\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n    print(""Greeter client received (crt): "" + response.message)\r\n```\r\n\r\nServer Code:\r\n```\r\ndef serve_ssl():\r\n    print('Starting up grpc server (ssl)...', os.getpid())\r\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)\r\n\r\n    private_key_file = 'self.key'\r\n    with open(private_key_file, 'rb') as f:\r\n        private_key = f.read()\r\n\r\n    cert_file = 'self.crt'\r\n    with open(cert_file, 'rb') as f:\r\n        cert_chain = f.read()\r\n\r\n    server_cred = grpc.ssl_server_credentials(((private_key, cert_chain,),))\r\n    server.add_secure_port('[::]:16506', server_cred)\r\n\r\n    server.start()\r\n    print('Started at', 'localhost:16506 (ssl)', os.getpid())\r\n    try:\r\n        while True:\r\n            time.sleep(_ONE_DAY_IN_SECONDS)\r\n    except KeyboardInterrupt:\r\n        print('Stoping grpc server ', os.getpid(), 'localhost:16506')\r\n        server.stop(0)\r\n```\r\n\r\nhelloworld.proto\r\n```\r\nsyntax = ""proto3"";\r\n\r\n//option java_multiple_files = true;\r\n//option java_package = ""io.grpc.examples.helloworld"";\r\n//option java_outer_classname = ""HelloWorldProto"";\r\n//option objc_class_prefix = ""HLW"";\r\n\r\npackage helloworld;\r\n\r\n// The greeting service definition.\r\nservice Greeter {\r\n  // Sends a greeting\r\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\r\n}\r\n\r\n// The request message containing the user's name.\r\nmessage HelloRequest {\r\n  string name = 1;\r\n}\r\n\r\n// The response message containing the greetings\r\nmessage HelloReply {\r\n  string message = 1;\r\n}\r\n```\r\n\r\n \r\n### What did you expect to see?\r\n A successful grpc call\r\n ```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nGreeter client received (crt): Hello, you!\r\n```\r\n\r\n### What did you see instead?\r\n`TypeError: expected bytes, str found`\r\n ```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nTypeError: expected bytes, str found\r\nException ignored in: 'grpc._cython.cygrpc.SSLChannelCredentials.c'\r\nTypeError: expected bytes, str found\r\nTraceback (most recent call last):\r\n  File ""greeter_client.py"", line 61, in <module>\r\n    run_secure_crt()\r\n  File ""greeter_client.py"", line 41, in run_secure_crt\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n  File ""/Users/ginaulak/anaconda/envs/py35/lib/python3.6/site-packages/grpc/_channel.py"", line 532, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File ""/Users/ginaulak/anaconda/envs/py35/lib/python3.6/site-packages/grpc/_channel.py"", line 466, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with:\r\n\tstatus = StatusCode.UNKNOWN\r\n\tdetails = ""lame client channel""\r\n\tdebug_error_string = ""{""created"":""@1538040595.935106000"",""description"":""lame client channel"",""file"":""src/core/lib/surface/lame_client.cc"",""file_line"":90}""\r\n>\r\n```\r\n\r\n\r\n### Anything else we should know about your project / environment?\r\nI tried with the dev build mentioned in : https://github.com/grpc/grpc/issues/16623 Unfortunately, it does not work for me. I also tried with grpcio 1.8.6, did not work.\r\n\r\nIt works with grpcio 1.7.0 though. However, I also want to have Tensorflow 1.10 in my project which require grpcio>=1.8.6 \r\nIf i try with grpcio==1.7.0, it is working fine.\r\n```\r\n$ python greeter_client.py \r\nSecure Client call (crt)\r\nGreeter client received (crt): Hello, you!\r\n```",python -m grpc_tools.protoc --python_out=. --grpc_python_out=. --proto_path=. helloworld.proto
16591,"Completion queue shutdown doesn't cancel pending alarms<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n1.13, 1.14\r\nC++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nGCC 8.2 \r\n \r\n### What did you do?\r\n\r\n### What did you expect to see?\r\nI would expect it the fail the assert on `ok`, or at least return immediately.\r\n\r\n### What did you see instead?\r\n```\r\n> $ time build/bin/foo                                                                                                                                                                                                                                                 [\xb1mainline \u25cf\u25cf]\r\nbuild/bin/foo  0.00s user 0.00s system 0% cpu 5.005 total\r\n``` \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/question|disposition/wontfix|lang/c++,vjpai,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n1.13, 1.14\r\nC++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nGCC 8.2 \r\n \r\n### What did you do?\r\n```cpp\r\n#include <chrono>\r\n\r\n#include <grpc/support/log.h>\r\n#include <grpcpp/alarm.h>\r\n#include <grpcpp/grpcpp.h>\r\n\r\nusing namespace std::chrono_literals;\r\n\r\nint main() {\r\n  ::grpc::ServerBuilder builder;\r\n  builder.AddListeningPort(""[::]:0"", grpc::InsecureServerCredentials());\r\n  auto          cq     = builder.AddCompletionQueue();\r\n  auto          server = builder.BuildAndStart();\r\n  void*         tag;\r\n  bool          ok;\r\n  ::grpc::Alarm alarm;\r\n  alarm.Set(cq.get(), ::std::chrono::system_clock::now() + 5s, (void*)1);\r\n\r\n  server->Shutdown();\r\n  cq->Shutdown();\r\n\r\n  GPR_ASSERT(cq->Next(&tag, &ok));\r\n  GPR_ASSERT(ok);\r\n  GPR_ASSERT(tag == (void*)1);\r\n\r\n  return 0;\r\n}\r\n```\r\n### What did you expect to see?\r\nI would expect it the fail the assert on `ok`, or at least return immediately.\r\n\r\n### What did you see instead?\r\n```\r\n> $ time build/bin/foo                                                                                                                                                                                                                                                 [\xb1mainline \u25cf\u25cf]\r\nbuild/bin/foo  0.00s user 0.00s system 0% cpu 5.005 total\r\n``` \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","cpp\r\n#include <chrono>\r\n\r\n#include <grpc/support/log.h>\r\n#include <grpcpp/alarm.h>\r\n#include <grpcpp/grpcpp.h>\r\n\r\nusing namespace std::chrono_literals;\r\n\r\nint main() {\r\n  ::grpc::ServerBuilder builder;\r\n  builder.AddListeningPort(""[::]:0"", grpc::InsecureServerCredentials());\r\n  auto          cq     = builder.AddCompletionQueue();\r\n  auto          server = builder.BuildAndStart();\r\n  void*         tag;\r\n  bool          ok;\r\n  ::grpc::Alarm alarm;\r\n  alarm.Set(cq.get(), ::std::chrono::system_clock::now() + 5s, (void*)1);\r\n\r\n  server->Shutdown();\r\n  cq->Shutdown();\r\n\r\n  GPR_ASSERT(cq->Next(&tag, &ok));\r\n  GPR_ASSERT(ok);\r\n  GPR_ASSERT(tag == (void*)1);\r\n\r\n  return 0;\r\n}\r\n"
16573,"Add template-friendly function in generated C++ WithAsyncMethod classesThis PR generates one template-friendly function in `WithAsyncMethod_*` class in C++ code generator. The objective is to call the family of `WithAsyncMethod_Foo::RequestFoo` functions without knowing the exact names of the functions. \r\n\r\nFor instance, I would like to allow application frameworks to write code like below to invoke all the nested `WithAsyncMethod_*::Request*` functions belonging to `AsyncService`.\r\n\r\n\r\nSuch generic code is possible if there is a duplicate of `WithAsyncMethod_*::Request*` function with  a generic name---`Request`. See the diffs for `tests/monster_test.grpc.fb.h` [here](https://github.com/google/flatbuffers/pull/4901/files#diff-62b42c0d54d6d3f7044e45159eebcf8b). Here's how generic code might work. There's some additional discussion in [this](https://github.com/google/flatbuffers/pull/4901) PR.\r\n\r\n\r\n\r\nThe current proposal is to call it `Request` to avoid any possibility of collision with a user-defined rpc operation. An alternative is `operator()`.\r\n\r\nThe changes required in C++ code generator are next to trivial.",kind/enhancement|lang/c++|priority/P2|disposition/stale,vjpai,"This PR generates one template-friendly function in `WithAsyncMethod_*` class in C++ code generator. The objective is to call the family of `WithAsyncMethod_Foo::RequestFoo` functions without knowing the exact names of the functions. \r\n\r\nFor instance, I would like to allow application frameworks to write code like below to invoke all the nested `WithAsyncMethod_*::Request*` functions belonging to `AsyncService`.\r\n\r\n```cpp\r\nint nested_async_methods() {\r\n  MyGame::Example::MonsterStorage::AsyncService async;\r\n  // Somehow call WithAsyncMethod_Retrieve::RequestRetrieve \r\n  // and WithAsyncMethod_Store::RequestStore.\r\n  call_nested(async);\r\n  return 0;\r\n}\r\n```\r\nSuch generic code is possible if there is a duplicate of `WithAsyncMethod_*::Request*` function with  a generic name---`Request`. See the diffs for `tests/monster_test.grpc.fb.h` [here](https://github.com/google/flatbuffers/pull/4901/files#diff-62b42c0d54d6d3f7044e45159eebcf8b). Here's how generic code might work. There's some additional discussion in [this](https://github.com/google/flatbuffers/pull/4901) PR.\r\n\r\n```cpp\r\ntemplate <class AsyncService>\r\nstruct CallNested {\r\n  static void call(AsyncService &s) {}\r\n};\r\n\r\ntemplate <class Nested, template <class> class WithAsyncMethod>\r\nstruct CallNested<WithAsyncMethod<Nested>> {\r\n  static void call(WithAsyncMethod<Nested> &async) {\r\n    // The next line is the objective. Of course, in real code, it's not all zeros.\r\n    async.Request(0, 0, 0, 0, 0, 0);\r\n    CallNested<Nested>::call(async);\r\n  };\r\n};\r\n\r\ntemplate <class AsyncService>\r\nvoid call_nested(AsyncService &async) {\r\n  CallNested<AsyncService>::call(async);\r\n}\r\n```\r\n\r\nThe current proposal is to call it `Request` to avoid any possibility of collision with a user-defined rpc operation. An alternative is `operator()`.\r\n\r\nThe changes required in C++ code generator are next to trivial.",cpp\r\nint nested_async_methods() {\r\n  MyGame::Example::MonsterStorage::AsyncService async;\r\n  // Somehow call WithAsyncMethod_Retrieve::RequestRetrieve \r\n  // and WithAsyncMethod_Store::RequestStore.\r\n  call_nested(async);\r\n  return 0;\r\n}\r\n
16433,"GRPC_VERBOSITY set programmatically in linux is not honored<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n C# - Grpc.Core 1.14.1\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux - Debian 9.5\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n .NET Core 2.1\r\n`dotnet --info` output:\r\n```\r\n.NET Core SDK (reflecting any global.json):\r\n Version:   2.1.401\r\n Commit:    91b1c13032\r\n\r\nRuntime Environment:\r\n OS Name:     debian\r\n OS Version:  9\r\n OS Platform: Linux\r\n RID:         debian.9-x64\r\n Base Path:   /usr/share/dotnet/sdk/2.1.401/\r\n\r\nHost (useful for support):\r\n  Version: 2.1.3\r\n  Commit:  124038c13e\r\n\r\n.NET Core SDKs installed:\r\n  2.1.401 [/usr/share/dotnet/sdk]\r\n\r\n.NET Core runtimes installed:\r\n  Microsoft.AspNetCore.All 2.1.3 [/usr/share/dotnet/shared/Microsoft.AspNetCore.All]\r\n  Microsoft.AspNetCore.App 2.1.3 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App]\r\n  Microsoft.NETCore.App 2.1.3 [/usr/share/dotnet/shared/Microsoft.NETCore.App]\r\n\r\nTo install additional .NET Core runtimes or SDKs:\r\n  https://aka.ms/dotnet-download\r\n```\r\n \r\n### What did you do?\r\nCreated the following console application:\r\n\r\n\r\nAnd then ran it from the terminal using\r\n```\r\n$ dotnet run\r\n```\r\n \r\n### What did you expect to see?\r\n\r\nAn INFO message logged about a subchannel being created, such as  \r\n```\r\nI0822 17:11:44.704390 140224188356352 src/core/ext/filters/client_channel/subchannel.cc:630: New connected subchannel at 0x7f87ec005600 for subchannel 0x7f87e8005fa0\r\nConnected\r\n```\r\n \r\n### What did you see instead?\r\n\r\nThe log message was missing:\r\n```\r\nConnected\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nIf you instead set the environment variable before running, like this, then the expected output is displayed:\r\n```\r\n$ export GRPC_VERBOSITY=INFO\r\n$ dotnet run\r\n```\r\n",kind/enhancement|lang/C#|platform/Linux|priority/P3,jtattermusch,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n C# - Grpc.Core 1.14.1\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux - Debian 9.5\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n .NET Core 2.1\r\n`dotnet --info` output:\r\n```\r\n.NET Core SDK (reflecting any global.json):\r\n Version:   2.1.401\r\n Commit:    91b1c13032\r\n\r\nRuntime Environment:\r\n OS Name:     debian\r\n OS Version:  9\r\n OS Platform: Linux\r\n RID:         debian.9-x64\r\n Base Path:   /usr/share/dotnet/sdk/2.1.401/\r\n\r\nHost (useful for support):\r\n  Version: 2.1.3\r\n  Commit:  124038c13e\r\n\r\n.NET Core SDKs installed:\r\n  2.1.401 [/usr/share/dotnet/sdk]\r\n\r\n.NET Core runtimes installed:\r\n  Microsoft.AspNetCore.All 2.1.3 [/usr/share/dotnet/shared/Microsoft.AspNetCore.All]\r\n  Microsoft.AspNetCore.App 2.1.3 [/usr/share/dotnet/shared/Microsoft.AspNetCore.App]\r\n  Microsoft.NETCore.App 2.1.3 [/usr/share/dotnet/shared/Microsoft.NETCore.App]\r\n\r\nTo install additional .NET Core runtimes or SDKs:\r\n  https://aka.ms/dotnet-download\r\n```\r\n \r\n### What did you do?\r\nCreated the following console application:\r\n```csharp\r\nusing System;\r\nusing System.Threading.Tasks;\r\nusing Grpc.Core;\r\n\r\nnamespace grpc_channel_test\r\n{\r\n    class Program\r\n    {\r\n        static async Task Main(string[] args)\r\n        {\r\n            Environment.SetEnvironmentVariable(""GRPC_VERBOSITY"", ""INFO"");\r\n\r\n            var c = new Channel(""bigtable.googleapis.com"", 443, ChannelCredentials.Insecure);\r\n            await c.ConnectAsync();\r\n            Console.WriteLine(""Connected"");\r\n            await c.ShutdownAsync();\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAnd then ran it from the terminal using\r\n```\r\n$ dotnet run\r\n```\r\n \r\n### What did you expect to see?\r\n\r\nAn INFO message logged about a subchannel being created, such as  \r\n```\r\nI0822 17:11:44.704390 140224188356352 src/core/ext/filters/client_channel/subchannel.cc:630: New connected subchannel at 0x7f87ec005600 for subchannel 0x7f87e8005fa0\r\nConnected\r\n```\r\n \r\n### What did you see instead?\r\n\r\nThe log message was missing:\r\n```\r\nConnected\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nIf you instead set the environment variable before running, like this, then the expected output is displayed:\r\n```\r\n$ export GRPC_VERBOSITY=INFO\r\n$ dotnet run\r\n```\r\n","csharp\r\nusing System;\r\nusing System.Threading.Tasks;\r\nusing Grpc.Core;\r\n\r\nnamespace grpc_channel_test\r\n{\r\n    class Program\r\n    {\r\n        static async Task Main(string[] args)\r\n        {\r\n            Environment.SetEnvironmentVariable(""GRPC_VERBOSITY"", ""INFO"");\r\n\r\n            var c = new Channel(""bigtable.googleapis.com"", 443, ChannelCredentials.Insecure);\r\n            await c.ConnectAsync();\r\n            Console.WriteLine(""Connected"");\r\n            await c.ShutdownAsync();\r\n        }\r\n    }\r\n}\r\n"
16427,"GRPC client deadlocks if TCP connection fails synchronously<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n1.14.1, Javascript\r\n \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Debian 8.11 (node:8 docker image).\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Node.js 8.11\r\n \r\n### What did you do?\r\nPlease see this repo: https://github.com/alexeevg/grpc-deadlock for scripts reproducing the problem.\r\nIn short, I tried making a gRPC call over IPv6 from within a Docker container with IPv6 disabled. \r\n\r\n \r\n### What did you expect to see?\r\n I expected the client to invoke the callback with connection error. Also, I expected the process to remain responsive while the call is in progress, event loop should not be blocked.\r\n \r\n### What did you see instead?\r\n \r\nNode.js process deadlocked deep inside gRPC core native code.\r\nAttaching gdb to the hanging process resulted in the following stacktrace:\r\n```\r\n(gdb) bt\r\n#0  __lll_lock_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135\r\n#1  0x00007fba9b8fb479 in _L_lock_909 () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#2  0x00007fba9b8fb2a0 in __GI___pthread_mutex_lock (mutex=0x4052f18) at ../nptl/pthread_mutex_lock.c:79\r\n#3  0x00007fba992f6319 in gpr_mu_lock (mu=<optimized out>) at ../deps/grpc/src/core/lib/gpr/sync_posix.cc:47\r\n#4  0x00007fba9928047f in connected (arg=0x4052f10, error=0x40541e0)\r\n    at ../deps/grpc/src/core/ext/transport/chttp2/client/chttp2_connector.cc:172\r\n#5  0x00007fba9925c2c1 in exec_ctx_run (error=0x40541e0, closure=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:40\r\n#6  grpc_core::ExecCtx::Flush (this=this@entry=0x7fff4adf27d0) at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:128\r\n#7  0x00007fba992e8ffe in ~ExecCtx (this=0x7fff4adf27d0, __in_chrg=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.h:108\r\n#8  custom_connect_callback (socket=<optimized out>, error=0x40541e0)\r\n    at ../deps/grpc/src/core/lib/iomgr/tcp_client_custom.cc:100\r\n#9  0x00007fba9928016a in chttp2_connector_connect (con=0x4052f10, args=0x7fff4adf2940, result=0x4053290,\r\n    notify=0x40532a0) at ../deps/grpc/src/core/ext/transport/chttp2/client/chttp2_connector.cc:215\r\n#10 0x00007fba9928e7dc in continue_connect_locked (c=0x4053260)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:408\r\n#11 maybe_start_connecting_locked (c=0x4053260) at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:483\r\n#12 0x00007fba9928f415 in grpc_subchannel_notify_on_state_change (c=0x4053260, interested_parties=0xdeafbeef,\r\n    state=0x405281c, notify=<optimized out>) at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:532\r\n#13 0x00007fba992d9e07 in grpc_core::SubchannelData<grpc_core::(anonymous namespace)::PickFirst::PickFirstSubchannelList, grpc_core::(anonymous namespace)::PickFirst::PickFirstSubchannelData>::StartConnectivityWatchLocked (\r\n    this=<optimized out>) at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/subchannel_list.h:304\r\n#14 0x00007fba992dac70 in StartPickingLocked (this=0x40523d0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc:249\r\n#15 grpc_core::(anonymous namespace)::PickFirst::PickLocked (this=0x40523d0, pick=0x404ceb0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc:270\r\n#16 0x00007fba99289e2a in StartLocked (elem=<optimized out>)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2687\r\n#17 process_service_config_and_start_lb_pick_locked (elem=0x404cd80)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2815\r\n#18 grpc_core::ResolverResultWaiter::DoneLocked (arg=0x40514a0, error=0x0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2920\r\n#19 0x00007fba992a2332 in grpc_combiner_continue_exec_ctx () at ../deps/grpc/src/core/lib/iomgr/combiner.cc:260\r\n#20 0x00007fba9925c2e4 in grpc_core::ExecCtx::Flush (this=this@entry=0x7fff4adf2b00)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:131\r\n#21 0x00007fba992a364a in ~ExecCtx (this=0x7fff4adf2b00, __in_chrg=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.h:108\r\n#22 grpc_custom_resolve_callback (r=0x3fc2bc0, result=0x3fa6e80, error=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/resolve_address_custom.cc:85\r\n#23 0x0000000001410b75 in uv__work_done (handle=0x2179ab0 <default_loop_struct+176>)\r\n    at ../deps/uv/src/threadpool.c:251\r\n#24 0x0000000001412c8b in uv__async_io (loop=0x2179a00 <default_loop_struct>, w=<optimized out>,\r\n    events=<optimized out>) at ../deps/uv/src/unix/async.c:118\r\n#25 0x0000000001424d48 in uv__io_poll (loop=loop@entry=0x2179a00 <default_loop_struct>, timeout=25)\r\n    at ../deps/uv/src/unix/linux-core.c:400\r\n#26 0x0000000001413616 in uv_run (loop=0x2179a00 <default_loop_struct>, mode=UV_RUN_DEFAULT)\r\n    at ../deps/uv/src/unix/core.c:368\r\n#27 0x00000000008d443d in node::Start(uv_loop_s*, int, char const* const*, int, char const* const*) ()\r\n#28 0x00000000008cc8fd in node::Start(int, char**) ()\r\n#29 0x00007fba9b567b45 in __libc_start_main (main=0x89b0c0 <main>, argc=2, argv=0x7fff4adf7208, init=<optimized out>,\r\n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fff4adf71f8) at libc-start.c:287\r\n#30 0x000000000089b1b1 in _start ()\r\n``` \r\n\r\nRunning `strace -e trace=network -f npm test` produced the following log:\r\n```\r\n77    socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 12\r\n77    setsockopt(12, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(12, {sa_family=AF_INET, sin_port=htons(1234), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress)\r\n77    getsockopt(12, SOL_SOCKET, SO_ERROR, [111], [4]) = 0\r\n77    socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 13\r\n77    setsockopt(13, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(13, {sa_family=AF_INET, sin_port=htons(1234), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress)\r\n77    getsockopt(13, SOL_SOCKET, SO_ERROR, [111], [4]) = 0\r\n77    socket(PF_INET6, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 14\r\n77    setsockopt(14, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(14, {sa_family=AF_INET6, sin6_port=htons(1234), inet_pton(AF_INET6, ""::1"", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, 28) = -1 EADDRNOTAVAIL (Cannot assign requested address)\r\n```\r\n\r\nRunning test with `GRPC_TRACE=connectivity_state,tcp` produced the following log:\r\n```\r\nD0822 16:19:46.249122621      25 dns_resolver.cc:280]        Start resolving.\r\nI0822 16:19:46.249524376      25 connectivity_state.cc:92]   CONWATCH: 0x3936548 pick_first: get IDLE\r\nI0822 16:19:46.249582095      25 connectivity_state.cc:116]  CONWATCH: 0x3936548 pick_first: from IDLE [cur=IDLE] notify=0x39a2788\r\nI0822 16:19:46.249603812      25 connectivity_state.cc:164]  SET: 0x3921498 client_channel: IDLE --> IDLE [resolver_result] error=(nil) ""No Error""\r\nI0822 16:19:46.249648189      25 connectivity_state.cc:116]  CONWATCH: 0x3937360 subchannel: from IDLE [cur=IDLE] notify=0x39122b8\r\nI0822 16:19:46.249664640      25 connectivity_state.cc:164]  SET: 0x3937360 subchannel: IDLE --> CONNECTING [state_change] error=(nil) ""No Error""\r\nI0822 16:19:46.249684799      25 connectivity_state.cc:190]  NOTIFY: 0x3937360 subchannel: 0x39122b8\r\nI0822 16:19:46.249761199      25 tcp_client_custom.cc:139]   CLIENT_CONNECT: 0x39354b0 ipv6:[::1]:1234: asynchronously connecting\r\nI0822 16:19:46.249906749      25 tcp_client_custom.cc:69]    CLIENT_CONNECT: ipv6:[::1]:1234: on_alarm: error=""Cancelled""\r\n```\r\n\r\nI suppose the problem is that TCP connection fails immediately with `EADDRNOTAVAIL (Cannot assign requested address)` and `chttp2_connector` isn't prepared to handle synchronous connection errors.\r\nAs a result, error handler attempts to acquire a mutex protecting the connector at https://github.com/grpc/grpc/blob/v1.14.1/src/core/ext/transport/chttp2/client/chttp2_connector.cc#L172.\r\nBut this mutex is already held at https://github.com/grpc/grpc/blob/v1.14.1/src/core/ext/transport/chttp2/client/chttp2_connector.cc#L215, and the mutex is non-recursive.\r\n\r\n\r\n",kind/bug|lang/node|lang/core|priority/P2,yashykt,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n1.14.1, Javascript\r\n \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Debian 8.11 (node:8 docker image).\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Node.js 8.11\r\n \r\n### What did you do?\r\nPlease see this repo: https://github.com/alexeevg/grpc-deadlock for scripts reproducing the problem.\r\nIn short, I tried making a gRPC call over IPv6 from within a Docker container with IPv6 disabled. \r\n```javascript\r\nconst grpc = require('grpc');\r\n\r\nconst {Calculator} = grpc.load('./proto/math.proto').test;\r\nconst client = new Calculator('[::1]:1234', grpc.credentials.createInsecure()); // there is no server listening at [::1]:1234\r\n\r\nclient.add({a: 1, b: 2}, (err, result) => {\r\n  // the callback is never called, the calling process hangs\r\n});\r\n```\r\n \r\n### What did you expect to see?\r\n I expected the client to invoke the callback with connection error. Also, I expected the process to remain responsive while the call is in progress, event loop should not be blocked.\r\n \r\n### What did you see instead?\r\n \r\nNode.js process deadlocked deep inside gRPC core native code.\r\nAttaching gdb to the hanging process resulted in the following stacktrace:\r\n```\r\n(gdb) bt\r\n#0  __lll_lock_wait () at ../nptl/sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135\r\n#1  0x00007fba9b8fb479 in _L_lock_909 () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#2  0x00007fba9b8fb2a0 in __GI___pthread_mutex_lock (mutex=0x4052f18) at ../nptl/pthread_mutex_lock.c:79\r\n#3  0x00007fba992f6319 in gpr_mu_lock (mu=<optimized out>) at ../deps/grpc/src/core/lib/gpr/sync_posix.cc:47\r\n#4  0x00007fba9928047f in connected (arg=0x4052f10, error=0x40541e0)\r\n    at ../deps/grpc/src/core/ext/transport/chttp2/client/chttp2_connector.cc:172\r\n#5  0x00007fba9925c2c1 in exec_ctx_run (error=0x40541e0, closure=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:40\r\n#6  grpc_core::ExecCtx::Flush (this=this@entry=0x7fff4adf27d0) at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:128\r\n#7  0x00007fba992e8ffe in ~ExecCtx (this=0x7fff4adf27d0, __in_chrg=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.h:108\r\n#8  custom_connect_callback (socket=<optimized out>, error=0x40541e0)\r\n    at ../deps/grpc/src/core/lib/iomgr/tcp_client_custom.cc:100\r\n#9  0x00007fba9928016a in chttp2_connector_connect (con=0x4052f10, args=0x7fff4adf2940, result=0x4053290,\r\n    notify=0x40532a0) at ../deps/grpc/src/core/ext/transport/chttp2/client/chttp2_connector.cc:215\r\n#10 0x00007fba9928e7dc in continue_connect_locked (c=0x4053260)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:408\r\n#11 maybe_start_connecting_locked (c=0x4053260) at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:483\r\n#12 0x00007fba9928f415 in grpc_subchannel_notify_on_state_change (c=0x4053260, interested_parties=0xdeafbeef,\r\n    state=0x405281c, notify=<optimized out>) at ../deps/grpc/src/core/ext/filters/client_channel/subchannel.cc:532\r\n#13 0x00007fba992d9e07 in grpc_core::SubchannelData<grpc_core::(anonymous namespace)::PickFirst::PickFirstSubchannelList, grpc_core::(anonymous namespace)::PickFirst::PickFirstSubchannelData>::StartConnectivityWatchLocked (\r\n    this=<optimized out>) at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/subchannel_list.h:304\r\n#14 0x00007fba992dac70 in StartPickingLocked (this=0x40523d0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc:249\r\n#15 grpc_core::(anonymous namespace)::PickFirst::PickLocked (this=0x40523d0, pick=0x404ceb0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc:270\r\n#16 0x00007fba99289e2a in StartLocked (elem=<optimized out>)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2687\r\n#17 process_service_config_and_start_lb_pick_locked (elem=0x404cd80)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2815\r\n#18 grpc_core::ResolverResultWaiter::DoneLocked (arg=0x40514a0, error=0x0)\r\n    at ../deps/grpc/src/core/ext/filters/client_channel/client_channel.cc:2920\r\n#19 0x00007fba992a2332 in grpc_combiner_continue_exec_ctx () at ../deps/grpc/src/core/lib/iomgr/combiner.cc:260\r\n#20 0x00007fba9925c2e4 in grpc_core::ExecCtx::Flush (this=this@entry=0x7fff4adf2b00)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.cc:131\r\n#21 0x00007fba992a364a in ~ExecCtx (this=0x7fff4adf2b00, __in_chrg=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/exec_ctx.h:108\r\n#22 grpc_custom_resolve_callback (r=0x3fc2bc0, result=0x3fa6e80, error=<optimized out>)\r\n    at ../deps/grpc/src/core/lib/iomgr/resolve_address_custom.cc:85\r\n#23 0x0000000001410b75 in uv__work_done (handle=0x2179ab0 <default_loop_struct+176>)\r\n    at ../deps/uv/src/threadpool.c:251\r\n#24 0x0000000001412c8b in uv__async_io (loop=0x2179a00 <default_loop_struct>, w=<optimized out>,\r\n    events=<optimized out>) at ../deps/uv/src/unix/async.c:118\r\n#25 0x0000000001424d48 in uv__io_poll (loop=loop@entry=0x2179a00 <default_loop_struct>, timeout=25)\r\n    at ../deps/uv/src/unix/linux-core.c:400\r\n#26 0x0000000001413616 in uv_run (loop=0x2179a00 <default_loop_struct>, mode=UV_RUN_DEFAULT)\r\n    at ../deps/uv/src/unix/core.c:368\r\n#27 0x00000000008d443d in node::Start(uv_loop_s*, int, char const* const*, int, char const* const*) ()\r\n#28 0x00000000008cc8fd in node::Start(int, char**) ()\r\n#29 0x00007fba9b567b45 in __libc_start_main (main=0x89b0c0 <main>, argc=2, argv=0x7fff4adf7208, init=<optimized out>,\r\n    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fff4adf71f8) at libc-start.c:287\r\n#30 0x000000000089b1b1 in _start ()\r\n``` \r\n\r\nRunning `strace -e trace=network -f npm test` produced the following log:\r\n```\r\n77    socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 12\r\n77    setsockopt(12, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(12, {sa_family=AF_INET, sin_port=htons(1234), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress)\r\n77    getsockopt(12, SOL_SOCKET, SO_ERROR, [111], [4]) = 0\r\n77    socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 13\r\n77    setsockopt(13, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(13, {sa_family=AF_INET, sin_port=htons(1234), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress)\r\n77    getsockopt(13, SOL_SOCKET, SO_ERROR, [111], [4]) = 0\r\n77    socket(PF_INET6, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 14\r\n77    setsockopt(14, SOL_TCP, TCP_NODELAY, [1], 4) = 0\r\n77    connect(14, {sa_family=AF_INET6, sin6_port=htons(1234), inet_pton(AF_INET6, ""::1"", &sin6_addr), sin6_flowinfo=0, sin6_scope_id=0}, 28) = -1 EADDRNOTAVAIL (Cannot assign requested address)\r\n```\r\n\r\nRunning test with `GRPC_TRACE=connectivity_state,tcp` produced the following log:\r\n```\r\nD0822 16:19:46.249122621      25 dns_resolver.cc:280]        Start resolving.\r\nI0822 16:19:46.249524376      25 connectivity_state.cc:92]   CONWATCH: 0x3936548 pick_first: get IDLE\r\nI0822 16:19:46.249582095      25 connectivity_state.cc:116]  CONWATCH: 0x3936548 pick_first: from IDLE [cur=IDLE] notify=0x39a2788\r\nI0822 16:19:46.249603812      25 connectivity_state.cc:164]  SET: 0x3921498 client_channel: IDLE --> IDLE [resolver_result] error=(nil) ""No Error""\r\nI0822 16:19:46.249648189      25 connectivity_state.cc:116]  CONWATCH: 0x3937360 subchannel: from IDLE [cur=IDLE] notify=0x39122b8\r\nI0822 16:19:46.249664640      25 connectivity_state.cc:164]  SET: 0x3937360 subchannel: IDLE --> CONNECTING [state_change] error=(nil) ""No Error""\r\nI0822 16:19:46.249684799      25 connectivity_state.cc:190]  NOTIFY: 0x3937360 subchannel: 0x39122b8\r\nI0822 16:19:46.249761199      25 tcp_client_custom.cc:139]   CLIENT_CONNECT: 0x39354b0 ipv6:[::1]:1234: asynchronously connecting\r\nI0822 16:19:46.249906749      25 tcp_client_custom.cc:69]    CLIENT_CONNECT: ipv6:[::1]:1234: on_alarm: error=""Cancelled""\r\n```\r\n\r\nI suppose the problem is that TCP connection fails immediately with `EADDRNOTAVAIL (Cannot assign requested address)` and `chttp2_connector` isn't prepared to handle synchronous connection errors.\r\nAs a result, error handler attempts to acquire a mutex protecting the connector at https://github.com/grpc/grpc/blob/v1.14.1/src/core/ext/transport/chttp2/client/chttp2_connector.cc#L172.\r\nBut this mutex is already held at https://github.com/grpc/grpc/blob/v1.14.1/src/core/ext/transport/chttp2/client/chttp2_connector.cc#L215, and the mutex is non-recursive.\r\n\r\n\r\n","javascript\r\nconst grpc = require('grpc');\r\n\r\nconst {Calculator} = grpc.load('./proto/math.proto').test;\r\nconst client = new Calculator('[::1]:1234', grpc.credentials.createInsecure()); // there is no server listening at [::1]:1234\r\n\r\nclient.add({a: 1, b: 2}, (err, result) => {\r\n  // the callback is never called, the calling process hangs\r\n});\r\n"
16403,"Support eager port binding in ServerBuilderI am exploring an implementation of gRPC Python server on top of the C++ Server API. Most things work right out of the gate, but there is a difference between the C++ and Python APIs in when they actually bind to a port (and return the bound port number or fail): Python immediately binds on `grpc.Server.add_[in]secure_port` whereas C++ `ServerBuilder` only actually binds on `BuildAndStart()`, which makes it hard to construct the server. (In general, `AddListeningPort` API is a bit non-intuitive in C++ as the pointer you pass to it is not populated right after the call, but only after `BuildAndStart()` is invoked.)\r\n\r\nSomething like the following would help with the supported functionality, but may require moving things around in the way we construct `grpc::Server`:\r\n\r\n\r\n\r\n(Interestingly, the desired behavior from Python perspective matches the `grpc::Server` internals well, and I considered inheriting a `PyServer` class from `grpc::Server` and instantiating it via its `protected` constructor, but realized it is clearly marked as ""PRIVATE API and will become `private` pretty soon"".)\r\n\r\n[cl/210822191 is a temporary workaround]",kind/enhancement|disposition/help wanted|lang/c++|area/api|priority/P2,ctiller,"I am exploring an implementation of gRPC Python server on top of the C++ Server API. Most things work right out of the gate, but there is a difference between the C++ and Python APIs in when they actually bind to a port (and return the bound port number or fail): Python immediately binds on `grpc.Server.add_[in]secure_port` whereas C++ `ServerBuilder` only actually binds on `BuildAndStart()`, which makes it hard to construct the server. (In general, `AddListeningPort` API is a bit non-intuitive in C++ as the pointer you pass to it is not populated right after the call, but only after `BuildAndStart()` is invoked.)\r\n\r\nSomething like the following would help with the supported functionality, but may require moving things around in the way we construct `grpc::Server`:\r\n\r\n```c++\r\nint ServerBuilder::BindToEndpoint(const std::string& addr_uri, std::shared_ptr<ServerCredentials> creds)\r\n```\r\n\r\n(Interestingly, the desired behavior from Python perspective matches the `grpc::Server` internals well, and I considered inheriting a `PyServer` class from `grpc::Server` and instantiating it via its `protected` constructor, but realized it is clearly marked as ""PRIVATE API and will become `private` pretty soon"".)\r\n\r\n[cl/210822191 is a temporary workaround]","c++\r\nint ServerBuilder::BindToEndpoint(const std::string& addr_uri, std::shared_ptr<ServerCredentials> creds)\r\n"
16367,"Add new C# serialization APIThis PR is based on recent experiments with zero-copy parsing of received messages. It turns out a few degrees of optimization can be utilized when serializing/deserializing messages, but a new API is needed, as the existing API  just uses a function to converts msg of type T to a newly allocated byte array (and a newly created byte array to a msg of type T. The existing API basically necessitates copying of the buffer around and also prohibits us from pooling the buffers in any way.\r\n\r\nThis PR adds a new serialization API that is more general and extensible, basically it consists of two functions:\r\n- `Action<T, SerializationContext> ContextualSerializer`  - when serializing, one gets access to an instance of a message  and a `SerializationContext` that provides methods by which we store the serialized data in the context (`Complete(payload)` in the simplest and least efficent case) before returning.\r\n- `Func<DeserializationContext, T> ContextualDeserializer` - when deserializing, one gets access to a `DeserializationContext` which provides methods to access the payload at varying levels of efficiency\r\n\r\nRight now the SerializationContext and DeserializationContext only provide the least efficient (but simplest to use) methods that basically mimic current idea of serialization (convert msg to a new byte array and back), but the idea is that more efficient methods of accessing the payload will be added over time (I have some prototypes what the new methods could look like).\r\n\r\nThe changes to the marshaller (and the way backward compatibility is ensured) are inspired by  https://github.com/grpc/grpc/pull/13440/files#diff-2058a085f39531e466d41c163feccbe5 (which got never merged because there were some concerns around it), but the advantage of the new approach is that using abstract classes SerializationContext and DeserializationContext makes the serialization/deserialization much more extensible (with the older PR there was a risk that the newly introduces API was pretty rigid and it was likely we would be needing yet another new API soon).\r\n\r\n## Possible future additions to SerializationContext:\r\n- Add `IBufferWriter`-like methods that will allow serializing data into a grpc-owned buffer that can be made available to C core without any extra copies and that can be reused (https://github.com/dotnet/corefx/blob/master/src/System.Memory/src/System/Buffers/IBufferWriter.cs)\r\n\r\n\r\n## Possible future additions to DeserializationContext\r\nfor efficient deserialization I have some prototypes and this new deserializtion methods could look like this:\r\n\r\n\r\nand  an advanced method that will allow reading data from C core's byte_buffer  slice by slice directly\r\n(and that will utilize the new Span<> type to make that safely possible).\r\n",lang/C#|release notes: yes,apolcyn,"This PR is based on recent experiments with zero-copy parsing of received messages. It turns out a few degrees of optimization can be utilized when serializing/deserializing messages, but a new API is needed, as the existing API  just uses a function to converts msg of type T to a newly allocated byte array (and a newly created byte array to a msg of type T. The existing API basically necessitates copying of the buffer around and also prohibits us from pooling the buffers in any way.\r\n\r\nThis PR adds a new serialization API that is more general and extensible, basically it consists of two functions:\r\n- `Action<T, SerializationContext> ContextualSerializer`  - when serializing, one gets access to an instance of a message  and a `SerializationContext` that provides methods by which we store the serialized data in the context (`Complete(payload)` in the simplest and least efficent case) before returning.\r\n- `Func<DeserializationContext, T> ContextualDeserializer` - when deserializing, one gets access to a `DeserializationContext` which provides methods to access the payload at varying levels of efficiency\r\n\r\nRight now the SerializationContext and DeserializationContext only provide the least efficient (but simplest to use) methods that basically mimic current idea of serialization (convert msg to a new byte array and back), but the idea is that more efficient methods of accessing the payload will be added over time (I have some prototypes what the new methods could look like).\r\n\r\nThe changes to the marshaller (and the way backward compatibility is ensured) are inspired by  https://github.com/grpc/grpc/pull/13440/files#diff-2058a085f39531e466d41c163feccbe5 (which got never merged because there were some concerns around it), but the advantage of the new approach is that using abstract classes SerializationContext and DeserializationContext makes the serialization/deserialization much more extensible (with the older PR there was a risk that the newly introduces API was pretty rigid and it was likely we would be needing yet another new API soon).\r\n\r\n## Possible future additions to SerializationContext:\r\n- Add `IBufferWriter`-like methods that will allow serializing data into a grpc-owned buffer that can be made available to C core without any extra copies and that can be reused (https://github.com/dotnet/corefx/blob/master/src/System.Memory/src/System/Buffers/IBufferWriter.cs)\r\n\r\n\r\n## Possible future additions to DeserializationContext\r\nfor efficient deserialization I have some prototypes and this new deserializtion methods could look like this:\r\n```csharp\r\n        /// <summary>\r\n        /// Gets the entire payload as a rented buffer.\r\n        /// Caller is reponsible for disposing the rented buffer once the done processing it to signal that the buffer can be reclaimed by gRPC\r\n        /// and used again for deserialization of another message.\r\n        /// In most cases, using this method involves copying to the entire buffer (as the payload is internally\r\n        /// delivered in multiple buffer segments and they need to be joined together to form a single monolithic buffer).\r\n        /// On the other hand, this method is much more efficient in terms of GC pressure than <see cref=""PayloadAsNewBuffer""/>\r\n        /// as the buffer is reused rather than thrown away. This comes at the cost of the additional complexity where one needs\r\n        /// to call <c>Dispose()</c> once done reading the data.\r\n        /// </summary>\r\n        /// <returns>a rented buffer the entire payload or null if payload is null.</returns>\r\n        internal abstract IMemoryOwner<byte> PayloadAsRentedBuffer();\r\n```\r\n\r\nand  an advanced method that will allow reading data from C core's byte_buffer  slice by slice directly\r\n(and that will utilize the new Span<> type to make that safely possible).\r\n```csharp\r\n        /// <summary>\r\n        /// Tries to get next segment of the payload.\r\n        /// This is the most efficient method of accessing the payload, no additional copying or allocation is made in most cases.\r\n        /// Throws an exception if payload is null.\r\n        /// </summary>\r\n        /// <param name=""bufferSegment"">will be set to the next buffer segment if operation is successful.</param>\r\n        /// <returns><c>true</c> the next segment was read, <c>false</c> if all segments have already been read.</returns>\r\n        internal abstract bool TryGetNextBufferSegment(out Span<byte> bufferSegment);\r\n```","csharp\r\n        /// <summary>\r\n        /// Gets the entire payload as a rented buffer.\r\n        /// Caller is reponsible for disposing the rented buffer once the done processing it to signal that the buffer can be reclaimed by gRPC\r\n        /// and used again for deserialization of another message.\r\n        /// In most cases, using this method involves copying to the entire buffer (as the payload is internally\r\n        /// delivered in multiple buffer segments and they need to be joined together to form a single monolithic buffer).\r\n        /// On the other hand, this method is much more efficient in terms of GC pressure than <see cref=""PayloadAsNewBuffer""/>\r\n        /// as the buffer is reused rather than thrown away. This comes at the cost of the additional complexity where one needs\r\n        /// to call <c>Dispose()</c> once done reading the data.\r\n        /// </summary>\r\n        /// <returns>a rented buffer the entire payload or null if payload is null.</returns>\r\n        internal abstract IMemoryOwner<byte> PayloadAsRentedBuffer();\r\n"
16366,"status.proto in python (can't get full details)<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n 1.14.1 python\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n MacOS HighSierra 10.13.6\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n python 3.6.5\r\n \r\n### What did you do?\r\nIf possible, provide a recipe for reproducing the error. Try being specific and include code snippets if helpful.\r\n In golang code I use custom status.ptoto details:\r\n\r\n**proto**:\r\n```\r\nmessage EnumRequestType {\r\n    enum Request {\r\n        ResolveUsername = 0;\r\n        GetFullChannel = 1;\r\n        GetHistory = 2;\r\n        GetMessages = 3;\r\n        GetMessagesViews = 4;\r\n        Unknown = 5;\r\n    }\r\n}\r\n\r\nmessage EnumNotFoundDetailCode {\r\n    enum Code {\r\n        SUBSCRIPTION_NOT_EXISTS = 0;\r\n        SESSION_NOT_EXISTS = 1;\r\n        LOCKED = 2;\r\n        BANNED = 3;\r\n        RATE_LIMIT_EXCEEDED = 4;\r\n    }\r\n}\r\n\r\nmessage RateLimitExceededDetails {\r\n    uint64 phone = 1;\r\n    EnumRequestType.Request request = 2;\r\n    string rate_limit = 3;\r\n    int64 reset_time_left = 4; //milliseconds\r\n}\r\n\r\nmessage NotFoundDetails {\r\n    EnumNotFoundDetailCode.Code code = 1;\r\n    oneof details {\r\n        RateLimitExceededDetails rate_limit_exceeded_details = 2;\r\n    }\r\n}\r\n```\r\n\r\n**go server code**:\r\n\r\n\r\n**go client code**:\r\n\r\n\r\nIn python I only have a representation string of this details.\r\n\r\n**python client code**:\r\n\r\n\r\n### What did you expect to see?\r\n \r\n Full protobuf `NotFoundDetails` message\r\n### What did you see instead?\r\n `'rate limit exceeded  [phone: 79166667818, request: GetHistory, rate limit: 1_per_second, reset time left: 973ms]'`\r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNo",kind/enhancement|lang/Python|priority/P3,lidizheng,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n 1.14.1 python\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n MacOS HighSierra 10.13.6\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n python 3.6.5\r\n \r\n### What did you do?\r\nIf possible, provide a recipe for reproducing the error. Try being specific and include code snippets if helpful.\r\n In golang code I use custom status.ptoto details:\r\n\r\n**proto**:\r\n```\r\nmessage EnumRequestType {\r\n    enum Request {\r\n        ResolveUsername = 0;\r\n        GetFullChannel = 1;\r\n        GetHistory = 2;\r\n        GetMessages = 3;\r\n        GetMessagesViews = 4;\r\n        Unknown = 5;\r\n    }\r\n}\r\n\r\nmessage EnumNotFoundDetailCode {\r\n    enum Code {\r\n        SUBSCRIPTION_NOT_EXISTS = 0;\r\n        SESSION_NOT_EXISTS = 1;\r\n        LOCKED = 2;\r\n        BANNED = 3;\r\n        RATE_LIMIT_EXCEEDED = 4;\r\n    }\r\n}\r\n\r\nmessage RateLimitExceededDetails {\r\n    uint64 phone = 1;\r\n    EnumRequestType.Request request = 2;\r\n    string rate_limit = 3;\r\n    int64 reset_time_left = 4; //milliseconds\r\n}\r\n\r\nmessage NotFoundDetails {\r\n    EnumNotFoundDetailCode.Code code = 1;\r\n    oneof details {\r\n        RateLimitExceededDetails rate_limit_exceeded_details = 2;\r\n    }\r\n}\r\n```\r\n\r\n**go server code**:\r\n\r\n```golang\r\nif err == al.ErrSessionLocked {\r\n\tif st, e := status.New(codes.NotFound, err.Error()).WithDetails(&grpcgrievoustlg.NotFoundDetails{Code: grpcgrievoustlg.EnumNotFoundDetailCode_LOCKED}); e != nil {\r\n\t\terr = e\r\n\t} else {\r\n\t\terr = st.Err()\r\n\t}\r\n}\r\n```\r\n**go client code**:\r\n\r\n```golang\r\nif st, ok := status.FromError(err); !ok {\r\n\tlog.Println(st.Err())\r\n} else if st.Code() != codes.OK {\r\n\tif st.Code() == codes.NotFound {\r\n\t\tdetails := st.Details()[0].(*grpcgrievoustlg.NotFoundDetails)\r\n\t\tswitch details.Code {\r\n\t\tcase grpcgrievoustlg.EnumNotFoundDetailCode_RATE_LIMIT_EXCEEDED:\r\n\t\t\tlog.Println(st.Err())\r\n\t\tdefault:\r\n\t\t\tlog.Println(st.Err())\r\n\t\t}\r\n\t} else {\r\n\t\tlog.Println(st.Err())\r\n\t}\r\n}\r\n```\r\nIn python I only have a representation string of this details.\r\n\r\n**python client code**:\r\n\r\n```python\r\nif e.code() != grpc.StatusCode.NOT_FOUND:\r\n    raise SessionManagerException(e)\r\ndetails=e.details()\r\n```\r\n### What did you expect to see?\r\n \r\n Full protobuf `NotFoundDetails` message\r\n### What did you see instead?\r\n `'rate limit exceeded  [phone: 79166667818, request: GetHistory, rate limit: 1_per_second, reset time left: 973ms]'`\r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNo","golang\r\nif err == al.ErrSessionLocked {\r\n\tif st, e := status.New(codes.NotFound, err.Error()).WithDetails(&grpcgrievoustlg.NotFoundDetails{Code: grpcgrievoustlg.EnumNotFoundDetailCode_LOCKED}); e != nil {\r\n\t\terr = e\r\n\t} else {\r\n\t\terr = st.Err()\r\n\t}\r\n}\r\n"
16290,"Python grpcio-1.14.1 Segmentation Faults caused by Channel.close() when used with connectivity-state subscriptions\r\n### Questions\r\n\r\n- **What version of gRPC and what language are you using?**\r\n\r\n  Language: **Python** `2.7.15`, `3.6.6`\r\n  \r\n  gRPC:  **`grpcio-1.14.1`** &nbsp; &nbsp; (same issue in `1.14.0`, `1.13.1`, `1.13.0`)\r\n\r\n- **What operating system and version?**\r\n\r\n  Linux: Fedora 28,  Ubuntu 16.04\r\n    \r\n- **What runtime / compiler are you using**\r\n\r\n  Python `2.7.15` and `3.6.6`, using the `grpcio` package straight from [pypi.org](https://pypi.org)\r\n\r\n \r\n### What did you do?\r\n\r\nWe upgraded from gRPC `1.11.1` to `1.14.1` and tried to use the new ``Channel.close()`` method.\r\n\r\nNow gRPC is causing **Segmentation Faults**.  \r\n\r\nThis seems to be correlated to an interaction between *subscriptions* to Channel \r\nconnectivity-state and the Channel's *`close()` method.* \r\n\r\nThis is issue is not present in grpc `1.11.1` (there is no `close()` method after all).\r\n\r\nAs fas as I understand, this is the **correct** procedure:\r\n1. Create a channel\r\n2. Add a subscription callback\r\n3. Invoke some RPCs\r\n4. Unsubscribe the callback\r\n5. Close the channel\r\n\r\nYet even doing the *correct* thing, we are getting occasional segfaults.\r\n\r\nWorse yet, if your code misses a step or gets anything out of order -- you don't\r\nget an Exception -- you get **segfaults**. For example:\r\n\r\n1. Call `subscribe()` *after* the channel is closed --> *segfault every time*\r\n2. Call `unsubscribe()` *after* the channel is closed --> frequent segfaults\r\n3. Forget to unsubscribe the callback and close the channel --> frequent segfaults\r\n\r\n\r\n### What did you expect to see?\r\n\r\n- In all cases, I expect gRPC **will not segfault**\r\n\r\n- In the case where all events are done with correct procedure, I expect no\r\n  Exceptions, no noisy and misleading log messages.\r\n  \r\n- If `subscribe` or `unsubscribe` are called *after* the channel is closed,\r\n  I would expect to get a python Exception that I can handle.\r\n\r\n- If `unsubscribe` is *not called* before the channel is closed, I expect to\r\n  see no error, or at most a logged error. gRPC should close the channel and\r\n  discard the subscribed callback function.\r\n\r\n \r\n### What did you see instead?\r\n\r\n1. **Segmentation faults**\r\n2. Lots of noisy logged exceptions. The message accompanying these log errors\r\n   is misleading as well.\r\n\r\n----\r\n\r\n### Example code\r\n\r\nThese examples use the `helloworld` protos and run against the `greeter_server.py`\r\nfrom the `examples` directory of this repo.\r\n\r\n\r\n### Example 1: call subscribe() *after* the channel is closed\r\n\r\nThis will immediately result in a segfault every time:\r\n\r\n\r\n\r\n#### Output: Example 1 \r\n\r\n- **1.1**\r\n  ```\r\n  $ python subscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n\r\n----\r\n\r\n\r\n### Example 2: call unsubscribe() after the channel is closed\r\n\r\nThis randomly (but frequently) results in a segfault:\r\n\r\n\r\n\r\n#### Output: Example 2 \r\n\r\n- **2.1**\r\n  ```\r\n  $ python unsubscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n  \r\n- **2.2**\r\n  ```\r\n  $ python unsubscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Exception in thread Thread-1:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  ****  Cut for brevity  ****\r\n  \r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Exception in thread Thread-8:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Response 4: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Example 3: forget to call unsubscribe() before closing the channel\r\n\r\nThis randomly (but frequently) results in a segfault:\r\n\r\n\r\n\r\n#### Output: Example 3 \r\n\r\n- **3.1**\r\n  ```\r\n  $ python forget_to_unsubscribe.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Exception in thread Thread-1:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  ****  Cut for brevity  ****\r\n  \r\n  Response 4: Hello, Stranger!\r\n  Exception in thread Thread-10:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Segmentation fault\r\n  ```\r\n\r\n\r\n----\r\n\r\n\r\n### Example 4: Do everything right and still get segfaults\r\n\r\nAs far as I know, this example does everything correctly.\r\nIt does not segfault *very often*, but it still happens. Maybe worth mentioning\r\nthat we have not observed this to segfault in Python 3.\r\n\r\n\r\n\r\n#### Output: Example 4\r\n\r\n- **4.1**\r\n  ```\r\n  $ python the_right_way.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n- **4.2**\r\n  ```\r\n  $ python the_right_way.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Exception in thread Thread-10:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  Response 4: Hello, Stranger!\r\n  \r\n  Response 5: Hello, Stranger!\r\n  Response 6: Hello, Stranger!\r\n  Response 7: Hello, Stranger!\r\n  Response 8: Hello, Stranger!\r\n  Response 9: Hello, Stranger!\r\n  Response 10: Hello, Stranger!\r\n  Response 11: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Example 5: If Example 4 didn't segfault, try it with a bunch of threads!\r\n\r\nIf the ""correct"" code seems like its working, try running the same thing with \r\nwith a small threadpool.\r\n\r\n\r\n\r\n#### Output: Example 5\r\n\r\n- **5.1**\r\n  ```\r\n  $ python the_right_way_threaded.py \r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 4: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Response 5: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n- **5.2**\r\n  ```\r\n  $ python the_right_way_threaded.py \r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 3: Hello, Stranger!Response 2: Hello, Stranger!\r\n  \r\n  Response 6: Hello, Stranger!\r\n  Response 4: Hello, Stranger!\r\n  Exception in thread Thread-3:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nWe are trying to upgrade from grpcio `1.11.1` to `1.14.x`. These issues have\r\nbeen a complete show-stopper for us.\r\n\r\nI anticipate that only a small fraction of users make use of these Channel\r\nconnectivity-state subscriptions. However, they are quite important to our\r\nimplementation and our growing gRPC-based platform.\r\n\r\nThanks!\r\n",kind/bug|lang/Python|priority/P2,ericgribkoff,"\r\n### Questions\r\n\r\n- **What version of gRPC and what language are you using?**\r\n\r\n  Language: **Python** `2.7.15`, `3.6.6`\r\n  \r\n  gRPC:  **`grpcio-1.14.1`** &nbsp; &nbsp; (same issue in `1.14.0`, `1.13.1`, `1.13.0`)\r\n\r\n- **What operating system and version?**\r\n\r\n  Linux: Fedora 28,  Ubuntu 16.04\r\n    \r\n- **What runtime / compiler are you using**\r\n\r\n  Python `2.7.15` and `3.6.6`, using the `grpcio` package straight from [pypi.org](https://pypi.org)\r\n\r\n \r\n### What did you do?\r\n\r\nWe upgraded from gRPC `1.11.1` to `1.14.1` and tried to use the new ``Channel.close()`` method.\r\n\r\nNow gRPC is causing **Segmentation Faults**.  \r\n\r\nThis seems to be correlated to an interaction between *subscriptions* to Channel \r\nconnectivity-state and the Channel's *`close()` method.* \r\n\r\nThis is issue is not present in grpc `1.11.1` (there is no `close()` method after all).\r\n\r\nAs fas as I understand, this is the **correct** procedure:\r\n1. Create a channel\r\n2. Add a subscription callback\r\n3. Invoke some RPCs\r\n4. Unsubscribe the callback\r\n5. Close the channel\r\n\r\nYet even doing the *correct* thing, we are getting occasional segfaults.\r\n\r\nWorse yet, if your code misses a step or gets anything out of order -- you don't\r\nget an Exception -- you get **segfaults**. For example:\r\n\r\n1. Call `subscribe()` *after* the channel is closed --> *segfault every time*\r\n2. Call `unsubscribe()` *after* the channel is closed --> frequent segfaults\r\n3. Forget to unsubscribe the callback and close the channel --> frequent segfaults\r\n\r\n\r\n### What did you expect to see?\r\n\r\n- In all cases, I expect gRPC **will not segfault**\r\n\r\n- In the case where all events are done with correct procedure, I expect no\r\n  Exceptions, no noisy and misleading log messages.\r\n  \r\n- If `subscribe` or `unsubscribe` are called *after* the channel is closed,\r\n  I would expect to get a python Exception that I can handle.\r\n\r\n- If `unsubscribe` is *not called* before the channel is closed, I expect to\r\n  see no error, or at most a logged error. gRPC should close the channel and\r\n  discard the subscribed callback function.\r\n\r\n \r\n### What did you see instead?\r\n\r\n1. **Segmentation faults**\r\n2. Lots of noisy logged exceptions. The message accompanying these log errors\r\n   is misleading as well.\r\n\r\n----\r\n\r\n### Example code\r\n\r\nThese examples use the `helloworld` protos and run against the `greeter_server.py`\r\nfrom the `examples` directory of this repo.\r\n\r\n\r\n### Example 1: call subscribe() *after* the channel is closed\r\n\r\nThis will immediately result in a segfault every time:\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n    \r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response: {}'.format(response.message))\r\n    \r\n    channel.close()\r\n    # Don't subscribe to a closed channel. This is no exception, it's a segfault!\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n#### Output: Example 1 \r\n\r\n- **1.1**\r\n  ```\r\n  $ python subscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n\r\n----\r\n\r\n\r\n### Example 2: call unsubscribe() after the channel is closed\r\n\r\nThis randomly (but frequently) results in a segfault:\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    for count in range(50):\r\n        run_once(count)\r\n\r\ndef run_once(count):\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response {}: {}'.format(count, response.message))\r\n\r\n    channel.close()                                         #  <-----\r\n    # Yikes! Don't unsubscribe after closing the channel!\r\n    channel.unsubscribe(connectivity_callback)              #  <-----\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n#### Output: Example 2 \r\n\r\n- **2.1**\r\n  ```\r\n  $ python unsubscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n  \r\n- **2.2**\r\n  ```\r\n  $ python unsubscribe_after_close.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Exception in thread Thread-1:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  ****  Cut for brevity  ****\r\n  \r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Exception in thread Thread-8:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Response 4: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Example 3: forget to call unsubscribe() before closing the channel\r\n\r\nThis randomly (but frequently) results in a segfault:\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    for count in range(50):\r\n        run_once(count)\r\n\r\ndef run_once(count):\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response {}: {}'.format(count, response.message))\r\n\r\n    # Danger! Don't forget to unsubscribe your callback!\r\n    ## channel.unsubscribe(connectivity_callback)           #  <-----\r\n    channel.close()                                         #  <-----\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n#### Output: Example 3 \r\n\r\n- **3.1**\r\n  ```\r\n  $ python forget_to_unsubscribe.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Exception in thread Thread-1:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  ****  Cut for brevity  ****\r\n  \r\n  Response 4: Hello, Stranger!\r\n  Exception in thread Thread-10:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Segmentation fault\r\n  ```\r\n\r\n\r\n----\r\n\r\n\r\n### Example 4: Do everything right and still get segfaults\r\n\r\nAs far as I know, this example does everything correctly.\r\nIt does not segfault *very often*, but it still happens. Maybe worth mentioning\r\nthat we have not observed this to segfault in Python 3.\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    for count in range(50):\r\n        run_once(count)\r\n\r\ndef run_once(count):\r\n    # This is how it's supposed to work right?\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response {}: {}'.format(count, response.message))\r\n\r\n    channel.unsubscribe(connectivity_callback)\r\n    channel.close()\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n#### Output: Example 4\r\n\r\n- **4.1**\r\n  ```\r\n  $ python the_right_way.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n- **4.2**\r\n  ```\r\n  $ python the_right_way.py\r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Exception in thread Thread-10:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  Response 4: Hello, Stranger!\r\n  \r\n  Response 5: Hello, Stranger!\r\n  Response 6: Hello, Stranger!\r\n  Response 7: Hello, Stranger!\r\n  Response 8: Hello, Stranger!\r\n  Response 9: Hello, Stranger!\r\n  Response 10: Hello, Stranger!\r\n  Response 11: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Example 5: If Example 4 didn't segfault, try it with a bunch of threads!\r\n\r\nIf the ""correct"" code seems like its working, try running the same thing with \r\nwith a small threadpool.\r\n\r\n```python\r\nfrom __future__ import print_function\r\nfrom concurrent import futures\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    executor = futures.ThreadPoolExecutor(max_workers=8)\r\n    calls = [executor.submit(run_once, count) for count in range(128)]\r\n    futures.wait(calls)\r\n\r\ndef run_once(count):\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response {}: {}'.format(count, response.message))\r\n\r\n    channel.unsubscribe(connectivity_callback)\r\n    channel.close()\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n    \r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n#### Output: Example 5\r\n\r\n- **5.1**\r\n  ```\r\n  $ python the_right_way_threaded.py \r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 2: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 4: Hello, Stranger!\r\n  Response 3: Hello, Stranger!\r\n  Response 5: Hello, Stranger!\r\n  Segmentation fault\r\n  ```\r\n\r\n- **5.2**\r\n  ```\r\n  $ python the_right_way_threaded.py \r\n  \r\n  Response 0: Hello, Stranger!\r\n  Response 1: Hello, Stranger!\r\n  Response 3: Hello, Stranger!Response 2: Hello, Stranger!\r\n  \r\n  Response 6: Hello, Stranger!\r\n  Response 4: Hello, Stranger!\r\n  Exception in thread Thread-3:\r\n  Traceback (most recent call last):\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 801, in __bootstrap_inner\r\n      self.run()\r\n    File ""/home/eheller/.pythonz/pythons/CPython-2.7.15/lib/python2.7/threading.py"", line 754, in run\r\n      self.__target(*self.__args, **self.__kwargs)\r\n    File ""/home/eheller/pew_envs/grpc-seg14/lib/python2.7/site-packages/grpc/_channel.py"", line 801, in _poll_connectivity\r\n      time.time() + 0.2)\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 451, in grpc._cython.cygrpc.Channel.watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 336, in grpc._cython.cygrpc._watch_connectivity_state\r\n    File ""src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi"", line 344, in grpc._cython.cygrpc._watch_connectivity_state\r\n  ValueError: Cannot invoke RPC on closed channel!\r\n  \r\n  Segmentation fault\r\n  ```\r\n\r\n----\r\n\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nWe are trying to upgrade from grpcio `1.11.1` to `1.14.x`. These issues have\r\nbeen a complete show-stopper for us.\r\n\r\nI anticipate that only a small fraction of users make use of these Channel\r\nconnectivity-state subscriptions. However, they are quite important to our\r\nimplementation and our growing gRPC-based platform.\r\n\r\nThanks!\r\n","python\r\nfrom __future__ import print_function\r\nimport grpc\r\nimport helloworld_pb2\r\nimport helloworld_pb2_grpc\r\n\r\ndef run():\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n    \r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='Stranger'))\r\n    print('Response: {}'.format(response.message))\r\n    \r\n    channel.close()\r\n    # Don't subscribe to a closed channel. This is no exception, it's a segfault!\r\n    channel.subscribe(connectivity_callback, try_to_connect=True)\r\n\r\ndef connectivity_callback(state):\r\n    pass  # no-op\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n"
16013,"[Ruby] Segmentation Fault with puma<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n\r\nHi :sun_with_face: I found segmentation fault maybe caused by grpc with puma.\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nRuby\r\n\r\n\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n\r\n\r\n### What did you do?\r\n\r\nReproducable code is here.\r\nhttps://gitlab.com/nownabe/grpc_puma_segmentation_fault\r\n\r\nIt's a simple rack app with gRPC.\r\nWhen cluster mode Puma with `preload_app!` runs this app, requests are not finished and segmentation faults occur.\r\n\r\n[rackapp](https://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/config.ru)\r\n[puma config](https://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/puma.rb)\r\n\r\n\r\nOriginally, my Rails app tried to send request logs to Stackdriver Logging using google-cloud-logging gem but requests were stacked at [`Google::Cloud::Logging::Project#write_entries`](http://googlecloudplatform.github.io/google-cloud-ruby/#/docs/google-cloud-logging/v1.5.1/google/cloud/logging/project?method=write_entries-instance) \r\n \r\n### What did you expect to see?\r\n \r\nNo segmentation fault.\r\n \r\n### What did you see instead?\r\n \r\nhttps://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/log/stderr.log\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/bug|lang/ruby|priority/P2|disposition/stale,apolcyn,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n\r\nHi :sun_with_face: I found segmentation fault maybe caused by grpc with puma.\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nRuby\r\n\r\n```bash\r\n$ bundle exec gem list grpc\r\n\r\n*** LOCAL GEMS ***\r\n\r\ngrpc (1.13.0 x86_64-linux)\r\ngrpc-tools (1.13.0)\r\n```\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n```bash\r\n$ uname -a\r\nLinux mypc 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n```bash\r\n $ ruby -v\r\nruby 2.5.1p57 (2018-03-29 revision 63029) [x86_64-linux]\r\n```\r\n\r\n### What did you do?\r\n\r\nReproducable code is here.\r\nhttps://gitlab.com/nownabe/grpc_puma_segmentation_fault\r\n\r\nIt's a simple rack app with gRPC.\r\nWhen cluster mode Puma with `preload_app!` runs this app, requests are not finished and segmentation faults occur.\r\n\r\n[rackapp](https://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/config.ru)\r\n[puma config](https://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/puma.rb)\r\n\r\n\r\nOriginally, my Rails app tried to send request logs to Stackdriver Logging using google-cloud-logging gem but requests were stacked at [`Google::Cloud::Logging::Project#write_entries`](http://googlecloudplatform.github.io/google-cloud-ruby/#/docs/google-cloud-logging/v1.5.1/google/cloud/logging/project?method=write_entries-instance) \r\n \r\n### What did you expect to see?\r\n \r\nNo segmentation fault.\r\n \r\n### What did you see instead?\r\n \r\nhttps://gitlab.com/nownabe/grpc_puma_segmentation_fault/blob/master/log/stderr.log\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",bash\r\n$ bundle exec gem list grpc\r\n\r\n*** LOCAL GEMS ***\r\n\r\ngrpc (1.13.0 x86_64-linux)\r\ngrpc-tools (1.13.0)\r\n
15972,"grpc c++ bidirectional stream crash with  ""proto_buffer_writer.h:65 assertion failed: !byte_buffer->Valid()"" ### What version of gRPC and what language are you using?\r\nversion: Release 1.10.1;\r\nlanguage: c++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux version 4.15.0-21deepin-generic\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n gcc version 7.2.0 (Debian 7.2.0-20)\r\n \r\n### What did you do?\r\n```header\r\nclass GrpcTransport\r\n    {\r\n    public:\r\n        enum class Type\r\n        {\r\n            CONNECT = 1,\r\n            READ = 2,\r\n            WRITE = 3,\r\n            FINISH = 4\r\n        };\r\n\r\n    public:\r\n        GrpcTransport(const std::string &ip, size_t max_size);\r\n\r\n        ~GrpcTransport();\r\n\r\n        bool append(const clientMsg& msg);\r\n\r\n    private:\r\n        void init();\r\n\r\n        void flushToServer();\r\n\r\n        void cqHandleFunc();\r\n\r\n        void getServerInfo();\r\n\r\n    private:\r\n        int _waitTime;\r\n        bool _running;\r\n        bool _health;\r\n        bool _initSign;\r\n        size_t _max_size; \r\n        std::thread _writeThread;\r\n        std::thread _cqHandleThread;\r\n        std::shared_ptr<grpc::Channel> _channel;\r\n        std::unique_ptr<Connection::Stub> _stub;\r\n        grpc::CompletionQueue _cq;\r\n        std::unique_ptr<grpc::ClientContext> _context;\r\n        std::unique_ptr<grpc::ClientAsyncReaderWriter<clientMsg, ServerMsg>> _stream;\r\n        boost::lockfree::spsc_queue<clientMsg> _queue;\r\n        ServerMsg _serverMessage;\r\n    };\r\n```\r\n\r\n\r\nIn the code, i set the restart the bidi-stream method.\r\nWhen i start the code, it often corrupt with 'proto_buffer_writer.h:65]   assertion failed: !byte_buffer->Valid()' or 'channel_cc.cc:131]          assertion failed: GRPC_CALL_OK == grpc_call_start_batch(call->call(), cops, nops, ops, nullptr)'.\r\n\r\n(1) If i set the time between of each request with a long time, it's works well.\r\nBut if i set the time between each request is so samll, it will corrupt with 'proto_buffer_writer.h:65]   assertion failed: !byte_buffer->Valid()'.\r\n(2)The 'channel_cc.cc:131]          assertion failed: GRPC_CALL_OK == grpc_call_start_batch(call->call(), cops, nops, ops, nullptr)' often appear when i start the code, not all the time.\r\n \r\n### What did you expect to see?\r\n I want to know how to use the async api for grpc bidi-stream.\r\n \r\n### What did you see instead?\r\nI want to see some demo about the async stream for client/server.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/question|lang/c++|priority/P2|disposition/stale,vjpai," ### What version of gRPC and what language are you using?\r\nversion: Release 1.10.1;\r\nlanguage: c++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux version 4.15.0-21deepin-generic\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n gcc version 7.2.0 (Debian 7.2.0-20)\r\n \r\n### What did you do?\r\n```header\r\nclass GrpcTransport\r\n    {\r\n    public:\r\n        enum class Type\r\n        {\r\n            CONNECT = 1,\r\n            READ = 2,\r\n            WRITE = 3,\r\n            FINISH = 4\r\n        };\r\n\r\n    public:\r\n        GrpcTransport(const std::string &ip, size_t max_size);\r\n\r\n        ~GrpcTransport();\r\n\r\n        bool append(const clientMsg& msg);\r\n\r\n    private:\r\n        void init();\r\n\r\n        void flushToServer();\r\n\r\n        void cqHandleFunc();\r\n\r\n        void getServerInfo();\r\n\r\n    private:\r\n        int _waitTime;\r\n        bool _running;\r\n        bool _health;\r\n        bool _initSign;\r\n        size_t _max_size; \r\n        std::thread _writeThread;\r\n        std::thread _cqHandleThread;\r\n        std::shared_ptr<grpc::Channel> _channel;\r\n        std::unique_ptr<Connection::Stub> _stub;\r\n        grpc::CompletionQueue _cq;\r\n        std::unique_ptr<grpc::ClientContext> _context;\r\n        std::unique_ptr<grpc::ClientAsyncReaderWriter<clientMsg, ServerMsg>> _stream;\r\n        boost::lockfree::spsc_queue<clientMsg> _queue;\r\n        ServerMsg _serverMessage;\r\n    };\r\n```\r\n\r\n```cpp\r\n   GrpcTransport::GrpcTransport(const std::string &ip, size_t max_size)\r\n            : _waitTime(1),\r\n              _running(false),\r\n              _health(false),\r\n              _initSign(false),\r\n              _writeSign(false),\r\n              _max_size(max_size),\r\n              _channel(grpc::CreateChannel(ip, grpc::InsecureChannelCredentials())),\r\n              _stub(Connection::NewStub(_channel)),\r\n              _cq(),\r\n              _context(new grpc::ClientContext),\r\n              _queue(_max_size) \r\n    {\r\n        init();\r\n    }\r\n\r\n    GrpcTransport::~GrpcTransport()\r\n    {\r\n        if (_running)\r\n        {\r\n            _running = false;      \r\n            _cq.Shutdown();\r\n            _writeThread.join();    \r\n            _cqHandleThread.join(); \r\n        }\r\n    }\r\n\r\n    void GrpcTransport::init()\r\n    {\r\n        _cqHandleThread = std::move(std::thread([this] { cqHandleFunc(); }));\r\n        auto res = _channel->WaitForConnected(std::chrono::system_clock::now() + std::chrono::milliseconds(500));\r\n        if (res)\r\n        {\r\n            _health = true;\r\n            _stream = _stub->Asyncchat(_context.get(), &_cq, reinterpret_cast<void *>(Type::CONNECT));\r\n        } \r\n        _running = true;\r\n        _writeThread = std::move(std::thread([this] { flushToServer(); }));\r\n    }\r\n\r\n    bool GrpcTransport::getTransportHealthStatus() const noexcept\r\n    {\r\n        return _health;\r\n    }\r\n\r\n    bool GrpcTransport::append(const clientMsg& msg)\r\n    {\r\n        return _queue.push(message);\r\n    }\r\n\r\n    void GrpcTransport::flushToServer()\r\n    {\r\n        while (_running)\r\n        {\r\n            if (!_health)\r\n            {\r\n                auto state = _channel->GetState(true);\r\n                if (GRPC_CHANNEL_SHUTDOWN == state)\r\n                {\r\n                    _running = false;\r\n                    _cq.Shutdown();\r\n                    break;\r\n                } else\r\n                {\r\n                    auto res = _channel->WaitForConnected(\r\n                            std::chrono::system_clock::now() + std::chrono::seconds(_waitTime));\r\n                    if (res)   \r\n                    {\r\n                        _waitTime = 1;\r\n                        _health = true; \r\n                        _initSign = false;\r\n                        _context.reset(new grpc::ClientContext);\r\n                        _stream = _stub->Asyncchat(_context.get(), &_cq, reinterpret_cast<void *>(Type::CONNECT));\r\n                        _stream->Read(&_serverMessage, reinterpret_cast<void *>(Type::READ));\r\n                    } else\r\n                    {\r\n                        _waitTime = (_waitTime * 2 < 300) ? (_waitTime * 2) : 300;\r\n                        continue;  \r\n                    }\r\n                }\r\n            }  \r\n\r\n            if (!_initSign)\r\n            {\r\n                std::this_thread::sleep_for(std::chrono::milliseconds(100));\r\n                continue;\r\n            }\r\n\r\n            auto message = new SdkMessage;\r\n            auto res = _queue.pop(*message);\r\n            if (!res)\r\n            {\r\n                std::this_thread::sleep_for(std::chrono::milliseconds(20));\r\n                continue;\r\n            }\r\n            _stream->Write(*message, reinterpret_cast<void *>(message));\r\n        }\r\n        if (_health)\r\n        {\r\n            _stream->WritesDone(reinterpret_cast<void *>(Type::FINISH));\r\n        }\r\n    }\r\n\r\n    void GrpcTransport::cqHandleFunc()\r\n    {\r\n        while (true)\r\n        {\r\n            void *got_tag;\r\n            bool ok = false;\r\n            if (!_cq.Next(&got_tag, &ok)) \r\n            {\r\n                return;\r\n            }\r\n\r\n            if (!ok)\r\n            {\r\n                _health = false;  \r\n            }\r\n\r\n            switch (static_cast<Type >(reinterpret_cast<long>(got_tag)))\r\n            {\r\n                case Type::READ:\r\n                    if (ok)\r\n                    {\r\n                    } \r\n                    break;\r\n                case Type::CONNECT:\r\n                    if (ok)\r\n                    {\r\n                        _initSign = true;\r\n                        _stream->Read(&_serverMessage, reinterpret_cast<void *>(Type::READ));\r\n                    } \r\n                    break;\r\n                case Type::FINISH:\r\n                    _context->TryCancel();\r\n                    _cq.Shutdown();\r\n                    break;\r\n                default:\r\n                {\r\n                    auto message = static_cast<SdkMessage *>(got_tag);\r\n                    delete message;\r\n                }\r\n            }\r\n        }   //while\r\n    }   //cqHandleFunc\r\n```\r\nIn the code, i set the restart the bidi-stream method.\r\nWhen i start the code, it often corrupt with 'proto_buffer_writer.h:65]   assertion failed: !byte_buffer->Valid()' or 'channel_cc.cc:131]          assertion failed: GRPC_CALL_OK == grpc_call_start_batch(call->call(), cops, nops, ops, nullptr)'.\r\n\r\n(1) If i set the time between of each request with a long time, it's works well.\r\nBut if i set the time between each request is so samll, it will corrupt with 'proto_buffer_writer.h:65]   assertion failed: !byte_buffer->Valid()'.\r\n(2)The 'channel_cc.cc:131]          assertion failed: GRPC_CALL_OK == grpc_call_start_batch(call->call(), cops, nops, ops, nullptr)' often appear when i start the code, not all the time.\r\n \r\n### What did you expect to see?\r\n I want to know how to use the async api for grpc bidi-stream.\r\n \r\n### What did you see instead?\r\nI want to see some demo about the async stream for client/server.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","cpp\r\n   GrpcTransport::GrpcTransport(const std::string &ip, size_t max_size)\r\n            : _waitTime(1),\r\n              _running(false),\r\n              _health(false),\r\n              _initSign(false),\r\n              _writeSign(false),\r\n              _max_size(max_size),\r\n              _channel(grpc::CreateChannel(ip, grpc::InsecureChannelCredentials())),\r\n              _stub(Connection::NewStub(_channel)),\r\n              _cq(),\r\n              _context(new grpc::ClientContext),\r\n              _queue(_max_size) \r\n    {\r\n        init();\r\n    }\r\n\r\n    GrpcTransport::~GrpcTransport()\r\n    {\r\n        if (_running)\r\n        {\r\n            _running = false;      \r\n            _cq.Shutdown();\r\n            _writeThread.join();    \r\n            _cqHandleThread.join(); \r\n        }\r\n    }\r\n\r\n    void GrpcTransport::init()\r\n    {\r\n        _cqHandleThread = std::move(std::thread([this] { cqHandleFunc(); }));\r\n        auto res = _channel->WaitForConnected(std::chrono::system_clock::now() + std::chrono::milliseconds(500));\r\n        if (res)\r\n        {\r\n            _health = true;\r\n            _stream = _stub->Asyncchat(_context.get(), &_cq, reinterpret_cast<void *>(Type::CONNECT));\r\n        } \r\n        _running = true;\r\n        _writeThread = std::move(std::thread([this] { flushToServer(); }));\r\n    }\r\n\r\n    bool GrpcTransport::getTransportHealthStatus() const noexcept\r\n    {\r\n        return _health;\r\n    }\r\n\r\n    bool GrpcTransport::append(const clientMsg& msg)\r\n    {\r\n        return _queue.push(message);\r\n    }\r\n\r\n    void GrpcTransport::flushToServer()\r\n    {\r\n        while (_running)\r\n        {\r\n            if (!_health)\r\n            {\r\n                auto state = _channel->GetState(true);\r\n                if (GRPC_CHANNEL_SHUTDOWN == state)\r\n                {\r\n                    _running = false;\r\n                    _cq.Shutdown();\r\n                    break;\r\n                } else\r\n                {\r\n                    auto res = _channel->WaitForConnected(\r\n                            std::chrono::system_clock::now() + std::chrono::seconds(_waitTime));\r\n                    if (res)   \r\n                    {\r\n                        _waitTime = 1;\r\n                        _health = true; \r\n                        _initSign = false;\r\n                        _context.reset(new grpc::ClientContext);\r\n                        _stream = _stub->Asyncchat(_context.get(), &_cq, reinterpret_cast<void *>(Type::CONNECT));\r\n                        _stream->Read(&_serverMessage, reinterpret_cast<void *>(Type::READ));\r\n                    } else\r\n                    {\r\n                        _waitTime = (_waitTime * 2 < 300) ? (_waitTime * 2) : 300;\r\n                        continue;  \r\n                    }\r\n                }\r\n            }  \r\n\r\n            if (!_initSign)\r\n            {\r\n                std::this_thread::sleep_for(std::chrono::milliseconds(100));\r\n                continue;\r\n            }\r\n\r\n            auto message = new SdkMessage;\r\n            auto res = _queue.pop(*message);\r\n            if (!res)\r\n            {\r\n                std::this_thread::sleep_for(std::chrono::milliseconds(20));\r\n                continue;\r\n            }\r\n            _stream->Write(*message, reinterpret_cast<void *>(message));\r\n        }\r\n        if (_health)\r\n        {\r\n            _stream->WritesDone(reinterpret_cast<void *>(Type::FINISH));\r\n        }\r\n    }\r\n\r\n    void GrpcTransport::cqHandleFunc()\r\n    {\r\n        while (true)\r\n        {\r\n            void *got_tag;\r\n            bool ok = false;\r\n            if (!_cq.Next(&got_tag, &ok)) \r\n            {\r\n                return;\r\n            }\r\n\r\n            if (!ok)\r\n            {\r\n                _health = false;  \r\n            }\r\n\r\n            switch (static_cast<Type >(reinterpret_cast<long>(got_tag)))\r\n            {\r\n                case Type::READ:\r\n                    if (ok)\r\n                    {\r\n                    } \r\n                    break;\r\n                case Type::CONNECT:\r\n                    if (ok)\r\n                    {\r\n                        _initSign = true;\r\n                        _stream->Read(&_serverMessage, reinterpret_cast<void *>(Type::READ));\r\n                    } \r\n                    break;\r\n                case Type::FINISH:\r\n                    _context->TryCancel();\r\n                    _cq.Shutdown();\r\n                    break;\r\n                default:\r\n                {\r\n                    auto message = static_cast<SdkMessage *>(got_tag);\r\n                    delete message;\r\n                }\r\n            }\r\n        }   //while\r\n    }   //cqHandleFunc\r\n"
15760,"epollex: Errors due to `pollable` accessing a potentially orphaned `owner_fd`in epollex implementation, the `pollable` struct (see below) does not maintain a ref to `owner_fd` \r\n\r\n\r\nSince a `pollable` can outlive its `owner_fd`, the `owner_fd` could be orphaned/destroyed without the `pollable` knowing about it - and this causes a problem if the `pollable` tries to access `owner_fd` later (for example, see the function `pollset_add_fd_locked()` that tries to access the `owner_fd` field!)\r\n\r\n\r\n",kind/bug|area/core|lang/c++|priority/P2,sreecha,"in epollex implementation, the `pollable` struct (see below) does not maintain a ref to `owner_fd` \r\n\r\n```C++\r\nstruct pollable {\r\n  ...\r\n  // only for type fd... one ref to the owner fd\r\n  grpc_fd* owner_fd;\r\n...\r\n};\r\n```\r\nSince a `pollable` can outlive its `owner_fd`, the `owner_fd` could be orphaned/destroyed without the `pollable` knowing about it - and this causes a problem if the `pollable` tries to access `owner_fd` later (for example, see the function `pollset_add_fd_locked()` that tries to access the `owner_fd` field!)\r\n\r\n\r\n",C++\r\nstruct pollable {\r\n  ...\r\n  // only for type fd... one ref to the owner fd\r\n  grpc_fd* owner_fd;\r\n...\r\n};\r\n
15757,"Add functionality to allow dependency injection via ServerCallContext### What version of gRPC and what language are you using?\r\n \r\n1.12.0 csharp\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux, Windows\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nmsbuild\r\n \r\n### What did you do?\r\n\r\nWe wanted to implement a server application that makes some nifty use of dependency injection. To do so we make use of the Inceptor api that was added with 1.10.0. Currently there's however only a hacky way to add context aware objects to the ServerCallContext instance as outlined below:\r\n\r\n\r\n\r\nYes, we indeed hack the dependency injection mechanism into the current implementation of the ServerCallContext by means of a custom request header metadata entry.\r\n \r\n### What did you expect to see?\r\n \r\nWe expected to have a way to inject a scoped System.IServiceProvider into ServerCallContext instances. Having a possibility to deep clone an existing ServerCallContext instance we could pass it on to services via the Interceptor api.\r\n\r\nAdding a property with type System.IServiceProvider to ServerCallContext and making the constructor of ServerCallContext public would allow effective dependency injection mechanisms to be implemented. The following is an example of how \r\n\r\n\r\n",kind/enhancement|lang/C#|priority/P2,jtattermusch,"### What version of gRPC and what language are you using?\r\n \r\n1.12.0 csharp\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux, Windows\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nmsbuild\r\n \r\n### What did you do?\r\n\r\nWe wanted to implement a server application that makes some nifty use of dependency injection. To do so we make use of the Inceptor api that was added with 1.10.0. Currently there's however only a hacky way to add context aware objects to the ServerCallContext instance as outlined below:\r\n\r\n```csharp\r\n    public interface IServerCallContextAccessor\r\n    {\r\n        ServerCallContext ServerCallContext { get; }\r\n    }\r\n\r\n    public interface IServerCallContextProvider\r\n    {\r\n        ServerCallContext ServerCallContext { get; set; }\r\n    }\r\n\r\n    public class ServerRequestScope : Interceptor\r\n    {\r\n        public IServiceProvider ServiceProvider { get; }\r\n\r\n        public ServerRequestScope(IServiceProvider serviceProvider)\r\n        {\r\n            ServiceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));\r\n        }\r\n\r\n        public override async Task<TResponse> UnaryServerHandler<TRequest, TResponse>(TRequest request, ServerCallContext context, UnaryServerMethod<TRequest, TResponse> continuation)\r\n        {\r\n            using (var serviceScope = ServiceProvider.CreateScope())\r\n            {\r\n                // initialize the server call context to allow future invocations to GetService<IServerCallContextAccessor>() to return the current server call context\r\n                var serviceProvider = serviceScope.ServiceProvider;\r\n                serviceProvider.GetRequiredService<IServerCallContextProvider>().ServerCallContext = context;\r\n                context.RequestHeaders.Add(new ServiceProviderMetadataEntry(serviceProvider));\r\n\r\n                return await base.UnaryServerHandler(request, context, continuation);\r\n            }\r\n        }\r\n    }\r\n\r\n    public class ServiceProviderMetadataEntry : Metadata.Entry\r\n    {\r\n        public const string MetadataKey = ""X-ServiceProvider"";\r\n\r\n        public IServiceProvider ServiceProvider { get; }\r\n\r\n        public ServiceProviderMetadataEntry(IServiceProvider serviceProvider)\r\n            : base(MetadataKey, string.Empty)\r\n        {\r\n            ServiceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));\r\n        }\r\n    }\r\n\r\n        public static IServiceProvider GetServiceProvider(this ServerCallContext context)\r\n        {\r\n            if (context == null)\r\n            {\r\n                throw new ArgumentNullException(nameof(context));\r\n            }\r\n\r\n            foreach (var requestHeader in context.RequestHeaders)\r\n            {\r\n                if (ServiceProviderMetadataEntry.MetadataKey.Equals(requestHeader.Key, StringComparison.InvariantCultureIgnoreCase))\r\n                {\r\n                    if (requestHeader is ServiceProviderMetadataEntry serviceProviderMetadataEntry)\r\n                    {\r\n                        return serviceProviderMetadataEntry.ServiceProvider;\r\n                    }\r\n                }\r\n            }\r\n\r\n            throw new InvalidOperationException($""{ServiceProviderMetadataEntry.MetadataKey} request header metadata not found."");\r\n        }\r\n\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            // configure service collection\r\n            var serviceCollection = new ServiceCollection()\r\n                .AddScoped<IServerCallContextProvider, ServerCallContextProvider>()\r\n                .AddScoped<IServerCallContextAccessor>(t => new ServerCallContextAccessor(t.GetRequiredService<IServerCallContextProvider>().ServerCallContext));\r\n\r\n            // build service provider\r\n            var serviceProvider = serviceCollection.BuildServiceProvider();\r\n\r\n            // initialize the server along with service controllers\r\n            int port = 50051;\r\n            Server server = new Server\r\n            {\r\n                Services =\r\n                {\r\n                    SecurityService.BindService(new SecurityService()) // inner-most service\r\n                        .Intercept(new GenericExceptionHandler()) // a middlware that gracefully swallows all exceptions that could possibly occur\r\n                        .Intercept(new ServerRequestScope(serviceProvider)) // inject the request scope, providing the service provider request scope from here on\r\n                },\r\n                Ports = { new ServerPort(""0.0.0.0"", port, ServerCredentials.Insecure), },\r\n            };\r\n            server.Start();\r\n\r\n            foreach (var serverPort in server.Ports)\r\n            {\r\n                Console.WriteLine($""Server listening on {serverPort.Host}:{serverPort.BoundPort}"");\r\n            }\r\n\r\n            Console.WriteLine(""Press any key to stop the server..."");\r\n            Console.ReadKey();\r\n\r\n            server.ShutdownAsync().Wait();\r\n        }\r\n    }\r\n```\r\n\r\nYes, we indeed hack the dependency injection mechanism into the current implementation of the ServerCallContext by means of a custom request header metadata entry.\r\n \r\n### What did you expect to see?\r\n \r\nWe expected to have a way to inject a scoped System.IServiceProvider into ServerCallContext instances. Having a possibility to deep clone an existing ServerCallContext instance we could pass it on to services via the Interceptor api.\r\n\r\nAdding a property with type System.IServiceProvider to ServerCallContext and making the constructor of ServerCallContext public would allow effective dependency injection mechanisms to be implemented. The following is an example of how \r\n\r\n```csharp\r\n    public class ServerRequestScope : Interceptor\r\n    {\r\n        public IServiceProvider ServiceProvider { get; }\r\n\r\n        public ServerRequestScope(IServiceProvider serviceProvider)\r\n        {\r\n            ServiceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));\r\n        }\r\n\r\n        public override async Task<TResponse> UnaryServerHandler<TRequest, TResponse>(TRequest request, ServerCallContext context, UnaryServerMethod<TRequest, TResponse> continuation)\r\n        {\r\n            using (var serviceScope = ServiceProvider.CreateScope())\r\n            {\r\n                // initialize the server call context to allow future invocations to GetService<IServerCallContextAccessor>() to return the current server call context\r\n                var serviceProvider = serviceScope.ServiceProvider;\r\n                serviceProvider.GetRequiredService<IServerCallContextProvider>().ServerCallContext = context;\r\n\r\n                // clone copy server call context with a customized service provider\r\n                var scopedServerCallContext = new ServerCallContext(context, serviceProvider);\r\n                return await base.UnaryServerHandler(request, scopedServerCallContext, continuation);\r\n            }\r\n        }\r\n    }\r\n```\r\n","csharp\r\n    public interface IServerCallContextAccessor\r\n    {\r\n        ServerCallContext ServerCallContext { get; }\r\n    }\r\n\r\n    public interface IServerCallContextProvider\r\n    {\r\n        ServerCallContext ServerCallContext { get; set; }\r\n    }\r\n\r\n    public class ServerRequestScope : Interceptor\r\n    {\r\n        public IServiceProvider ServiceProvider { get; }\r\n\r\n        public ServerRequestScope(IServiceProvider serviceProvider)\r\n        {\r\n            ServiceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));\r\n        }\r\n\r\n        public override async Task<TResponse> UnaryServerHandler<TRequest, TResponse>(TRequest request, ServerCallContext context, UnaryServerMethod<TRequest, TResponse> continuation)\r\n        {\r\n            using (var serviceScope = ServiceProvider.CreateScope())\r\n            {\r\n                // initialize the server call context to allow future invocations to GetService<IServerCallContextAccessor>() to return the current server call context\r\n                var serviceProvider = serviceScope.ServiceProvider;\r\n                serviceProvider.GetRequiredService<IServerCallContextProvider>().ServerCallContext = context;\r\n                context.RequestHeaders.Add(new ServiceProviderMetadataEntry(serviceProvider));\r\n\r\n                return await base.UnaryServerHandler(request, context, continuation);\r\n            }\r\n        }\r\n    }\r\n\r\n    public class ServiceProviderMetadataEntry : Metadata.Entry\r\n    {\r\n        public const string MetadataKey = ""X-ServiceProvider"";\r\n\r\n        public IServiceProvider ServiceProvider { get; }\r\n\r\n        public ServiceProviderMetadataEntry(IServiceProvider serviceProvider)\r\n            : base(MetadataKey, string.Empty)\r\n        {\r\n            ServiceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));\r\n        }\r\n    }\r\n\r\n        public static IServiceProvider GetServiceProvider(this ServerCallContext context)\r\n        {\r\n            if (context == null)\r\n            {\r\n                throw new ArgumentNullException(nameof(context));\r\n            }\r\n\r\n            foreach (var requestHeader in context.RequestHeaders)\r\n            {\r\n                if (ServiceProviderMetadataEntry.MetadataKey.Equals(requestHeader.Key, StringComparison.InvariantCultureIgnoreCase))\r\n                {\r\n                    if (requestHeader is ServiceProviderMetadataEntry serviceProviderMetadataEntry)\r\n                    {\r\n                        return serviceProviderMetadataEntry.ServiceProvider;\r\n                    }\r\n                }\r\n            }\r\n\r\n            throw new InvalidOperationException($""{ServiceProviderMetadataEntry.MetadataKey} request header metadata not found."");\r\n        }\r\n\r\n    class Program\r\n    {\r\n        static void Main(string[] args)\r\n        {\r\n            // configure service collection\r\n            var serviceCollection = new ServiceCollection()\r\n                .AddScoped<IServerCallContextProvider, ServerCallContextProvider>()\r\n                .AddScoped<IServerCallContextAccessor>(t => new ServerCallContextAccessor(t.GetRequiredService<IServerCallContextProvider>().ServerCallContext));\r\n\r\n            // build service provider\r\n            var serviceProvider = serviceCollection.BuildServiceProvider();\r\n\r\n            // initialize the server along with service controllers\r\n            int port = 50051;\r\n            Server server = new Server\r\n            {\r\n                Services =\r\n                {\r\n                    SecurityService.BindService(new SecurityService()) // inner-most service\r\n                        .Intercept(new GenericExceptionHandler()) // a middlware that gracefully swallows all exceptions that could possibly occur\r\n                        .Intercept(new ServerRequestScope(serviceProvider)) // inject the request scope, providing the service provider request scope from here on\r\n                },\r\n                Ports = { new ServerPort(""0.0.0.0"", port, ServerCredentials.Insecure), },\r\n            };\r\n            server.Start();\r\n\r\n            foreach (var serverPort in server.Ports)\r\n            {\r\n                Console.WriteLine($""Server listening on {serverPort.Host}:{serverPort.BoundPort}"");\r\n            }\r\n\r\n            Console.WriteLine(""Press any key to stop the server..."");\r\n            Console.ReadKey();\r\n\r\n            server.ShutdownAsync().Wait();\r\n        }\r\n    }\r\n"
15738,"Python max receive message length not respected### What version of gRPC and what language are you using?\r\nPython pip module grpcio v1.12.1\r\nPython pip module grpcio-tools v1.12.1\r\nPython pip module protobuf v3.5.1\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n `lsb_release -a`\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.4 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n\r\n`uname -a`\r\nLinux lendbuzz 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 2.7.12\r\npip 10.0.1\r\n \r\n### What did you do?\r\nUpgraded from 0.11b to 1.12.1 recently. Introduced message length errors in document uploads for us, for both getS3Data and insertS3Data (we route S3 uploads through our servers to encrypt docs). Managed to fix getS3Data w/ `grpc.max_send_message_length`, the other is still eluding me. Here is some excerpted/modified code (so it will fit here and be clearer, there might be typos):\r\n\r\nserver:\r\n``\r\nclient runner:\r\n``\r\nBaseClient:\r\n``\r\nwhere S3Request, add_BaseServicer_to_server, and BaseStub are given by the appropriate protobuf\r\n\r\n### What did you expect to see?\r\nSuccessful send of binary PDF data over the channel\r\n \r\n### What did you see instead?\r\n````log\r\nTraceback (most recent call last):\r\n  File ""/home/vagrant/lendbuzz/servers/grpc_base.py"", line 125, in send\r\n    resp = fn(request, TIMEOUT_FOR_GRPC)\r\n  File ""/home/vagrant/.local/lib/python2.7/site-packages/grpc/_channel.py"", line 500, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File ""/home/vagrant/.local/lib/python2.7/site-packages/grpc/_channel.py"", line 434, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\n_Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.RESOURCE_EXHAUSTED, Received message larger than max (4217991 vs. 4194304))>\r\n````\r\n \r\n#### Make sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\nHere is a gist with the output when GRPC_VERBOSITY=DEBUG GRPC_TRACE=channel,client_channel,server_channel: https://gist.github.com/frankpinto/3bcdedff7daf7116ed1ce19486f504bd\r\n \r\n### Anything else we should know about your project / environment?",kind/question|lang/Python|priority/P2,kpayson64,"### What version of gRPC and what language are you using?\r\nPython pip module grpcio v1.12.1\r\nPython pip module grpcio-tools v1.12.1\r\nPython pip module protobuf v3.5.1\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n `lsb_release -a`\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.4 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n\r\n`uname -a`\r\nLinux lendbuzz 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 2.7.12\r\npip 10.0.1\r\n \r\n### What did you do?\r\nUpgraded from 0.11b to 1.12.1 recently. Introduced message length errors in document uploads for us, for both getS3Data and insertS3Data (we route S3 uploads through our servers to encrypt docs). Managed to fix getS3Data w/ `grpc.max_send_message_length`, the other is still eluding me. Here is some excerpted/modified code (so it will fit here and be clearer, there might be typos):\r\n\r\nserver:\r\n````python\r\n    self.service = BaseService()\r\n    self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    add_BaseServicer_to_server(self.service, self.server)\r\n````\r\nclient runner:\r\n````python\r\n    grpc_request = S3Request()\r\n    grpc_request.raw_data = base64.b64decode(req_json['raw_data']) # 4.2MB PDF\r\n\r\n    client = BaseClient()\r\n    grpc_resp = client.send('insertS3Data', grpc_request, full_resp=True)\r\n````\r\nBaseClient:\r\n````python\r\ndef __init__(self):\r\n    channel = grpc.insecure_channel(hostname + ':' + str(port), options=[\r\n          ('grpc.max_send_message_length', 50 * 1024 * 1024),\r\n          ('grpc.max_receive_message_length', 50 * 1024 * 1024)\r\n      ])\r\n    self.stub = BaseStub()\r\ndef send(self, method_name, request, key='data', full_resp=False):\r\n    fn = getattr(self.stub, method_name)\r\n    resp = fn(request, TIMEOUT_FOR_GRPC) # fails here\r\n````\r\nwhere S3Request, add_BaseServicer_to_server, and BaseStub are given by the appropriate protobuf\r\n\r\n### What did you expect to see?\r\nSuccessful send of binary PDF data over the channel\r\n \r\n### What did you see instead?\r\n````log\r\nTraceback (most recent call last):\r\n  File ""/home/vagrant/lendbuzz/servers/grpc_base.py"", line 125, in send\r\n    resp = fn(request, TIMEOUT_FOR_GRPC)\r\n  File ""/home/vagrant/.local/lib/python2.7/site-packages/grpc/_channel.py"", line 500, in __call__\r\n    return _end_unary_response_blocking(state, call, False, None)\r\n  File ""/home/vagrant/.local/lib/python2.7/site-packages/grpc/_channel.py"", line 434, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\n_Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.RESOURCE_EXHAUSTED, Received message larger than max (4217991 vs. 4194304))>\r\n````\r\n \r\n#### Make sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n\r\nHere is a gist with the output when GRPC_VERBOSITY=DEBUG GRPC_TRACE=channel,client_channel,server_channel: https://gist.github.com/frankpinto/3bcdedff7daf7116ed1ce19486f504bd\r\n \r\n### Anything else we should know about your project / environment?","python\r\n    self.service = BaseService()\r\n    self.server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    add_BaseServicer_to_server(self.service, self.server)\r\n"
15737,"Bazel can't compile Cython code that use files from core not within public header files### What version of gRPC and what language are you using?\r\n\r\ngRPC version: 1.13.0-dev (Built from source)  \r\nLanguage: Python\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOperating System: Ubuntu  \r\nVersion: 16.04 Xenial  \r\nKernel: Kernel: x86_64 Linux 4.13.0-1013-gcp\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython version: 2.7.12 (Probably not relevant in this issue)  \r\nBazel version: 0.14.1\r\n\r\n### What did you do?\r\n\r\n**NOTE**: This setup is identical to #15736. Only difference is that I have NOT commented out the 2 lines in the 2 respective `cygrpc` Cython files.\r\n\r\nI'm currently working on the GSoC project, ""[Enable building of gRPC Python with Bazel](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/summerofcode/ideas.md)"". For non gRPC related reasons, I have a new minified project setup (with only the relevant Python parts) which has gRPC as a third-party Bazel project. The structure of my minified project looks like this:\r\n```\r\n- grpc/ # <-- This is basically src/python/grpcio/grpc\r\n-- _cython/\r\n--- __credentials/\r\n--- _cygrpc/\r\n--- __init__.py\r\n--- BUILD\r\n--- cygrpc.pxd\r\n--- cygrpc.pyx\r\n--- grpc_python.bzl\r\n--- ...\r\n-- beta/\r\n-- experimental/\r\n-- framework/\r\n-- __init_.py\r\n-- ...\r\n-- BUILD\r\n-- requirements.txt\r\n- tests/ # <-- This is basically src/python/grpcio_tests/tests\r\n- third_party/\r\n- WORKSPACE\r\n- ...\r\n```\r\n\r\nI'm able to import gRPC as a third party Bazel project with a setup similar to [this commit](https://github.com/google/ced/commit/b55567e0479d7d441da3d5e3a901947bb7ebbc18) in [google/ced](https://github.com/google/ced).\r\n\r\nMy BUILD file for gRPC Python (`grpc/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\nand my BUILD file for `_cython` (`grpc/_cython/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\nThe `pyx_library` rule is borrowed from `tensorflow` and looks like this:\r\n\r\n<details><summary>Click to view .bzl file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\n### What did you see?\r\n\r\nMy BUILD fails (command used: `bazel build -s --explain=explain.txt --verbose_explanations //grpc:all`).\r\n\r\nThe error log I'm seeing is:\r\n```\r\nINFO: Analysed 9 targets (0 packages loaded).\r\nINFO: Found 9 targets...\r\nINFO: Writing explanation of rebuilds to 'explain.txt'\r\nSUBCOMMAND: # //grpc/_cython:cygrpc.so [action 'Compiling grpc/_cython/cygrpc.cpp']\r\n(cd /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__ && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/ghostwriternr/peco/peco_linux_amd64 \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer \r\n'-std=c++0x' -MD -MF bazel-out/k8-fastbuild/bin/grpc/_cython/_objs/cygrpc.so/grpc/_cython/cygrpc.pic.d '-frandom-seed=bazel-out/k8-fastbuild/bin/grpc/_cyth\r\non/_objs/cygrpc.so/grpc/_cython/cygrpc.pic.o' -fPIC -iquote . -iquote bazel-out/k8-fastbuild/genfiles -iquote external/local_config_python -iquote bazel-ou\r\nt/k8-fastbuild/genfiles/external/local_config_python -iquote external/bazel_tools -iquote bazel-out/k8-fastbuild/genfiles/external/bazel_tools -isystem ext\r\nernal/local_config_python/python_include -isystem bazel-out/k8-fastbuild/genfiles/external/local_config_python/python_include -isystem bazel-out/k8-fastbui\r\nld/bin/external/local_config_python/python_include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""red\r\nacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-fastbuild/genfiles/grpc/_cython/cygrpc.cpp -o bazel-out/k8-fastbuild/bin/grpc/_cython/_objs/cygrpc.so/grpc/\r\n_cython/cygrpc.pic.o)\r\nERROR: /home/ghostwriternr/grpc-python-temp/grpc/_cython/BUILD:5:1: C++ compilation of rule '//grpc/_cython:cygrpc.so' failed (Exit 1)\r\nbazel-out/k8-fastbuild/genfiles/grpc/_cython/cygrpc.cpp:615:38: fatal error: src/core/lib/iomgr/error.h: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 0.368s, Critical Path: 0.11s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI suspect this error: `fatal error: src/core/lib/iomgr/error.h: No such file or directory` is coming from [`src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pxd.pxi`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pxd.pxi#L22) that uses `src/core/lib/iomgr/error.h`, which isn't one of the public headers and thus probably can't be used externally.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nProbably irrelevant, but I'm attaching the WORKSPACE file for my minified project here. It is _extremely hacky_ right now and has some rendundant bits added intentionally to get things working sooner, since this setup is to be discarded eventually.\r\n\r\n<details><summary>Click to view WORKSPACE file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>",kind/bug|area/core|lang/Python|priority/P2|infra/Bazel|disposition/stale,nicolasnoble,"### What version of gRPC and what language are you using?\r\n\r\ngRPC version: 1.13.0-dev (Built from source)  \r\nLanguage: Python\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOperating System: Ubuntu  \r\nVersion: 16.04 Xenial  \r\nKernel: Kernel: x86_64 Linux 4.13.0-1013-gcp\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython version: 2.7.12 (Probably not relevant in this issue)  \r\nBazel version: 0.14.1\r\n\r\n### What did you do?\r\n\r\n**NOTE**: This setup is identical to #15736. Only difference is that I have NOT commented out the 2 lines in the 2 respective `cygrpc` Cython files.\r\n\r\nI'm currently working on the GSoC project, ""[Enable building of gRPC Python with Bazel](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/summerofcode/ideas.md)"". For non gRPC related reasons, I have a new minified project setup (with only the relevant Python parts) which has gRPC as a third-party Bazel project. The structure of my minified project looks like this:\r\n```\r\n- grpc/ # <-- This is basically src/python/grpcio/grpc\r\n-- _cython/\r\n--- __credentials/\r\n--- _cygrpc/\r\n--- __init__.py\r\n--- BUILD\r\n--- cygrpc.pxd\r\n--- cygrpc.pyx\r\n--- grpc_python.bzl\r\n--- ...\r\n-- beta/\r\n-- experimental/\r\n-- framework/\r\n-- __init_.py\r\n-- ...\r\n-- BUILD\r\n-- requirements.txt\r\n- tests/ # <-- This is basically src/python/grpcio_tests/tests\r\n- third_party/\r\n- WORKSPACE\r\n- ...\r\n```\r\n\r\nI'm able to import gRPC as a third party Bazel project with a setup similar to [this commit](https://github.com/google/ced/commit/b55567e0479d7d441da3d5e3a901947bb7ebbc18) in [google/ced](https://github.com/google/ced).\r\n\r\nMy BUILD file for gRPC Python (`grpc/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n```python\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""requirement"")\r\n\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\npy_binary(\r\n    name = ""grpcio"",\r\n    srcs = [""__init__.py""],\r\n    deps = [\r\n        "":utilities"",\r\n        "":auth"",\r\n        "":plugin_wrapping"",\r\n        "":channel"",\r\n        "":interceptor"",\r\n        "":server"",\r\n        ""//grpc/_cython:cygrpc"",\r\n        # ""@grpccore//:grpc"",\r\n        requirement('enum34'),\r\n        requirement('six'),\r\n    ],\r\n    data = [\r\n        ""@grpccore//:grpc"",\r\n    ],\r\n    main = ""__init__.py"",\r\n)\r\n\r\npy_library(\r\n    name = ""auth"",\r\n    srcs = [""_auth.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""channel"",\r\n    srcs = [""_channel.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":grpcio_metadata"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""common"",\r\n    srcs = [""_common.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""grpcio_metadata"",\r\n    srcs = [""_grpcio_metadata.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""interceptor"",\r\n    srcs = [""_interceptor.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""plugin_wrapping"",\r\n    srcs = [""_plugin_wrapping.py""],\r\n    deps = [\r\n        "":common"",\r\n    ]\r\n)\r\n\r\npy_library(\r\n    name = ""server"",\r\n    srcs = [""_server.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":interceptor"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""utilities"",\r\n    srcs = [""_utilities.py""],\r\n    deps = [\r\n        "":common"",\r\n    ],\r\n)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nand my BUILD file for `_cython` (`grpc/_cython/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n```python\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\nload(""//grpc/_cython:grpc_python.bzl"", ""pyx_library"")\r\n\r\npyx_library(\r\n    name = ""cygrpc"",\r\n    srcs = [\r\n        ""__init__.py"",\r\n        ""cygrpc.pxd"",\r\n        ""cygrpc.pyx"",\r\n        ""_cygrpc/grpc_string.pyx.pxi"",\r\n        ""_cygrpc/arguments.pyx.pxi"",\r\n        ""_cygrpc/call.pyx.pxi"",\r\n        ""_cygrpc/channel.pyx.pxi"",\r\n        ""_cygrpc/credentials.pyx.pxi"",\r\n        ""_cygrpc/completion_queue.pyx.pxi"",\r\n        ""_cygrpc/event.pyx.pxi"",\r\n        ""_cygrpc/metadata.pyx.pxi"",\r\n        ""_cygrpc/operation.pyx.pxi"",\r\n        ""_cygrpc/records.pyx.pxi"",\r\n        ""_cygrpc/security.pyx.pxi"",\r\n        ""_cygrpc/server.pyx.pxi"",\r\n        ""_cygrpc/tag.pyx.pxi"",\r\n        ""_cygrpc/time.pyx.pxi"",\r\n        ""_cygrpc/grpc_gevent.pyx.pxi"",\r\n        ""_cygrpc/grpc.pxi"",\r\n        ""_cygrpc/arguments.pxd.pxi"",\r\n        ""_cygrpc/call.pxd.pxi"",\r\n        ""_cygrpc/channel.pxd.pxi"",\r\n        ""_cygrpc/credentials.pxd.pxi"",\r\n        ""_cygrpc/completion_queue.pxd.pxi"",\r\n        ""_cygrpc/event.pxd.pxi"",\r\n        ""_cygrpc/metadata.pxd.pxi"",\r\n        ""_cygrpc/operation.pxd.pxi"",\r\n        ""_cygrpc/records.pxd.pxi"",\r\n        ""_cygrpc/security.pxd.pxi"",\r\n        ""_cygrpc/server.pxd.pxi"",\r\n        ""_cygrpc/tag.pxd.pxi"",\r\n        ""_cygrpc/time.pxd.pxi"",\r\n        ""_cygrpc/grpc_gevent.pxd.pxi"",\r\n    ],\r\n)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nThe `pyx_library` rule is borrowed from `tensorflow` and looks like this:\r\n\r\n<details><summary>Click to view .bzl file</summary>\r\n<p>\r\n\r\n```python\r\n""""""Custom rules for gRPC Python""""""\r\n\r\ndef pyx_library(\r\n    name,\r\n    deps=[],\r\n    py_deps=[],\r\n    srcs=[],\r\n    **kwargs):\r\n  """"""Compiles a group of .pyx / .pxd / .py files.\r\n  First runs Cython to create .cpp files for each input .pyx or .py + .pxd\r\n  pair. Then builds a shared object for each, passing ""deps"" to each cc_binary\r\n  rule (includes Python headers by default). Finally, creates a py_library rule\r\n  with the shared objects and any pure Python ""srcs"", with py_deps as its\r\n  dependencies; the shared objects can be imported like normal Python files.\r\n  Args:\r\n    name: Name for the rule.\r\n    deps: C/C++ dependencies of the Cython (e.g. Numpy headers).\r\n    py_deps: Pure Python dependencies of the final library.\r\n    srcs: .py, .pyx, or .pxd files to either compile or pass through.\r\n    **kwargs: Extra keyword arguments passed to the py_library.\r\n  """"""\r\n  # First filter out files that should be run compiled vs. passed through.\r\n  py_srcs = []\r\n  pyx_srcs = []\r\n  pxd_srcs = []\r\n  for src in srcs:\r\n    if src.endswith("".pyx"") or (src.endswith("".py"")\r\n                                and src[:-3] + "".pxd"" in srcs):\r\n      pyx_srcs.append(src)\r\n    elif src.endswith("".py""):\r\n      py_srcs.append(src)\r\n    else:\r\n      pxd_srcs.append(src)\r\n    if src.endswith(""__init__.py""):\r\n      pxd_srcs.append(src)\r\n\r\n  # Invoke cython to produce the shared object libraries.\r\n  for filename in pyx_srcs:\r\n    native.genrule(\r\n        name = filename + ""_cython_translation"",\r\n        srcs = [filename],\r\n        outs = [filename.split(""."")[0] + "".cpp""],\r\n        # Optionally use PYTHON_BIN_PATH on Linux platforms so that python 3\r\n        # works. Windows has issues with cython_binary so skip PYTHON_BIN_PATH.\r\n        cmd = ""PYTHONHASHSEED=0 $(location @cython//:cython_binary) --cplus $(SRCS) --output-file $(OUTS)"",\r\n        tools = [""@cython//:cython_binary""] + pxd_srcs,\r\n    )\r\n\r\n  shared_objects = []\r\n  for src in pyx_srcs:\r\n    stem = src.split(""."")[0]\r\n    shared_object_name = stem + "".so""\r\n    native.cc_binary(\r\n        name=shared_object_name,\r\n        srcs=[stem + "".cpp""],\r\n        deps=deps + [""//third_party/python_runtime:headers""],\r\n        linkshared = 1,\r\n    )\r\n    shared_objects.append(shared_object_name)\r\n\r\n  # Now create a py_library with these shared objects as data.\r\n  native.py_library(\r\n      name=name,\r\n      srcs=py_srcs,\r\n      deps=py_deps,\r\n      srcs_version = ""PY2AND3"",\r\n      data=shared_objects,\r\n      **kwargs\r\n  )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### What did you see?\r\n\r\nMy BUILD fails (command used: `bazel build -s --explain=explain.txt --verbose_explanations //grpc:all`).\r\n\r\nThe error log I'm seeing is:\r\n```\r\nINFO: Analysed 9 targets (0 packages loaded).\r\nINFO: Found 9 targets...\r\nINFO: Writing explanation of rebuilds to 'explain.txt'\r\nSUBCOMMAND: # //grpc/_cython:cygrpc.so [action 'Compiling grpc/_cython/cygrpc.cpp']\r\n(cd /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__ && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/ghostwriternr/peco/peco_linux_amd64 \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer \r\n'-std=c++0x' -MD -MF bazel-out/k8-fastbuild/bin/grpc/_cython/_objs/cygrpc.so/grpc/_cython/cygrpc.pic.d '-frandom-seed=bazel-out/k8-fastbuild/bin/grpc/_cyth\r\non/_objs/cygrpc.so/grpc/_cython/cygrpc.pic.o' -fPIC -iquote . -iquote bazel-out/k8-fastbuild/genfiles -iquote external/local_config_python -iquote bazel-ou\r\nt/k8-fastbuild/genfiles/external/local_config_python -iquote external/bazel_tools -iquote bazel-out/k8-fastbuild/genfiles/external/bazel_tools -isystem ext\r\nernal/local_config_python/python_include -isystem bazel-out/k8-fastbuild/genfiles/external/local_config_python/python_include -isystem bazel-out/k8-fastbui\r\nld/bin/external/local_config_python/python_include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""red\r\nacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-fastbuild/genfiles/grpc/_cython/cygrpc.cpp -o bazel-out/k8-fastbuild/bin/grpc/_cython/_objs/cygrpc.so/grpc/\r\n_cython/cygrpc.pic.o)\r\nERROR: /home/ghostwriternr/grpc-python-temp/grpc/_cython/BUILD:5:1: C++ compilation of rule '//grpc/_cython:cygrpc.so' failed (Exit 1)\r\nbazel-out/k8-fastbuild/genfiles/grpc/_cython/cygrpc.cpp:615:38: fatal error: src/core/lib/iomgr/error.h: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 0.368s, Critical Path: 0.11s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI suspect this error: `fatal error: src/core/lib/iomgr/error.h: No such file or directory` is coming from [`src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pxd.pxi`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pxd.pxi#L22) that uses `src/core/lib/iomgr/error.h`, which isn't one of the public headers and thus probably can't be used externally.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nProbably irrelevant, but I'm attaching the WORKSPACE file for my minified project here. It is _extremely hacky_ right now and has some rendundant bits added intentionally to get things working sooner, since this setup is to be discarded eventually.\r\n\r\n<details><summary>Click to view WORKSPACE file</summary>\r\n<p>\r\n\r\n```python\r\nload(""//third_party/py:python_configure.bzl"", ""python_configure"")\r\n\r\nnew_local_repository(\r\n   name = ""cython"",\r\n   path = __workspace_dir__ + ""/third_party/cython/"",\r\n   build_file = ""//third_party:cython.BUILD"",\r\n)\r\n\r\nlocal_repository(\r\n    name = ""grpccore"",\r\n    path = ""/home/ghostwriternr/grpc"",\r\n)\r\n\r\n#\r\n# gRPC bits\r\n#\r\n\r\nlocal_repository(\r\n  name = ""grpc"",\r\n  path = ""/home/ghostwriternr/grpc"",\r\n)\r\n\r\nnew_http_archive(\r\n    name = ""com_github_madler_zlib"",\r\n    build_file = __workspace_dir__ + ""/third_party/zlib.BUILD"",\r\n    strip_prefix = ""zlib-cacf7f1d4e3d44d871b605da3b647f07d718623f"",\r\n    url = ""https://github.com/madler/zlib/archive/cacf7f1d4e3d44d871b605da3b647f07d718623f.tar.gz"",\r\n)\r\n\r\nhttp_archive(\r\n    name = ""boringssl"",\r\n    # on the master-with-bazel branch\r\n    url = ""https://boringssl.googlesource.com/boringssl/+archive/886e7d75368e3f4fab3f4d0d3584e4abfc557755.tar.gz"",\r\n)\r\n\r\nnew_local_repository(\r\n    name = ""com_github_cares_cares"",\r\n    path = __workspace_dir__ + ""/third_party/cares/cares"",\r\n    build_file = ""//third_party/cares:cares.BUILD"",\r\n)\r\n\r\n# required binds...\r\nbind(\r\n    name = ""protobuf"",\r\n    actual = ""@com_google_protobuf//:protobuf"",\r\n)\r\n\r\nbind(\r\n    name = ""protobuf_clib"",\r\n    actual = ""@com_google_protobuf//:protoc_lib"",\r\n)\r\n\r\nbind(\r\n    name = ""protocol_compiler"",\r\n    actual = ""@com_google_protobuf//:protoc"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc_cpp_plugin"",\r\n    actual = ""@grpc//:grpc_cpp_plugin"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc++"",\r\n    actual = ""@grpc//:grpc++"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc++_codegen_proto"",\r\n    actual = ""@grpc//:grpc++_codegen_proto"",\r\n)\r\n\r\nbind(\r\n    name = ""protobuf_headers"",\r\n    actual = ""@com_google_protobuf//:protobuf_headers"",\r\n)\r\n\r\nbind(\r\n    name = ""zlib"",\r\n    actual = ""@com_github_madler_zlib//:z"",\r\n)\r\n\r\nbind(\r\n    name = ""libssl"",\r\n    actual = ""@boringssl//:ssl"",\r\n)\r\n\r\nbind(\r\n    name = ""cares"",\r\n    actual = ""@com_github_cares_cares//:ares"",\r\n)\r\n\r\nbind(\r\n    name = ""nanopb"",\r\n    actual = ""@grpc//third_party/nanopb"",\r\n)\r\n\r\ngit_repository(\r\n    name = ""io_bazel_rules_python"",\r\n    remote = ""https://github.com/bazelbuild/rules_python.git"",\r\n    commit = ""8b5d0683a7d878b28fffe464779c8a53659fc645"",\r\n)\r\n\r\npython_configure(name=""local_config_python"")\r\n\r\nload(""@io_bazel_rules_python//python:pip.bzl"", ""pip_repositories"", ""pip_import"")\r\n\r\npip_repositories()\r\npip_import(\r\n    name = ""grpc_python_dependencies"",\r\n    requirements = ""//grpc:requirements.txt"",\r\n)\r\n\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""pip_install"")\r\npip_install()\r\n```\r\n\r\n</p>\r\n</details>","python\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""requirement"")\r\n\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\npy_binary(\r\n    name = ""grpcio"",\r\n    srcs = [""__init__.py""],\r\n    deps = [\r\n        "":utilities"",\r\n        "":auth"",\r\n        "":plugin_wrapping"",\r\n        "":channel"",\r\n        "":interceptor"",\r\n        "":server"",\r\n        ""//grpc/_cython:cygrpc"",\r\n        # ""@grpccore//:grpc"",\r\n        requirement('enum34'),\r\n        requirement('six'),\r\n    ],\r\n    data = [\r\n        ""@grpccore//:grpc"",\r\n    ],\r\n    main = ""__init__.py"",\r\n)\r\n\r\npy_library(\r\n    name = ""auth"",\r\n    srcs = [""_auth.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""channel"",\r\n    srcs = [""_channel.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":grpcio_metadata"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""common"",\r\n    srcs = [""_common.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""grpcio_metadata"",\r\n    srcs = [""_grpcio_metadata.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""interceptor"",\r\n    srcs = [""_interceptor.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""plugin_wrapping"",\r\n    srcs = [""_plugin_wrapping.py""],\r\n    deps = [\r\n        "":common"",\r\n    ]\r\n)\r\n\r\npy_library(\r\n    name = ""server"",\r\n    srcs = [""_server.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":interceptor"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""utilities"",\r\n    srcs = [""_utilities.py""],\r\n    deps = [\r\n        "":common"",\r\n    ],\r\n)\r\n"
15736,"Bazel can't compile Cython code that uses files not used by any existing build rule### What version of gRPC and what language are you using?\r\n\r\ngRPC version: 1.13.0-dev (Built from source)  \r\nLanguage: Python\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOperating System: Ubuntu  \r\nVersion: 16.04 Xenial  \r\nKernel: Kernel: x86_64 Linux 4.13.0-1013-gcp\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython version: 2.7.12 (Probably not relevant in this issue)  \r\nBazel version: 0.14.1\r\n\r\n### What did you do?\r\n\r\nI'm currently working on the GSoC project, ""[Enable building of gRPC Python with Bazel](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/summerofcode/ideas.md)"". For non gRPC related reasons, I have a new minified project setup (with only the relevant Python parts) which has gRPC as a third-party Bazel project. The structure of my minified project looks like this:\r\n```\r\n- grpc/ # <-- This is basically src/python/grpcio/grpc\r\n-- _cython/\r\n--- __credentials/\r\n--- _cygrpc/\r\n--- __init__.py\r\n--- BUILD\r\n--- cygrpc.pxd\r\n--- cygrpc.pyx\r\n--- grpc_python.bzl\r\n--- ...\r\n-- beta/\r\n-- experimental/\r\n-- framework/\r\n-- __init_.py\r\n-- ...\r\n-- BUILD\r\n-- requirements.txt\r\n- tests/ # <-- This is basically src/python/grpcio_tests/tests\r\n- third_party/\r\n- WORKSPACE\r\n- ...\r\n```\r\n\r\nI'm able to import gRPC as a third party Bazel project with a setup similar to [this commit](https://github.com/google/ced/commit/b55567e0479d7d441da3d5e3a901947bb7ebbc18) in [google/ced](https://github.com/google/ced).\r\n\r\nMy BUILD file for gRPC Python (`grpc/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\nand my BUILD file for `_cython` (`grpc/_cython/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\nThe `pyx_library` rule is borrowed from `tensorflow` and looks like this:\r\n\r\n<details><summary>Click to view .bzl file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>\r\n\r\nMy build is currently succeeding (command used: `bazel build -s --explain=explain.txt --verbose_explanations //grpc:all`). I had to comment out the line `include ""_cygrpc/grpc_gevent.pyx.pxi""` from [`src/python/grpcio/grpc/_cython/cygrpc.pyx`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/cygrpc.pyx#L39) and the line `include ""_cygrpc/grpc_gevent.pxd.pxi""` from [`src/python/grpcio/grpc/_cython/cygrpc.pxd\r\n`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/cygrpc.pxd#L32) for reasons described in #15737 (most probably not directly related).\r\n\r\nMy BUILD file for `tests` looks like this:\r\n\r\n\r\n\r\n### What did you see?\r\n\r\nWhen I run ANY test (command used: `bazel test --test_output=streamed //tests:all`), I see the following error:\r\n```\r\nINFO: Analysed target //tests:logging_pool (4 packages loaded).\r\nINFO: Found 1 test target...\r\nTraceback (most recent call last):\r\n  File ""/home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-fastb\r\nuild/bin/tests/logging_pool.runfiles/__main__/tests/unit/framework/foundation/_logging_pool_test.py"", line 19, in <module>\r\n    from grpc.framework.foundation import logging_pool\r\n  File ""/home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-fastb\r\nuild/bin/tests/logging_pool.runfiles/__main__/grpc/__init__.py"", line 22, in <module>\r\n    from grpc._cython import cygrpc as _cygrpc\r\nImportError: /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-\r\nfastbuild/bin/tests/logging_pool.runfiles/__main__/grpc/_cython/cygrpc.so: undefined symbol: gpr_malloc\r\nFAIL: //tests:logging_pool (see /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__/bazel-out/k8-fastb\r\nuild/testlogs/tests/logging_pool/test.log)\r\nTarget //tests:logging_pool up-to-date:\r\n  bazel-bin/tests/logging_pool\r\nINFO: Elapsed time: 0.667s, Critical Path: 0.22s\r\nINFO: 2 processes: 1 linux-sandbox, 1 local.\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n//tests:logging_pool                                                     FAILED in 0.1s\r\n  /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__/bazel-out/k8-fastbuild/testlogs/tests/logging_po\r\nol/test.log\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n```\r\n\r\nI'm assuming this error: `undefined symbol: gpr_malloc` is showing up from [`src/python/grpcio/_cython/_cygrpc/grpc.pxi`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/_cygrpc/grpc.pxi#L28) which is using declarations from `grpc/support/alloc.h`. This file happens to be listen in `GPR_PUBLIC_HDRS` within the [core `BUILD` file](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/BUILD#L74), but is not listed under `srcs` or `deps` (or relevant params) in any of the build rules.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nProbably irrelevant, but I'm attaching the WORKSPACE file for my minified project here. It is _extremely hacky_ right now and has some rendundant bits added intentionally to get things working sooner, since this setup is to be discarded eventually.\r\n\r\n<details><summary>Click to view WORKSPACE file</summary>\r\n<p>\r\n\r\n\r\n\r\n</p>\r\n</details>",kind/enhancement|area/core|lang/Python|priority/P2|infra/Bazel|disposition/stale,nicolasnoble,"### What version of gRPC and what language are you using?\r\n\r\ngRPC version: 1.13.0-dev (Built from source)  \r\nLanguage: Python\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOperating System: Ubuntu  \r\nVersion: 16.04 Xenial  \r\nKernel: Kernel: x86_64 Linux 4.13.0-1013-gcp\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython version: 2.7.12 (Probably not relevant in this issue)  \r\nBazel version: 0.14.1\r\n\r\n### What did you do?\r\n\r\nI'm currently working on the GSoC project, ""[Enable building of gRPC Python with Bazel](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/summerofcode/ideas.md)"". For non gRPC related reasons, I have a new minified project setup (with only the relevant Python parts) which has gRPC as a third-party Bazel project. The structure of my minified project looks like this:\r\n```\r\n- grpc/ # <-- This is basically src/python/grpcio/grpc\r\n-- _cython/\r\n--- __credentials/\r\n--- _cygrpc/\r\n--- __init__.py\r\n--- BUILD\r\n--- cygrpc.pxd\r\n--- cygrpc.pyx\r\n--- grpc_python.bzl\r\n--- ...\r\n-- beta/\r\n-- experimental/\r\n-- framework/\r\n-- __init_.py\r\n-- ...\r\n-- BUILD\r\n-- requirements.txt\r\n- tests/ # <-- This is basically src/python/grpcio_tests/tests\r\n- third_party/\r\n- WORKSPACE\r\n- ...\r\n```\r\n\r\nI'm able to import gRPC as a third party Bazel project with a setup similar to [this commit](https://github.com/google/ced/commit/b55567e0479d7d441da3d5e3a901947bb7ebbc18) in [google/ced](https://github.com/google/ced).\r\n\r\nMy BUILD file for gRPC Python (`grpc/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n```python\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""requirement"")\r\n\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\npy_binary(\r\n    name = ""grpcio"",\r\n    srcs = [""__init__.py""],\r\n    deps = [\r\n        "":utilities"",\r\n        "":auth"",\r\n        "":plugin_wrapping"",\r\n        "":channel"",\r\n        "":interceptor"",\r\n        "":server"",\r\n        ""//grpc/_cython:cygrpc"",\r\n        # ""@grpccore//:grpc"",\r\n        requirement('enum34'),\r\n        requirement('six'),\r\n    ],\r\n    data = [\r\n        ""@grpccore//:grpc"",\r\n    ],\r\n    main = ""__init__.py"",\r\n)\r\n\r\npy_library(\r\n    name = ""auth"",\r\n    srcs = [""_auth.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""channel"",\r\n    srcs = [""_channel.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":grpcio_metadata"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""common"",\r\n    srcs = [""_common.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""grpcio_metadata"",\r\n    srcs = [""_grpcio_metadata.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""interceptor"",\r\n    srcs = [""_interceptor.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""plugin_wrapping"",\r\n    srcs = [""_plugin_wrapping.py""],\r\n    deps = [\r\n        "":common"",\r\n    ]\r\n)\r\n\r\npy_library(\r\n    name = ""server"",\r\n    srcs = [""_server.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":interceptor"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""utilities"",\r\n    srcs = [""_utilities.py""],\r\n    deps = [\r\n        "":common"",\r\n    ],\r\n)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nand my BUILD file for `_cython` (`grpc/_cython/BUILD`) looks like this:\r\n<details><summary>Click to view BUILD file</summary>\r\n<p>\r\n\r\n```python\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\nload(""//grpc/_cython:grpc_python.bzl"", ""pyx_library"")\r\n\r\npyx_library(\r\n    name = ""cygrpc"",\r\n    srcs = [\r\n        ""__init__.py"",\r\n        ""cygrpc.pxd"",\r\n        ""cygrpc.pyx"",\r\n        ""_cygrpc/grpc_string.pyx.pxi"",\r\n        ""_cygrpc/arguments.pyx.pxi"",\r\n        ""_cygrpc/call.pyx.pxi"",\r\n        ""_cygrpc/channel.pyx.pxi"",\r\n        ""_cygrpc/credentials.pyx.pxi"",\r\n        ""_cygrpc/completion_queue.pyx.pxi"",\r\n        ""_cygrpc/event.pyx.pxi"",\r\n        ""_cygrpc/metadata.pyx.pxi"",\r\n        ""_cygrpc/operation.pyx.pxi"",\r\n        ""_cygrpc/records.pyx.pxi"",\r\n        ""_cygrpc/security.pyx.pxi"",\r\n        ""_cygrpc/server.pyx.pxi"",\r\n        ""_cygrpc/tag.pyx.pxi"",\r\n        ""_cygrpc/time.pyx.pxi"",\r\n        ""_cygrpc/grpc_gevent.pyx.pxi"",\r\n        ""_cygrpc/grpc.pxi"",\r\n        ""_cygrpc/arguments.pxd.pxi"",\r\n        ""_cygrpc/call.pxd.pxi"",\r\n        ""_cygrpc/channel.pxd.pxi"",\r\n        ""_cygrpc/credentials.pxd.pxi"",\r\n        ""_cygrpc/completion_queue.pxd.pxi"",\r\n        ""_cygrpc/event.pxd.pxi"",\r\n        ""_cygrpc/metadata.pxd.pxi"",\r\n        ""_cygrpc/operation.pxd.pxi"",\r\n        ""_cygrpc/records.pxd.pxi"",\r\n        ""_cygrpc/security.pxd.pxi"",\r\n        ""_cygrpc/server.pxd.pxi"",\r\n        ""_cygrpc/tag.pxd.pxi"",\r\n        ""_cygrpc/time.pxd.pxi"",\r\n        ""_cygrpc/grpc_gevent.pxd.pxi"",\r\n    ],\r\n)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nThe `pyx_library` rule is borrowed from `tensorflow` and looks like this:\r\n\r\n<details><summary>Click to view .bzl file</summary>\r\n<p>\r\n\r\n```python\r\n""""""Custom rules for gRPC Python""""""\r\n\r\ndef pyx_library(\r\n    name,\r\n    deps=[],\r\n    py_deps=[],\r\n    srcs=[],\r\n    **kwargs):\r\n  """"""Compiles a group of .pyx / .pxd / .py files.\r\n  First runs Cython to create .cpp files for each input .pyx or .py + .pxd\r\n  pair. Then builds a shared object for each, passing ""deps"" to each cc_binary\r\n  rule (includes Python headers by default). Finally, creates a py_library rule\r\n  with the shared objects and any pure Python ""srcs"", with py_deps as its\r\n  dependencies; the shared objects can be imported like normal Python files.\r\n  Args:\r\n    name: Name for the rule.\r\n    deps: C/C++ dependencies of the Cython (e.g. Numpy headers).\r\n    py_deps: Pure Python dependencies of the final library.\r\n    srcs: .py, .pyx, or .pxd files to either compile or pass through.\r\n    **kwargs: Extra keyword arguments passed to the py_library.\r\n  """"""\r\n  # First filter out files that should be run compiled vs. passed through.\r\n  py_srcs = []\r\n  pyx_srcs = []\r\n  pxd_srcs = []\r\n  for src in srcs:\r\n    if src.endswith("".pyx"") or (src.endswith("".py"")\r\n                                and src[:-3] + "".pxd"" in srcs):\r\n      pyx_srcs.append(src)\r\n    elif src.endswith("".py""):\r\n      py_srcs.append(src)\r\n    else:\r\n      pxd_srcs.append(src)\r\n    if src.endswith(""__init__.py""):\r\n      pxd_srcs.append(src)\r\n\r\n  # Invoke cython to produce the shared object libraries.\r\n  for filename in pyx_srcs:\r\n    native.genrule(\r\n        name = filename + ""_cython_translation"",\r\n        srcs = [filename],\r\n        outs = [filename.split(""."")[0] + "".cpp""],\r\n        # Optionally use PYTHON_BIN_PATH on Linux platforms so that python 3\r\n        # works. Windows has issues with cython_binary so skip PYTHON_BIN_PATH.\r\n        cmd = ""PYTHONHASHSEED=0 $(location @cython//:cython_binary) --cplus $(SRCS) --output-file $(OUTS)"",\r\n        tools = [""@cython//:cython_binary""] + pxd_srcs,\r\n    )\r\n\r\n  shared_objects = []\r\n  for src in pyx_srcs:\r\n    stem = src.split(""."")[0]\r\n    shared_object_name = stem + "".so""\r\n    native.cc_binary(\r\n        name=shared_object_name,\r\n        srcs=[stem + "".cpp""],\r\n        deps=deps + [""//third_party/python_runtime:headers""],\r\n        linkshared = 1,\r\n    )\r\n    shared_objects.append(shared_object_name)\r\n\r\n  # Now create a py_library with these shared objects as data.\r\n  native.py_library(\r\n      name=name,\r\n      srcs=py_srcs,\r\n      deps=py_deps,\r\n      srcs_version = ""PY2AND3"",\r\n      data=shared_objects,\r\n      **kwargs\r\n  )\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\nMy build is currently succeeding (command used: `bazel build -s --explain=explain.txt --verbose_explanations //grpc:all`). I had to comment out the line `include ""_cygrpc/grpc_gevent.pyx.pxi""` from [`src/python/grpcio/grpc/_cython/cygrpc.pyx`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/cygrpc.pyx#L39) and the line `include ""_cygrpc/grpc_gevent.pxd.pxi""` from [`src/python/grpcio/grpc/_cython/cygrpc.pxd\r\n`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/cygrpc.pxd#L32) for reasons described in #15737 (most probably not directly related).\r\n\r\nMy BUILD file for `tests` looks like this:\r\n\r\n```python\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\npy_test(\r\n    name = ""logging_pool"",\r\n    srcs = [""unit/framework/foundation/_logging_pool_test.py""],\r\n    main = ""unit/framework/foundation/_logging_pool_test.py"",\r\n    size = ""small"",\r\n    deps = [\r\n        ""//grpc:grpcio"",\r\n    ],\r\n)\r\n```\r\n\r\n### What did you see?\r\n\r\nWhen I run ANY test (command used: `bazel test --test_output=streamed //tests:all`), I see the following error:\r\n```\r\nINFO: Analysed target //tests:logging_pool (4 packages loaded).\r\nINFO: Found 1 test target...\r\nTraceback (most recent call last):\r\n  File ""/home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-fastb\r\nuild/bin/tests/logging_pool.runfiles/__main__/tests/unit/framework/foundation/_logging_pool_test.py"", line 19, in <module>\r\n    from grpc.framework.foundation import logging_pool\r\n  File ""/home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-fastb\r\nuild/bin/tests/logging_pool.runfiles/__main__/grpc/__init__.py"", line 22, in <module>\r\n    from grpc._cython import cygrpc as _cygrpc\r\nImportError: /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/sandbox/linux-sandbox/2/execroot/__main__/bazel-out/k8-\r\nfastbuild/bin/tests/logging_pool.runfiles/__main__/grpc/_cython/cygrpc.so: undefined symbol: gpr_malloc\r\nFAIL: //tests:logging_pool (see /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__/bazel-out/k8-fastb\r\nuild/testlogs/tests/logging_pool/test.log)\r\nTarget //tests:logging_pool up-to-date:\r\n  bazel-bin/tests/logging_pool\r\nINFO: Elapsed time: 0.667s, Critical Path: 0.22s\r\nINFO: 2 processes: 1 linux-sandbox, 1 local.\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n//tests:logging_pool                                                     FAILED in 0.1s\r\n  /home/ghostwriternr/.cache/bazel/_bazel_ghostwriternr/ff59205a9bb6842e755c8b9fab73e6c5/execroot/__main__/bazel-out/k8-fastbuild/testlogs/tests/logging_po\r\nol/test.log\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n```\r\n\r\nI'm assuming this error: `undefined symbol: gpr_malloc` is showing up from [`src/python/grpcio/_cython/_cygrpc/grpc.pxi`](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/src/python/grpcio/grpc/_cython/_cygrpc/grpc.pxi#L28) which is using declarations from `grpc/support/alloc.h`. This file happens to be listen in `GPR_PUBLIC_HDRS` within the [core `BUILD` file](https://github.com/grpc/grpc/blob/9b52965ba4157d098bb951c892e18a1361917200/BUILD#L74), but is not listed under `srcs` or `deps` (or relevant params) in any of the build rules.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nProbably irrelevant, but I'm attaching the WORKSPACE file for my minified project here. It is _extremely hacky_ right now and has some rendundant bits added intentionally to get things working sooner, since this setup is to be discarded eventually.\r\n\r\n<details><summary>Click to view WORKSPACE file</summary>\r\n<p>\r\n\r\n```python\r\nload(""//third_party/py:python_configure.bzl"", ""python_configure"")\r\n\r\nnew_local_repository(\r\n   name = ""cython"",\r\n   path = __workspace_dir__ + ""/third_party/cython/"",\r\n   build_file = ""//third_party:cython.BUILD"",\r\n)\r\n\r\nlocal_repository(\r\n    name = ""grpccore"",\r\n    path = ""/home/ghostwriternr/grpc"",\r\n)\r\n\r\n#\r\n# gRPC bits\r\n#\r\n\r\nlocal_repository(\r\n  name = ""grpc"",\r\n  path = ""/home/ghostwriternr/grpc"",\r\n)\r\n\r\nnew_http_archive(\r\n    name = ""com_github_madler_zlib"",\r\n    build_file = __workspace_dir__ + ""/third_party/zlib.BUILD"",\r\n    strip_prefix = ""zlib-cacf7f1d4e3d44d871b605da3b647f07d718623f"",\r\n    url = ""https://github.com/madler/zlib/archive/cacf7f1d4e3d44d871b605da3b647f07d718623f.tar.gz"",\r\n)\r\n\r\nhttp_archive(\r\n    name = ""boringssl"",\r\n    # on the master-with-bazel branch\r\n    url = ""https://boringssl.googlesource.com/boringssl/+archive/886e7d75368e3f4fab3f4d0d3584e4abfc557755.tar.gz"",\r\n)\r\n\r\nnew_local_repository(\r\n    name = ""com_github_cares_cares"",\r\n    path = __workspace_dir__ + ""/third_party/cares/cares"",\r\n    build_file = ""//third_party/cares:cares.BUILD"",\r\n)\r\n\r\n# required binds...\r\nbind(\r\n    name = ""protobuf"",\r\n    actual = ""@com_google_protobuf//:protobuf"",\r\n)\r\n\r\nbind(\r\n    name = ""protobuf_clib"",\r\n    actual = ""@com_google_protobuf//:protoc_lib"",\r\n)\r\n\r\nbind(\r\n    name = ""protocol_compiler"",\r\n    actual = ""@com_google_protobuf//:protoc"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc_cpp_plugin"",\r\n    actual = ""@grpc//:grpc_cpp_plugin"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc++"",\r\n    actual = ""@grpc//:grpc++"",\r\n)\r\n\r\nbind(\r\n    name = ""grpc++_codegen_proto"",\r\n    actual = ""@grpc//:grpc++_codegen_proto"",\r\n)\r\n\r\nbind(\r\n    name = ""protobuf_headers"",\r\n    actual = ""@com_google_protobuf//:protobuf_headers"",\r\n)\r\n\r\nbind(\r\n    name = ""zlib"",\r\n    actual = ""@com_github_madler_zlib//:z"",\r\n)\r\n\r\nbind(\r\n    name = ""libssl"",\r\n    actual = ""@boringssl//:ssl"",\r\n)\r\n\r\nbind(\r\n    name = ""cares"",\r\n    actual = ""@com_github_cares_cares//:ares"",\r\n)\r\n\r\nbind(\r\n    name = ""nanopb"",\r\n    actual = ""@grpc//third_party/nanopb"",\r\n)\r\n\r\ngit_repository(\r\n    name = ""io_bazel_rules_python"",\r\n    remote = ""https://github.com/bazelbuild/rules_python.git"",\r\n    commit = ""8b5d0683a7d878b28fffe464779c8a53659fc645"",\r\n)\r\n\r\npython_configure(name=""local_config_python"")\r\n\r\nload(""@io_bazel_rules_python//python:pip.bzl"", ""pip_repositories"", ""pip_import"")\r\n\r\npip_repositories()\r\npip_import(\r\n    name = ""grpc_python_dependencies"",\r\n    requirements = ""//grpc:requirements.txt"",\r\n)\r\n\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""pip_install"")\r\npip_install()\r\n```\r\n\r\n</p>\r\n</details>","python\r\nload(""@grpc_python_dependencies//:requirements.bzl"", ""requirement"")\r\n\r\npackage(default_visibility = [""//visibility:public""])\r\n\r\npy_binary(\r\n    name = ""grpcio"",\r\n    srcs = [""__init__.py""],\r\n    deps = [\r\n        "":utilities"",\r\n        "":auth"",\r\n        "":plugin_wrapping"",\r\n        "":channel"",\r\n        "":interceptor"",\r\n        "":server"",\r\n        ""//grpc/_cython:cygrpc"",\r\n        # ""@grpccore//:grpc"",\r\n        requirement('enum34'),\r\n        requirement('six'),\r\n    ],\r\n    data = [\r\n        ""@grpccore//:grpc"",\r\n    ],\r\n    main = ""__init__.py"",\r\n)\r\n\r\npy_library(\r\n    name = ""auth"",\r\n    srcs = [""_auth.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""channel"",\r\n    srcs = [""_channel.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":grpcio_metadata"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""common"",\r\n    srcs = [""_common.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""grpcio_metadata"",\r\n    srcs = [""_grpcio_metadata.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""interceptor"",\r\n    srcs = [""_interceptor.py""],\r\n)\r\n\r\npy_library(\r\n    name = ""plugin_wrapping"",\r\n    srcs = [""_plugin_wrapping.py""],\r\n    deps = [\r\n        "":common"",\r\n    ]\r\n)\r\n\r\npy_library(\r\n    name = ""server"",\r\n    srcs = [""_server.py""],\r\n    deps = [\r\n        "":common"",\r\n        "":interceptor"",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = ""utilities"",\r\n    srcs = [""_utilities.py""],\r\n    deps = [\r\n        "":common"",\r\n    ],\r\n)\r\n"
15574,"Empty service generates invalid C# code### What version of gRPC and what language are you using?\r\ngRPC 1.12.0, C# \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nMacOS High Sierra 10.13.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n\r\n### What did you do?\r\nTried to generate grpc C# code for an empty service:\r\n\r\n```protobuf\r\nsyntax = ""proto3"";\r\nservice EmptyService { }\r\n``` \r\n \r\n### What did you expect to see?\r\nGenerated that compiled cleanly.\r\n \r\n### What did you see instead?\r\n\r\nThe generated `BindService` method for the empty service is incomplete:\r\n\r\n\r\n\r\nThe compiler complains about a missing semicolon and not being able to convert a `ServerServiceDefinitonBuilder` to a `ServerServiceDefinition`.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/bug|lang/C#|priority/P2,jtattermusch,"### What version of gRPC and what language are you using?\r\ngRPC 1.12.0, C# \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nMacOS High Sierra 10.13.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n\r\n### What did you do?\r\nTried to generate grpc C# code for an empty service:\r\n\r\n```protobuf\r\nsyntax = ""proto3"";\r\nservice EmptyService { }\r\n``` \r\n \r\n### What did you expect to see?\r\nGenerated that compiled cleanly.\r\n \r\n### What did you see instead?\r\n\r\nThe generated `BindService` method for the empty service is incomplete:\r\n\r\n```csharp\r\n    /// <summary>Creates service definition that can be registered with a server</summary>\r\n    /// <param name=""serviceImpl"">An object implementing the server-side handling logic.</param>\r\n    public static grpc::ServerServiceDefinition BindService(EmptyServiceBase serviceImpl)\r\n    {\r\n      return grpc::ServerServiceDefinition.CreateBuilder()\r\n    }\r\n```\r\n\r\nThe compiler complains about a missing semicolon and not being able to convert a `ServerServiceDefinitonBuilder` to a `ServerServiceDefinition`.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n","csharp\r\n    /// <summary>Creates service definition that can be registered with a server</summary>\r\n    /// <param name=""serviceImpl"">An object implementing the server-side handling logic.</param>\r\n    public static grpc::ServerServiceDefinition BindService(EmptyServiceBase serviceImpl)\r\n    {\r\n      return grpc::ServerServiceDefinition.CreateBuilder()\r\n    }\r\n"
15441,"PHP failed to use closure in function to create credential<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\n```\r\nPHP 7.2.5 (cli) (built: Apr 26 2018 12:07:32) ( NTS )\r\nCopyright (c) 1997-2018 The PHP Group\r\nZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies\r\n    with Zend OPcache v7.2.5, Copyright (c) 1999-2018, by Zend Technologies\r\n``` \r\n\r\nGRPC 1.12.0\r\nPB 3.5.2\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nmacOS 10.13.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nPHP\r\n \r\n### What did you do?\r\n\r\n\r\n \r\n### What did you expect to see?\r\n \r\nWorks the same\r\n \r\n### What did you see instead?\r\n \r\n```\r\nPHP Fatal error:  Uncaught Error: Cannot call overloaded function for non-object in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php:44\r\nStack trace:\r\n#0 /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php(44): Grpc\\Call->startBatch(Array)\r\n#1 /Workspace/vendor/grpc/grpc/src/lib/BaseStub.php(242): Grpc\\UnaryCall->start(Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array)\r\n#2 /Workspace/vendor/wen/wenapis/Wener/Service/V1/InfoServiceClient.php(31): Grpc\\BaseStub->_simpleRequest('/Wener.servic...', Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array, Array)\r\n#3 /Workspace/ignored/t3.php(39): Wener\\Service\\V1\\InfoServiceClient->GetInfo(Object(Wener\\Service\\V1\\GetInfoRequest))\r\n#4 {main}\r\n  thrown in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php on line 44\r\n\r\nFatal error: Uncaught Error: Cannot call overloaded function for non-object in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php:44\r\nStack trace:\r\n#0 /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php(44): Grpc\\Call->startBatch(Array)\r\n#1 /Workspace/vendor/grpc/grpc/src/lib/BaseStub.php(242): Grpc\\UnaryCall->start(Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array)\r\n#2 /Workspace/vendor/wen/wenapis/Wener/Service/V1/InfoServiceClient.php(31): Grpc\\BaseStub->_simpleRequest('/Wener.servic...', Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array, Array)\r\n#3 /Workspace/ignored/t3.php(39): Wener\\Service\\V1\\InfoServiceClient->GetInfo(Object(Wener\\Service\\V1\\GetInfoRequest))\r\n#4 {main}\r\n  thrown in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php on line 44\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNO",kind/bug|lang/php|priority/P2,ZhouyihaiDing,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\n```\r\nPHP 7.2.5 (cli) (built: Apr 26 2018 12:07:32) ( NTS )\r\nCopyright (c) 1997-2018 The PHP Group\r\nZend Engine v3.2.0, Copyright (c) 1998-2018 Zend Technologies\r\n    with Zend OPcache v7.2.5, Copyright (c) 1999-2018, by Zend Technologies\r\n``` \r\n\r\nGRPC 1.12.0\r\nPB 3.5.2\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nmacOS 10.13.4\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nPHP\r\n \r\n### What did you do?\r\n\r\n```php\r\n<?php\r\n\r\nrequire __DIR__ . '/../vendor/autoload.php';\r\n\r\nuse Wener\\Api;\r\n\r\n\r\n$updateMetadata = function ($context) {\r\n//    echo ""CALL $context->service_url : $context->method_name"" . PHP_EOL;\r\n    return [];\r\n};\r\n\r\nfunction createCred(){\r\n    $updateMetadata = function ($context) {\r\n        return [];\r\n    };\r\n    $cred = Grpc\\ChannelCredentials::createComposite(\r\n        Grpc\\ChannelCredentials::createSsl(file_get_contents('roots.pem')),\r\n        Grpc\\CallCredentials::createFromPlugin($updateMetadata));\r\n    return $cred;\r\n}\r\n\r\n$cred = Grpc\\ChannelCredentials::createComposite(\r\n    Grpc\\ChannelCredentials::createSsl(file_get_contents('roots.pem')),\r\n    Grpc\\CallCredentials::createFromPlugin($updateMetadata));\r\n\r\n// This works\r\n$client = new \\Wener\\Service\\V1\\InfoServiceClient(""myservice.com:443"", [\r\n    'credentials' => $cred,\r\n]);\r\n\r\nlist($reply, $status) = $client->GetInfo(new \\Wener\\Service\\V1\\GetInfoRequest())->wait();\r\n\r\n// This not work\r\n$client = new \\Wener\\Service\\V1\\InfoServiceClient(""myservice.com:443"", [\r\n    'credentials' => createCred(),\r\n]);\r\n\r\nlist($reply, $status) = $client->GetInfo(new \\Wener\\Service\\V1\\GetInfoRequest())->wait();\r\n```\r\n \r\n### What did you expect to see?\r\n \r\nWorks the same\r\n \r\n### What did you see instead?\r\n \r\n```\r\nPHP Fatal error:  Uncaught Error: Cannot call overloaded function for non-object in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php:44\r\nStack trace:\r\n#0 /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php(44): Grpc\\Call->startBatch(Array)\r\n#1 /Workspace/vendor/grpc/grpc/src/lib/BaseStub.php(242): Grpc\\UnaryCall->start(Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array)\r\n#2 /Workspace/vendor/wen/wenapis/Wener/Service/V1/InfoServiceClient.php(31): Grpc\\BaseStub->_simpleRequest('/Wener.servic...', Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array, Array)\r\n#3 /Workspace/ignored/t3.php(39): Wener\\Service\\V1\\InfoServiceClient->GetInfo(Object(Wener\\Service\\V1\\GetInfoRequest))\r\n#4 {main}\r\n  thrown in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php on line 44\r\n\r\nFatal error: Uncaught Error: Cannot call overloaded function for non-object in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php:44\r\nStack trace:\r\n#0 /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php(44): Grpc\\Call->startBatch(Array)\r\n#1 /Workspace/vendor/grpc/grpc/src/lib/BaseStub.php(242): Grpc\\UnaryCall->start(Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array)\r\n#2 /Workspace/vendor/wen/wenapis/Wener/Service/V1/InfoServiceClient.php(31): Grpc\\BaseStub->_simpleRequest('/Wener.servic...', Object(Wener\\Service\\V1\\GetInfoRequest), Array, Array, Array)\r\n#3 /Workspace/ignored/t3.php(39): Wener\\Service\\V1\\InfoServiceClient->GetInfo(Object(Wener\\Service\\V1\\GetInfoRequest))\r\n#4 {main}\r\n  thrown in /Workspace/vendor/grpc/grpc/src/lib/UnaryCall.php on line 44\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNO","php\r\n<?php\r\n\r\nrequire __DIR__ . '/../vendor/autoload.php';\r\n\r\nuse Wener\\Api;\r\n\r\n\r\n$updateMetadata = function ($context) {\r\n//    echo ""CALL $context->service_url : $context->method_name"" . PHP_EOL;\r\n    return [];\r\n};\r\n\r\nfunction createCred(){\r\n    $updateMetadata = function ($context) {\r\n        return [];\r\n    };\r\n    $cred = Grpc\\ChannelCredentials::createComposite(\r\n        Grpc\\ChannelCredentials::createSsl(file_get_contents('roots.pem')),\r\n        Grpc\\CallCredentials::createFromPlugin($updateMetadata));\r\n    return $cred;\r\n}\r\n\r\n$cred = Grpc\\ChannelCredentials::createComposite(\r\n    Grpc\\ChannelCredentials::createSsl(file_get_contents('roots.pem')),\r\n    Grpc\\CallCredentials::createFromPlugin($updateMetadata));\r\n\r\n// This works\r\n$client = new \\Wener\\Service\\V1\\InfoServiceClient(""myservice.com:443"", [\r\n    'credentials' => $cred,\r\n]);\r\n\r\nlist($reply, $status) = $client->GetInfo(new \\Wener\\Service\\V1\\GetInfoRequest())->wait();\r\n\r\n// This not work\r\n$client = new \\Wener\\Service\\V1\\InfoServiceClient(""myservice.com:443"", [\r\n    'credentials' => createCred(),\r\n]);\r\n\r\nlist($reply, $status) = $client->GetInfo(new \\Wener\\Service\\V1\\GetInfoRequest())->wait();\r\n"
15387,"PHP: Autoload failure for 'Grpc\\InterceptorChannel'<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n PHP, dev-master\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nLinux (Debian 9) \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nn/a \r\n \r\n### What did you do?\r\nI'm implementing an interceptor for PHP:\r\n\r\n\r\nThe `Grpc\\InterceptorChannel` class is defined under `lib/Grpc/Internal/InterceptorChannel.php` but composer autoloading is looking for it under `lib/Grpc/InterceptorChannel.php`. The unit tests pass because the test file explicitly calls `require` on the `lib/Grpc/Internal/InterceptorChannel.php` file.\r\n \r\n### What did you expect to see?\r\nNo error message.\r\n \r\n### What did you see instead?\r\n`Error: Class 'Grpc\\InterceptorChannel' not found`\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/bug|lang/php|priority/P2,ZhouyihaiDing,"<!--\r\n\r\nThis form is for bug reports and feature requests ONLY!\r\nFor general questions and troubleshooting, please ask/look for answers here:\r\n- grpc.io mailing list: https://groups.google.com/forum/#!forum/grpc-io\r\n- StackOverflow, with ""grpc"" tag: http://stackoverflow.com/questions/tagged/grpc\r\n\r\nIssues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. https://github.com/grpc/grpc-LANGUAGE/issues/new)\r\n-->\r\n \r\n### What version of gRPC and what language are you using?\r\n PHP, dev-master\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nLinux (Debian 9) \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nn/a \r\n \r\n### What did you do?\r\nI'm implementing an interceptor for PHP:\r\n```php\r\n$channel = new Channel('localhost' . $this->port, [\r\n     'credentials' => ChannelCredentials::createInsecure()\r\n]);\r\n$interceptor = new TraceInterceptor();\r\n\r\n// raises class not found error in lib/Grpc/Interceptor.php:81\r\n$channel = Interceptor::intercept($this->channel, $interceptor);\r\n```\r\n\r\nThe `Grpc\\InterceptorChannel` class is defined under `lib/Grpc/Internal/InterceptorChannel.php` but composer autoloading is looking for it under `lib/Grpc/InterceptorChannel.php`. The unit tests pass because the test file explicitly calls `require` on the `lib/Grpc/Internal/InterceptorChannel.php` file.\r\n \r\n### What did you expect to see?\r\nNo error message.\r\n \r\n### What did you see instead?\r\n`Error: Class 'Grpc\\InterceptorChannel' not found`\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","php\r\n$channel = new Channel('localhost' . $this->port, [\r\n     'credentials' => ChannelCredentials::createInsecure()\r\n]);\r\n$interceptor = new TraceInterceptor();\r\n\r\n// raises class not found error in lib/Grpc/Interceptor.php:81\r\n$channel = Interceptor::intercept($this->channel, $interceptor);\r\n"
15333,"Build errors when compiling with BUILD_SHARED_LIBS=ON on Windows### What version of gRPC and what language are you using?\r\n\r\ngRPC v1.11.x\r\nC++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nWindows\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nMicrosoft Visual Studio 15 2017 (aka MSVC++ 14.1 \r\n \r\n### What did you do?\r\n\r\n\r\n\r\n### What did you expect to see?\r\n \r\nA successful build.\r\n\r\n### What did you see instead?\r\n\r\nA number of build errors, see [build.log](https://github.com/grpc/grpc/files/1992454/build.log) for details.\r\n\r\nAlso see [configure.log](https://github.com/grpc/grpc/files/1992427/configure.log) for the configuration step output.\r\n\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nI do not believe so.\r\n\r\n\r\n",kind/bug|lang/c++|platform/Windows|priority/P2,apolcyn,"### What version of gRPC and what language are you using?\r\n\r\ngRPC v1.11.x\r\nC++\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nWindows\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nMicrosoft Visual Studio 15 2017 (aka MSVC++ 14.1 \r\n \r\n### What did you do?\r\n\r\n```console\r\ngit clone -b v1.11.x https://github.com/grpc/grpc.git\r\ncd grpc\r\ngit submodule update --init --recursive\r\ncmake -H. -B.build -DBUILD_SHARED_LIBS=ON > configure.log 2>&1\r\ncmake --build .build -- /m > build.log 2>&1\r\n```\r\n\r\n### What did you expect to see?\r\n \r\nA successful build.\r\n\r\n### What did you see instead?\r\n\r\nA number of build errors, see [build.log](https://github.com/grpc/grpc/files/1992454/build.log) for details.\r\n\r\nAlso see [configure.log](https://github.com/grpc/grpc/files/1992427/configure.log) for the configuration step output.\r\n\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nI do not believe so.\r\n\r\n\r\n",console\r\ngit clone -b v1.11.x https://github.com/grpc/grpc.git\r\ncd grpc\r\ngit submodule update --init --recursive\r\ncmake -H. -B.build -DBUILD_SHARED_LIBS=ON > configure.log 2>&1\r\ncmake --build .build -- /m > build.log 2>&1\r\n
15314,Ruby - Deadline Regression in v1.8.0+gRPC:  versions 1.8.0+.  This is a non-issue in < 1.8.0\r\nOS: Mac OS X  10.13.4\r\nRuby:  2.4.1p111\r\n \r\nThe specs for the Etcdv3 client driver that's tracking our timeout / deadline logic began failing in unpredictable ways once I upgraded the Ruby gRPC versions to 1.8.0+. \r\n\r\nhttps://github.com/davissp14/etcdv3-ruby\r\n\r\nTo reproduce:\r\n\r\nI am running this against a local Etcd v3.3.3.\r\n\r\n\r\n```\r\n$ gem install etcdv3\r\n```\r\n\r\n```\r\n$ irb\r\n```\r\n\r\n\r\n\r\n \r\n### What did you expect to see?\r\n\r\nI expect every Range call to throw a `DeadlineExceeded` Exception \r\n \r\n\r\n### What did you see instead?\r\n\r\nI see about 1/4 of the Range calls throwing an Exception.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope,kind/bug|lang/ruby|priority/P1|platform/macOS|disposition/never stale,apolcyn,"gRPC:  versions 1.8.0+.  This is a non-issue in < 1.8.0\r\nOS: Mac OS X  10.13.4\r\nRuby:  2.4.1p111\r\n \r\nThe specs for the Etcdv3 client driver that's tracking our timeout / deadline logic began failing in unpredictable ways once I upgraded the Ruby gRPC versions to 1.8.0+. \r\n\r\nhttps://github.com/davissp14/etcdv3-ruby\r\n\r\nTo reproduce:\r\n\r\nI am running this against a local Etcd v3.3.3.\r\n\r\n\r\n```\r\n$ gem install etcdv3\r\n```\r\n\r\n```\r\n$ irb\r\n```\r\n\r\n```ruby \r\nrequire ""etcdv3""\r\n\r\nstub = Etcdserverpb::KV::Stub.new(""localhost:2379"", :this_channel_is_insecure)\r\nreq = Etcdserverpb::RangeRequest.new(key: ""hey"")\r\n\r\ncount = 0\r\n10000.times do\r\n   begin\r\n     stub.range(req, deadline: Time.now.to_f + 0)\r\n     count += 1\r\n   rescue\r\n   end\r\nend\r\n\r\nputs count\r\n```\r\n\r\n \r\n### What did you expect to see?\r\n\r\nI expect every Range call to throw a `DeadlineExceeded` Exception \r\n \r\n\r\n### What did you see instead?\r\n\r\nI see about 1/4 of the Range calls throwing an Exception.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope","ruby \r\nrequire ""etcdv3""\r\n\r\nstub = Etcdserverpb::KV::Stub.new(""localhost:2379"", :this_channel_is_insecure)\r\nreq = Etcdserverpb::RangeRequest.new(key: ""hey"")\r\n\r\ncount = 0\r\n10000.times do\r\n   begin\r\n     stub.range(req, deadline: Time.now.to_f + 0)\r\n     count += 1\r\n   rescue\r\n   end\r\nend\r\n\r\nputs count\r\n"
15207,"Subchannel sharing does not seem to be working### Should this be an issue in the gRPC issue tracker?\r\nYes\r\n\r\n### What version of gRPC and what language are you using?\r\nC# - Grpc.Core 1.11.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nWindows 10 - 10.0.14393\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n.NET Core 2.0\r\n\r\n'dotnet --info' output:\r\n```\r\n.NET Command Line Tools (2.1.100)\r\n\r\nProduct Information:\r\n Version:            2.1.100\r\n Commit SHA-1 hash:  b9e74c6520\r\n\r\nRuntime Environment:\r\n OS Name:     Windows\r\n OS Version:  10.0.14393\r\n OS Platform: Windows\r\n RID:         win10-x64\r\n Base Path:   C:\\Program Files\\dotnet\\sdk\\2.1.100\\\r\n\r\nMicrosoft .NET Core Shared Framework Host\r\n\r\n  Version  : 2.0.5\r\n  Build    : 17373eb129b3b05aa18ece963f8795d65ef8ea54\r\n```\r\n \r\n### What did you do?\r\nRan the following code in a console application with appropriate credentials:\r\n\r\n \r\n### What did you expect to see?\r\nI was under the impression that subchannels would be shared for channels with the same endpoint and options. It appears that would be the case here for all the channels.\r\n \r\n### What did you see instead?\r\nI see exactly 100 log messages that say something like `New connected subchannel at 000001E9587DD850 for subchannel 000001E958653B40`, meaning 1 subchannel has been used for each channel created. This can also be verified by running netstat before pressing the enter key and seeing 100 connections in the ESTABLISHED state. No sharing is happening. \r\n\r\n### Anything else we should know about your project / environment?\r\nNothing\r\n",kind/bug|lang/C#|priority/P2|area/client channel,jtattermusch,"### Should this be an issue in the gRPC issue tracker?\r\nYes\r\n\r\n### What version of gRPC and what language are you using?\r\nC# - Grpc.Core 1.11.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nWindows 10 - 10.0.14393\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n.NET Core 2.0\r\n\r\n'dotnet --info' output:\r\n```\r\n.NET Command Line Tools (2.1.100)\r\n\r\nProduct Information:\r\n Version:            2.1.100\r\n Commit SHA-1 hash:  b9e74c6520\r\n\r\nRuntime Environment:\r\n OS Name:     Windows\r\n OS Version:  10.0.14393\r\n OS Platform: Windows\r\n RID:         win10-x64\r\n Base Path:   C:\\Program Files\\dotnet\\sdk\\2.1.100\\\r\n\r\nMicrosoft .NET Core Shared Framework Host\r\n\r\n  Version  : 2.0.5\r\n  Build    : 17373eb129b3b05aa18ece963f8795d65ef8ea54\r\n```\r\n \r\n### What did you do?\r\nRan the following code in a console application with appropriate credentials:\r\n```csharp\r\nEnvironment.SetEnvironmentVariable(""GRPC_TRACE"", ""subchannel"");\r\nEnvironment.SetEnvironmentVariable(""GRPC_VERBOSITY"", ""DEBUG"");\r\n\r\nvar channels = new List<Channel>();\r\nfor (int i = 0; i < 100; i++)\r\n{\r\n    channels.Add(new Channel(""bigtable.googleapis.com"", 443, channelCredentials));\r\n}\r\nforeach (var channel in channels)\r\n{\r\n    await channel.ConnectAsync();\r\n}\r\nConsole.WriteLine(""Channels connected, press enter to close them"");\r\nConsole.ReadLine();\r\nConsole.WriteLine(""Closing channels"");\r\nforeach (var channel in channels)\r\n{\r\n    await channel.ShutdownAsync();\r\n}\r\n```\r\n \r\n### What did you expect to see?\r\nI was under the impression that subchannels would be shared for channels with the same endpoint and options. It appears that would be the case here for all the channels.\r\n \r\n### What did you see instead?\r\nI see exactly 100 log messages that say something like `New connected subchannel at 000001E9587DD850 for subchannel 000001E958653B40`, meaning 1 subchannel has been used for each channel created. This can also be verified by running netstat before pressing the enter key and seeing 100 connections in the ESTABLISHED state. No sharing is happening. \r\n\r\n### Anything else we should know about your project / environment?\r\nNothing\r\n","csharp\r\nEnvironment.SetEnvironmentVariable(""GRPC_TRACE"", ""subchannel"");\r\nEnvironment.SetEnvironmentVariable(""GRPC_VERBOSITY"", ""DEBUG"");\r\n\r\nvar channels = new List<Channel>();\r\nfor (int i = 0; i < 100; i++)\r\n{\r\n    channels.Add(new Channel(""bigtable.googleapis.com"", 443, channelCredentials));\r\n}\r\nforeach (var channel in channels)\r\n{\r\n    await channel.ConnectAsync();\r\n}\r\nConsole.WriteLine(""Channels connected, press enter to close them"");\r\nConsole.ReadLine();\r\nConsole.WriteLine(""Closing channels"");\r\nforeach (var channel in channels)\r\n{\r\n    await channel.ShutdownAsync();\r\n}\r\n"
15100,"Building grpc for csharp c# fails with ""ResponseNotReady""### What version of gRPC and what language are you using?\r\n 1.11.0 c#\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nArch Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Python 2.7.14\r\n \r\n### What did you do?\r\nI try to compile the c# nuget package as requested here: https://github.com/grpc/grpc/issues/8636#issuecomment-382278725 with this tutorial:\r\nhttps://github.com/grpc/grpc/blob/master/src/csharp/README.md\r\n\r\nI already fixed some of the compile errors, which I uploaded here as fix:\r\nhttps://github.com/grpc/grpc/pull/15099\r\n \r\n### What did you expect to see?\r\n The code compile.\r\n \r\n### What did you see instead?\r\n\r\n \r\n### Anything else we should know about your project / environment?\r\nIts a fresh install, dependencies might be missing. I could not find any list of dependencies, please give me suggestions on how to fix it.\r\n",lang/C#,jtattermusch,"### What version of gRPC and what language are you using?\r\n 1.11.0 c#\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nArch Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Python 2.7.14\r\n \r\n### What did you do?\r\nI try to compile the c# nuget package as requested here: https://github.com/grpc/grpc/issues/8636#issuecomment-382278725 with this tutorial:\r\nhttps://github.com/grpc/grpc/blob/master/src/csharp/README.md\r\n\r\nI already fixed some of the compile errors, which I uploaded here as fix:\r\nhttps://github.com/grpc/grpc/pull/15099\r\n \r\n### What did you expect to see?\r\n The code compile.\r\n \r\n### What did you see instead?\r\n```bash\r\n[nico@archlinuxpc grpc]$ python2 tools/run_tests/run_tests.py -c dbg -l csharp --build_only\r\nUnexpected error getting flaky tests: Traceback (most recent call last):\r\n  File ""tools/run_tests/run_tests.py"", line 1434, in <module>\r\n    for test in get_bqtest_data():\r\n  File ""tools/run_tests/run_tests.py"", line 74, in get_bqtest_data\r\n    bq = big_query_utils.create_big_query()\r\n  File ""/home/user/git/grpc/tools/gcp/utils/big_query_utils.py"", line 33, in create_big_query\r\n    creds = GoogleCredentials.get_application_default()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1271, in get_application_default\r\n    return GoogleCredentials._get_implicit_credentials()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1256, in _get_implicit_credentials\r\n    credentials = checker()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1187, in _implicit_credentials_from_gce\r\n    if not _in_gce_environment():\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1042, in _in_gce_environment\r\n    if NO_GCE_CHECK != 'True' and _detect_gce_environment():\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 999, in _detect_gce_environment\r\n    http, _GCE_METADATA_URI, headers=_GCE_HEADERS)\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/transport.py"", line 282, in request\r\n    connection_type=connection_type)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1693, in request\r\n    (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1433, in _request\r\n    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1389, in _conn_request\r\n    response = conn.getresponse()\r\n  File ""/usr/lib/python2.7/httplib.py"", line 1108, in getresponse\r\n    raise ResponseNotReady()\r\nResponseNotReady\r\n\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFLAKE: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1129]\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFLAKE: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1132]\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFAILED: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1135, time=0.0sec]\r\nFAILED: Some tests failed\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\nIts a fresh install, dependencies might be missing. I could not find any list of dependencies, please give me suggestions on how to fix it.\r\n","bash\r\n[nico@archlinuxpc grpc]$ python2 tools/run_tests/run_tests.py -c dbg -l csharp --build_only\r\nUnexpected error getting flaky tests: Traceback (most recent call last):\r\n  File ""tools/run_tests/run_tests.py"", line 1434, in <module>\r\n    for test in get_bqtest_data():\r\n  File ""tools/run_tests/run_tests.py"", line 74, in get_bqtest_data\r\n    bq = big_query_utils.create_big_query()\r\n  File ""/home/user/git/grpc/tools/gcp/utils/big_query_utils.py"", line 33, in create_big_query\r\n    creds = GoogleCredentials.get_application_default()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1271, in get_application_default\r\n    return GoogleCredentials._get_implicit_credentials()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1256, in _get_implicit_credentials\r\n    credentials = checker()\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1187, in _implicit_credentials_from_gce\r\n    if not _in_gce_environment():\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 1042, in _in_gce_environment\r\n    if NO_GCE_CHECK != 'True' and _detect_gce_environment():\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/client.py"", line 999, in _detect_gce_environment\r\n    http, _GCE_METADATA_URI, headers=_GCE_HEADERS)\r\n  File ""/usr/lib/python2.7/site-packages/oauth2client/transport.py"", line 282, in request\r\n    connection_type=connection_type)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1693, in request\r\n    (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1433, in _request\r\n    (response, content) = self._conn_request(conn, request_uri, method, body, headers)\r\n  File ""/usr/lib/python2.7/site-packages/httplib2/__init__.py"", line 1389, in _conn_request\r\n    response = conn.getresponse()\r\n  File ""/usr/lib/python2.7/httplib.py"", line 1108, in getresponse\r\n    raise ResponseNotReady()\r\nResponseNotReady\r\n\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFLAKE: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1129]\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFLAKE: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1132]\r\nWAITING: 2 queued, 1 jobs running, 0 complete, 0 failed (load 1.00) next: make @\r\n++ dirname tools/run_tests/helper_scripts/pre_build_csharp.sh\r\n+ cd tools/run_tests/helper_scripts/../../../src/csharp\r\n+ dotnet restore Grpc.sln\r\ntools/run_tests/helper_scripts/pre_build_csharp.sh: line 21: dotnet: command not found\r\nFAILED: tools/run_tests/helper_scripts/pre_build_csharp.sh [ret=127, pid=1135, time=0.0sec]\r\nFAILED: Some tests failed\r\n"
14895,"Get startBatch failed, when send an error in server callback \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\ntrue\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nV 1.10.1 on NodeJS\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nMacOS Sierra Version 10.12.6\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nbabel 6.24 \r\nnodeJS v8.8.1\r\n\r\n### What did you do?\r\n\r\n\r\n\r\n\r\n\r\n \r\n### What did you expect to see?\r\n \r\n I expect that should not throw an exception and the server should stay running\r\n\r\n### What did you see instead?\r\n \r\nprocess of server is terminated with an exception\r\n\r\n```\r\n/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:112\r\n  call.startBatch(end_batch, function (){});\r\n       ^\r\n\r\nError: startBatch failed\r\n    at sendUnaryResponse (/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:112:8)\r\n    at sendUnaryData (/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:599:9)\r\n    at Object.GetPort (/Users/thakerng/Desktop/poc/test/services/port-service/port-service.js:16:3)\r\n    at Object.wrapper [as func] (/Users/thakerng/Desktop/poc/test/node_modules/lodash/lodash.js:4941:19)\r\n    at /Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:592:13\r\n```\r\n",lang/node,murgatroid99," \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\ntrue\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nV 1.10.1 on NodeJS\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nMacOS Sierra Version 10.12.6\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nbabel 6.24 \r\nnodeJS v8.8.1\r\n\r\n### What did you do?\r\n\r\n```javascript\r\n// .proto file\r\nsyntax = ""proto3"";\r\n\r\npackage odini;\r\n\r\nmessage PortQueryRequest {\r\n  string uid = 1;\r\n}\r\n\r\nmessage PortResponse {\r\n  string uid = 1;\r\n  float portValue = 2;\r\n}\r\n\r\nservice PortService {\r\n  rpc GetPort (PortQueryRequest) returns (PortResponse) {}\r\n}\r\n```\r\n```javascript\r\n// server\r\n\r\nimport grpc from 'grpc'\r\nimport path from 'path'\r\n\r\nconst PROTO_PATH = path.resolve(__dirname,'../protos/port-service.proto')\r\nconst testProto = grpc.load(PROTO_PATH).test\r\n\r\nconst server = new grpc.Server()\r\n\r\nconst GetPort = (call, done) => {\r\n  done({\r\n    code: 400,\r\n    message: ""invalid input"",\r\n    status: grpc.status.INTERNAL\r\n  })\r\n}\r\n\r\nserver.addProtoService(odiniProto.PortService.service, { GetPort })\r\nserver.bind('0.0.0.0:50051', grpc.ServerCredentials.createInsecure());\r\nserver.start()\r\n\r\n```\r\n\r\n```javascript\r\n// client\r\n\r\nimport grpc from 'grpc'\r\nimport path from 'path'\r\n\r\nconst PROTO_PATH = path.resolve(__dirname,'../protos/port-service.proto')\r\nconst testProto = grpc.load(PROTO_PATH).test\r\n\r\nconst client = new odiniProto.PortService('127.0.0.1:50051',grpc.credentials.createInsecure())\r\nclient.getPort({uid: ""13""}, function(err, response) {\r\n  if (err) console.error(err.message)\r\n  console.log(response);\r\n})\r\n\r\n```\r\n \r\n### What did you expect to see?\r\n \r\n I expect that should not throw an exception and the server should stay running\r\n\r\n### What did you see instead?\r\n \r\nprocess of server is terminated with an exception\r\n\r\n```\r\n/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:112\r\n  call.startBatch(end_batch, function (){});\r\n       ^\r\n\r\nError: startBatch failed\r\n    at sendUnaryResponse (/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:112:8)\r\n    at sendUnaryData (/Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:599:9)\r\n    at Object.GetPort (/Users/thakerng/Desktop/poc/test/services/port-service/port-service.js:16:3)\r\n    at Object.wrapper [as func] (/Users/thakerng/Desktop/poc/test/node_modules/lodash/lodash.js:4941:19)\r\n    at /Users/thakerng/Desktop/poc/test/node_modules/grpc/src/server.js:592:13\r\n```\r\n","javascript\r\n// .proto file\r\nsyntax = ""proto3"";\r\n\r\npackage odini;\r\n\r\nmessage PortQueryRequest {\r\n  string uid = 1;\r\n}\r\n\r\nmessage PortResponse {\r\n  string uid = 1;\r\n  float portValue = 2;\r\n}\r\n\r\nservice PortService {\r\n  rpc GetPort (PortQueryRequest) returns (PortResponse) {}\r\n}\r\n"
14808,"Segmentation fault when calculate memory usage### Should this be an issue in the gRPC issue tracker?\r\nyes\r\n\r\n### What version of gRPC and what language are you using?\r\nlang: ruby\r\ngRPC gem: 1.10.0 and 1.11.0-dev (local build)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nmacOS 10.13.3\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n- ruby: 2.5.0p0\r\n- gRPC: 1.11.0-dev\r\n\t- built by clang from master locally \r\n\t- Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n\t- Target: x86_64-apple-darwin17.4.0\r\n \r\n### What did you do?\r\nI used `ObjectSpace.memsize_of_all` to get  the client's side memory usage at run time.\r\n\r\n**server.rb**\r\n```server.rb\r\n# GreeterServer is simple server that implements the Helloworld Greeter server.\r\n\r\nclass GreeterService < Helloworld::Greeter::Service\r\n  def say_hello(hello_req, _unused_call)\r\n    Helloworld::HelloReply.new(message: ""Hello #{hello_req.name}"")\r\n  end\r\nend\r\n\r\ndef main\r\n  s = GRPC::RpcServer.new\r\n  s.add_http2_port('0.0.0.0:50051', :this_port_is_insecure)\r\n  s.handle(GreeterService)\r\n  s.run_till_terminated\r\nend\r\n\r\nmain\r\n\r\n```\r\n\r\n**client.rb**\r\n\r\n \r\n### What did you expect to see?\r\ngot emory usage per `sleep(0.2)`.\r\n \r\n### What did you see instead?\r\nSegmentation fault.\r\n\r\n```\r\nclient.rb:13: [BUG] Segmentation fault at 0x0000000000000005\r\nruby 2.5.0p0 (2017-12-25 revision 61468) [x86_64-darwin17]\r\n\r\n-- Crash Report log information --------------------------------------------\r\n   See Crash Report log file under the one of following:\r\n     * ~/Library/Logs/DiagnosticReports\r\n     * /Library/Logs/DiagnosticReports\r\n   for more details.\r\nDon't forget to include the above Crash Report log file in bug reports.\r\n\r\n-- Control frame information -----------------------------------------------\r\nc:0009 p:---- s:0040 e:000039 CFUNC  :memsize_of\r\nc:0008 p:0035 s:0035 e:000033 BLOCK  client.rb:13 [FINISH]\r\nc:0007 p:---- s:0030 e:000029 CFUNC  :each_object\r\nc:0006 p:0011 s:0026 e:000025 METHOD client.rb:11\r\nc:0005 p:0052 s:0022 e:000021 BLOCK  client.rb:25 [FINISH]\r\nc:0004 p:---- s:0017 e:000016 CFUNC  :loop\r\nc:0003 p:0008 s:0013 e:000012 METHOD client.rb:21\r\nc:0002 p:0102 s:0008 E:002260 EVAL   client.rb:36 [FINISH]\r\nc:0001 p:0000 s:0003 E:000fb0 (none) [FINISH]\r\n\r\n-- Ruby level backtrace information ----------------------------------------\r\nclient.rb:36:in `<main>'\r\nclient.rb:21:in `call_hello_world'\r\nclient.rb:21:in `loop'\r\nclient.rb:25:in `block in call_hello_world'\r\nclient.rb:11:in `print_call_metadata'\r\nclient.rb:11:in `each_object'\r\nclient.rb:13:in `block in print_call_metadata'\r\nclient.rb:13:in `memsize_of'\r\n\r\n-- Machine register context ------------------------------------------------\r\n rax: 0x0000000000000005 rbx: 0x00007f826d330ec0 rcx: 0x0000000000000000\r\n rdx: 0x000000010bd6bcf8 rdi: 0x00007f826b63de58 rsi: 0x0000000000000001\r\n rbp: 0x00007ffee3f45b00 rsp: 0x00007ffee3f45b00  r8: 0x000000010bf43f90\r\n  r9: 0x00007f826d871d50 r10: 0x0000000000000000 r11: 0x000000010c19e128\r\n r12: 0x0000000000000000 r13: 0x0000000000000000 r14: 0x0000000000000003\r\n r15: 0x0000000000000000 rip: 0x000000010c3ef219 rfl: 0x0000000000010206\r\n\r\n-- C level backtrace information -------------------------------------------\r\n0   ruby                                0x000000010bf58e49 rb_print_backtrace + 25\r\n1   ruby                                0x000000010bf58f58 rb_vm_bugreport + 136\r\n2   ruby                                0x000000010bd4b262 rb_bug_context + 450\r\n3   ruby                                0x000000010be9d51e sigsegv + 94\r\n4   libsystem_platform.dylib            0x00007fff695bbf5a _sigtramp + 26\r\n5   grpc_c.bundle                       0x000000010c3ef219 md_ary_datasize + 73\r\n6   ruby                                0x000000010bd6a21b rb_objspace_data_type_memsize + 91\r\n7   ruby                                0x000000010bd6bd0b obj_memsize_of + 843\r\n8   ruby                                0x000000010bd6b9ba rb_obj_memsize_of + 26\r\n9   objspace.bundle                     0x000000010c3da329 memsize_of_m + 25\r\n10  ruby                                0x000000010bf43fba call_cfunc_1 + 42\r\n11  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n12  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n13  ruby                                0x000000010bf2987e vm_exec_core + 8974\r\n14  ruby                                0x000000010bf39016 vm_exec + 182\r\n15  ruby                                0x000000010bf50638 invoke_block + 216\r\n16  ruby                                0x000000010bf504a2 invoke_iseq_block_from_c + 434\r\n17  ruby                                0x000000010bf5019e invoke_block_from_c_bh + 158\r\n18  ruby                                0x000000010bf500eb vm_yield + 107\r\n19  ruby                                0x000000010bf33a73 rb_yield_0 + 35\r\n20  ruby                                0x000000010bf33a3c rb_yield_1 + 28\r\n21  ruby                                0x000000010bf33ab4 rb_yield + 52\r\n22  ruby                                0x000000010bd7e62b os_obj_of_i + 139\r\n23  ruby                                0x000000010bd6a97c objspace_each_objects + 412\r\n24  ruby                                0x000000010bd56fc5 rb_ensure + 245\r\n25  ruby                                0x000000010bd6a6d6 rb_objspace_each_objects + 182\r\n26  ruby                                0x000000010bd7e582 os_obj_of + 50\r\n27  ruby                                0x000000010bd749cf os_each_obj + 159\r\n28  ruby                                0x000000010bf43f5a call_cfunc_m1 + 42\r\n29  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n30  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n31  ruby                                0x000000010bf29459 vm_exec_core + 7913\r\n32  ruby                                0x000000010bf39016 vm_exec + 182\r\n33  ruby                                0x000000010bf50638 invoke_block + 216\r\n34  ruby                                0x000000010bf504a2 invoke_iseq_block_from_c + 434\r\n35  ruby                                0x000000010bf5019e invoke_block_from_c_bh + 158\r\n36  ruby                                0x000000010bf500eb vm_yield + 107\r\n37  ruby                                0x000000010bf33a73 rb_yield_0 + 35\r\n38  ruby                                0x000000010bf51723 loop_i + 19\r\n39  ruby                                0x000000010bd569d5 rb_rescue2 + 517\r\n40  ruby                                0x000000010bf36024 rb_f_loop + 148\r\n41  ruby                                0x000000010bf43f83 call_cfunc_0 + 35\r\n42  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n43  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n44  ruby                                0x000000010bf3d92e vm_call_method_each_type + 190\r\n45  ruby                                0x000000010bf3d753 vm_call_method + 307\r\n46  ruby                                0x000000010bf3d615 vm_call_general + 53\r\n47  ruby                                0x000000010bf29459 vm_exec_core + 7913\r\n48  ruby                                0x000000010bf39016 vm_exec + 182\r\n49  ruby                                0x000000010bf39d8b rb_iseq_eval_main + 43\r\n50  ruby                                0x000000010bd55d78 ruby_exec_internal + 232\r\n51  ruby                                0x000000010bd55c81 ruby_exec_node + 33\r\n52  ruby                                0x000000010bd55c40 ruby_run_node + 64\r\n53  ruby                                0x000000010bcb5b0f main + 95\r\n\r\n-- Other runtime information -----------------------------------------------\r\n\r\n* Loaded script: client.rb\r\n\r\n* Loaded features:\r\n\r\n    0 enumerator.so\r\n    1 thread.rb\r\n    2 rational.so\r\n    3 complex.so\r\n    4 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/enc/encdb.bundle\r\n    5 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/enc/trans/transdb.bundle\r\n    6 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/rbconfig.rb\r\n    7 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/compatibility.rb\r\n    8 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/defaults.rb\r\n    9 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/deprecate.rb\r\n   10 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/errors.rb\r\n   11 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/version.rb\r\n   12 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/requirement.rb\r\n   13 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/platform.rb\r\n   14 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/basic_specification.rb\r\n   15 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/stub_specification.rb\r\n   16 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util/list.rb\r\n   17 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/stringio.bundle\r\n   18 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/specification.rb\r\n   19 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/exceptions.rb\r\n   20 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/dependency.rb\r\n   21 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/core_ext/kernel_gem.rb\r\n   22 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/monitor.rb\r\n   23 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/core_ext/kernel_require.rb\r\n   24 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems.rb\r\n   25 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/path_support.rb\r\n   26 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/version.rb\r\n   27 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/core_ext/name_error.rb\r\n   28 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/levenshtein.rb\r\n   29 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/jaro_winkler.rb\r\n   30 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checker.rb\r\n   31 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/delegate.rb\r\n   32 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers/class_name_checker.rb\r\n   33 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers/variable_name_checker.rb\r\n   34 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers.rb\r\n   35 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/method_name_checker.rb\r\n   36 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/key_error_checker.rb\r\n   37 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/null_checker.rb\r\n   38 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/formatters/plain_formatter.rb\r\n   39 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean.rb\r\n   40 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/version.rb\r\n   41 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/compatibility_guard.rb\r\n   42 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/pathname.bundle\r\n   43 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/pathname.rb\r\n   44 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/constants.rb\r\n   45 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util.rb\r\n   46 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/user_interaction.rb\r\n   47 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/etc.bundle\r\n   48 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/config_file.rb\r\n   49 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/rubygems_integration.rb\r\n   50 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/current_ruby.rb\r\n   51 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/shared_helpers.rb\r\n   52 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/fileutils/lib/fileutils.rb\r\n   53 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendored_fileutils.rb\r\n   54 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/errors.rb\r\n   55 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/environment_preserver.rb\r\n   56 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/plugin/api.rb\r\n   57 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/plugin.rb\r\n   58 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/git.rb\r\n   59 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/installed.rb\r\n   60 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/specific_file.rb\r\n   61 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/local.rb\r\n   62 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/lock.rb\r\n   63 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/vendor.rb\r\n   64 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source.rb\r\n   65 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/gem_helpers.rb\r\n   66 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/match_platform.rb\r\n   67 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/rubygems_ext.rb\r\n   68 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/build_metadata.rb\r\n   69 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler.rb\r\n   70 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/rfc2396_parser.rb\r\n   71 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/rfc3986_parser.rb\r\n   72 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/common.rb\r\n   73 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/generic.rb\r\n   74 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ftp.rb\r\n   75 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/http.rb\r\n   76 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/https.rb\r\n   77 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ldap.rb\r\n   78 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ldaps.rb\r\n   79 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/mailto.rb\r\n   80 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri.rb\r\n   81 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/settings.rb\r\n   82 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/yaml_serializer.rb\r\n   83 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/ext/builder.rb\r\n   84 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/feature_flag.rb\r\n   85 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source.rb\r\n   86 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/path.rb\r\n   87 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/git.rb\r\n   88 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/rubygems.rb\r\n   89 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/lockfile_parser.rb\r\n   90 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/set.rb\r\n   91 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/definition.rb\r\n   92 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dependency.rb\r\n   93 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ruby_dsl.rb\r\n   94 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dsl.rb\r\n   95 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source_list.rb\r\n   96 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/metadata.rb\r\n   97 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/lazy_specification.rb\r\n   98 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/index.rb\r\n   99 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/tsort.rb\r\n  100 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/forwardable/impl.rb\r\n  101 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/forwardable.rb\r\n  102 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/spec_set.rb\r\n  103 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/gemspec.rb\r\n  104 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/version.rb\r\n  105 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui.rb\r\n  106 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui/silent.rb\r\n  107 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui/rg_proxy.rb\r\n  108 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/text.rb\r\n  109 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util/licenses.rb\r\n  110 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/remote_specification.rb\r\n  111 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/compatibility.rb\r\n  112 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/gem_metadata.rb\r\n  113 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/delegates/specification_provider.rb\r\n  114 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/errors.rb\r\n  115 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/action.rb\r\n  116 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/add_edge_no_circular.rb\r\n  117 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/add_vertex.rb\r\n  118 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/delete_edge.rb\r\n  119 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/detach_vertex_named.rb\r\n  120 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/set_payload.rb\r\n  121 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/tag.rb\r\n  122 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/log.rb\r\n  123 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/vertex.rb\r\n  124 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph.rb\r\n  125 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/state.rb\r\n  126 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/modules/specification_provider.rb\r\n  127 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/delegates/resolution_state.rb\r\n  128 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/resolution.rb\r\n  129 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/resolver.rb\r\n  130 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/modules/ui.rb\r\n  131 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo.rb\r\n  132 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendored_molinillo.rb\r\n  133 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/resolver/spec_group.rb\r\n  134 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/resolver.rb\r\n  135 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/gem_version_promoter.rb\r\n  136 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/runtime.rb\r\n  137 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dep_proxy.rb\r\n  138 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/stub_specification.rb\r\n  139 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/endpoint_specification.rb\r\n  140 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ruby_version.rb\r\n  141 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/bundler_version_finder.rb\r\n  142 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/setup.rb\r\n  143 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/objspace.bundle\r\n  144 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/grpc_c.bundle\r\n  145 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/grpc.rb\r\n  146 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/errors.rb\r\n  147 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/logconfig.rb\r\n  148 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/notifier.rb\r\n  149 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/core/time_consts.rb\r\n  150 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/weakref.rb\r\n  151 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/bidi_call.rb\r\n  152 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/active_call.rb\r\n  153 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/client_stub.rb\r\n  154 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/rpc_desc.rb\r\n  155 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/service.rb\r\n  156 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/rpc_server.rb\r\n  157 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/interceptor_registry.rb\r\n  158 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/interceptors.rb\r\n  159 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc.rb\r\n  160 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf/message_exts.rb\r\n  161 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/2.5/protobuf_c.bundle\r\n  162 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf/repeated_field.rb\r\n  163 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf.rb\r\n  164 /Users/everysick/dev/grpc-test/lib/helloworld_pb.rb\r\n  165 /Users/everysick/dev/grpc-test/lib/helloworld_services_pb.rb\r\n\r\n[NOTE]\r\nYou may have encountered a bug in the Ruby interpreter or extension libraries.\r\nBug reports are welcome.\r\nFor details: http://www.ruby-lang.org/bugreport.html\r\n\r\n[IMPORTANT]\r\nDon't forget to include the Crash Report log file under\r\nDiagnosticReports directory in bug reports.\r\n\r\nzsh: abort (core dumped)  bundle exec ruby client.rb\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nThis SEGV is caused by the early release of `grpc_metadata_array` that wraped by [grpc_rb_md_ary_data_type](https://github.com/grpc/grpc/blob/baa14a975ef92ee6fb301f0e684f56f18f2c55a7/src/ruby/ext/grpc/rb_call.c#L118).\r\n\r\nThe entities of `grpc_metadata_array` owned by `run_batch_stack` is released after execution of `run_batch`. It does not consider ruby level's reference. When I use `ObjectSpace.memsize_of` for instance of  `GRPC::Core::MetadataArray`, we refer to `grpc_metadata_array` in [this function](https://github.com/grpc/grpc/blob/baa14a975ef92ee6fb301f0e684f56f18f2c55a7/src/ruby/ext/grpc/rb_call.c#L106). But the `grpc_metadata_array` had released, and SEGV occurs.\r\n\r\n`grpc_rb_md_ary_data_type` was introduced by https://github.com/grpc/grpc/pull/1295.\r\n\r\nI think dsize function poineter of `grpc_rb_md_ary_data_type` should be `GRPC_RB_MEMSIZE_UNAVAILABLE`. I wrote a patch. and I want to send it.\r\n",lang/ruby,apolcyn,"### Should this be an issue in the gRPC issue tracker?\r\nyes\r\n\r\n### What version of gRPC and what language are you using?\r\nlang: ruby\r\ngRPC gem: 1.10.0 and 1.11.0-dev (local build)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nmacOS 10.13.3\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n- ruby: 2.5.0p0\r\n- gRPC: 1.11.0-dev\r\n\t- built by clang from master locally \r\n\t- Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n\t- Target: x86_64-apple-darwin17.4.0\r\n \r\n### What did you do?\r\nI used `ObjectSpace.memsize_of_all` to get  the client's side memory usage at run time.\r\n\r\n**server.rb**\r\n```server.rb\r\n# GreeterServer is simple server that implements the Helloworld Greeter server.\r\n\r\nclass GreeterService < Helloworld::Greeter::Service\r\n  def say_hello(hello_req, _unused_call)\r\n    Helloworld::HelloReply.new(message: ""Hello #{hello_req.name}"")\r\n  end\r\nend\r\n\r\ndef main\r\n  s = GRPC::RpcServer.new\r\n  s.add_http2_port('0.0.0.0:50051', :this_port_is_insecure)\r\n  s.handle(GreeterService)\r\n  s.run_till_terminated\r\nend\r\n\r\nmain\r\n\r\n```\r\n\r\n**client.rb**\r\n```client.rb\r\n# GreeterServer is simple server that implements the Helloworld Greeter server.\r\n\r\ndef main\r\n  loop do\r\n    puts ""#{ObjectSpace.memsize_of_all(GRPC::Core::MetadataArray)} byte""\r\n\r\n    stub = Helloworld::Greeter::Stub.new('localhost:50051', :this_channel_is_insecure)\r\n    message = stub.say_hello(Helloworld::HelloRequest.new(name: 'world')).message\r\n\r\n    sleep(0.2)\r\n  end\r\nend\r\n\r\nmain\r\n```\r\n \r\n### What did you expect to see?\r\ngot emory usage per `sleep(0.2)`.\r\n \r\n### What did you see instead?\r\nSegmentation fault.\r\n\r\n```\r\nclient.rb:13: [BUG] Segmentation fault at 0x0000000000000005\r\nruby 2.5.0p0 (2017-12-25 revision 61468) [x86_64-darwin17]\r\n\r\n-- Crash Report log information --------------------------------------------\r\n   See Crash Report log file under the one of following:\r\n     * ~/Library/Logs/DiagnosticReports\r\n     * /Library/Logs/DiagnosticReports\r\n   for more details.\r\nDon't forget to include the above Crash Report log file in bug reports.\r\n\r\n-- Control frame information -----------------------------------------------\r\nc:0009 p:---- s:0040 e:000039 CFUNC  :memsize_of\r\nc:0008 p:0035 s:0035 e:000033 BLOCK  client.rb:13 [FINISH]\r\nc:0007 p:---- s:0030 e:000029 CFUNC  :each_object\r\nc:0006 p:0011 s:0026 e:000025 METHOD client.rb:11\r\nc:0005 p:0052 s:0022 e:000021 BLOCK  client.rb:25 [FINISH]\r\nc:0004 p:---- s:0017 e:000016 CFUNC  :loop\r\nc:0003 p:0008 s:0013 e:000012 METHOD client.rb:21\r\nc:0002 p:0102 s:0008 E:002260 EVAL   client.rb:36 [FINISH]\r\nc:0001 p:0000 s:0003 E:000fb0 (none) [FINISH]\r\n\r\n-- Ruby level backtrace information ----------------------------------------\r\nclient.rb:36:in `<main>'\r\nclient.rb:21:in `call_hello_world'\r\nclient.rb:21:in `loop'\r\nclient.rb:25:in `block in call_hello_world'\r\nclient.rb:11:in `print_call_metadata'\r\nclient.rb:11:in `each_object'\r\nclient.rb:13:in `block in print_call_metadata'\r\nclient.rb:13:in `memsize_of'\r\n\r\n-- Machine register context ------------------------------------------------\r\n rax: 0x0000000000000005 rbx: 0x00007f826d330ec0 rcx: 0x0000000000000000\r\n rdx: 0x000000010bd6bcf8 rdi: 0x00007f826b63de58 rsi: 0x0000000000000001\r\n rbp: 0x00007ffee3f45b00 rsp: 0x00007ffee3f45b00  r8: 0x000000010bf43f90\r\n  r9: 0x00007f826d871d50 r10: 0x0000000000000000 r11: 0x000000010c19e128\r\n r12: 0x0000000000000000 r13: 0x0000000000000000 r14: 0x0000000000000003\r\n r15: 0x0000000000000000 rip: 0x000000010c3ef219 rfl: 0x0000000000010206\r\n\r\n-- C level backtrace information -------------------------------------------\r\n0   ruby                                0x000000010bf58e49 rb_print_backtrace + 25\r\n1   ruby                                0x000000010bf58f58 rb_vm_bugreport + 136\r\n2   ruby                                0x000000010bd4b262 rb_bug_context + 450\r\n3   ruby                                0x000000010be9d51e sigsegv + 94\r\n4   libsystem_platform.dylib            0x00007fff695bbf5a _sigtramp + 26\r\n5   grpc_c.bundle                       0x000000010c3ef219 md_ary_datasize + 73\r\n6   ruby                                0x000000010bd6a21b rb_objspace_data_type_memsize + 91\r\n7   ruby                                0x000000010bd6bd0b obj_memsize_of + 843\r\n8   ruby                                0x000000010bd6b9ba rb_obj_memsize_of + 26\r\n9   objspace.bundle                     0x000000010c3da329 memsize_of_m + 25\r\n10  ruby                                0x000000010bf43fba call_cfunc_1 + 42\r\n11  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n12  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n13  ruby                                0x000000010bf2987e vm_exec_core + 8974\r\n14  ruby                                0x000000010bf39016 vm_exec + 182\r\n15  ruby                                0x000000010bf50638 invoke_block + 216\r\n16  ruby                                0x000000010bf504a2 invoke_iseq_block_from_c + 434\r\n17  ruby                                0x000000010bf5019e invoke_block_from_c_bh + 158\r\n18  ruby                                0x000000010bf500eb vm_yield + 107\r\n19  ruby                                0x000000010bf33a73 rb_yield_0 + 35\r\n20  ruby                                0x000000010bf33a3c rb_yield_1 + 28\r\n21  ruby                                0x000000010bf33ab4 rb_yield + 52\r\n22  ruby                                0x000000010bd7e62b os_obj_of_i + 139\r\n23  ruby                                0x000000010bd6a97c objspace_each_objects + 412\r\n24  ruby                                0x000000010bd56fc5 rb_ensure + 245\r\n25  ruby                                0x000000010bd6a6d6 rb_objspace_each_objects + 182\r\n26  ruby                                0x000000010bd7e582 os_obj_of + 50\r\n27  ruby                                0x000000010bd749cf os_each_obj + 159\r\n28  ruby                                0x000000010bf43f5a call_cfunc_m1 + 42\r\n29  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n30  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n31  ruby                                0x000000010bf29459 vm_exec_core + 7913\r\n32  ruby                                0x000000010bf39016 vm_exec + 182\r\n33  ruby                                0x000000010bf50638 invoke_block + 216\r\n34  ruby                                0x000000010bf504a2 invoke_iseq_block_from_c + 434\r\n35  ruby                                0x000000010bf5019e invoke_block_from_c_bh + 158\r\n36  ruby                                0x000000010bf500eb vm_yield + 107\r\n37  ruby                                0x000000010bf33a73 rb_yield_0 + 35\r\n38  ruby                                0x000000010bf51723 loop_i + 19\r\n39  ruby                                0x000000010bd569d5 rb_rescue2 + 517\r\n40  ruby                                0x000000010bf36024 rb_f_loop + 148\r\n41  ruby                                0x000000010bf43f83 call_cfunc_0 + 35\r\n42  ruby                                0x000000010bf42d4d vm_call_cfunc_with_frame + 605\r\n43  ruby                                0x000000010bf3e44d vm_call_cfunc + 173\r\n44  ruby                                0x000000010bf3d92e vm_call_method_each_type + 190\r\n45  ruby                                0x000000010bf3d753 vm_call_method + 307\r\n46  ruby                                0x000000010bf3d615 vm_call_general + 53\r\n47  ruby                                0x000000010bf29459 vm_exec_core + 7913\r\n48  ruby                                0x000000010bf39016 vm_exec + 182\r\n49  ruby                                0x000000010bf39d8b rb_iseq_eval_main + 43\r\n50  ruby                                0x000000010bd55d78 ruby_exec_internal + 232\r\n51  ruby                                0x000000010bd55c81 ruby_exec_node + 33\r\n52  ruby                                0x000000010bd55c40 ruby_run_node + 64\r\n53  ruby                                0x000000010bcb5b0f main + 95\r\n\r\n-- Other runtime information -----------------------------------------------\r\n\r\n* Loaded script: client.rb\r\n\r\n* Loaded features:\r\n\r\n    0 enumerator.so\r\n    1 thread.rb\r\n    2 rational.so\r\n    3 complex.so\r\n    4 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/enc/encdb.bundle\r\n    5 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/enc/trans/transdb.bundle\r\n    6 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/rbconfig.rb\r\n    7 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/compatibility.rb\r\n    8 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/defaults.rb\r\n    9 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/deprecate.rb\r\n   10 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/errors.rb\r\n   11 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/version.rb\r\n   12 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/requirement.rb\r\n   13 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/platform.rb\r\n   14 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/basic_specification.rb\r\n   15 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/stub_specification.rb\r\n   16 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util/list.rb\r\n   17 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/stringio.bundle\r\n   18 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/specification.rb\r\n   19 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/exceptions.rb\r\n   20 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/dependency.rb\r\n   21 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/core_ext/kernel_gem.rb\r\n   22 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/monitor.rb\r\n   23 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/core_ext/kernel_require.rb\r\n   24 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems.rb\r\n   25 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/path_support.rb\r\n   26 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/version.rb\r\n   27 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/core_ext/name_error.rb\r\n   28 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/levenshtein.rb\r\n   29 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/jaro_winkler.rb\r\n   30 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checker.rb\r\n   31 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/delegate.rb\r\n   32 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers/class_name_checker.rb\r\n   33 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers/variable_name_checker.rb\r\n   34 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/name_error_checkers.rb\r\n   35 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/method_name_checker.rb\r\n   36 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/key_error_checker.rb\r\n   37 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/spell_checkers/null_checker.rb\r\n   38 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean/formatters/plain_formatter.rb\r\n   39 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/did_you_mean-1.2.0/lib/did_you_mean.rb\r\n   40 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/version.rb\r\n   41 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/compatibility_guard.rb\r\n   42 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/pathname.bundle\r\n   43 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/pathname.rb\r\n   44 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/constants.rb\r\n   45 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util.rb\r\n   46 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/user_interaction.rb\r\n   47 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/etc.bundle\r\n   48 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/config_file.rb\r\n   49 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/rubygems_integration.rb\r\n   50 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/current_ruby.rb\r\n   51 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/shared_helpers.rb\r\n   52 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/fileutils/lib/fileutils.rb\r\n   53 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendored_fileutils.rb\r\n   54 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/errors.rb\r\n   55 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/environment_preserver.rb\r\n   56 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/plugin/api.rb\r\n   57 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/plugin.rb\r\n   58 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/git.rb\r\n   59 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/installed.rb\r\n   60 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/specific_file.rb\r\n   61 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/local.rb\r\n   62 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/lock.rb\r\n   63 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source/vendor.rb\r\n   64 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/source.rb\r\n   65 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/gem_helpers.rb\r\n   66 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/match_platform.rb\r\n   67 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/rubygems_ext.rb\r\n   68 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/build_metadata.rb\r\n   69 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler.rb\r\n   70 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/rfc2396_parser.rb\r\n   71 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/rfc3986_parser.rb\r\n   72 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/common.rb\r\n   73 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/generic.rb\r\n   74 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ftp.rb\r\n   75 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/http.rb\r\n   76 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/https.rb\r\n   77 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ldap.rb\r\n   78 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/ldaps.rb\r\n   79 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri/mailto.rb\r\n   80 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/uri.rb\r\n   81 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/settings.rb\r\n   82 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/yaml_serializer.rb\r\n   83 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/ext/builder.rb\r\n   84 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/feature_flag.rb\r\n   85 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source.rb\r\n   86 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/path.rb\r\n   87 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/git.rb\r\n   88 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/rubygems.rb\r\n   89 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/lockfile_parser.rb\r\n   90 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/set.rb\r\n   91 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/definition.rb\r\n   92 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dependency.rb\r\n   93 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ruby_dsl.rb\r\n   94 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dsl.rb\r\n   95 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source_list.rb\r\n   96 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/metadata.rb\r\n   97 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/lazy_specification.rb\r\n   98 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/index.rb\r\n   99 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/tsort.rb\r\n  100 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/forwardable/impl.rb\r\n  101 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/forwardable.rb\r\n  102 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/spec_set.rb\r\n  103 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/source/gemspec.rb\r\n  104 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/version.rb\r\n  105 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui.rb\r\n  106 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui/silent.rb\r\n  107 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ui/rg_proxy.rb\r\n  108 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/text.rb\r\n  109 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/util/licenses.rb\r\n  110 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/remote_specification.rb\r\n  111 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/compatibility.rb\r\n  112 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/gem_metadata.rb\r\n  113 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/delegates/specification_provider.rb\r\n  114 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/errors.rb\r\n  115 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/action.rb\r\n  116 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/add_edge_no_circular.rb\r\n  117 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/add_vertex.rb\r\n  118 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/delete_edge.rb\r\n  119 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/detach_vertex_named.rb\r\n  120 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/set_payload.rb\r\n  121 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/tag.rb\r\n  122 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/log.rb\r\n  123 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph/vertex.rb\r\n  124 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/dependency_graph.rb\r\n  125 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/state.rb\r\n  126 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/modules/specification_provider.rb\r\n  127 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/delegates/resolution_state.rb\r\n  128 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/resolution.rb\r\n  129 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/resolver.rb\r\n  130 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo/modules/ui.rb\r\n  131 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendor/molinillo/lib/molinillo.rb\r\n  132 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/vendored_molinillo.rb\r\n  133 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/resolver/spec_group.rb\r\n  134 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/resolver.rb\r\n  135 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/gem_version_promoter.rb\r\n  136 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/runtime.rb\r\n  137 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/dep_proxy.rb\r\n  138 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/stub_specification.rb\r\n  139 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/endpoint_specification.rb\r\n  140 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/ruby_version.rb\r\n  141 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/rubygems/bundler_version_finder.rb\r\n  142 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/gems/2.5.0/gems/bundler-1.16.1/lib/bundler/setup.rb\r\n  143 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/x86_64-darwin17/objspace.bundle\r\n  144 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/grpc_c.bundle\r\n  145 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/grpc.rb\r\n  146 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/errors.rb\r\n  147 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/logconfig.rb\r\n  148 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/notifier.rb\r\n  149 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/core/time_consts.rb\r\n  150 /Users/everysick/.rbenv/versions/2.5.0/lib/ruby/2.5.0/weakref.rb\r\n  151 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/bidi_call.rb\r\n  152 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/active_call.rb\r\n  153 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/client_stub.rb\r\n  154 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/rpc_desc.rb\r\n  155 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/service.rb\r\n  156 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/rpc_server.rb\r\n  157 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/interceptor_registry.rb\r\n  158 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc/generic/interceptors.rb\r\n  159 /Users/everysick/src/github.com/Everysick/grpc/src/ruby/lib/grpc.rb\r\n  160 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf/message_exts.rb\r\n  161 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/2.5/protobuf_c.bundle\r\n  162 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf/repeated_field.rb\r\n  163 /Users/everysick/dev/grpc-test/vendor/bundle/ruby/2.5.0/gems/google-protobuf-3.5.1.2-universal-darwin/lib/google/protobuf.rb\r\n  164 /Users/everysick/dev/grpc-test/lib/helloworld_pb.rb\r\n  165 /Users/everysick/dev/grpc-test/lib/helloworld_services_pb.rb\r\n\r\n[NOTE]\r\nYou may have encountered a bug in the Ruby interpreter or extension libraries.\r\nBug reports are welcome.\r\nFor details: http://www.ruby-lang.org/bugreport.html\r\n\r\n[IMPORTANT]\r\nDon't forget to include the Crash Report log file under\r\nDiagnosticReports directory in bug reports.\r\n\r\nzsh: abort (core dumped)  bundle exec ruby client.rb\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nThis SEGV is caused by the early release of `grpc_metadata_array` that wraped by [grpc_rb_md_ary_data_type](https://github.com/grpc/grpc/blob/baa14a975ef92ee6fb301f0e684f56f18f2c55a7/src/ruby/ext/grpc/rb_call.c#L118).\r\n\r\nThe entities of `grpc_metadata_array` owned by `run_batch_stack` is released after execution of `run_batch`. It does not consider ruby level's reference. When I use `ObjectSpace.memsize_of` for instance of  `GRPC::Core::MetadataArray`, we refer to `grpc_metadata_array` in [this function](https://github.com/grpc/grpc/blob/baa14a975ef92ee6fb301f0e684f56f18f2c55a7/src/ruby/ext/grpc/rb_call.c#L106). But the `grpc_metadata_array` had released, and SEGV occurs.\r\n\r\n`grpc_rb_md_ary_data_type` was introduced by https://github.com/grpc/grpc/pull/1295.\r\n\r\nI think dsize function poineter of `grpc_rb_md_ary_data_type` should be `GRPC_RB_MEMSIZE_UNAVAILABLE`. I wrote a patch. and I want to send it.\r\n","client.rb\r\n# GreeterServer is simple server that implements the Helloworld Greeter server.\r\n\r\ndef main\r\n  loop do\r\n    puts ""#{ObjectSpace.memsize_of_all(GRPC::Core::MetadataArray)} byte""\r\n\r\n    stub = Helloworld::Greeter::Stub.new('localhost:50051', :this_channel_is_insecure)\r\n    message = stub.say_hello(Helloworld::HelloRequest.new(name: 'world')).message\r\n\r\n    sleep(0.2)\r\n  end\r\nend\r\n\r\nmain\r\n"
14511,"CMake build problems for v1.10.xPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nv1.10.x / C++.   \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nLinux, Ubuntu, 16.04\r\n\r\n\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n\r\n \r\n### What did you do?\r\n\r\nTry to compile the code using cmake:\r\n\r\n\r\n\r\nThe build fails to complete.\r\n\r\n\r\n### What did you expect to see?\r\n \r\nA successful build\r\n\r\n### What did you see instead?\r\n\r\nThe build fails with:\r\n\r\n \r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nCannot think of anything, but feel free to ask for specifics.\r\n",area/build,jtattermusch,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nv1.10.x / C++.   \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nLinux, Ubuntu, 16.04\r\n\r\n```console\r\n$ uname -a \r\nLinux ae79522b78cd 4.9.0-5-amd64 #1 SMP Debian 4.9.65-3+deb9u2 (2018-01-04) x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release \r\nNAME=""Ubuntu""\r\nVERSION=""16.04.3 LTS (Xenial Xerus)""\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=""Ubuntu 16.04.3 LTS""\r\nVERSION_ID=""16.04""\r\nHOME_URL=""http://www.ubuntu.com/""\r\nSUPPORT_URL=""http://help.ubuntu.com/""\r\nBUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""\r\nVERSION_CODENAME=xenial\r\nUBUNTU_CODENAME=xenial\r\n```\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n```console\r\n$ g++ --version\r\ng++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```\r\n \r\n### What did you do?\r\n\r\nTry to compile the code using cmake:\r\n\r\n```console\r\n$ git clone -bv1.10.x https://github.com/grpc/grpc.git grpc-v1.10.x\r\n$ cd grpc-v1.10.x\r\n$ git submodule update --init --recursive\r\n$ sudo docker run -it --rm  --volume $PWD:/v --workdir /v ubuntu:16.04 /bin/bash\r\n# apt update && apt install -y cmake g++ golang\r\n# cmake -H. -B.build -DCMAKE_BUILD_TYPE=Release\r\n...\r\n...\r\n-- Build files have been written to: /v/.build\r\n# cmake --build .build -- -j $(nproc)\r\n...\r\nMakefile:127: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n\r\nThe build fails to complete.\r\n\r\n\r\n### What did you expect to see?\r\n \r\nA successful build\r\n\r\n### What did you see instead?\r\n\r\nThe build fails with:\r\n\r\n```console\r\n# cmake --build .build -- -j $(nproc)\r\n...\r\n..\r\n/v/third_party/boringssl/ssl/tls_record.cc: In function 'bssl::OpenRecordResult bssl::OpenRecord(SSL*, bssl::Span<unsigned char>*, size_t*, uint8_t*, bssl::Span<unsigned char>)':\r\n/v/third_party/boringssl/ssl/tls_record.cc:595:44: error: 'type' may be used uninitialized in this function [-Werror=maybe-uninitialized]\r\n       if (type != SSL3_RT_APPLICATION_DATA && type != SSL3_RT_ALERT) {\r\n                                            ^\r\n``` \r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nCannot think of anything, but feel free to ask for specifics.\r\n","console\r\n$ uname -a \r\nLinux ae79522b78cd 4.9.0-5-amd64 #1 SMP Debian 4.9.65-3+deb9u2 (2018-01-04) x86_64 x86_64 x86_64 GNU/Linux\r\n$ cat /etc/os-release \r\nNAME=""Ubuntu""\r\nVERSION=""16.04.3 LTS (Xenial Xerus)""\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=""Ubuntu 16.04.3 LTS""\r\nVERSION_ID=""16.04""\r\nHOME_URL=""http://www.ubuntu.com/""\r\nSUPPORT_URL=""http://help.ubuntu.com/""\r\nBUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""\r\nVERSION_CODENAME=xenial\r\nUBUNTU_CODENAME=xenial\r\n"
14408,"Client freezes with round_robin load balancing### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nv1.9.x / C++ \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux, Debian, kernel 4.9.0\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n\r\n \r\n### What did you do?\r\n\r\nPlease find a program to reproduce the issue attached.  Basically create a server with multiple ports, a client that uses `round_robin` across them, then make the server close all the ports from time to time, while the client continuously makes (blocking) requests to the server.\r\n\r\n[grpc-freeze-standalone.tar.gz](https://github.com/grpc/grpc/files/1718031/grpc-freeze-standalone.tar.gz)\r\n\r\n### What did you expect to see?\r\n \r\nThe client should continue to issue RPCs.\r\n \r\n### What did you see instead?\r\n \r\nEventually the client stops making RPCs, all threads become blocked and there is no progress.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nI do not think so, but feel free to contact me if you have any questions.\r\n\r\n",area/client channel,dgquintas,"### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nv1.9.x / C++ \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux, Debian, kernel 4.9.0\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n```console \r\n$ g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Debian 6.3.0-18' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 6.3.0 20170516 (Debian 6.3.0-18) \r\n```\r\n \r\n### What did you do?\r\n\r\nPlease find a program to reproduce the issue attached.  Basically create a server with multiple ports, a client that uses `round_robin` across them, then make the server close all the ports from time to time, while the client continuously makes (blocking) requests to the server.\r\n\r\n[grpc-freeze-standalone.tar.gz](https://github.com/grpc/grpc/files/1718031/grpc-freeze-standalone.tar.gz)\r\n\r\n### What did you expect to see?\r\n \r\nThe client should continue to issue RPCs.\r\n \r\n### What did you see instead?\r\n \r\nEventually the client stops making RPCs, all threads become blocked and there is no progress.\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nI do not think so, but feel free to contact me if you have any questions.\r\n\r\n","console \r\n$ g++ -v\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/6/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Debian 6.3.0-18' --with-bugurl=file:///usr/share/doc/gcc-6/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-6 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-6-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-6-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-6-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 6.3.0 20170516 (Debian 6.3.0-18) \r\n"
14338,"Channel creation/destruction may leave dangling thread.### What version of gRPC and what language are you using?\r\n- Python==3.5.2\r\n- grpcio==1.8.4\r\n- grpcio-tools==1.8.4\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n - Linux Ubuntu 16.04\r\n - Windows 7\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n - Python==3.5.2\r\n\r\n### What did you do?\r\n\r\nHere is an example of a minimal function call we wrote\r\n\r\nAnd here is the corresponding test:\r\n\r\n\r\n### What did you expect to see?\r\n\r\nTest passes leaving no lingering non-demonic threads around.\r\n\r\n### What did you see instead?\r\nFollowing the above the context in which it was run will have accrued a unterminated non-demonic thread.  Code like the following (enforced by our CI) will fail:\r\n\r\n\r\n**Our CI requires that tests do not leave Threads of this nature after the test has completed**, and it will  flags tests that leave unterminated threads around as having failed failed.  It seems there is no (documented) way to terminate these lingering threads.   \r\n \r\n### Anything else we should know about your project / environment?\r\nI believe this is linked to #12531.  The behavior is intermittent, and does not fail in all cases.  Including sleep/timeout retry logic does not seem to help.\r\n\r\n",kind/bug|lang/Python|priority/P1|disposition/requires reporter action,meawoppl,"### What version of gRPC and what language are you using?\r\n- Python==3.5.2\r\n- grpcio==1.8.4\r\n- grpcio-tools==1.8.4\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n - Linux Ubuntu 16.04\r\n - Windows 7\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n - Python==3.5.2\r\n\r\n### What did you do?\r\n\r\nHere is an example of a minimal function call we wrote\r\n```Python\r\ndef wait_for_channel_ready(channel: grpc.Channel, timeout_ms: numbers.Number):\r\n    """"""\r\n    Wait for `channel` to enter the ready state.  If it does not enter this state\r\n    withing `timeout_seconds`, raise a TimeoutError.\r\n\r\n    :param channel: a grcp.Channel instance.\r\n    :param timeout_ms: Time in milliseconds to wait for Channel to be ready.\r\n    """"""\r\n    fut = grpc.channel_ready_future(channel)\r\n    try:\r\n        fut.result(timeout=timeout_ms / 1000)\r\n    except grpc.FutureTimeoutError as e:\r\n        fut.cancel()\r\n        raise TimeoutError(\r\n            ""Timeout waiting on {} for {} ms"".format(channel, timeout_ms)) from e\r\n```\r\nAnd here is the corresponding test:\r\n```Python\r\ndef test_waiter_times_out():\r\n     channel = grpc.insecure_channel(""localhost:51234"")  # No one answers here\r\n     with nose.tools.assert_raises(TimeoutError):\r\n        grpc_helpers.wait_for_channel_ready(channel, 5)\r\n\r\n    # This tries to trigger the __del__ handlers that grpc has decided\r\n    # actually work in Python:  https://github.com/grpc/grpc/issues/12531\r\n    del channel\r\n    gc.collect()\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nTest passes leaving no lingering non-demonic threads around.\r\n\r\n### What did you see instead?\r\nFollowing the above the context in which it was run will have accrued a unterminated non-demonic thread.  Code like the following (enforced by our CI) will fail:\r\n```Python\r\nimport threading\r\nfor t in threading.enumerate():\r\n   assert t.daemon, t\r\n```\r\n\r\n**Our CI requires that tests do not leave Threads of this nature after the test has completed**, and it will  flags tests that leave unterminated threads around as having failed failed.  It seems there is no (documented) way to terminate these lingering threads.   \r\n \r\n### Anything else we should know about your project / environment?\r\nI believe this is linked to #12531.  The behavior is intermittent, and does not fail in all cases.  Including sleep/timeout retry logic does not seem to help.\r\n\r\n","Python\r\ndef wait_for_channel_ready(channel: grpc.Channel, timeout_ms: numbers.Number):\r\n    """"""\r\n    Wait for `channel` to enter the ready state.  If it does not enter this state\r\n    withing `timeout_seconds`, raise a TimeoutError.\r\n\r\n    :param channel: a grcp.Channel instance.\r\n    :param timeout_ms: Time in milliseconds to wait for Channel to be ready.\r\n    """"""\r\n    fut = grpc.channel_ready_future(channel)\r\n    try:\r\n        fut.result(timeout=timeout_ms / 1000)\r\n    except grpc.FutureTimeoutError as e:\r\n        fut.cancel()\r\n        raise TimeoutError(\r\n            ""Timeout waiting on {} for {} ms"".format(channel, timeout_ms)) from e\r\n"
14246,"Ruby implementation polls streaming RPC endpoints### What version of gRPC and what language are you using?\r\nClient: Ruby\r\nServer: Python\r\ngRPC 1.7.0 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nN/A\r\n \r\n### What did you do?\r\nWhen using the Ruby client to open a connection to an RPC endpoint with a streaming reply, the connection appears to be polled. I see in my logs (the server is Python), that the connection is rapidly opened and closed. Other client implementations open the connection once and leave it open until it is explicitly closed by the client.\r\n\r\nRuby client snippet:\r\n\r\n\r\nServer logs:\r\n```\r\n[2017-12-01 20:35:45,608] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:45,929] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:49,282] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:49,684] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:53,034] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:53,428] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:56,792] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:57,194] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:00,574] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:00,928] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:04,366] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:04,726] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:08,147] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:08,429] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:11,803] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:12,206] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:15,582] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:15,928] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:19,305] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:19,708] [handlers](INFO) GetMissionState stream finished\r\n```\r\n\r\nFor my environment, I'm acquiring a condition variable to a message (the mission state) that updates rapidly. I notify the condition variable whenever the mission state changes to notify the open connections that are streaming it. This polling behavior is potentially causing performance issues because it has to wait to acquire the condition variable with each new connection that is opened instead of just waiting to be notified.",lang/ruby,apolcyn,"### What version of gRPC and what language are you using?\r\nClient: Ruby\r\nServer: Python\r\ngRPC 1.7.0 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n Linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nN/A\r\n \r\n### What did you do?\r\nWhen using the Ruby client to open a connection to an RPC endpoint with a streaming reply, the connection appears to be polled. I see in my logs (the server is Python), that the connection is rapidly opened and closed. Other client implementations open the connection once and leave it open until it is explicitly closed by the client.\r\n\r\nRuby client snippet:\r\n```ruby\r\n    resps = stub.get_mission_state(A17::Api::OnboardService::GetMissionStateRequest.new)\r\n    resps.each do |state_reply|\r\n        [do stuff with the repo]\r\n    end\r\n```\r\n\r\nServer logs:\r\n```\r\n[2017-12-01 20:35:45,608] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:45,929] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:49,282] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:49,684] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:53,034] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:53,428] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:35:56,792] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:35:57,194] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:00,574] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:00,928] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:04,366] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:04,726] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:08,147] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:08,429] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:11,803] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:12,206] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:15,582] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:15,928] [handlers](INFO) GetMissionState stream finished\r\n[2017-12-01 20:36:19,305] [handlers](INFO) GetMissionState stream started\r\n[2017-12-01 20:36:19,708] [handlers](INFO) GetMissionState stream finished\r\n```\r\n\r\nFor my environment, I'm acquiring a condition variable to a message (the mission state) that updates rapidly. I notify the condition variable whenever the mission state changes to notify the open connections that are streaming it. This polling behavior is potentially causing performance issues because it has to wait to acquire the condition variable with each new connection that is opened instead of just waiting to be notified.",ruby\r\n    resps = stub.get_mission_state(A17::Api::OnboardService::GetMissionStateRequest.new)\r\n    resps.each do |state_reply|\r\n        [do stuff with the repo]\r\n    end\r\n
14213,"Generate C# gRPC services as interfacesCurrent state of C# gRPC services code generation builds proto contracts as abstract classes to be extended by the end user. Same thing concerns client code generation. \r\n\r\n\r\n\r\nHowever it seems, that more natural way of working with code-based contract representation in C# is via interface implementation - this is also reasonable choice, as C# base class generated by proto.exe doesn't contain any code except set of virtual methods defined in gRCP proto file, which have no actual implementation.\r\n\r\nAnother case is client code implementation - at the present moment client is generated as a class. This makes harder to abstract over it. It would be great if a client could implement an interface contract (the same as used by proposed server implementation). This way we could abstract over it i.e. to use dependency injection and make unit testing easier.\r\n\r\nProposed code gen:\r\n\r\n\r\n\r\nThis proposition is not a breaking change, it could be implemented as an optional parameter to gRPC C# code gen.\r\n\r\nPS: personally I'd like to participate in implementing such a feature, but I'd need help (I haven't contributed to gRPC so far). Is there any support channel for new contributors?",lang/C#,jtattermusch,"Current state of C# gRPC services code generation builds proto contracts as abstract classes to be extended by the end user. Same thing concerns client code generation. \r\n\r\n```csharp\r\n// C# code generated for server from RouteGuide.proto file\r\npublic abstract partial class RouteGuideBase \r\n{\r\n    // methods defined in proto file\r\n}\r\n\r\n// C# code generated for client from RouteGuide.proto file\r\npublic partial class RouteGuideClient : grpc::ClientBase<RouteGuideClient> \r\n{\r\n    // methods defined in proto file\r\n}\r\n```\r\n\r\nHowever it seems, that more natural way of working with code-based contract representation in C# is via interface implementation - this is also reasonable choice, as C# base class generated by proto.exe doesn't contain any code except set of virtual methods defined in gRCP proto file, which have no actual implementation.\r\n\r\nAnother case is client code implementation - at the present moment client is generated as a class. This makes harder to abstract over it. It would be great if a client could implement an interface contract (the same as used by proposed server implementation). This way we could abstract over it i.e. to use dependency injection and make unit testing easier.\r\n\r\nProposed code gen:\r\n\r\n```csharp\r\n// C# code generated for server from RouteGuide.proto file\r\npublic partial interface IRouteGuide // keep I prefix for C# naming convention\r\n{\r\n    // methods defined in proto file\r\n}\r\n\r\n// C# code generated for client from RouteGuide.proto file - implements the same interface as server\r\npublic partial class RouteGuideClient : grpc::ClientBase<RouteGuideClient>,  IRouteGuide \r\n{\r\n    // methods defined in proto file\r\n}\r\n```\r\n\r\nThis proposition is not a breaking change, it could be implemented as an optional parameter to gRPC C# code gen.\r\n\r\nPS: personally I'd like to participate in implementing such a feature, but I'd need help (I haven't contributed to gRPC so far). Is there any support channel for new contributors?",csharp\r\n// C# code generated for server from RouteGuide.proto file\r\npublic abstract partial class RouteGuideBase \r\n{\r\n    // methods defined in proto file\r\n}\r\n\r\n// C# code generated for client from RouteGuide.proto file\r\npublic partial class RouteGuideClient : grpc::ClientBase<RouteGuideClient> \r\n{\r\n    // methods defined in proto file\r\n}\r\n
14021,"C# memory leak with multiple channels- Grpc.Core v1.8.0 and 1.8.3 behave very badly; Grpc.Core v1.7.3 is somewhat better (details below)\r\n- Windows 10\r\n\r\nSimple repro using `Google.Cloud.Speech.V1` library just for the generated protos and gRPC code:\r\n\r\n\r\n\r\nProject file:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>netcoreapp2.0</TargetFramework>\r\n  </PropertyGroup>\r\n  <ItemGroup>\r\n    <PackageReference Include=""Google.Cloud.Speech.V1"" Version=""1.0.0"" />\r\n    <PackageReference Include=""Grpc.Core"" Version=""1.8.3"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nOutput when using Grpc.Core v1.7.x:\r\n\r\n```text\r\n0: 376984\r\n1: 382392\r\n2: 382392\r\n3: 382960\r\n4: 382960\r\n5: 382960\r\n6: 382960\r\n7: 382960\r\n8: 382960\r\n9: 382960\r\n(stable)\r\n```\r\n\r\nWhen using Grpc.Core v1.8.x:\r\n\r\n```text\r\n0: 461312\r\n1: 553512\r\n2: 640344\r\n3: 728456\r\n4: 815368\r\n5: 902816\r\n6: 990232\r\n7: 1077648\r\n8: 1164728\r\n9: 1250880\r\n(keeps increasing)\r\n```\r\n\r\nNow even 1.7.x isn't perfect: if you comment out the call to `call.ResponseStream.MoveNext(System.Threading.CancellationToken.None).Wait();`, there's still a small leak, although it's much better than with 1.8.x:\r\n\r\n```text\r\n0: 376976\r\n1: 382504\r\n2: 383136\r\n3: 383280\r\n4: 383424\r\n5: 383648\r\n6: 383792\r\n7: 383936\r\n8: 384080\r\n9: 384224\r\n```\r\n\r\nUsing a single channel instead of multiple channels avoids the leak even with 1.8.x, and even without calling `MoveNext()` to observe the end of the stream.",kind/bug|lang/C#,jtattermusch,"- Grpc.Core v1.8.0 and 1.8.3 behave very badly; Grpc.Core v1.7.3 is somewhat better (details below)\r\n- Windows 10\r\n\r\nSimple repro using `Google.Cloud.Speech.V1` library just for the generated protos and gRPC code:\r\n\r\n```csharp\r\nusing Google.Apis.Auth.OAuth2;\r\nusing Google.Cloud.Speech.V1;\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\nusing System;\r\n\r\npublic class Test\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        GoogleCredential googleCredential = GoogleCredential.GetApplicationDefault();\r\n        ChannelCredentials channelCreds = googleCredential.ToChannelCredentials();\r\n        for (int i = 0; i < 100; ++i)\r\n        {\r\n            var channel = new Channel(SpeechClient.DefaultEndpoint.Host, channelCreds);\r\n            var speech = new Speech.SpeechClient(channel);\r\n            var call = speech.StreamingRecognize();\r\n            call.RequestStream.CompleteAsync().Wait();\r\n            call.ResponseStream.MoveNext(System.Threading.CancellationToken.None).Wait();\r\n            channel.ShutdownAsync().Wait();\r\n            Console.WriteLine($""{i}: {GC.GetTotalMemory(true)}"");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nProject file:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>netcoreapp2.0</TargetFramework>\r\n  </PropertyGroup>\r\n  <ItemGroup>\r\n    <PackageReference Include=""Google.Cloud.Speech.V1"" Version=""1.0.0"" />\r\n    <PackageReference Include=""Grpc.Core"" Version=""1.8.3"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nOutput when using Grpc.Core v1.7.x:\r\n\r\n```text\r\n0: 376984\r\n1: 382392\r\n2: 382392\r\n3: 382960\r\n4: 382960\r\n5: 382960\r\n6: 382960\r\n7: 382960\r\n8: 382960\r\n9: 382960\r\n(stable)\r\n```\r\n\r\nWhen using Grpc.Core v1.8.x:\r\n\r\n```text\r\n0: 461312\r\n1: 553512\r\n2: 640344\r\n3: 728456\r\n4: 815368\r\n5: 902816\r\n6: 990232\r\n7: 1077648\r\n8: 1164728\r\n9: 1250880\r\n(keeps increasing)\r\n```\r\n\r\nNow even 1.7.x isn't perfect: if you comment out the call to `call.ResponseStream.MoveNext(System.Threading.CancellationToken.None).Wait();`, there's still a small leak, although it's much better than with 1.8.x:\r\n\r\n```text\r\n0: 376976\r\n1: 382504\r\n2: 383136\r\n3: 383280\r\n4: 383424\r\n5: 383648\r\n6: 383792\r\n7: 383936\r\n8: 384080\r\n9: 384224\r\n```\r\n\r\nUsing a single channel instead of multiple channels avoids the leak even with 1.8.x, and even without calling `MoveNext()` to observe the end of the stream.","csharp\r\nusing Google.Apis.Auth.OAuth2;\r\nusing Google.Cloud.Speech.V1;\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\nusing System;\r\n\r\npublic class Test\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        GoogleCredential googleCredential = GoogleCredential.GetApplicationDefault();\r\n        ChannelCredentials channelCreds = googleCredential.ToChannelCredentials();\r\n        for (int i = 0; i < 100; ++i)\r\n        {\r\n            var channel = new Channel(SpeechClient.DefaultEndpoint.Host, channelCreds);\r\n            var speech = new Speech.SpeechClient(channel);\r\n            var call = speech.StreamingRecognize();\r\n            call.RequestStream.CompleteAsync().Wait();\r\n            call.ResponseStream.MoveNext(System.Threading.CancellationToken.None).Wait();\r\n            channel.ShutdownAsync().Wait();\r\n            Console.WriteLine($""{i}: {GC.GetTotalMemory(true)}"");\r\n        }\r\n    }\r\n}\r\n"
13855,`grpcio==1.8.x` release logs messages that shouldn't be thereThis happens consistently (i.e. every time) on the first call to `grpc.secure_channel` and logs three messages about the `GRPC_LINUX_EPOLL` environment variable.\r\n\r\n```\r\n$ python -m virtualenv --python=python3.6 venv\r\n$ venv/bin/python -m pip install grpcio google-auth requests\r\n$ venv/bin/python -m pip show grpcio\r\nName: grpcio\r\nVersion: 1.8.2\r\n...\r\n$ venv/bin/python\r\n```\r\n,kind/bug|lang/Python,mehrdada,"This happens consistently (i.e. every time) on the first call to `grpc.secure_channel` and logs three messages about the `GRPC_LINUX_EPOLL` environment variable.\r\n\r\n```\r\n$ python -m virtualenv --python=python3.6 venv\r\n$ venv/bin/python -m pip install grpcio google-auth requests\r\n$ venv/bin/python -m pip show grpcio\r\nName: grpcio\r\nVersion: 1.8.2\r\n...\r\n$ venv/bin/python\r\n```\r\n```python\r\n>>> target = 'pubsub.googleapis.com:443'\r\n>>> scopes = ('https://www.googleapis.com/auth/pubsub',),\r\n>>> \r\n>>> import google.auth\r\n>>> credentials, _ = google.auth.default(scopes=scopes)\r\n>>> \r\n>>> import google.auth.transport.requests\r\n>>> request = google.auth.transport.requests.Request()\r\n>>> \r\n>>> import google.auth.transport.grpc\r\n>>> metadata_plugin = google.auth.transport.grpc.AuthMetadataPlugin(\r\n...     credentials, request)\r\n>>> \r\n>>> import grpc\r\n>>> google_auth_credentials = grpc.metadata_call_credentials(metadata_plugin)\r\n>>> ssl_credentials = grpc.ssl_channel_credentials()\r\n>>> composite_credentials = grpc.composite_channel_credentials(\r\n...     ssl_credentials, google_auth_credentials)\r\n>>> \r\n>>> # Now the logging message happens\r\n... \r\n>>> channel = grpc.secure_channel(target, composite_credentials)\r\nE1221 08:57:47.373061113   18747 ev_epollex_linux.cc:1482]   Skipping epollex becuase GRPC_LINUX_EPOLL is not defined.\r\nE1221 08:57:47.373136726   18747 ev_epoll1_linux.cc:1261]    Skipping epoll1 becuase GRPC_LINUX_EPOLL is not defined.\r\nE1221 08:57:47.373148066   18747 ev_epollsig_linux.cc:1761]  Skipping epollsig becuase GRPC_LINUX_EPOLL is not defined.\r\n```","python\r\n>>> target = 'pubsub.googleapis.com:443'\r\n>>> scopes = ('https://www.googleapis.com/auth/pubsub',),\r\n>>> \r\n>>> import google.auth\r\n>>> credentials, _ = google.auth.default(scopes=scopes)\r\n>>> \r\n>>> import google.auth.transport.requests\r\n>>> request = google.auth.transport.requests.Request()\r\n>>> \r\n>>> import google.auth.transport.grpc\r\n>>> metadata_plugin = google.auth.transport.grpc.AuthMetadataPlugin(\r\n...     credentials, request)\r\n>>> \r\n>>> import grpc\r\n>>> google_auth_credentials = grpc.metadata_call_credentials(metadata_plugin)\r\n>>> ssl_credentials = grpc.ssl_channel_credentials()\r\n>>> composite_credentials = grpc.composite_channel_credentials(\r\n...     ssl_credentials, google_auth_credentials)\r\n>>> \r\n>>> # Now the logging message happens\r\n... \r\n>>> channel = grpc.secure_channel(target, composite_credentials)\r\nE1221 08:57:47.373061113   18747 ev_epollex_linux.cc:1482]   Skipping epollex becuase GRPC_LINUX_EPOLL is not defined.\r\nE1221 08:57:47.373136726   18747 ev_epoll1_linux.cc:1261]    Skipping epoll1 becuase GRPC_LINUX_EPOLL is not defined.\r\nE1221 08:57:47.373148066   18747 ev_epollsig_linux.cc:1761]  Skipping epollsig becuase GRPC_LINUX_EPOLL is not defined.\r\n"
13725,"C# connectivity watcher causes lots of IDE outputPlease answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nC# running in VS 15.5.1, targeting net461, tested with gRPC 1.7.3 and 1.8.0-pre2.\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nWindows 10, with Creators Update\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nVisual Studio C# compiler\r\n \r\n### What did you do?\r\n\r\nProject file:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>net461</TargetFramework>\r\n  </PropertyGroup>\r\n\r\n  <ItemGroup>\r\n    <PackageReference Include=""Grpc.Core"" Version=""1.8.0-pre2"" />\r\n    <PackageReference Include=""Grpc.Auth"" Version=""1.8.0-pre2"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nProgram file:\r\n\r\n\r\n\r\nRun in the debugger - it doesn't matter whether it's a Release or Debug configuration build.\r\n \r\n### What did you expect to see?\r\n\r\nRelatively little output in the VS Output window\r\n \r\n### What did you see instead?\r\n\r\nOnce per second:\r\n\r\n```text\r\nException thrown: 'System.Threading.Tasks.TaskCanceledException' in mscorlib.dll\r\n```\r\n\r\nIf the TaskCanceledException is set to ""break when thrown"", the stack trace shows:\r\n\r\n```text \tat System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) in f:\\dd\\ndp\\clr\\src\\BCL\\system\\runtime\\compilerservices\\TaskAwaiter.cs:line 194\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(System.Threading.Tasks.Task task)\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(System.Threading.Tasks.Task task)\r\nGrpc.Core.dll!Grpc.Core.Channel.RunConnectivityWatcherAsync()\tUnknown\tNo symbols loaded.\r\nmscorlib.dll!System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.Run()\r\nmscorlib.dll!System.Runtime.CompilerServices.AsyncMethodBuilderCore.OutputAsyncCausalityEvents.AnonymousMethod__0()\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.OutputWaitEtwEvents.AnonymousMethod__0()\r\nmscorlib.dll!System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(System.Action action, bool allowInlining, ref System.Threading.Tasks.Task currentTask)\r\nmscorlib.dll!System.Threading.Tasks.Task.FinishContinuations()\r\nmscorlib.dll!System.Threading.Tasks.Task<System.__Canon>.TrySetCanceled(System.Threading.CancellationToken tokenToRecord, object cancellationException)\r\nmscorlib.dll!System.Threading.Tasks.TaskCompletionSource<object>.TrySetCanceled(System.Threading.CancellationToken cancellationToken)\r\nmscorlib.dll!System.Threading.Tasks.TaskCompletionSource<System.__Canon>.SetCanceled()\r\nGrpc.Core.dll!Grpc.Core.Internal.BatchContextSafeHandle.Grpc.Core.Internal.IOpCompletionCallback.OnComplete(bool success)\tUnknown\tNo symbols loaded.\r\nGrpc.Core.dll!Grpc.Core.Internal.GrpcThreadPool.RunCompletionQueueEventCallback(Grpc.Core.Internal.IOpCompletionCallback callback, bool success)\tUnknown\tNo symbols loaded.\r\nmscorlib.dll!System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem()\r\nmscorlib.dll!System.Threading.ThreadPoolWorkQueue.Dispatch()\r\n```\r\n\r\nLooking at the [code involved](https://github.com/grpc/grpc/blob/866db6e9b1d8b1424b1e6640db4ffd955aaea276/src/csharp/Grpc.Core/Channel.cs#L308), the exception is being caught - it's not clear whether there's some way to avoid it being shown in the IDE output window. ",lang/C#,jtattermusch,"Please answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nC# running in VS 15.5.1, targeting net461, tested with gRPC 1.7.3 and 1.8.0-pre2.\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nWindows 10, with Creators Update\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nVisual Studio C# compiler\r\n \r\n### What did you do?\r\n\r\nProject file:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>net461</TargetFramework>\r\n  </PropertyGroup>\r\n\r\n  <ItemGroup>\r\n    <PackageReference Include=""Grpc.Core"" Version=""1.8.0-pre2"" />\r\n    <PackageReference Include=""Grpc.Auth"" Version=""1.8.0-pre2"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nProgram file:\r\n\r\n```csharp\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\nusing System;\r\n\r\nclass Program\r\n{\r\n    static void Main()\r\n    {\r\n        var endpoint = ""datastore.googleapis.com:443"";\r\n        var auth = GoogleGrpcCredentials.GetApplicationDefaultAsync().Result;\r\n        var channel = new Channel(endpoint, auth);\r\n        Console.ReadLine();\r\n        GC.KeepAlive(channel);\r\n    }\r\n}\r\n```\r\n\r\nRun in the debugger - it doesn't matter whether it's a Release or Debug configuration build.\r\n \r\n### What did you expect to see?\r\n\r\nRelatively little output in the VS Output window\r\n \r\n### What did you see instead?\r\n\r\nOnce per second:\r\n\r\n```text\r\nException thrown: 'System.Threading.Tasks.TaskCanceledException' in mscorlib.dll\r\n```\r\n\r\nIf the TaskCanceledException is set to ""break when thrown"", the stack trace shows:\r\n\r\n```text \tat System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) in f:\\dd\\ndp\\clr\\src\\BCL\\system\\runtime\\compilerservices\\TaskAwaiter.cs:line 194\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(System.Threading.Tasks.Task task)\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(System.Threading.Tasks.Task task)\r\nGrpc.Core.dll!Grpc.Core.Channel.RunConnectivityWatcherAsync()\tUnknown\tNo symbols loaded.\r\nmscorlib.dll!System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.Run()\r\nmscorlib.dll!System.Runtime.CompilerServices.AsyncMethodBuilderCore.OutputAsyncCausalityEvents.AnonymousMethod__0()\r\nmscorlib.dll!System.Runtime.CompilerServices.TaskAwaiter.OutputWaitEtwEvents.AnonymousMethod__0()\r\nmscorlib.dll!System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(System.Action action, bool allowInlining, ref System.Threading.Tasks.Task currentTask)\r\nmscorlib.dll!System.Threading.Tasks.Task.FinishContinuations()\r\nmscorlib.dll!System.Threading.Tasks.Task<System.__Canon>.TrySetCanceled(System.Threading.CancellationToken tokenToRecord, object cancellationException)\r\nmscorlib.dll!System.Threading.Tasks.TaskCompletionSource<object>.TrySetCanceled(System.Threading.CancellationToken cancellationToken)\r\nmscorlib.dll!System.Threading.Tasks.TaskCompletionSource<System.__Canon>.SetCanceled()\r\nGrpc.Core.dll!Grpc.Core.Internal.BatchContextSafeHandle.Grpc.Core.Internal.IOpCompletionCallback.OnComplete(bool success)\tUnknown\tNo symbols loaded.\r\nGrpc.Core.dll!Grpc.Core.Internal.GrpcThreadPool.RunCompletionQueueEventCallback(Grpc.Core.Internal.IOpCompletionCallback callback, bool success)\tUnknown\tNo symbols loaded.\r\nmscorlib.dll!System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, object state, bool preserveSyncCtx)\r\nmscorlib.dll!System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem()\r\nmscorlib.dll!System.Threading.ThreadPoolWorkQueue.Dispatch()\r\n```\r\n\r\nLooking at the [code involved](https://github.com/grpc/grpc/blob/866db6e9b1d8b1424b1e6640db4ffd955aaea276/src/csharp/Grpc.Core/Channel.cs#L308), the exception is being caught - it's not clear whether there's some way to avoid it being shown in the IDE output window. ","csharp\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\nusing System;\r\n\r\nclass Program\r\n{\r\n    static void Main()\r\n    {\r\n        var endpoint = ""datastore.googleapis.com:443"";\r\n        var auth = GoogleGrpcCredentials.GetApplicationDefaultAsync().Result;\r\n        var channel = new Channel(endpoint, auth);\r\n        Console.ReadLine();\r\n        GC.KeepAlive(channel);\r\n    }\r\n}\r\n"
13636,"cmake build broken with -DgRPC_BUILD_TESTS=ONPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nmaster, v1.8.x, and v1.7.x seem to have this problem. \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nTested on Linux (Ubuntu 17.04 and Ubuntu 16.04), presumably the problem appears in more platforms.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\ngcc-6.3.0, also with clang-3.8, but presumably also fails with other compilers.\r\n \r\n### What did you do?\r\n\r\n\r\n \r\n### What did you expect to see?\r\n \r\nA successful build.\r\n \r\n### What did you see instead?\r\n\r\nThe build fails trying to compile the `ares` target.  This target is found around [here](https://github.com/grpc/grpc/blob/6fa206de8f8a1444fff19a84945a424c0cabb41c/CMakeLists.txt#L4686)  \r\n### Anything else we should know about your project / environment?\r\n\r\nI would be sending a script to reproduce the problem and PR to fix it if you are interested.\r\n",area/build,jtattermusch,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nmaster, v1.8.x, and v1.7.x seem to have this problem. \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nTested on Linux (Ubuntu 17.04 and Ubuntu 16.04), presumably the problem appears in more platforms.\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\ngcc-6.3.0, also with clang-3.8, but presumably also fails with other compilers.\r\n \r\n### What did you do?\r\n\r\n```console\r\n$ git clone https://github.com/grpc/grpc.git\r\n$ cd grpc\r\n$ git submodule update --init --recursive\r\n$ mkdir .build && cd .build\r\n$ cmake -DgRPC_BUILD_TESTS=ON ..\r\n$ make\r\n```\r\n \r\n### What did you expect to see?\r\n \r\nA successful build.\r\n \r\n### What did you see instead?\r\n\r\nThe build fails trying to compile the `ares` target.  This target is found around [here](https://github.com/grpc/grpc/blob/6fa206de8f8a1444fff19a84945a424c0cabb41c/CMakeLists.txt#L4686)  \r\n### Anything else we should know about your project / environment?\r\n\r\nI would be sending a script to reproduce the problem and PR to fix it if you are interested.\r\n",console\r\n$ git clone https://github.com/grpc/grpc.git\r\n$ cd grpc\r\n$ git submodule update --init --recursive\r\n$ mkdir .build && cd .build\r\n$ cmake -DgRPC_BUILD_TESTS=ON ..\r\n$ make\r\n
13634,"Updates the ruby generator RubyTypeOf to correctly account for underscores in packagesPrior to this change, when the ruby generator tried to reference an entity that was not part of the same package (or a direct parent package) and the package contains underscores, the result would simply uppercase the first character. It should however uppercase each letter that proceeds an underscore and remove underscores.\r\n\r\ni.e.\r\n\r\n```\r\npackage my_package.service;\r\n\r\nimport ""my_package/data.proto"";\r\n\r\nservice MyService {\r\n  rpc Test (data.Request) returns data.Response {}\r\n}\r\n```\r\n\r\nWas\r\n\r\n\r\n\r\nShould be:\r\n\r\n",lang/ruby,apolcyn,"Prior to this change, when the ruby generator tried to reference an entity that was not part of the same package (or a direct parent package) and the package contains underscores, the result would simply uppercase the first character. It should however uppercase each letter that proceeds an underscore and remove underscores.\r\n\r\ni.e.\r\n\r\n```\r\npackage my_package.service;\r\n\r\nimport ""my_package/data.proto"";\r\n\r\nservice MyService {\r\n  rpc Test (data.Request) returns data.Response {}\r\n}\r\n```\r\n\r\nWas\r\n\r\n```ruby\r\n   # ...\r\n   rpc :Test, My_package::Data::Request, My_package::Data::Response\r\n   # ...\r\n```\r\n\r\nShould be:\r\n\r\n```ruby\r\n   # ...\r\n   rpc :Test, MyPackage::Data::Request, My_package::Data::Response\r\n   # ...\r\n```","ruby\r\n   # ...\r\n   rpc :Test, My_package::Data::Request, My_package::Data::Response\r\n   # ...\r\n"
13412,"PHP gRPC extension breaks fork functionsPlease answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nLatest on PECL, 1.7.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nMac OS X, 10.12.6\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n```\r\n\u21d2  cc -v\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n``` \r\n### What did you do?\r\n\r\nScript to reproduce: \r\n\r\n\r\n\r\n \r\n### What did you expect to see?\r\n\r\nThis is the script output with gRPC extension disabled:\r\n```\r\n\u21d2  php grpc-fork-bug.php\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\nchild: exiting...\r\nparent: child exited successfully\r\nparent: finished.\r\n```\r\n \r\n### What did you see instead?\r\n\r\nThis is the script output with the extension enabled: \r\n```\r\n\u21d2  php grpc-fork-bug.php\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\nchild: exiting...\r\n[HANG]\r\n```\r\n\r\nHere's a syscall trace of the process while its hung: \r\n\r\n```\r\n\u21d2  sudo dtruss -af php grpc-fork-bug.php\r\nPassword:\r\n\tPID/THRD  RELATIVE  ELAPSD    CPU SYSCALL(args) \t\t = return\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\n98193/0xff925:      4990      62      1 madvise(0x10895E000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:      5430      61      0 madvise(0x1086D7000, 0x7000, 0x5)\t\t = 0 0\r\n98193/0xff925:      5496      26     23 open(""/dev/dtracehelper\\0"", 0x2, 0x107C69000)\t\t = 3 0\r\n98193/0xff925:      8026    2766   2528 ioctl(0x3, 0x80086804, 0x7FFF57F956F8)\t\t = 0 0\r\n98193/0xff925:      8042      17     13 close(0x3)\t\t = 0 0\r\n98193/0xff925:      8175       3      0 thread_selfid(0x3, 0x80086804, 0x7FFF57F956F8)\t\t = 1046821 0\r\n98193/0xff925:      8178       4      1 bsdthread_register(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = 1073741919 0\r\n98193/0xff925:      8220       2      0 ulock_wake(0x1, 0x7FFF57F93CFC, 0x0)\t\t = -1 Err#2\r\n98193/0xff925:      8231       2      0 issetugid(0x1, 0x7FFF57F93CFC, 0x0)\t\t = 0 0\r\n98193/0xff925:      8377       4      2 mprotect(0x1087C3000, 0x88, 0x1)\t\t = 0 0\r\n98193/0xff925:      8381       2      1 mprotect(0x108CDA000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8382       2      0 mprotect(0x108CF0000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8393       2      0 mprotect(0x108CF1000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8394       2      0 mprotect(0x108D07000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8409       2      0 mprotect(0x1089AF000, 0x1000, 0x1)\t\t = 0 0\r\n98193/0xff925:      8411       3      1 mprotect(0x1087C3000, 0x88, 0x3)\t\t = 0 0\r\n98193/0xff925:      8423       2      1 mprotect(0x1087C3000, 0x88, 0x1)\t\t = 0 0\r\n98193/0xff925:      9149      70      0 getpid(0x1087C3000, 0x88, 0x1)\t\t = 98193 0\r\n98193/0xff925:      9154       4      2 stat64(""/AppleInternal/XBS/.isChrooted\\0"", 0x7FFF57F93BB8, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:      9157       2      0 stat64(""/AppleInternal\\0"", 0x7FFF57F93C50, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:      9240       3      1 csops(0x17F91, 0x7, 0x7FFF57F936E0)\t\t = -1 Err#22\r\n98193/0xff925:      9265      28     23 sysctl([CTL_KERN, 14, 1, 98193, 0, 0] (4), 0x7FFF57F93838, 0x7FFF57F93830, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:      9280       6      0 ulock_wake(0x1, 0x7FFF57F93C60, 0x0)\t\t = -1 Err#2\r\n98193/0xff925:      9315       5      2 csops(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = -1 Err#22\r\n98193/0xff925:      9465       4      0 getuid(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:      9831       2      0 getuid(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:     10374      25     20 stat64(""/System/Library/PrivateFrameworks/Heimdal.framework/Heimdal\\0"", 0x7FFF57F931E8, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:     10526     132     16 stat64(""/System/Library/Frameworks/GSS.framework/GSS\\0"", 0x7FFF57F931E8, 0x7FFF57F92FC0)\t = 0 0\r\n98193/0xff925:     10818       4      0 sigaction(0xD, 0x7FFF57F96898, 0x7FFF57F968C0)\t\t = 0 0\r\n98193/0xff925:     10836       1      0 sigaction(0x1, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x2, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x3, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x4, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10838       1      0 sigaction(0x5, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10838       1      0 sigaction(0x6, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10839       2      0 sigaction(0x7, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10839       1      0 sigaction(0x8, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xA, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xB, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xC, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10841       1      0 sigaction(0xD, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10841       1      0 sigaction(0xE, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0xF, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0x10, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0x12, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10843       1      0 sigaction(0x13, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10843       1      0 sigaction(0x14, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10844       2      0 sigaction(0x15, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10844       2      0 sigaction(0x16, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10845       2      0 sigaction(0x17, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x18, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x19, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x1A, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1B, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1C, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1D, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10848       1      0 sigaction(0x1E, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10848       1      0 sigaction(0x1F, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10927       9      3 mmap(0x0, 0x200000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     10934       4      1 munmap(0x108D08000, 0x200000)\t\t = 0 0\r\n98193/0xff925:     10935       2      0 mmap(0x0, 0x3FF000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     10937       2      1 munmap(0x108D08000, 0xF8000)\t\t = 0 0\r\n98193/0xff925:     10938       2      0 munmap(0x109000000, 0x107000)\t\t = 0 0\r\n98193/0xff925:     10951       7      3 sysctl([CTL_HW, 7, 0, 0, 0, 0] (2), 0x7FFFDC53BE00, 0x7FFF57F96270, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     10963       9      6 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 3 0\r\n98193/0xff925:     10965       3      1 fstat64(0x3, 0x7FFF57F95D00, 0x1)\t\t = 0 0\r\n98193/0xff925:     10968       4      2 fcntl_nocancel(0x3, 0x32, 0x7FFF57F95F00)\t\t = 0 0\r\n98193/0xff925:     10973       6      4 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     10978       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95C70, 0x7FFF57F95F00)\t\t = 0 0\r\n98193/0xff925:     11142      17     15 open_nocancel(""/usr/share/locale/en_US.UTF-8/LC_CTYPE\\0"", 0x0, 0x5)\t\t = 3 0\r\n98193/0xff925:     11143       2      0 fcntl_nocancel(0x3, 0x3, 0x0)\t\t = 0 0\r\n98193/0xff925:     11145       2      0 getrlimit(0x1008, 0x7FFF57F95D38, 0x0)\t\t = 0 0\r\n98193/0xff925:     11152       4      1 fstat64(0x3, 0x7FFF57F95DF8, 0x0)\t\t = 0 0\r\n98193/0xff925:     11155       2      0 fstat64(0x3, 0x7FFF57F95BE8, 0x0)\t\t = 0 0\r\n98193/0xff925:     11157       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     11158       1      0 lseek(0x3, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     11167      11      8 read_nocancel(0x3, ""RuneMagAUTF-8\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11177       3      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11194       3      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11209       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11225       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11240       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11285      37     36 read_nocancel(0x3, ""@\\004\\031\\0"", 0xDE80)\t\t = 56960 0\r\n98193/0xff925:     11299       5      3 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11335      27     25 access(""/etc/localtime\\0"", 0x4, 0xDE80)\t\t = 0 0\r\n98193/0xff925:     11353      19     17 open_nocancel(""/etc/localtime\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     11355       3      1 fstat64(0x3, 0x7FFF57F933E0, 0x0)\t\t = 0 0\r\n98193/0xff925:     11357       3      1 read_nocancel(0x3, ""TZif\\0"", 0x2A64)\t\t = 791 0\r\n98193/0xff925:     11358       2      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11490       8      5 shm_open(0x7FFFD3847DE7, 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     11495       7      4 mmap(0x0, 0x1000, 0x1, 0x1, 0x3, 0x0)\t\t = 0x1089B0000 0\r\n98193/0xff925:     11497       3      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11569       7      4 lstat64(""/Users/zack.angelo/.rvm/gems/ruby-2.2.4/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11583       9      7 lstat64(""/Users/zack.angelo/.rvm/gems/ruby-2.2.4@global/bin/php\\0"", 0x7FFF57F95990, 0x1)\t = -1 Err#2\r\n98193/0xff925:     11585       3      1 lstat64(""/Users/zack.angelo/.rvm/rubies/ruby-2.2.4/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11589       5      3 lstat64(""/Users/zack.angelo/.nvm/versions/node/v4.4.7/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11592       3      1 lstat64(""/Users/zack.angelo/google-cloud-sdk/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11594       3      1 lstat64(""/usr/local/sbin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11599       5      4 lstat64(""/usr/local/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = 0 0\r\n98193/0xff925:     11609       3      1 readlink(""/usr/local/bin/php\\0"", 0x7FFF57F95AE0, 0x400)\t\t = 33 0\r\n98193/0xff925:     11618       6      5 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22/bin/php\\0"", 0x7FFF57F95860, 0x400)\t\t = 0 0\r\n98193/0xff925:     11622       3      1 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22/bin\\0"", 0x7FFF57F95730, 0x400)\t\t = 0 0\r\n98193/0xff925:     11626       3      1 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22\\0"", 0x7FFF57F95600, 0x400)\t\t = 0 0\r\n98193/0xff925:     11628       2      1 lstat64(""/usr/local/bin/../Cellar/php71\\0"", 0x7FFF57F954D0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11631       2      1 lstat64(""/usr/local/bin/../Cellar\\0"", 0x7FFF57F953A0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11633       2      1 lstat64(""/usr/local/bin\\0"", 0x7FFF57F95140, 0x400)\t\t = 0 0\r\n98193/0xff925:     11636       2      1 lstat64(""/usr/local\\0"", 0x7FFF57F95010, 0x400)\t\t = 0 0\r\n98193/0xff925:     11638       2      1 lstat64(""/usr\\0"", 0x7FFF57F94EE0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11660      16     14 access(""/usr/local/Cellar/php71/7.1.11_22/bin/php\\0"", 0x1, 0x400)\t\t = 0 0\r\n98193/0xff925:     11667       3      1 stat64(""/usr/local/Cellar/php71/7.1.11_22/bin/php\\0"", 0x7FFF57F963F0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11702       5      3 open_nocancel(""/usr/local/Cellar/php71/7.1.11_22/bin/php-cli.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11706       5      3 open_nocancel(""/usr/local/etc/php/7.1/php-cli.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11709      12      2 open_nocancel(""/usr/local/Cellar/php71/7.1.11_22/bin/php.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11714       6      4 open_nocancel(""/usr/local/etc/php/7.1/php.ini\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     11729       3      1 ioctl(0x3, 0x4004667A, 0x7FFF57F95B3C)\t\t = -1 Err#25\r\n98193/0xff925:     11729       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F95B40)\t\t = -1 Err#25\r\n98193/0xff925:     11737       3      1 fstat64(0x3, 0x7FFF57F95BB0, 0x7FFF57F95B40)\t\t = 0 0\r\n98193/0xff925:     11743       8      5 mmap(0x0, 0x1162B, 0x1, 0x2, 0x3, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     11748       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     12086      10      6 munmap(0x108D08000, 0x1162B)\t\t = 0 0\r\n98193/0xff925:     12097       8      5 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     12114      12      9 open_nocancel(""/usr/local/etc/php/7.1/conf.d\\0"", 0x1100004, 0x0)\t\t = 3 0\r\n98193/0xff925:     12117       5      2 fstatfs64(0x3, 0x7FFF57F953D8, 0x0)\t\t = 0 0\r\n98193/0xff925:     12140      23     20 getdirentries64(0x3, 0x7F814F002C00, 0x1000)\t\t = 136 0\r\n98193/0xff925:     12145       6      3 getdirentries64(0x3, 0x7F814F002C00, 0x1000)\t\t = 0 0\r\n98193/0xff925:     12148       3      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     12162       6      4 stat64(""/usr/local/etc/php/7.1/conf.d/ext-intl.ini\\0"", 0x7FFF57F95E70, 0x1000)\t\t = 0 0\r\n98193/0xff925:     12167       5      3 open_nocancel(""/usr/local/etc/php/7.1/conf.d/ext-intl.ini\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     12169       3      0 ioctl(0x3, 0x4004667A, 0x7FFF57F95B3C)\t\t = -1 Err#25\r\n98193/0xff925:     12169       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F95B40)\t\t = -1 Err#25\r\n98193/0xff925:     12171       2      0 fstat64(0x3, 0x7FFF57F95BB0, 0x7FFF57F95B40)\t\t = 0 0\r\n98193/0xff925:     12176       8      4 mmap(0x0, 0x14F, 0x1, 0x2, 0x3, 0x0)\t\t = 0x1089B1000 0\r\n98193/0xff925:     12177       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     12226       7      3 munmap(0x1089B1000, 0x14F)\t\t = 0 0\r\n98193/0xff925:     12231       7      4 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     13270      16     12 stat64(""/usr/local/Cellar/php71/7.1.11_22/lib/php/extensions/no-debug-non-zts-20160303/grpc.so\\0"", 0x7FFF57F95778, 0x1)\t\t = 0 0\r\n98193/0xff925:     13281       7      5 open(""/usr/local/Cellar/php71/7.1.11_22/lib/php/extensions/no-debug-non-zts-20160303/grpc.so\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     13330     219     47 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     13386      47     42 mmap(0x109000000, 0x219000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x109000000 0\r\n98193/0xff925:     13391       7      4 mmap(0x109219000, 0x1A000, 0x3, 0x12, 0x3, 0x219000)\t\t = 0x109219000 0\r\n98193/0xff925:     13394       4      2 mmap(0x109239000, 0x51BDC, 0x1, 0x12, 0x3, 0x233000)\t\t = 0x109239000 0\r\n98193/0xff925:     13404       4      1 madvise(0x109239000, 0x2000, 0x2)\t\t = 0 0\r\n98193/0xff925:     13408       4      2 close(0x3)\t\t = 0 0\r\n98193/0xff925:     13957       4      0 madvise(0x109239000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:     14224      15     12 stat64(""/usr/local/opt/php71-intl/intl.so\\0"", 0x7FFF57F95778, 0x5)\t\t = 0 0\r\n98193/0xff925:     14233       8      5 open(""/usr/local/opt/php71-intl/intl.so\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14239       9      5 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14269      26     23 mmap(0x108D57000, 0x3F000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x108D57000 0\r\n98193/0xff925:     14273       5      2 mmap(0x108D96000, 0xA000, 0x3, 0x12, 0x3, 0x3F000)\t\t = 0x108D96000 0\r\n98193/0xff925:     14276       5      2 mmap(0x108DA1000, 0x1D520, 0x1, 0x12, 0x3, 0x49000)\t\t = 0x108DA1000 0\r\n98193/0xff925:     14283       2      0 madvise(0x108DA1000, 0x4000, 0x2)\t\t = 0 0\r\n98193/0xff925:     14286       4      2 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14300      12     10 stat64(""/usr/local/opt/icu4c/lib/libicui18n.59.dylib\\0"", 0x7FFF57F95538, 0x2)\t\t = 0 0\r\n98193/0xff925:     14307       6      4 open(""/usr/local/opt/icu4c/lib/libicui18n.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14310       5      3 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14332      20     17 mmap(0x10928B000, 0x15E000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x10928B000 0\r\n98193/0xff925:     14335       4      2 mmap(0x1093E9000, 0xF000, 0x3, 0x12, 0x3, 0x15E000)\t\t = 0x1093E9000 0\r\n98193/0xff925:     14337       4      1 mmap(0x1093F8000, 0xCE03C, 0x1, 0x12, 0x3, 0x16D000)\t\t = 0x1093F8000 0\r\n98193/0xff925:     14342       2      0 madvise(0x1093F8000, 0x2000, 0x2)\t\t = 0 0\r\n98193/0xff925:     14347       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14355       7      6 stat64(""/usr/local/opt/icu4c/lib/libicuuc.59.dylib\\0"", 0x7FFF57F95538, 0x2)\t\t = 0 0\r\n98193/0xff925:     14361       5      3 open(""/usr/local/opt/icu4c/lib/libicuuc.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14364       5      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14384      18     16 mmap(0x1094C7000, 0x103000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x1094C7000 0\r\n98193/0xff925:     14387       4      2 mmap(0x1095CA000, 0x12000, 0x3, 0x12, 0x3, 0x103000)\t\t = 0x1095CA000 0\r\n98193/0xff925:     14389       4      2 mmap(0x1095DD000, 0x58EA4, 0x1, 0x12, 0x3, 0x115000)\t\t = 0x1095DD000 0\r\n98193/0xff925:     14398       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14404       5      4 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.1.dylib\\0"", 0x7FFF57F95538, 0x1)\t\t = 0 0\r\n98193/0xff925:     14409       4      3 open(""/usr/local/opt/icu4c/lib/libicudata.59.1.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14412       5      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14433      19     17 mmap(0x109636000, 0x1914000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x109636000 0\r\n98193/0xff925:     14435       4      1 mmap(0x10AF4A000, 0x60, 0x1, 0x12, 0x3, 0x1914000)\t\t = 0x10AF4A000 0\r\n98193/0xff925:     14445       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14452       7      5 stat64(""/usr/local/opt/icu4c/lib/libicuio.59.dylib\\0"", 0x7FFF57F95538, 0x1)\t\t = 0 0\r\n98193/0xff925:     14458       5      3 open(""/usr/local/opt/icu4c/lib/libicuio.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14460       4      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14483      21     19 mmap(0x108DBF000, 0x9000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x108DBF000 0\r\n98193/0xff925:     14486       4      2 mmap(0x108DC8000, 0x2000, 0x3, 0x12, 0x3, 0x9000)\t\t = 0x108DC8000 0\r\n98193/0xff925:     14488       4      1 mmap(0x108DCA000, 0x3DA0, 0x1, 0x12, 0x3, 0xB000)\t\t = 0x108DCA000 0\r\n98193/0xff925:     14497       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14509       3      1 stat64(""/\\0"", 0x7FFF57F93780, 0x1)\t\t = 0 0\r\n98193/0xff925:     14518       6      4 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14521       4      2 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14524       3      2 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14527       4      2 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14529       4      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94490, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14532       4      2 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14535       4      2 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14538       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14541       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14544       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.dylib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14546       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.dylib\\0"", 0x7FFF57F94490, 0x3FF)\t = 21 0\r\n98193/0xff925:     14549       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14557       6      4 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F953C8, 0x7FFF57F95090)\t = 0 0\r\n98193/0xff925:     14563       3      1 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14565       3      2 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14567       3      1 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14571       4      2 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14572       3      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94380, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14575       3      1 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14577       4      1 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14579       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14582       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14585       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.dylib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14587       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.dylib\\0"", 0x7FFF57F94380, 0x3FF)\t = 19 0\r\n98193/0xff925:     14590       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14596       4      3 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F952B8, 0x7FFF57F94F80)\t = 0 0\r\n98193/0xff925:     14603       3      1 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14605       3      1 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14608       3      1 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14610       3      1 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14611       2      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94470, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14614       3      1 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14616       3      1 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14618       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14621       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14624       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.dylib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14625       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.dylib\\0"", 0x7FFF57F94470, 0x3FF)\t = 19 0\r\n98193/0xff925:     14628       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14634       4      2 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F953A8, 0x7FFF57F95070)\t = 0 0\r\n98193/0xff925:     15262       5      0 madvise(0x1093F8000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:     22471       9      2 madvise(0x108DA1000, 0x4000, 0x5)\t\t = 0 0\r\n98193/0xff925:     23425      27     20 stat64(""/usr/lib/libz.dylib\\0"", 0x7FFF57F95658, 0x5)\t\t = 0 0\r\n98193/0xff925:     25263      39     33 socket(0x1E, 0x2, 0x0)\t\t = 3 0\r\n98193/0xff925:     25275      12     10 close(0x3)\t\t = 0 0\r\n98193/0xff925:     26714      13      5 sysctl([CTL_KERN, 8, 0, 0, 0, 0] (2), 0x7FFF57F961E4, 0x7FFF57F961E8, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     29304      12      8 open_nocancel(""grpc-fork-bug.php\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     29310       3      1 fstat64(0x3, 0x7FFF57F960D8, 0x1B6)\t\t = 0 0\r\n98193/0xff925:     29360     249     49 read_nocancel(0x3, ""<?php \\n\\n$pid = pcntl_fork(); \\n\\nif($pid === -1) { \\n\\techo \\""error forking\\\\n\\""; \\n} elseif ($pid) { //parent \\n\\techo \\""parent: waiting for child...\\\\n\\""; \\n\\tpcntl_waitpid(0, $status);\\n\\techo \\""parent: child exited successfully\\\\n\\""; \\n} elseif ($pid === 0) { //child\\n\\techo"", 0x1000)\t\t = 371 0\r\n98193/0xff925:     29370       3      0 lseek(0x3, 0x0, 0x1)\t\t = 371 0\r\n98193/0xff925:     29377       8      5 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 4 0\r\n98193/0xff925:     29379       3      1 fstat64(0x4, 0x7FFF57F95C40, 0x1)\t\t = 0 0\r\n98193/0xff925:     29381       4      2 fcntl_nocancel(0x4, 0x32, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29384       4      1 close_nocancel(0x4)\t\t = 0 0\r\n98193/0xff925:     29387       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95BB0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29397       4      2 lstat64(""/Users/zack.angelo/git/grpc-fork-bug/grpc-fork-bug.php\\0"", 0x7FFF57F958B0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29399       2      1 lstat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95780, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29401       3      1 lstat64(""/Users/zack.angelo/git\\0"", 0x7FFF57F95650, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29403       2      1 lstat64(""/Users/zack.angelo\\0"", 0x7FFF57F95520, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29404       2      1 lstat64(""/Users\\0"", 0x7FFF57F953F0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29425       4      1 sigaction(0x1B, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29426       1      0 sigaction(0x1B, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29426       1      0 sigaction(0x1, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29427       1      0 sigaction(0x1, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29427       1      0 sigaction(0x2, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29428       1      0 sigaction(0x2, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29428       1      0 sigaction(0x3, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29429       1      0 sigaction(0x3, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29430       1      0 sigaction(0xF, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29431       1      0 sigaction(0xF, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29431       1      0 sigaction(0x1E, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1E, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1F, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1F, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29433       1      0 sigaction(0x1B, 0x7FFF57F96098, 0x0)\t\t = 0 0\r\n98193/0xff925:     29441       2      0 sigprocmask(0x2, 0x7FFF57F960CC, 0x0)\t\t = 0x0 0\r\n98193/0xff925:     29521      10      4 mmap(0x0, 0x10000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x10AF6F000 0\r\n98193/0xff925:     30217      10      5 pipe(0x0, 0x10000, 0x3)\t\t = 4 0\r\n98193/0xff925:     30229       3      0 fcntl(0x4, 0x3, 0x0)\t\t = 0 0\r\n98193/0xff925:     30230       2      0 fcntl(0x4, 0x4, 0x4)\t\t = 0 0\r\n98193/0xff925:     30231       1      0 fcntl(0x5, 0x3, 0x0)\t\t = 1 0\r\n98193/0xff925:     30231       1      0 fcntl(0x5, 0x4, 0x5)\t\t = 0 0\r\n98193/0xff925:     30243       4      2 close(0x4)\t\t = 0 0\r\n98193/0xff925:     30244       2      1 close(0x5)\t\t = 0 0\r\n98193/0xff925:     30747      25     20 bsdthread_create(0x109010800, 0x7F814DC719C0, 0x80000)\t\t = 10268672 0\r\n98193/0xffb51:         6      69      0 thread_selfid(0x0, 0x0, 0x0)\t\t = 1047377 0\r\n98193/0xffb51:       102      13      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     30936       5      2 fstat64(0x0, 0x7FFF57F96060, 0x80000)\t\t = 0 0\r\n98193/0xff925:     30952       2      0 fstat64(0x0, 0x108E72038, 0x80000)\t\t = 0 0\r\n98193/0xff925:     30954       2      0 lseek(0x0, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30956       2      0 fstat64(0x1, 0x7FFF57F96060, 0x1)\t\t = 0 0\r\n98193/0xff925:     30957       1      0 fstat64(0x1, 0x108E72118, 0x1)\t\t = 0 0\r\n98193/0xff925:     30958       1      0 lseek(0x1, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30959       2      0 fstat64(0x2, 0x7FFF57F96060, 0x1)\t\t = 0 0\r\n98193/0xff925:     30960       1      0 fstat64(0x2, 0x108E721F8, 0x1)\t\t = 0 0\r\n98193/0xff925:     30961       1      0 lseek(0x2, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30976      10      8 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 4 0\r\n98193/0xff925:     30978       2      1 fstat64(0x4, 0x7FFF57F95600, 0x1)\t\t = 0 0\r\n98193/0xff925:     30981       5      2 fcntl_nocancel(0x4, 0x32, 0x7FFF57F95820)\t\t = 0 0\r\n98193/0xff925:     30984       4      2 close_nocancel(0x4)\t\t = 0 0\r\n98193/0xff925:     30988       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95570, 0x7FFF57F95820)\t\t = 0 0\r\n98193/0xff925:     31000       2      0 ioctl(0x3, 0x4004667A, 0x7FFF57F957BC)\t\t = -1 Err#25\r\n98193/0xff925:     31001       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F957C0)\t\t = -1 Err#25\r\n98193/0xff925:     31003       2      1 fstat64(0x3, 0x7FFF57F95830, 0x7FFF57F957C0)\t\t = 0 0\r\n98193/0xff925:     31011      10      7 mmap(0x0, 0x193, 0x1, 0x2, 0x3, 0x0)\t\t = 0x1089B2000 0\r\n98193/0xff925:     31125       8      5 munmap(0x1089B2000, 0x193)\t\t = 0 0\r\n98193/0xff925:     31131       7      5 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     33667    2512   2495 fork()\t\t = 98238 0\r\n98238/0xffb52:        80:        0:       0 fork()\t\t = 0 0\r\n98238/0xffb52:        80       4      0 thread_selfid(0x0, 0x0, 0x0)\t\t = 1047378 0\r\n98238/0xffb52:        85       6      3 bsdthread_register(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = -1 Err#22\r\n98193/0xff925:     33858      15     10 write(0x1, ""parent: waiting for child...\\n\\0"", 0x1D)\t\t = 29 0\r\n98238/0xffb52:       192       3      0 getpid(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = 98238 0\r\n98238/0xffb52:       197       4      1 __mac_syscall(0x7FFFD385FC46, 0x4, 0x7FFF57F95578)\t\t = -1 Err#45\r\n98238/0xffb52:       199       3      1 csops(0x17FBE, 0xB, 0x7FFF57F955B8)\t\t = -1 Err#22\r\n98238/0xffb52:       685      14      9 write(0x1, ""child: sleeping for 1s...\\n\\b\\0"", 0x1A)\t\t = 26 0\r\nchild: exiting...\r\n98238/0xffb52:       760 1000905     10 __semwait_signal(0x703, 0x0, 0x1)\t\t = -1 Err#60\r\n98193/0xffb51:       112 1004731      9 psynch_cvwait(0x109236098, 0x100000100, 0x0)\t\t = -1 Err#316\r\n98238/0xffb52:       793     111     16 write(0x1, ""child: exiting...\\n\\0"", 0x12)\t\t = 18 0\r\n98193/0xffb51:       196      75      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98238/0xffb52:      1457      71      2 close_nocancel(0x2)\t\t = 0 0\r\n98238/0xffb52:      1480       3      0 close_nocancel(0x1)\t\t = 0 0\r\n98238/0xffb52:      1502       3      0 close_nocancel(0x0)\t\t = 0 0\r\n98238/0xffb52:      2089      16     13 munmap(0x108D96000, 0xB000)\t\t = 0 0\r\n98238/0xffb52:      2093       5      2 munmap(0x108DA1000, 0x1E000)\t\t = 0 0\r\n98238/0xffb52:      2097       6      3 munmap(0x108D57000, 0x3F000)\t\t = 0 0\r\n98238/0xffb52:      2119       4      2 munmap(0x1093E9000, 0xF000)\t\t = 0 0\r\n98238/0xffb52:      2121       3      1 munmap(0x1093F8000, 0xCF000)\t\t = 0 0\r\n98238/0xffb52:      2125       6      4 munmap(0x10928B000, 0x15E000)\t\t = 0 0\r\n98238/0xffb52:      2135       3      1 munmap(0x1095CA000, 0x13000)\t\t = 0 0\r\n98238/0xffb52:      2137       2      0 munmap(0x1095DD000, 0x59000)\t\t = 0 0\r\n98238/0xffb52:      2140       5      3 munmap(0x1094C7000, 0x103000)\t\t = 0 0\r\n98238/0xffb52:      2149       3      1 munmap(0x10AF4A000, 0x1000)\t\t = 0 0\r\n98238/0xffb52:      2154       7      4 munmap(0x109636000, 0x1914000)\t\t = 0 0\r\n98238/0xffb52:      2168       3      1 munmap(0x108DC8000, 0x2000)\t\t = 0 0\r\n98238/0xffb52:      2169       3      0 munmap(0x108DCA000, 0x4000)\t\t = 0 0\r\n98238/0xffb52:      2172       4      2 munmap(0x108DBF000, 0x9000)\t\t = 0 0\r\n98238/0xffb52:      2302       6      2 psynch_cvbroad(0x109236098, 0x100, 0x100)\t\t = 2 0\r\n98193/0xffb51:       207 1005369     10 psynch_cvwait(0x109236098, 0x10100000200, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       238      69      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       247 1001302      8 psynch_cvwait(0x109236098, 0x20100000300, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       286      72      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       298 1004393     11 psynch_cvwait(0x109236098, 0x30100000400, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       335     139      3 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       347 1002852     10 psynch_cvwait(0x109236098, 0x40100000500, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       377      67      9 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n^C\r\n98193/0xffb51:       403 1002432     22 psynch_cvwait(0x109236098, 0x50100000600, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       442      84      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     33973 6645911      6 wait4(0x0, 0x7FFF57F95984, 0x0)\t\t = -1 Err#4\r\n98238/0xffb52:      2319 5642539      6 psynch_cvwait(0x1092360C8, 0x100000100, 0x0)\t\t = -1 Err#260\r\n```\r\n\r\n(it basically keeps calling `gettimeofday` and `psynch_cvwait`). \r\n",kind/bug|lang/php|priority/P2,ZhouyihaiDing,"Please answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n\r\nLatest on PECL, 1.7.0\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nMac OS X, 10.12.6\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n```\r\n\u21d2  cc -v\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n``` \r\n### What did you do?\r\n\r\nScript to reproduce: \r\n\r\n```php\r\n<?php \r\n\r\n$pid = pcntl_fork(); \r\n\r\nif($pid === -1) { \r\n\techo ""error forking\\n""; \r\n} elseif ($pid) { //parent \r\n\techo ""parent: waiting for child...\\n""; \r\n\tpcntl_waitpid(0, $status);\r\n\techo ""parent: child exited successfully\\n""; \r\n} elseif ($pid === 0) { //child\r\n\techo ""child: sleeping for 1s...\\n"";\r\n\tsleep(1); \r\n\techo ""child: exiting...\\n"";\r\n\texit(0); \r\n}\r\n\r\necho ""parent: finished.\\n""; \r\n```\r\n\r\n \r\n### What did you expect to see?\r\n\r\nThis is the script output with gRPC extension disabled:\r\n```\r\n\u21d2  php grpc-fork-bug.php\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\nchild: exiting...\r\nparent: child exited successfully\r\nparent: finished.\r\n```\r\n \r\n### What did you see instead?\r\n\r\nThis is the script output with the extension enabled: \r\n```\r\n\u21d2  php grpc-fork-bug.php\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\nchild: exiting...\r\n[HANG]\r\n```\r\n\r\nHere's a syscall trace of the process while its hung: \r\n\r\n```\r\n\u21d2  sudo dtruss -af php grpc-fork-bug.php\r\nPassword:\r\n\tPID/THRD  RELATIVE  ELAPSD    CPU SYSCALL(args) \t\t = return\r\nparent: waiting for child...\r\nchild: sleeping for 1s...\r\n98193/0xff925:      4990      62      1 madvise(0x10895E000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:      5430      61      0 madvise(0x1086D7000, 0x7000, 0x5)\t\t = 0 0\r\n98193/0xff925:      5496      26     23 open(""/dev/dtracehelper\\0"", 0x2, 0x107C69000)\t\t = 3 0\r\n98193/0xff925:      8026    2766   2528 ioctl(0x3, 0x80086804, 0x7FFF57F956F8)\t\t = 0 0\r\n98193/0xff925:      8042      17     13 close(0x3)\t\t = 0 0\r\n98193/0xff925:      8175       3      0 thread_selfid(0x3, 0x80086804, 0x7FFF57F956F8)\t\t = 1046821 0\r\n98193/0xff925:      8178       4      1 bsdthread_register(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = 1073741919 0\r\n98193/0xff925:      8220       2      0 ulock_wake(0x1, 0x7FFF57F93CFC, 0x0)\t\t = -1 Err#2\r\n98193/0xff925:      8231       2      0 issetugid(0x1, 0x7FFF57F93CFC, 0x0)\t\t = 0 0\r\n98193/0xff925:      8377       4      2 mprotect(0x1087C3000, 0x88, 0x1)\t\t = 0 0\r\n98193/0xff925:      8381       2      1 mprotect(0x108CDA000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8382       2      0 mprotect(0x108CF0000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8393       2      0 mprotect(0x108CF1000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8394       2      0 mprotect(0x108D07000, 0x1000, 0x0)\t\t = 0 0\r\n98193/0xff925:      8409       2      0 mprotect(0x1089AF000, 0x1000, 0x1)\t\t = 0 0\r\n98193/0xff925:      8411       3      1 mprotect(0x1087C3000, 0x88, 0x3)\t\t = 0 0\r\n98193/0xff925:      8423       2      1 mprotect(0x1087C3000, 0x88, 0x1)\t\t = 0 0\r\n98193/0xff925:      9149      70      0 getpid(0x1087C3000, 0x88, 0x1)\t\t = 98193 0\r\n98193/0xff925:      9154       4      2 stat64(""/AppleInternal/XBS/.isChrooted\\0"", 0x7FFF57F93BB8, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:      9157       2      0 stat64(""/AppleInternal\\0"", 0x7FFF57F93C50, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:      9240       3      1 csops(0x17F91, 0x7, 0x7FFF57F936E0)\t\t = -1 Err#22\r\n98193/0xff925:      9265      28     23 sysctl([CTL_KERN, 14, 1, 98193, 0, 0] (4), 0x7FFF57F93838, 0x7FFF57F93830, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:      9280       6      0 ulock_wake(0x1, 0x7FFF57F93C60, 0x0)\t\t = -1 Err#2\r\n98193/0xff925:      9315       5      2 csops(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = -1 Err#22\r\n98193/0xff925:      9465       4      0 getuid(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:      9831       2      0 getuid(0x17F91, 0x7, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:     10374      25     20 stat64(""/System/Library/PrivateFrameworks/Heimdal.framework/Heimdal\\0"", 0x7FFF57F931E8, 0x7FFF57F92FC0)\t\t = 0 0\r\n98193/0xff925:     10526     132     16 stat64(""/System/Library/Frameworks/GSS.framework/GSS\\0"", 0x7FFF57F931E8, 0x7FFF57F92FC0)\t = 0 0\r\n98193/0xff925:     10818       4      0 sigaction(0xD, 0x7FFF57F96898, 0x7FFF57F968C0)\t\t = 0 0\r\n98193/0xff925:     10836       1      0 sigaction(0x1, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x2, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x3, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10837       1      0 sigaction(0x4, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10838       1      0 sigaction(0x5, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10838       1      0 sigaction(0x6, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10839       2      0 sigaction(0x7, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10839       1      0 sigaction(0x8, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xA, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xB, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10840       1      0 sigaction(0xC, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10841       1      0 sigaction(0xD, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10841       1      0 sigaction(0xE, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0xF, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0x10, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10842       1      0 sigaction(0x12, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10843       1      0 sigaction(0x13, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10843       1      0 sigaction(0x14, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10844       2      0 sigaction(0x15, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10844       2      0 sigaction(0x16, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10845       2      0 sigaction(0x17, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x18, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x19, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10846       1      0 sigaction(0x1A, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1B, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1C, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10847       1      0 sigaction(0x1D, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10848       1      0 sigaction(0x1E, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10848       1      0 sigaction(0x1F, 0x0, 0x7FFF57F968B8)\t\t = 0 0\r\n98193/0xff925:     10927       9      3 mmap(0x0, 0x200000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     10934       4      1 munmap(0x108D08000, 0x200000)\t\t = 0 0\r\n98193/0xff925:     10935       2      0 mmap(0x0, 0x3FF000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     10937       2      1 munmap(0x108D08000, 0xF8000)\t\t = 0 0\r\n98193/0xff925:     10938       2      0 munmap(0x109000000, 0x107000)\t\t = 0 0\r\n98193/0xff925:     10951       7      3 sysctl([CTL_HW, 7, 0, 0, 0, 0] (2), 0x7FFFDC53BE00, 0x7FFF57F96270, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     10963       9      6 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 3 0\r\n98193/0xff925:     10965       3      1 fstat64(0x3, 0x7FFF57F95D00, 0x1)\t\t = 0 0\r\n98193/0xff925:     10968       4      2 fcntl_nocancel(0x3, 0x32, 0x7FFF57F95F00)\t\t = 0 0\r\n98193/0xff925:     10973       6      4 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     10978       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95C70, 0x7FFF57F95F00)\t\t = 0 0\r\n98193/0xff925:     11142      17     15 open_nocancel(""/usr/share/locale/en_US.UTF-8/LC_CTYPE\\0"", 0x0, 0x5)\t\t = 3 0\r\n98193/0xff925:     11143       2      0 fcntl_nocancel(0x3, 0x3, 0x0)\t\t = 0 0\r\n98193/0xff925:     11145       2      0 getrlimit(0x1008, 0x7FFF57F95D38, 0x0)\t\t = 0 0\r\n98193/0xff925:     11152       4      1 fstat64(0x3, 0x7FFF57F95DF8, 0x0)\t\t = 0 0\r\n98193/0xff925:     11155       2      0 fstat64(0x3, 0x7FFF57F95BE8, 0x0)\t\t = 0 0\r\n98193/0xff925:     11157       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     11158       1      0 lseek(0x3, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     11167      11      8 read_nocancel(0x3, ""RuneMagAUTF-8\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11177       3      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11194       3      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11209       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11225       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11240       2      1 read_nocancel(0x3, ""\\0"", 0x1000)\t\t = 4096 0\r\n98193/0xff925:     11285      37     36 read_nocancel(0x3, ""@\\004\\031\\0"", 0xDE80)\t\t = 56960 0\r\n98193/0xff925:     11299       5      3 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11335      27     25 access(""/etc/localtime\\0"", 0x4, 0xDE80)\t\t = 0 0\r\n98193/0xff925:     11353      19     17 open_nocancel(""/etc/localtime\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     11355       3      1 fstat64(0x3, 0x7FFF57F933E0, 0x0)\t\t = 0 0\r\n98193/0xff925:     11357       3      1 read_nocancel(0x3, ""TZif\\0"", 0x2A64)\t\t = 791 0\r\n98193/0xff925:     11358       2      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11490       8      5 shm_open(0x7FFFD3847DE7, 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     11495       7      4 mmap(0x0, 0x1000, 0x1, 0x1, 0x3, 0x0)\t\t = 0x1089B0000 0\r\n98193/0xff925:     11497       3      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     11569       7      4 lstat64(""/Users/zack.angelo/.rvm/gems/ruby-2.2.4/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11583       9      7 lstat64(""/Users/zack.angelo/.rvm/gems/ruby-2.2.4@global/bin/php\\0"", 0x7FFF57F95990, 0x1)\t = -1 Err#2\r\n98193/0xff925:     11585       3      1 lstat64(""/Users/zack.angelo/.rvm/rubies/ruby-2.2.4/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11589       5      3 lstat64(""/Users/zack.angelo/.nvm/versions/node/v4.4.7/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11592       3      1 lstat64(""/Users/zack.angelo/google-cloud-sdk/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11594       3      1 lstat64(""/usr/local/sbin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = -1 Err#2\r\n98193/0xff925:     11599       5      4 lstat64(""/usr/local/bin/php\\0"", 0x7FFF57F95990, 0x1)\t\t = 0 0\r\n98193/0xff925:     11609       3      1 readlink(""/usr/local/bin/php\\0"", 0x7FFF57F95AE0, 0x400)\t\t = 33 0\r\n98193/0xff925:     11618       6      5 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22/bin/php\\0"", 0x7FFF57F95860, 0x400)\t\t = 0 0\r\n98193/0xff925:     11622       3      1 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22/bin\\0"", 0x7FFF57F95730, 0x400)\t\t = 0 0\r\n98193/0xff925:     11626       3      1 lstat64(""/usr/local/bin/../Cellar/php71/7.1.11_22\\0"", 0x7FFF57F95600, 0x400)\t\t = 0 0\r\n98193/0xff925:     11628       2      1 lstat64(""/usr/local/bin/../Cellar/php71\\0"", 0x7FFF57F954D0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11631       2      1 lstat64(""/usr/local/bin/../Cellar\\0"", 0x7FFF57F953A0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11633       2      1 lstat64(""/usr/local/bin\\0"", 0x7FFF57F95140, 0x400)\t\t = 0 0\r\n98193/0xff925:     11636       2      1 lstat64(""/usr/local\\0"", 0x7FFF57F95010, 0x400)\t\t = 0 0\r\n98193/0xff925:     11638       2      1 lstat64(""/usr\\0"", 0x7FFF57F94EE0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11660      16     14 access(""/usr/local/Cellar/php71/7.1.11_22/bin/php\\0"", 0x1, 0x400)\t\t = 0 0\r\n98193/0xff925:     11667       3      1 stat64(""/usr/local/Cellar/php71/7.1.11_22/bin/php\\0"", 0x7FFF57F963F0, 0x400)\t\t = 0 0\r\n98193/0xff925:     11702       5      3 open_nocancel(""/usr/local/Cellar/php71/7.1.11_22/bin/php-cli.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11706       5      3 open_nocancel(""/usr/local/etc/php/7.1/php-cli.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11709      12      2 open_nocancel(""/usr/local/Cellar/php71/7.1.11_22/bin/php.ini\\0"", 0x0, 0x1B6)\t\t = -1 Err#2\r\n98193/0xff925:     11714       6      4 open_nocancel(""/usr/local/etc/php/7.1/php.ini\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     11729       3      1 ioctl(0x3, 0x4004667A, 0x7FFF57F95B3C)\t\t = -1 Err#25\r\n98193/0xff925:     11729       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F95B40)\t\t = -1 Err#25\r\n98193/0xff925:     11737       3      1 fstat64(0x3, 0x7FFF57F95BB0, 0x7FFF57F95B40)\t\t = 0 0\r\n98193/0xff925:     11743       8      5 mmap(0x0, 0x1162B, 0x1, 0x2, 0x3, 0x0)\t\t = 0x108D08000 0\r\n98193/0xff925:     11748       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     12086      10      6 munmap(0x108D08000, 0x1162B)\t\t = 0 0\r\n98193/0xff925:     12097       8      5 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     12114      12      9 open_nocancel(""/usr/local/etc/php/7.1/conf.d\\0"", 0x1100004, 0x0)\t\t = 3 0\r\n98193/0xff925:     12117       5      2 fstatfs64(0x3, 0x7FFF57F953D8, 0x0)\t\t = 0 0\r\n98193/0xff925:     12140      23     20 getdirentries64(0x3, 0x7F814F002C00, 0x1000)\t\t = 136 0\r\n98193/0xff925:     12145       6      3 getdirentries64(0x3, 0x7F814F002C00, 0x1000)\t\t = 0 0\r\n98193/0xff925:     12148       3      1 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     12162       6      4 stat64(""/usr/local/etc/php/7.1/conf.d/ext-intl.ini\\0"", 0x7FFF57F95E70, 0x1000)\t\t = 0 0\r\n98193/0xff925:     12167       5      3 open_nocancel(""/usr/local/etc/php/7.1/conf.d/ext-intl.ini\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     12169       3      0 ioctl(0x3, 0x4004667A, 0x7FFF57F95B3C)\t\t = -1 Err#25\r\n98193/0xff925:     12169       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F95B40)\t\t = -1 Err#25\r\n98193/0xff925:     12171       2      0 fstat64(0x3, 0x7FFF57F95BB0, 0x7FFF57F95B40)\t\t = 0 0\r\n98193/0xff925:     12176       8      4 mmap(0x0, 0x14F, 0x1, 0x2, 0x3, 0x0)\t\t = 0x1089B1000 0\r\n98193/0xff925:     12177       2      0 lseek(0x3, 0x0, 0x1)\t\t = 0 0\r\n98193/0xff925:     12226       7      3 munmap(0x1089B1000, 0x14F)\t\t = 0 0\r\n98193/0xff925:     12231       7      4 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     13270      16     12 stat64(""/usr/local/Cellar/php71/7.1.11_22/lib/php/extensions/no-debug-non-zts-20160303/grpc.so\\0"", 0x7FFF57F95778, 0x1)\t\t = 0 0\r\n98193/0xff925:     13281       7      5 open(""/usr/local/Cellar/php71/7.1.11_22/lib/php/extensions/no-debug-non-zts-20160303/grpc.so\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     13330     219     47 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     13386      47     42 mmap(0x109000000, 0x219000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x109000000 0\r\n98193/0xff925:     13391       7      4 mmap(0x109219000, 0x1A000, 0x3, 0x12, 0x3, 0x219000)\t\t = 0x109219000 0\r\n98193/0xff925:     13394       4      2 mmap(0x109239000, 0x51BDC, 0x1, 0x12, 0x3, 0x233000)\t\t = 0x109239000 0\r\n98193/0xff925:     13404       4      1 madvise(0x109239000, 0x2000, 0x2)\t\t = 0 0\r\n98193/0xff925:     13408       4      2 close(0x3)\t\t = 0 0\r\n98193/0xff925:     13957       4      0 madvise(0x109239000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:     14224      15     12 stat64(""/usr/local/opt/php71-intl/intl.so\\0"", 0x7FFF57F95778, 0x5)\t\t = 0 0\r\n98193/0xff925:     14233       8      5 open(""/usr/local/opt/php71-intl/intl.so\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14239       9      5 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14269      26     23 mmap(0x108D57000, 0x3F000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x108D57000 0\r\n98193/0xff925:     14273       5      2 mmap(0x108D96000, 0xA000, 0x3, 0x12, 0x3, 0x3F000)\t\t = 0x108D96000 0\r\n98193/0xff925:     14276       5      2 mmap(0x108DA1000, 0x1D520, 0x1, 0x12, 0x3, 0x49000)\t\t = 0x108DA1000 0\r\n98193/0xff925:     14283       2      0 madvise(0x108DA1000, 0x4000, 0x2)\t\t = 0 0\r\n98193/0xff925:     14286       4      2 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14300      12     10 stat64(""/usr/local/opt/icu4c/lib/libicui18n.59.dylib\\0"", 0x7FFF57F95538, 0x2)\t\t = 0 0\r\n98193/0xff925:     14307       6      4 open(""/usr/local/opt/icu4c/lib/libicui18n.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14310       5      3 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14332      20     17 mmap(0x10928B000, 0x15E000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x10928B000 0\r\n98193/0xff925:     14335       4      2 mmap(0x1093E9000, 0xF000, 0x3, 0x12, 0x3, 0x15E000)\t\t = 0x1093E9000 0\r\n98193/0xff925:     14337       4      1 mmap(0x1093F8000, 0xCE03C, 0x1, 0x12, 0x3, 0x16D000)\t\t = 0x1093F8000 0\r\n98193/0xff925:     14342       2      0 madvise(0x1093F8000, 0x2000, 0x2)\t\t = 0 0\r\n98193/0xff925:     14347       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14355       7      6 stat64(""/usr/local/opt/icu4c/lib/libicuuc.59.dylib\\0"", 0x7FFF57F95538, 0x2)\t\t = 0 0\r\n98193/0xff925:     14361       5      3 open(""/usr/local/opt/icu4c/lib/libicuuc.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14364       5      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14384      18     16 mmap(0x1094C7000, 0x103000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x1094C7000 0\r\n98193/0xff925:     14387       4      2 mmap(0x1095CA000, 0x12000, 0x3, 0x12, 0x3, 0x103000)\t\t = 0x1095CA000 0\r\n98193/0xff925:     14389       4      2 mmap(0x1095DD000, 0x58EA4, 0x1, 0x12, 0x3, 0x115000)\t\t = 0x1095DD000 0\r\n98193/0xff925:     14398       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14404       5      4 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.1.dylib\\0"", 0x7FFF57F95538, 0x1)\t\t = 0 0\r\n98193/0xff925:     14409       4      3 open(""/usr/local/opt/icu4c/lib/libicudata.59.1.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14412       5      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14433      19     17 mmap(0x109636000, 0x1914000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x109636000 0\r\n98193/0xff925:     14435       4      1 mmap(0x10AF4A000, 0x60, 0x1, 0x12, 0x3, 0x1914000)\t\t = 0x10AF4A000 0\r\n98193/0xff925:     14445       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14452       7      5 stat64(""/usr/local/opt/icu4c/lib/libicuio.59.dylib\\0"", 0x7FFF57F95538, 0x1)\t\t = 0 0\r\n98193/0xff925:     14458       5      3 open(""/usr/local/opt/icu4c/lib/libicuio.59.dylib\\0"", 0x0, 0x0)\t\t = 3 0\r\n98193/0xff925:     14460       4      2 pread(0x3, ""\\317\\372\\355\\376\\a\\0"", 0x1000, 0x0)\t\t = 4096 0\r\n98193/0xff925:     14483      21     19 mmap(0x108DBF000, 0x9000, 0x5, 0x12, 0x3, 0x0)\t\t = 0x108DBF000 0\r\n98193/0xff925:     14486       4      2 mmap(0x108DC8000, 0x2000, 0x3, 0x12, 0x3, 0x9000)\t\t = 0x108DC8000 0\r\n98193/0xff925:     14488       4      1 mmap(0x108DCA000, 0x3DA0, 0x1, 0x12, 0x3, 0xB000)\t\t = 0x108DCA000 0\r\n98193/0xff925:     14497       3      1 close(0x3)\t\t = 0 0\r\n98193/0xff925:     14509       3      1 stat64(""/\\0"", 0x7FFF57F93780, 0x1)\t\t = 0 0\r\n98193/0xff925:     14518       6      4 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14521       4      2 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14524       3      2 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14527       4      2 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14529       4      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94490, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14532       4      2 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14535       4      2 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14538       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14541       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14544       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.dylib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14546       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.dylib\\0"", 0x7FFF57F94490, 0x3FF)\t = 21 0\r\n98193/0xff925:     14549       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicui18n.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F95090)\t\t = 0 0\r\n98193/0xff925:     14557       6      4 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F953C8, 0x7FFF57F95090)\t = 0 0\r\n98193/0xff925:     14563       3      1 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14565       3      2 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14567       3      1 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14571       4      2 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14572       3      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94380, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14575       3      1 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14577       4      1 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14579       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14582       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14585       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.dylib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14587       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.dylib\\0"", 0x7FFF57F94380, 0x3FF)\t = 19 0\r\n98193/0xff925:     14590       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuuc.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F94F80)\t\t = 0 0\r\n98193/0xff925:     14596       4      3 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F952B8, 0x7FFF57F94F80)\t = 0 0\r\n98193/0xff925:     14603       3      1 getattrlist(""/usr\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14605       3      1 getattrlist(""/usr/local\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14608       3      1 getattrlist(""/usr/local/opt\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14610       3      1 getattrlist(""/usr/local/opt/icu4c\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14611       2      1 readlink(""/usr/local/opt/icu4c\\0"", 0x7FFF57F94470, 0x3FF)\t\t = 22 0\r\n98193/0xff925:     14614       3      1 getattrlist(""/usr/local/Cellar\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14616       3      1 getattrlist(""/usr/local/Cellar/icu4c\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14618       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14621       3      1 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14624       4      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.dylib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14625       2      1 readlink(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.dylib\\0"", 0x7FFF57F94470, 0x3FF)\t = 19 0\r\n98193/0xff925:     14628       3      2 getattrlist(""/usr/local/Cellar/icu4c/59.1_1/lib/libicuio.59.1.dylib\\0"", 0x114ED1364, 0x7FFF57F95070)\t\t = 0 0\r\n98193/0xff925:     14634       4      2 stat64(""/usr/local/opt/icu4c/lib/libicudata.59.dylib\\0"", 0x7FFF57F953A8, 0x7FFF57F95070)\t = 0 0\r\n98193/0xff925:     15262       5      0 madvise(0x1093F8000, 0x2000, 0x5)\t\t = 0 0\r\n98193/0xff925:     22471       9      2 madvise(0x108DA1000, 0x4000, 0x5)\t\t = 0 0\r\n98193/0xff925:     23425      27     20 stat64(""/usr/lib/libz.dylib\\0"", 0x7FFF57F95658, 0x5)\t\t = 0 0\r\n98193/0xff925:     25263      39     33 socket(0x1E, 0x2, 0x0)\t\t = 3 0\r\n98193/0xff925:     25275      12     10 close(0x3)\t\t = 0 0\r\n98193/0xff925:     26714      13      5 sysctl([CTL_KERN, 8, 0, 0, 0, 0] (2), 0x7FFF57F961E4, 0x7FFF57F961E8, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     29304      12      8 open_nocancel(""grpc-fork-bug.php\\0"", 0x0, 0x1B6)\t\t = 3 0\r\n98193/0xff925:     29310       3      1 fstat64(0x3, 0x7FFF57F960D8, 0x1B6)\t\t = 0 0\r\n98193/0xff925:     29360     249     49 read_nocancel(0x3, ""<?php \\n\\n$pid = pcntl_fork(); \\n\\nif($pid === -1) { \\n\\techo \\""error forking\\\\n\\""; \\n} elseif ($pid) { //parent \\n\\techo \\""parent: waiting for child...\\\\n\\""; \\n\\tpcntl_waitpid(0, $status);\\n\\techo \\""parent: child exited successfully\\\\n\\""; \\n} elseif ($pid === 0) { //child\\n\\techo"", 0x1000)\t\t = 371 0\r\n98193/0xff925:     29370       3      0 lseek(0x3, 0x0, 0x1)\t\t = 371 0\r\n98193/0xff925:     29377       8      5 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 4 0\r\n98193/0xff925:     29379       3      1 fstat64(0x4, 0x7FFF57F95C40, 0x1)\t\t = 0 0\r\n98193/0xff925:     29381       4      2 fcntl_nocancel(0x4, 0x32, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29384       4      1 close_nocancel(0x4)\t\t = 0 0\r\n98193/0xff925:     29387       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95BB0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29397       4      2 lstat64(""/Users/zack.angelo/git/grpc-fork-bug/grpc-fork-bug.php\\0"", 0x7FFF57F958B0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29399       2      1 lstat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95780, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29401       3      1 lstat64(""/Users/zack.angelo/git\\0"", 0x7FFF57F95650, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29403       2      1 lstat64(""/Users/zack.angelo\\0"", 0x7FFF57F95520, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29404       2      1 lstat64(""/Users\\0"", 0x7FFF57F953F0, 0x7FFF57F95E50)\t\t = 0 0\r\n98193/0xff925:     29425       4      1 sigaction(0x1B, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29426       1      0 sigaction(0x1B, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29426       1      0 sigaction(0x1, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29427       1      0 sigaction(0x1, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29427       1      0 sigaction(0x2, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29428       1      0 sigaction(0x2, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29428       1      0 sigaction(0x3, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29429       1      0 sigaction(0x3, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29430       1      0 sigaction(0xF, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29431       1      0 sigaction(0xF, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29431       1      0 sigaction(0x1E, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1E, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1F, 0x0, 0x7FFF57F96138)\t\t = 0 0\r\n98193/0xff925:     29432       1      0 sigaction(0x1F, 0x7FFF57F96108, 0x0)\t\t = 0 0\r\n98193/0xff925:     29433       1      0 sigaction(0x1B, 0x7FFF57F96098, 0x0)\t\t = 0 0\r\n98193/0xff925:     29441       2      0 sigprocmask(0x2, 0x7FFF57F960CC, 0x0)\t\t = 0x0 0\r\n98193/0xff925:     29521      10      4 mmap(0x0, 0x10000, 0x3, 0x1002, 0xFFFFFFFF, 0x0)\t\t = 0x10AF6F000 0\r\n98193/0xff925:     30217      10      5 pipe(0x0, 0x10000, 0x3)\t\t = 4 0\r\n98193/0xff925:     30229       3      0 fcntl(0x4, 0x3, 0x0)\t\t = 0 0\r\n98193/0xff925:     30230       2      0 fcntl(0x4, 0x4, 0x4)\t\t = 0 0\r\n98193/0xff925:     30231       1      0 fcntl(0x5, 0x3, 0x0)\t\t = 1 0\r\n98193/0xff925:     30231       1      0 fcntl(0x5, 0x4, 0x5)\t\t = 0 0\r\n98193/0xff925:     30243       4      2 close(0x4)\t\t = 0 0\r\n98193/0xff925:     30244       2      1 close(0x5)\t\t = 0 0\r\n98193/0xff925:     30747      25     20 bsdthread_create(0x109010800, 0x7F814DC719C0, 0x80000)\t\t = 10268672 0\r\n98193/0xffb51:         6      69      0 thread_selfid(0x0, 0x0, 0x0)\t\t = 1047377 0\r\n98193/0xffb51:       102      13      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     30936       5      2 fstat64(0x0, 0x7FFF57F96060, 0x80000)\t\t = 0 0\r\n98193/0xff925:     30952       2      0 fstat64(0x0, 0x108E72038, 0x80000)\t\t = 0 0\r\n98193/0xff925:     30954       2      0 lseek(0x0, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30956       2      0 fstat64(0x1, 0x7FFF57F96060, 0x1)\t\t = 0 0\r\n98193/0xff925:     30957       1      0 fstat64(0x1, 0x108E72118, 0x1)\t\t = 0 0\r\n98193/0xff925:     30958       1      0 lseek(0x1, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30959       2      0 fstat64(0x2, 0x7FFF57F96060, 0x1)\t\t = 0 0\r\n98193/0xff925:     30960       1      0 fstat64(0x2, 0x108E721F8, 0x1)\t\t = 0 0\r\n98193/0xff925:     30961       1      0 lseek(0x2, 0x0, 0x1)\t\t = 3311256 0\r\n98193/0xff925:     30976      10      8 open_nocancel("".\\0"", 0x0, 0x1)\t\t = 4 0\r\n98193/0xff925:     30978       2      1 fstat64(0x4, 0x7FFF57F95600, 0x1)\t\t = 0 0\r\n98193/0xff925:     30981       5      2 fcntl_nocancel(0x4, 0x32, 0x7FFF57F95820)\t\t = 0 0\r\n98193/0xff925:     30984       4      2 close_nocancel(0x4)\t\t = 0 0\r\n98193/0xff925:     30988       5      3 stat64(""/Users/zack.angelo/git/grpc-fork-bug\\0"", 0x7FFF57F95570, 0x7FFF57F95820)\t\t = 0 0\r\n98193/0xff925:     31000       2      0 ioctl(0x3, 0x4004667A, 0x7FFF57F957BC)\t\t = -1 Err#25\r\n98193/0xff925:     31001       1      0 ioctl(0x3, 0x40487413, 0x7FFF57F957C0)\t\t = -1 Err#25\r\n98193/0xff925:     31003       2      1 fstat64(0x3, 0x7FFF57F95830, 0x7FFF57F957C0)\t\t = 0 0\r\n98193/0xff925:     31011      10      7 mmap(0x0, 0x193, 0x1, 0x2, 0x3, 0x0)\t\t = 0x1089B2000 0\r\n98193/0xff925:     31125       8      5 munmap(0x1089B2000, 0x193)\t\t = 0 0\r\n98193/0xff925:     31131       7      5 close_nocancel(0x3)\t\t = 0 0\r\n98193/0xff925:     33667    2512   2495 fork()\t\t = 98238 0\r\n98238/0xffb52:        80:        0:       0 fork()\t\t = 0 0\r\n98238/0xffb52:        80       4      0 thread_selfid(0x0, 0x0, 0x0)\t\t = 1047378 0\r\n98238/0xffb52:        85       6      3 bsdthread_register(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = -1 Err#22\r\n98193/0xff925:     33858      15     10 write(0x1, ""parent: waiting for child...\\n\\0"", 0x1D)\t\t = 29 0\r\n98238/0xffb52:       192       3      0 getpid(0x7FFFD3854080, 0x7FFFD3854070, 0x2000)\t\t = 98238 0\r\n98238/0xffb52:       197       4      1 __mac_syscall(0x7FFFD385FC46, 0x4, 0x7FFF57F95578)\t\t = -1 Err#45\r\n98238/0xffb52:       199       3      1 csops(0x17FBE, 0xB, 0x7FFF57F955B8)\t\t = -1 Err#22\r\n98238/0xffb52:       685      14      9 write(0x1, ""child: sleeping for 1s...\\n\\b\\0"", 0x1A)\t\t = 26 0\r\nchild: exiting...\r\n98238/0xffb52:       760 1000905     10 __semwait_signal(0x703, 0x0, 0x1)\t\t = -1 Err#60\r\n98193/0xffb51:       112 1004731      9 psynch_cvwait(0x109236098, 0x100000100, 0x0)\t\t = -1 Err#316\r\n98238/0xffb52:       793     111     16 write(0x1, ""child: exiting...\\n\\0"", 0x12)\t\t = 18 0\r\n98193/0xffb51:       196      75      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98238/0xffb52:      1457      71      2 close_nocancel(0x2)\t\t = 0 0\r\n98238/0xffb52:      1480       3      0 close_nocancel(0x1)\t\t = 0 0\r\n98238/0xffb52:      1502       3      0 close_nocancel(0x0)\t\t = 0 0\r\n98238/0xffb52:      2089      16     13 munmap(0x108D96000, 0xB000)\t\t = 0 0\r\n98238/0xffb52:      2093       5      2 munmap(0x108DA1000, 0x1E000)\t\t = 0 0\r\n98238/0xffb52:      2097       6      3 munmap(0x108D57000, 0x3F000)\t\t = 0 0\r\n98238/0xffb52:      2119       4      2 munmap(0x1093E9000, 0xF000)\t\t = 0 0\r\n98238/0xffb52:      2121       3      1 munmap(0x1093F8000, 0xCF000)\t\t = 0 0\r\n98238/0xffb52:      2125       6      4 munmap(0x10928B000, 0x15E000)\t\t = 0 0\r\n98238/0xffb52:      2135       3      1 munmap(0x1095CA000, 0x13000)\t\t = 0 0\r\n98238/0xffb52:      2137       2      0 munmap(0x1095DD000, 0x59000)\t\t = 0 0\r\n98238/0xffb52:      2140       5      3 munmap(0x1094C7000, 0x103000)\t\t = 0 0\r\n98238/0xffb52:      2149       3      1 munmap(0x10AF4A000, 0x1000)\t\t = 0 0\r\n98238/0xffb52:      2154       7      4 munmap(0x109636000, 0x1914000)\t\t = 0 0\r\n98238/0xffb52:      2168       3      1 munmap(0x108DC8000, 0x2000)\t\t = 0 0\r\n98238/0xffb52:      2169       3      0 munmap(0x108DCA000, 0x4000)\t\t = 0 0\r\n98238/0xffb52:      2172       4      2 munmap(0x108DBF000, 0x9000)\t\t = 0 0\r\n98238/0xffb52:      2302       6      2 psynch_cvbroad(0x109236098, 0x100, 0x100)\t\t = 2 0\r\n98193/0xffb51:       207 1005369     10 psynch_cvwait(0x109236098, 0x10100000200, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       238      69      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       247 1001302      8 psynch_cvwait(0x109236098, 0x20100000300, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       286      72      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       298 1004393     11 psynch_cvwait(0x109236098, 0x30100000400, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       335     139      3 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xffb51:       347 1002852     10 psynch_cvwait(0x109236098, 0x40100000500, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       377      67      9 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n^C\r\n98193/0xffb51:       403 1002432     22 psynch_cvwait(0x109236098, 0x50100000600, 0x0)\t\t = -1 Err#316\r\n98193/0xffb51:       442      84      0 gettimeofday(0x7000009CAD58, 0x0, 0x0)\t\t = 0 0\r\n98193/0xff925:     33973 6645911      6 wait4(0x0, 0x7FFF57F95984, 0x0)\t\t = -1 Err#4\r\n98238/0xffb52:      2319 5642539      6 psynch_cvwait(0x1092360C8, 0x100000100, 0x0)\t\t = -1 Err#260\r\n```\r\n\r\n(it basically keeps calling `gettimeofday` and `psynch_cvwait`). \r\n","php\r\n<?php \r\n\r\n$pid = pcntl_fork(); \r\n\r\nif($pid === -1) { \r\n\techo ""error forking\\n""; \r\n} elseif ($pid) { //parent \r\n\techo ""parent: waiting for child...\\n""; \r\n\tpcntl_waitpid(0, $status);\r\n\techo ""parent: child exited successfully\\n""; \r\n} elseif ($pid === 0) { //child\r\n\techo ""child: sleeping for 1s...\\n"";\r\n\tsleep(1); \r\n\techo ""child: exiting...\\n"";\r\n\texit(0); \r\n}\r\n\r\necho ""parent: finished.\\n""; \r\n"
13318,"Escape argument/attribute name collides Python reserved keywordVersion of gRPC and language: protoc and Python plugin\r\n```\r\n$ protoc --version\r\nlibprotoc 3.4.0\r\n$ pip show grpcio\r\nName: grpcio\r\nVersion: 1.7.0\r\n...(snip)...\r\n $ pip show grpcio-tools\r\nName: grpcio-tools\r\nVersion: 1.7.0\r\n...(snip)...\r\n```\r\n\r\nOS: Linux Ubuntu 16.04 LTS\r\n \r\nRuntime and compiler:\r\n \r\n\r\nI'm trying to send a message from gRPC client written in Python and the message is like:\r\n```protobuf\r\nmessage TestMessage {\r\n  bool global = 1;\r\n  uint32 as = 2;\r\n}\r\n```\r\n\r\nThe gRPC server is written in Go, `global` and `as` does not collides the reserved keywords, but on Python, these variable name clash with reserved keywords.\r\nThen, I couldn't create the above message with keyword arguments and couldn't access these attributes:\r\n\r\n\r\n\r\n### Request\r\n\r\n[pep8](https://www.python.org/dev/peps/pep-0008/#function-and-method-arguments) says ""If a function argument's name clashes with a reserved keyword, it is generally better to append a single trailing underscore"", so I want to pass arguments appending ""_"" like:\r\n\r\nand also access attribute like:\r\n\r\n\r\nWhat do think about it? Does anyone know better workaround?",lang/Python|area/protoc plugins|disposition/requires reporter action,mehrdada,"Version of gRPC and language: protoc and Python plugin\r\n```\r\n$ protoc --version\r\nlibprotoc 3.4.0\r\n$ pip show grpcio\r\nName: grpcio\r\nVersion: 1.7.0\r\n...(snip)...\r\n $ pip show grpcio-tools\r\nName: grpcio-tools\r\nVersion: 1.7.0\r\n...(snip)...\r\n```\r\n\r\nOS: Linux Ubuntu 16.04 LTS\r\n \r\nRuntime and compiler:\r\n ```bash\r\n$ python --version\r\nPython 3.5.2\r\n$ gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n ```\r\n\r\nI'm trying to send a message from gRPC client written in Python and the message is like:\r\n```protobuf\r\nmessage TestMessage {\r\n  bool global = 1;\r\n  uint32 as = 2;\r\n}\r\n```\r\n\r\nThe gRPC server is written in Go, `global` and `as` does not collides the reserved keywords, but on Python, these variable name clash with reserved keywords.\r\nThen, I couldn't create the above message with keyword arguments and couldn't access these attributes:\r\n\r\n```python\r\n# couldn't create message with keyword arguments\r\nmsg = xxx_pb2.TestMessage(\r\n    global=True,  # Here raises syntax error\r\n    as=100,\r\n)\r\n# required to pass arguments like\r\nkwargs = {\r\n    'global': True,\r\n    'as': 100,\r\n}\r\nmsg= xxx_pb2.TestMessage(**kwargs)\r\n\r\n# Also couldn't access these attributes\r\nprint(msg.global)  # Here raises syntax error\r\n# required to use getattr() like\r\nprint(getattr(msg, 'global'))\r\n```\r\n\r\n### Request\r\n\r\n[pep8](https://www.python.org/dev/peps/pep-0008/#function-and-method-arguments) says ""If a function argument's name clashes with a reserved keyword, it is generally better to append a single trailing underscore"", so I want to pass arguments appending ""_"" like:\r\n```python\r\nmsg = xxx_pb2.TestMessage(\r\n    global_=True,\r\n    as_=100,\r\n)\r\n```\r\nand also access attribute like:\r\n```python\r\nprint(msg.global_)\r\n```\r\n\r\nWhat do think about it? Does anyone know better workaround?","bash\r\n$ python --version\r\nPython 3.5.2\r\n$ gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n "
13305,"""grpc_message"":""Did not read entire message"" \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n \r\n\r\n \r\n### What version of gRPC and what language are you using?\r\n I have tried with 1.7.2 and 1.6.1 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nUbuntu 17.10\r\n \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Have tried with Python 2.7.14 and Python 3.6.3\r\n \r\n### What did you do?\r\n\r\nTried to call a simple grpc method. I can make the equivilant call in Java, and it works perfectly.\r\n\r\n```proto\r\nservice NewChartBasesService {\r\n\trpc ChartRequest (ChartRequestRequestMessage) returns (ChartRequestResponseMessage){}\r\n}\r\n```\r\n\r\nand\r\n\r\n\r\n \r\n### What did you expect to see?\r\n I expected to get a result back from the server.\r\n \r\n### What did you see instead?\r\n \r\nHere is the full verbose log:\r\n\r\n```\r\n           \r\nD1108 22:22:12.070899086   13595 ev_posix.c:111]             Using polling engine: poll\r\nD1108 22:22:12.071947506   13595 dns_resolver.c:301]         Using native dns resolver\r\nD1108 22:22:12.072055639   13595 timer_manager.c:82]         Spawn timer thread\r\nI1108 22:22:12.072178672   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.072223668   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf990f5c18)\r\nD1108 22:22:12.081615538   13598 timer_generic.c:538]        TIMER CHECK BEGIN: now=6776.681088675 [10] next=9223372036854775807.000000000 [9223372036854775807] tls_min=0 glob_min=0\r\nD1108 22:22:12.081672094   13598 timer_generic.c:452]          .. shard[0]->min_deadline = 1\r\nD1108 22:22:12.081686442   13598 timer_generic.c:393]          .. shard[0]: heap_empty=true\r\nD1108 22:22:12.081697659   13598 timer_generic.c:368]          .. shard[0]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081710510   13598 timer_generic.c:470]          .. result --> 1, shard[0]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081722983   13598 timer_generic.c:393]          .. shard[1]: heap_empty=true\r\nD1108 22:22:12.081733894   13598 timer_generic.c:368]          .. shard[1]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081745142   13598 timer_generic.c:470]          .. result --> 1, shard[1]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081756354   13598 timer_generic.c:393]          .. shard[2]: heap_empty=true\r\nD1108 22:22:12.081767494   13598 timer_generic.c:368]          .. shard[2]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081779276   13598 timer_generic.c:470]          .. result --> 1, shard[2]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081790168   13598 timer_generic.c:393]          .. shard[3]: heap_empty=true\r\nD1108 22:22:12.081799723   13598 timer_generic.c:368]          .. shard[3]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081810519   13598 timer_generic.c:470]          .. result --> 1, shard[3]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082107160   13598 timer_generic.c:393]          .. shard[4]: heap_empty=true\r\nD1108 22:22:12.082124113   13598 timer_generic.c:368]          .. shard[4]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082136519   13598 timer_generic.c:470]          .. result --> 1, shard[4]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082147685   13598 timer_generic.c:393]          .. shard[5]: heap_empty=true\r\nD1108 22:22:12.082157693   13598 timer_generic.c:368]          .. shard[5]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082168372   13598 timer_generic.c:470]          .. result --> 1, shard[5]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082178563   13598 timer_generic.c:393]          .. shard[6]: heap_empty=true\r\nD1108 22:22:12.082188005   13598 timer_generic.c:368]          .. shard[6]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082198414   13598 timer_generic.c:470]          .. result --> 1, shard[6]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082208346   13598 timer_generic.c:393]          .. shard[7]: heap_empty=true\r\nD1108 22:22:12.082218805   13598 timer_generic.c:368]          .. shard[7]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082230245   13598 timer_generic.c:470]          .. result --> 1, shard[7]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082241254   13598 timer_generic.c:393]          .. shard[8]: heap_empty=true\r\nD1108 22:22:12.082251890   13598 timer_generic.c:368]          .. shard[8]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082262939   13598 timer_generic.c:470]          .. result --> 1, shard[8]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082274617   13598 timer_generic.c:393]          .. shard[9]: heap_empty=true\r\nD1108 22:22:12.082285668   13598 timer_generic.c:368]          .. shard[9]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082296661   13598 timer_generic.c:470]          .. result --> 1, shard[9]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082308105   13598 timer_generic.c:393]          .. shard[10]: heap_empty=true\r\nD1108 22:22:12.082319220   13598 timer_generic.c:368]          .. shard[10]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082330690   13598 timer_generic.c:470]          .. result --> 1, shard[10]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082342219   13598 timer_generic.c:393]          .. shard[11]: heap_empty=true\r\nD1108 22:22:12.082352635   13598 timer_generic.c:368]          .. shard[11]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082363971   13598 timer_generic.c:470]          .. result --> 1, shard[11]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082375114   13598 timer_generic.c:393]          .. shard[12]: heap_empty=true\r\nD1108 22:22:12.082384756   13598 timer_generic.c:368]          .. shard[12]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082395573   13598 timer_generic.c:470]          .. result --> 1, shard[12]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082405802   13598 timer_generic.c:393]          .. shard[13]: heap_empty=true\r\nD1108 22:22:12.082416210   13598 timer_generic.c:368]          .. shard[13]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082426949   13598 timer_generic.c:470]          .. result --> 1, shard[13]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082443146   13598 timer_generic.c:393]          .. shard[14]: heap_empty=true\r\nD1108 22:22:12.082457979   13598 timer_generic.c:368]          .. shard[14]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082472898   13598 timer_generic.c:470]          .. result --> 1, shard[14]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082487983   13598 timer_generic.c:393]          .. shard[15]: heap_empty=true\r\nD1108 22:22:12.082502426   13598 timer_generic.c:368]          .. shard[15]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082538111   13598 timer_generic.c:470]          .. result --> 1, shard[15]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082554404   13598 timer_generic.c:393]          .. shard[16]: heap_empty=true\r\nD1108 22:22:12.082569011   13598 timer_generic.c:368]          .. shard[16]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082584395   13598 timer_generic.c:470]          .. result --> 1, shard[16]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082600018   13598 timer_generic.c:393]          .. shard[17]: heap_empty=true\r\nD1108 22:22:12.082614646   13598 timer_generic.c:368]          .. shard[17]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082629306   13598 timer_generic.c:470]          .. result --> 1, shard[17]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082644052   13598 timer_generic.c:393]          .. shard[18]: heap_empty=true\r\nD1108 22:22:12.082658465   13598 timer_generic.c:368]          .. shard[18]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082673328   13598 timer_generic.c:470]          .. result --> 1, shard[18]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082687068   13598 timer_generic.c:393]          .. shard[19]: heap_empty=true\r\nD1108 22:22:12.082700742   13598 timer_generic.c:368]          .. shard[19]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082714987   13598 timer_generic.c:470]          .. result --> 1, shard[19]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082728283   13598 timer_generic.c:393]          .. shard[20]: heap_empty=true\r\nD1108 22:22:12.082741926   13598 timer_generic.c:368]          .. shard[20]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082756835   13598 timer_generic.c:470]          .. result --> 1, shard[20]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082771376   13598 timer_generic.c:393]          .. shard[21]: heap_empty=true\r\nD1108 22:22:12.082785153   13598 timer_generic.c:368]          .. shard[21]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082799051   13598 timer_generic.c:470]          .. result --> 1, shard[21]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082812917   13598 timer_generic.c:393]          .. shard[22]: heap_empty=true\r\nD1108 22:22:12.082827194   13598 timer_generic.c:368]          .. shard[22]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082841286   13598 timer_generic.c:470]          .. result --> 1, shard[22]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082855067   13598 timer_generic.c:393]          .. shard[23]: heap_empty=true\r\nD1108 22:22:12.082868909   13598 timer_generic.c:368]          .. shard[23]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082882964   13598 timer_generic.c:470]          .. result --> 1, shard[23]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082897455   13598 timer_generic.c:393]          .. shard[24]: heap_empty=true\r\nD1108 22:22:12.082910626   13598 timer_generic.c:368]          .. shard[24]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082924504   13598 timer_generic.c:470]          .. result --> 1, shard[24]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082939111   13598 timer_generic.c:393]          .. shard[25]: heap_empty=true\r\nD1108 22:22:12.082953715   13598 timer_generic.c:368]          .. shard[25]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082967154   13598 timer_generic.c:470]          .. result --> 1, shard[25]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082980708   13598 timer_generic.c:393]          .. shard[26]: heap_empty=true\r\nD1108 22:22:12.082994197   13598 timer_generic.c:368]          .. shard[26]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083007288   13598 timer_generic.c:470]          .. result --> 1, shard[26]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083022578   13598 timer_generic.c:393]          .. shard[27]: heap_empty=true\r\nD1108 22:22:12.083036132   13598 timer_generic.c:368]          .. shard[27]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083050055   13598 timer_generic.c:470]          .. result --> 1, shard[27]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083064169   13598 timer_generic.c:393]          .. shard[28]: heap_empty=true\r\nD1108 22:22:12.083077030   13598 timer_generic.c:368]          .. shard[28]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083090773   13598 timer_generic.c:470]          .. result --> 1, shard[28]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083105020   13598 timer_generic.c:393]          .. shard[29]: heap_empty=true\r\nD1108 22:22:12.083118301   13598 timer_generic.c:368]          .. shard[29]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083133234   13598 timer_generic.c:470]          .. result --> 1, shard[29]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083147636   13598 timer_generic.c:393]          .. shard[30]: heap_empty=true\r\nD1108 22:22:12.083161539   13598 timer_generic.c:368]          .. shard[30]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083175347   13598 timer_generic.c:470]          .. result --> 1, shard[30]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083189231   13598 timer_generic.c:393]          .. shard[31]: heap_empty=true\r\nD1108 22:22:12.083202790   13598 timer_generic.c:368]          .. shard[31]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083217392   13598 timer_generic.c:470]          .. result --> 1, shard[31]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083232886   13598 timer_generic.c:564]        TIMER CHECK END: r=1; next=6777.681259568 [1011]\r\nD1108 22:22:12.083248743   13598 timer_manager.c:181]        sleep for a 0.998518086 seconds\r\nI1108 22:22:12.082079797   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083297098   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083317735   13595 channel_create.c:88]        grpc_insecure_channel_create(target=10.5.0.5:50051, args=0x7fcf990fa458, reserved=(nil))\r\nD1108 22:22:12.083356566   13595 combiner.c:84]              C:0x2b98f20 create\r\nI1108 22:22:12.083414470   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083432138   13595 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nI1108 22:22:12.085079846   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085110209   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085122232   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085140031   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085152765   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085164029   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085174147   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf9911d348)\r\nI1108 22:22:12.085189035   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085199422   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085210908   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085221519   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085230735   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf9911d378)\r\nI1108 22:22:12.085245838   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085256476   13595 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nI1108 22:22:12.085276100   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085303520   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085321160   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085362139   13595 call.c:1780]                grpc_call_start_batch(call=0x2b9bd38, ops=0x2b9ca20, nops=6, tag=0x7fcf980c3320, reserved=(nil))\r\nI1108 22:22:12.085375297   13595 call.c:1426]                ops[0]: SEND_INITIAL_METADATA(nil)\r\nI1108 22:22:12.085385328   13595 call.c:1426]                ops[1]: SEND_MESSAGE ptr=0x2b98c10\r\nI1108 22:22:12.085393802   13595 call.c:1426]                ops[2]: SEND_CLOSE_FROM_CLIENT\r\nI1108 22:22:12.085402814   13595 call.c:1426]                ops[3]: RECV_INITIAL_METADATA ptr=0x7fcf9911d348\r\nI1108 22:22:12.085411791   13595 call.c:1426]                ops[4]: RECV_MESSAGE ptr=0x7fcf9d727d90\r\nI1108 22:22:12.085422998   13595 call.c:1426]                ops[5]: RECV_STATUS_ON_CLIENT metadata=0x7fcf9911d378 status=0x7fcf990f98a0 details=0x7fcf990f98a8\r\nI1108 22:22:12.085451645   13595 client_channel.c:1383]      OP[client-channel:0x2b9c3f0]:  SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.085463969   13595 client_channel.c:1421]      chand=0x2b98df0 calld=0x2b9c410: entering combiner\r\nD1108 22:22:12.085474306   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b9c6c0 last=1\r\nD1108 22:22:12.085484898   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\r\nD1108 22:22:12.085494641   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b9c6c0\r\nD1108 22:22:12.085504236   13595 client_channel.c:298]       chand=0x2b98df0: starting name resolution\r\nD1108 22:22:12.085552335   13595 client_channel.c:1108]      chand=0x2b98df0 calld=0x2b9c410: deferring pick pending resolver result\r\nD1108 22:22:12.085565301   13595 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nI1108 22:22:12.085577920   13595 init.c:173]                 grpc_shutdown(void)\r\nD1108 22:22:12.085581308   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b98a80 last=1\r\nI1108 22:22:12.085608938   13595 completion_queue.c:833]     grpc_completion_queue_next(cq=0x2b9b2a0, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 285604582, clock_type: 1 }, reserved=(nil))\r\nD1108 22:22:12.085614129   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085635250   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b98a80\r\nD1108 22:22:12.085655706   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b98e40 last=3\r\nI1108 22:22:12.085654726   13599 channel_connectivity.c:38]  grpc_channel_check_connectivity_state(channel=0x2b98d30, try_to_connect=0)\r\nD1108 22:22:12.085664751   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.085677411   13599 connectivity_state.c:84]    CONWATCH: 0x2b98e68 client_channel: get IDLE\r\nD1108 22:22:12.085687544   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085704218   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b98e40\r\nD1108 22:22:12.085712836   13597 client_channel.c:370]       chand=0x2b98df0: got resolver result: error=""No Error""\r\nD1108 22:22:12.085723549   13597 pick_first.c:687]           Pick First 0x7fcf94001470 created.\r\nI1108 22:22:12.085729416   13597 pick_first.c:335]           Pick First 0x7fcf94001470 received update with 1 addresses\r\nI1108 22:22:12.085776068   13597 pick_first.c:416]           Pick First 0x7fcf94001470 created subchannel 0x7fcf94001a70 for address uri ipv4:10.5.0.5:50051\r\nD1108 22:22:12.085789435   13597 client_channel.c:477]       chand=0x2b98df0: resolver result: lb_policy_name=""pick_first"" (changed), service_config=""(null)""\r\nD1108 22:22:12.085800177   13597 client_channel.c:559]       chand=0x2b98df0: initializing new LB policy\r\nD1108 22:22:12.085808176   13597 connectivity_state.c:96]    CONWATCH: 0x7fcf94001510 (null): get IDLE\r\nD1108 22:22:12.085815483   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b86840 last=3\r\nD1108 22:22:12.085822948   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from IDLE [cur=IDLE] notify=0x7fcf94001548\r\nD1108 22:22:12.085829578   13597 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to IDLE\r\nD1108 22:22:12.085837481   13597 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: IDLE --> IDLE [new_lb+resolver] error=(nil) ""No Error""\r\nD1108 22:22:12.085844512   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.085851633   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085859308   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b86840\r\nD1108 22:22:12.085866019   13597 client_channel.c:1092]      chand=0x2b98df0 calld=0x2b9c410: resolver returned, doing pick\r\nD1108 22:22:12.085873073   13597 client_channel.c:940]       chand=0x2b98df0 calld=0x2b9c410: applying service config to call\r\nD1108 22:22:12.085880010   13597 client_channel.c:1178]      chand=0x2b98df0 calld=0x2b9c410: starting pick on lb_policy=0x7fcf94001470\r\nD1108 22:22:12.085889027   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001b40 subchannel: from IDLE [cur=IDLE] notify=0x2b9d118\r\nD1108 22:22:12.085898531   13597 connectivity_state.c:171]   SET: 0x7fcf94001b40 subchannel: IDLE --> CONNECTING [state_change] error=(nil) ""No Error""\r\nD1108 22:22:12.085906071   13597 connectivity_state.c:198]   NOTIFY: 0x7fcf94001b40 subchannel: 0x2b9d118\r\nD1108 22:22:12.086250713   13597 tcp_client_posix.c:321]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: asynchronously connecting fd 0x7fcf940024f0\r\nD1108 22:22:12.086270834   13597 timer_generic.c:248]        TIMER 0x7fcf94002650: SET 6796.685390789 [20016] now 6776.685761027 [15] call 0x7fcf94002678[0x7fcf9bc187f0]\r\nD1108 22:22:12.086281744   13597 timer_generic.c:281]          .. add to shard 22 with queue_deadline_cap=1010 => is_first_timer=false\r\nD1108 22:22:12.086296484   13597 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nD1108 22:22:12.086306677   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf940014b0 last=1\r\nD1108 22:22:12.086315346   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086323664   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf940014b0\r\nD1108 22:22:12.086332680   13597 pick_first.c:469]           Pick First 0x7fcf94001470 connectivity changed. Updating selected: 0; Updating subchannels: 0; Checking 0 index (1 total); State: 1; \r\nD1108 22:22:12.086342928   13597 connectivity_state.c:171]   SET: 0x7fcf94001510 (null): IDLE --> CONNECTING [connecting_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.086350812   13597 connectivity_state.c:198]   NOTIFY: 0x7fcf94001510 (null): 0x7fcf94001548\r\nD1108 22:22:12.086358411   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf94001548 last=3\r\nD1108 22:22:12.086368003   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001b40 subchannel: from CONNECTING [cur=CONNECTING] notify=0x2b9d118\r\nD1108 22:22:12.086376276   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.086406709   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086418206   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf94001548\r\nD1108 22:22:12.086426461   13597 client_channel.c:262]       chand=0x2b98df0: lb_policy=0x7fcf94001470 state changed to CONNECTING\r\nD1108 22:22:12.086434772   13597 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to CONNECTING\r\nD1108 22:22:12.086443589   13597 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: IDLE --> CONNECTING [lb_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.086451894   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from CONNECTING [cur=CONNECTING] notify=0x7fcf94002838\r\nD1108 22:22:12.086459464   13597 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nD1108 22:22:12.086485488   13595 tcp_client_posix.c:142]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: on_writable: error=""No Error""\r\nD1108 22:22:12.086497308   13595 timer_generic.c:332]        TIMER 0x7fcf94002650: CANCEL pending=true\r\nI1108 22:22:12.086277793   13599 init.c:168]                 grpc_init(void)\r\nD1108 22:22:12.086525982   13595 combiner.c:84]              C:0x2b9d2e0 create\r\nI1108 22:22:12.086540307   13599 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nD1108 22:22:12.086549959   13595 tcp_client_posix.c:104]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: on_alarm: error=""Cancelled""\r\nI1108 22:22:12.086589114   13599 channel_connectivity.c:209] grpc_channel_watch_connectivity_state(channel=0x2b98d30, last_observed_state=0, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 286559581, clock_type: 1 }, cq=0x7fcf88002400, tag=0x7fcf980c3250)\r\nD1108 22:22:12.086590347   13595 timer_generic.c:248]        TIMER 0x2b9d680: SET 6796.685390789 [20016] now 6776.686081383 [15] call 0x2b9d6a8[0x7fcf9bc05a40]\r\nD1108 22:22:12.086612688   13595 timer_generic.c:281]          .. add to shard 4 with queue_deadline_cap=1010 => is_first_timer=false\r\nD1108 22:22:12.086623861   13595 timer_generic.c:332]        TIMER 0x2b9d680: CANCEL pending=true\r\nD1108 22:22:12.086643083   13595 combiner.c:84]              C:0x2ba1ec0 create\r\nD1108 22:22:12.086721770   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state IDLE -> WRITING [initial_write]\r\nD1108 22:22:12.086732264   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=(nil)\r\nD1108 22:22:12.086737406   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b80cb0 last=1\r\nD1108 22:22:12.086746183   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> WRITING+MORE [init]\r\nD1108 22:22:12.086751054   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55330 last=1\r\nD1108 22:22:12.086756015   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea68 last=3\r\nD1108 22:22:12.086778884   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2996f30 last=5\r\nD1108 22:22:12.086785003   13595 connectivity_state.c:171]   SET: 0x7fcf94001b40 subchannel: CONNECTING --> READY [connected] error=(nil) ""No Error""\r\nD1108 22:22:12.086789792   13595 connectivity_state.c:198]   NOTIFY: 0x7fcf94001b40 subchannel: 0x2b9d118\r\nD1108 22:22:12.086795663   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf940014b0 last=1\r\nD1108 22:22:12.086801474   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086806300   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b80cb0\r\nD1108 22:22:12.086810824   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=0x2ba1ec0\r\nD1108 22:22:12.086815308   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=9\r\nD1108 22:22:12.086819936   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086824458   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea68\r\nD1108 22:22:12.086836182   13595 bdp_estimator.c:67]         bdp[ipv4:10.5.0.5:50051]:sched acc=0 est=65536\r\nD1108 22:22:12.086845850   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=7\r\nD1108 22:22:12.086851282   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086856095   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2996f30\r\nD1108 22:22:12.086861347   13595 connectivity_state.c:121]   CONWATCH: 0x2b9eb90 client_transport: from READY [cur=READY] notify=0x2ba2480\r\nD1108 22:22:12.086866047   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.086870657   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.086875410   13595 combiner.c:265]             C:0x2ba1ec0 execute_final[0] c=0x2b9e9f0\r\nD1108 22:22:12.086883899   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING+MORE -> WRITING [begin writing]\r\nD1108 22:22:12.086888621   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.086895325   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 50 52 49 20 2a 20 48 54 54 50 2f 32 2e 30 0d 0a 0d 0a 53 4d 0d 0a 0d 0a 'PRI * HTTP/2.0....SM....'\r\nD1108 22:22:12.086901946   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 24 04 00 00 00 00 00 00 02 00 00 00 00 00 03 00 00 00 00 00 04 00 00 ff ff 00 05 00 01 00 0f 00 06 00 00 20 00 fe 03 00 00 00 01 '..$.................................. .......'\r\nD1108 22:22:12.086954507   13595 tcp_posix.c:523]            write: ""No Error""\r\nD1108 22:22:12.086962935   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea40 last=1\r\nD1108 22:22:12.086968821   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086973561   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55330\r\nD1108 22:22:12.086978216   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nD1108 22:22:12.086982894   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086987406   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf940014b0\r\nD1108 22:22:12.086992383   13595 pick_first.c:469]           Pick First 0x7fcf94001470 connectivity changed. Updating selected: 0; Updating subchannels: 0; Checking 0 index (1 total); State: 2; \r\nD1108 22:22:12.086998056   13595 connectivity_state.c:171]   SET: 0x7fcf94001510 (null): CONNECTING --> READY [connecting_ready] error=(nil) ""No Error""\r\nD1108 22:22:12.087002619   13595 connectivity_state.c:198]   NOTIFY: 0x7fcf94001510 (null): 0x7fcf94002838\r\nD1108 22:22:12.087006946   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf94002838 last=3\r\nI1108 22:22:12.087012223   13595 pick_first.c:573]           Pick First 0x7fcf94001470 selected subchannel 0x7fcf94001a70 (connected 0x2b9d880)\r\nI1108 22:22:12.087022177   13595 pick_first.c:586]           Servicing pending pick with selected subchannel 0x2b9d880\r\nD1108 22:22:12.087027065   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b9c500 last=5\r\nD1108 22:22:12.087033177   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2996f30 last=3\r\nD1108 22:22:12.087037869   13595 combiner.c:284]             C:0x2b98f20 finish old_state=7\r\nD1108 22:22:12.087042673   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087047247   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf94002838\r\nD1108 22:22:12.087051825   13595 client_channel.c:262]       chand=0x2b98df0: lb_policy=0x7fcf94001470 state changed to READY\r\nD1108 22:22:12.087056600   13595 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to READY\r\nD1108 22:22:12.087061387   13595 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: CONNECTING --> READY [lb_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.087066358   13595 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from READY [cur=READY] notify=0x2b83028\r\nD1108 22:22:12.087071052   13595 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.087075665   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087080096   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b9c500\r\nD1108 22:22:12.087084511   13595 client_channel.c:1160]      chand=0x2b98df0 calld=0x2b9c410: pick completed asynchronously\r\nD1108 22:22:12.087096753   13595 client_channel.c:983]       chand=0x2b98df0 calld=0x2b9c410: create subchannel_call=0x2ba2e48: error=""No Error""\r\nD1108 22:22:12.087102147   13595 client_channel.c:921]       chand=0x2b98df0 calld=0x2b9c410: sending 1 pending batches to subchannel_call=0x2ba2e48\r\nI1108 22:22:12.087115690   13595 http_client_filter.c:305]   OP[http-client:0x2ba2ec0]:  SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nI1108 22:22:12.087140656   13595 connected_channel.c:55]     OP[connected:0x2ba2ef0]:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.098483127   13595 chttp2_transport.c:1524]    perform_stream_op[s=0x2ba33c0]:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.098537808   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9c6c0 last=5\r\nD1108 22:22:12.098558022   13595 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.098575829   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098588922   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.098600039   13595 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.098621256   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098647880   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea40\r\nD1108 22:22:12.098662794   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> IDLE [finish writing]\r\nD1108 22:22:12.098679110   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=7\r\nD1108 22:22:12.098696322   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098710427   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2996f30\r\nD1108 22:22:12.098727604   13595 connectivity_state.c:121]   CONWATCH: 0x2b9eb90 client_transport: from READY [cur=READY] notify=0x7fcf940014b0\r\nD1108 22:22:12.098742052   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.098761473   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098775803   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9c6c0\r\nD1108 22:22:12.098818995   13595 chttp2_transport.c:1263]    perform_stream_op_locked:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA; on_complete = 0x2ba3050\r\nI1108 22:22:12.098846764   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :scheme: http\r\nI1108 22:22:12.098860479   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :method: POST\r\nI1108 22:22:12.098873366   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :path: /io.priceinsight.mt4manager.NewChartBasesService/ChartRequest\r\nI1108 22:22:12.098886778   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :authority: 10.5.0.5:50051\r\nI1108 22:22:12.098903192   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: te: trailers\r\nI1108 22:22:12.098916576   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: content-type: application/grpc\r\nI1108 22:22:12.098929708   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: user-agent: grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)\r\nI1108 22:22:12.098943903   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: grpc-accept-encoding: identity,deflate,gzip\r\nD1108 22:22:12.098959459   13595 chttp2_transport.c:1047]    HTTP:CLI: Allocating new grpc_chttp2_stream 0x2ba33c0 to id 1\r\nD1108 22:22:12.098973626   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55358 last=1\r\nD1108 22:22:12.098989211   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state IDLE -> WRITING [new_stream]\r\nD1108 22:22:12.099003721   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=0x2ba1ec0\r\nD1108 22:22:12.099020631   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> WRITING+MORE [op.send_message]\r\nD1108 22:22:12.099039713   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=4 flags=0x0003 desc=op->on_complete err=""No Error""\r\nD1108 22:22:12.099053888   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.099068620   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.099082984   13595 combiner.c:265]             C:0x2ba1ec0 execute_final[0] c=0x2b9e9f0\r\nD1108 22:22:12.099099283   13595 writing.c:226]              W:0x2b9e940 CLIENT[1] im-(sent,send)=(0,1) announce=5\r\nD1108 22:22:12.099122359   13595 hpack_encoder.c:417]        Encode: ':path: /io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', elem_interned=0 [2], k_interned=1, v_interned=0\r\nD1108 22:22:12.099150328   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING+MORE -> WRITING [begin writing]\r\nD1108 22:22:12.145577517   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.145613470   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 01 1b 01 04 00 00 00 01 40 07 '.........@.'\r\nD1108 22:22:12.145624954   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 73 63 68 65 6d 65 ':scheme'\r\nD1108 22:22:12.145634347   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 04 '.'\r\nD1108 22:22:12.145644461   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 68 74 74 70 'http'\r\nD1108 22:22:12.145653686   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 07 '@.'\r\nD1108 22:22:12.145663328   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 6d 65 74 68 6f 64 ':method'\r\nD1108 22:22:12.145672746   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 04 '.'\r\nD1108 22:22:12.145681996   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 50 4f 53 54 'POST'\r\nD1108 22:22:12.145691190   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 05 '..'\r\nD1108 22:22:12.145700476   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 70 61 74 68 ':path'\r\nD1108 22:22:12.145709865   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3d '='\r\nD1108 22:22:12.145721333   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest'\r\nD1108 22:22:12.145732446   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0a '@.'\r\nD1108 22:22:12.145742657   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 61 75 74 68 6f 72 69 74 79 ':authority'\r\nD1108 22:22:12.145753414   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 0e '.'\r\nD1108 22:22:12.145763860   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'\r\nD1108 22:22:12.145774653   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 02 '@.'\r\nD1108 22:22:12.145783912   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 74 65 'te'\r\nD1108 22:22:12.145792516   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 08 '.'\r\nD1108 22:22:12.145800996   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 74 72 61 69 6c 65 72 73 'trailers'\r\nD1108 22:22:12.145809088   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0c '@.'\r\nD1108 22:22:12.145817390   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type'\r\nD1108 22:22:12.145825490   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 10 '.'\r\nD1108 22:22:12.145834735   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\r\nD1108 22:22:12.145843706   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0a '@.'\r\nD1108 22:22:12.145852466   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 75 73 65 72 2d 61 67 65 6e 74 'user-agent'\r\nD1108 22:22:12.145860946   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a ':'\r\nD1108 22:22:12.145871465   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)'\r\nD1108 22:22:12.145882841   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 14 '@.'\r\nD1108 22:22:12.145894246   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding'\r\nD1108 22:22:12.145903756   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 15 '.'\r\nD1108 22:22:12.145913168   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'\r\nD1108 22:22:12.145922849   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 04 08 00 00 00 00 01 00 00 00 05 00 00 '...............'\r\nD1108 22:22:12.145931720   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 1b 00 01 00 00 00 01 00 00 00 00 16 '............'\r\nD1108 22:22:12.145941144   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 0a 06 45 55 52 55 53 44 18 b0 ae 8f a5 05 20 b0 d2 b7 b5 05 28 01 '..EURUSD...... .....(.'\r\nD1108 22:22:12.098636805   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146275960   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146287524   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146298342   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146306187   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146313381   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146321638   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146329516   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146334268   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146339684   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146346433   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146353217   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146358036   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146364289   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146368812   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146374306   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146379902   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146388849   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146398320   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146407358   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146415399   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146423985   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146432349   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146440182   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146448723   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146457347   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146464391   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146471936   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146479370   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146486294   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146495012   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146502942   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146533421   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146544608   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146553047   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146560035   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146567893   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146576404   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146582931   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146591965   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146598799   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146604194   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146613489   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146621605   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146628090   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146636657   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146643826   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146650993   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146659436   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146667787   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146674656   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146682948   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146690694   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146697793   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146705920   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146715077   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146721764   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146729705   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146740182   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146745453   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146763194   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146798593   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146812791   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146825867   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146839386   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146853180   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146879311   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146897102   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146917539   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146933692   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146947889   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146961504   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146975836   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146989854   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147002927   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147017208   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147031434   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147044908   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147059115   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147073896   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147086871   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147100857   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147115225   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147127987   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147140739   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147153092   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147164842   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147178883   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147192938   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147204738   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147217963   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147230838   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147245194   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147259974   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147274966   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147289306   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147302856   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147315485   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147327105   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147340017   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147352637   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147364582   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147377855   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087314000   13599 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf88001990 last=3\r\nI1108 22:22:12.197575213   13599 completion_queue.c:833]     grpc_completion_queue_next(cq=0x7fcf88002400, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 397562599, clock_type: 1 }, reserved=(nil))\r\nD1108 22:22:12.197657077   13599 resource_quota.c:776]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: alloc 8192; free_pool -> -8192\r\nD1108 22:22:12.197678372   13599 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55268 last=3\r\nD1108 22:22:12.146785999   13595 tcp_posix.c:523]            write: ""No Error""\r\nD1108 22:22:12.197707242   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea40 last=1\r\nD1108 22:22:12.197723416   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197730354   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55358\r\nD1108 22:22:12.197736908   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.197743170   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197748944   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55268\r\nD1108 22:22:12.197755425   13595 combiner.c:332]             C:0x2b9d2e0 grpc_combiner_execute_finally c=0x2af8278; ac=0x2b9d2e0\r\nD1108 22:22:12.197761273   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.197770991   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.197780222   13595 combiner.c:265]             C:0x2b9d2e0 execute_final[0] c=0x2af8278\r\nD1108 22:22:12.197790415   13595 resource_quota.c:297]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: grant alloc 8192 bytes; rq_free_pool -> 9223372036854767615\r\nD1108 22:22:12.197799800   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nD1108 22:22:12.197827348   13595 tcp_posix.c:219]            read: error=""No Error""\r\nD1108 22:22:12.197847965   13595 tcp_posix.c:224]            READ 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 18 04 00 00 00 00 00 00 04 00 00 ff ff 00 05 00 01 01 93 00 06 00 00 20 00 fe 03 00 00 00 01 00 00 00 04 01 00 00 00 00 00 00 66 01 05 00 00 00 01 40 07 3a 73 74 61 74 75 73 03 32 30 30 40 0c 63 6f 6e 74 65 6e 74 2d 74 79 70 65 10 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 00 0b 67 72 70 63 2d 73 74 61 74 75 73 02 31 33 00 0c 67 72 70 63 2d 6d 65 73 73 61 67 65 1b 44 69 64 20 6e 6f 74 20 72 65 61 64 20 65 6e 74 69 72 65 20 6d 65 73 73 61 67 65 '......................... ..................f......@.:status.200@.content-type.application/grpc..grpc-status.13..grpc-message.Did not read entire message'\r\nD1108 22:22:12.197863171   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea68 last=3\r\nD1108 22:22:12.197873207   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197883763   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea40\r\nD1108 22:22:12.197894285   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> IDLE [finish writing]\r\nD1108 22:22:12.197906345   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=3 flags=0x0003 desc=send_initial_metadata_finished err=""No Error""\r\nD1108 22:22:12.197917133   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=2 flags=0x0003 desc=finish_write_cb err=""No Error""\r\nD1108 22:22:12.197927005   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=1 flags=0x0003 desc=send_trailing_metadata_finished err=""No Error""\r\nD1108 22:22:12.197941278   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.197952038   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197960951   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea68\r\nD1108 22:22:12.197974971   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting INITIAL_WINDOW_SIZE = 65535\r\nD1108 22:22:12.197985398   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting MAX_FRAME_SIZE = 65939\r\nD1108 22:22:12.197994709   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting MAX_HEADER_LIST_SIZE = 8192\r\nD1108 22:22:12.198003805   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting GRPC_ALLOW_TRUE_BINARY_METADATA = 1\r\nI1108 22:22:12.198015369   13595 parsing.c:621]              parsing Trailers-Only\r\nI1108 22:22:12.198032911   13595 parsing.c:496]              HTTP:1:TRL:CLI: :status: 32 30 30 '200'\r\nI1108 22:22:12.198049785   13595 parsing.c:496]              HTTP:1:TRL:CLI: content-type: 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\r\nD1108 22:22:12.198064301   13595 hpack_parser.c:656]         Decode: 'grpc-status: 13', elem_interned=0 [2], k_interned=1, v_interned=0\r\nI1108 22:22:12.198075098   13595 parsing.c:496]              HTTP:1:TRL:CLI: grpc-status: 31 33 '13'\r\nD1108 22:22:12.198086337   13595 hpack_parser.c:656]         Decode: 'grpc-message: Did not read entire message', elem_interned=0 [2], k_interned=1, v_interned=0\r\nI1108 22:22:12.198098334   13595 parsing.c:496]              HTTP:1:TRL:CLI: grpc-message: 44 69 64 20 6e 6f 74 20 72 65 61 64 20 65 6e 74 69 72 65 20 6d 65 73 73 61 67 65 'Did not read entire message'\r\nD1108 22:22:12.198119435   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=0 flags=0x0003 desc=recv_trailing_metadata_finished err=""No Error""\r\nD1108 22:22:12.198140495   13595 call.c:695]                 get_final_status CLI\r\nD1108 22:22:12.198165446   13595 call.c:698]                   1: {""created"":""@1510132932.198137263"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.c"",""file_line"":989,""grpc_message"":""Did not read entire message"",""grpc_status"":13}\r\nI1108 22:22:12.198180951   13595 completion_queue.c:620]     cq_end_op_for_next(exec_ctx=0x7ffd26cda6d0, cq=0x2b9b2a0, tag=0x7fcf980c3320, error=""No Error"", done=0x7fcf9bc29c90, done_arg=0x2b9c618, storage=0x2b9c620)\r\nD1108 22:22:12.198202981   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.198214182   13595 resource_quota.c:776]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: alloc 8192; free_pool -> -8192\r\nD1108 22:22:12.198223860   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55268 last=1\r\nD1108 22:22:12.198234417   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\r\nD1108 22:22:12.198243794   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55268\r\nD1108 22:22:12.198252431   13595 combiner.c:332]             C:0x2b9d2e0 grpc_combiner_execute_finally c=0x2af8278; ac=0x2b9d2e0\r\nD1108 22:22:12.198261200   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.198305253   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=1\r\nD1108 22:22:12.198320349   13595 combiner.c:265]             C:0x2b9d2e0 execute_final[0] c=0x2af8278\r\nD1108 22:22:12.198335555   13595 resource_quota.c:297]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: grant alloc 8192 bytes; rq_free_pool -> 9223372036854759423\r\nD1108 22:22:12.198352133   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nI1108 22:22:12.198392592   13595 completion_queue.c:937]     RETURN_EVENT[0x2b9b2a0]: OP_COMPLETE: tag:0x7fcf980c3320 OK\r\nI1108 22:22:12.198432388   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7ffd26cda5e0)\r\nI1108 22:22:12\r\n```\r\n### Anything else we should know about your project / environment?\r\n\r\nServer is written in c++",kind/bug|lang/Python|priority/P2|disposition/requires reporter action,yang-g," \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n \r\n\r\n \r\n### What version of gRPC and what language are you using?\r\n I have tried with 1.7.2 and 1.6.1 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nUbuntu 17.10\r\n \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n Have tried with Python 2.7.14 and Python 3.6.3\r\n \r\n### What did you do?\r\n\r\nTried to call a simple grpc method. I can make the equivilant call in Java, and it works perfectly.\r\n\r\n```proto\r\nservice NewChartBasesService {\r\n\trpc ChartRequest (ChartRequestRequestMessage) returns (ChartRequestResponseMessage){}\r\n}\r\n```\r\n\r\nand\r\n```python\r\nchannel = grpc.insecure_channel(""10.5.0.5:50051"")\r\n\r\n    stub = NewChartBasesService_pb2_grpc.NewChartBasesServiceStub(channel)\r\n\r\n\r\n  \r\n\r\n    crrm = ChartRequestRequestMessage_pb2\r\n    request = crrm.ChartInfo__pb2.ChartInfo(symbol='HELLO', period=1, timesign=1, start=None, end=None)\r\n    response = stub.ChartRequest(request)\r\n\r\n\r\n    print(""printing results"")\r\n    for ri in response.rate_info:\r\n        print(ri)\r\n\r\n```\r\n\r\n \r\n### What did you expect to see?\r\n I expected to get a result back from the server.\r\n \r\n### What did you see instead?\r\n \r\nHere is the full verbose log:\r\n\r\n```\r\n           \r\nD1108 22:22:12.070899086   13595 ev_posix.c:111]             Using polling engine: poll\r\nD1108 22:22:12.071947506   13595 dns_resolver.c:301]         Using native dns resolver\r\nD1108 22:22:12.072055639   13595 timer_manager.c:82]         Spawn timer thread\r\nI1108 22:22:12.072178672   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.072223668   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf990f5c18)\r\nD1108 22:22:12.081615538   13598 timer_generic.c:538]        TIMER CHECK BEGIN: now=6776.681088675 [10] next=9223372036854775807.000000000 [9223372036854775807] tls_min=0 glob_min=0\r\nD1108 22:22:12.081672094   13598 timer_generic.c:452]          .. shard[0]->min_deadline = 1\r\nD1108 22:22:12.081686442   13598 timer_generic.c:393]          .. shard[0]: heap_empty=true\r\nD1108 22:22:12.081697659   13598 timer_generic.c:368]          .. shard[0]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081710510   13598 timer_generic.c:470]          .. result --> 1, shard[0]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081722983   13598 timer_generic.c:393]          .. shard[1]: heap_empty=true\r\nD1108 22:22:12.081733894   13598 timer_generic.c:368]          .. shard[1]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081745142   13598 timer_generic.c:470]          .. result --> 1, shard[1]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081756354   13598 timer_generic.c:393]          .. shard[2]: heap_empty=true\r\nD1108 22:22:12.081767494   13598 timer_generic.c:368]          .. shard[2]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081779276   13598 timer_generic.c:470]          .. result --> 1, shard[2]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.081790168   13598 timer_generic.c:393]          .. shard[3]: heap_empty=true\r\nD1108 22:22:12.081799723   13598 timer_generic.c:368]          .. shard[3]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.081810519   13598 timer_generic.c:470]          .. result --> 1, shard[3]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082107160   13598 timer_generic.c:393]          .. shard[4]: heap_empty=true\r\nD1108 22:22:12.082124113   13598 timer_generic.c:368]          .. shard[4]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082136519   13598 timer_generic.c:470]          .. result --> 1, shard[4]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082147685   13598 timer_generic.c:393]          .. shard[5]: heap_empty=true\r\nD1108 22:22:12.082157693   13598 timer_generic.c:368]          .. shard[5]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082168372   13598 timer_generic.c:470]          .. result --> 1, shard[5]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082178563   13598 timer_generic.c:393]          .. shard[6]: heap_empty=true\r\nD1108 22:22:12.082188005   13598 timer_generic.c:368]          .. shard[6]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082198414   13598 timer_generic.c:470]          .. result --> 1, shard[6]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082208346   13598 timer_generic.c:393]          .. shard[7]: heap_empty=true\r\nD1108 22:22:12.082218805   13598 timer_generic.c:368]          .. shard[7]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082230245   13598 timer_generic.c:470]          .. result --> 1, shard[7]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082241254   13598 timer_generic.c:393]          .. shard[8]: heap_empty=true\r\nD1108 22:22:12.082251890   13598 timer_generic.c:368]          .. shard[8]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082262939   13598 timer_generic.c:470]          .. result --> 1, shard[8]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082274617   13598 timer_generic.c:393]          .. shard[9]: heap_empty=true\r\nD1108 22:22:12.082285668   13598 timer_generic.c:368]          .. shard[9]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082296661   13598 timer_generic.c:470]          .. result --> 1, shard[9]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082308105   13598 timer_generic.c:393]          .. shard[10]: heap_empty=true\r\nD1108 22:22:12.082319220   13598 timer_generic.c:368]          .. shard[10]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082330690   13598 timer_generic.c:470]          .. result --> 1, shard[10]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082342219   13598 timer_generic.c:393]          .. shard[11]: heap_empty=true\r\nD1108 22:22:12.082352635   13598 timer_generic.c:368]          .. shard[11]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082363971   13598 timer_generic.c:470]          .. result --> 1, shard[11]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082375114   13598 timer_generic.c:393]          .. shard[12]: heap_empty=true\r\nD1108 22:22:12.082384756   13598 timer_generic.c:368]          .. shard[12]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082395573   13598 timer_generic.c:470]          .. result --> 1, shard[12]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082405802   13598 timer_generic.c:393]          .. shard[13]: heap_empty=true\r\nD1108 22:22:12.082416210   13598 timer_generic.c:368]          .. shard[13]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082426949   13598 timer_generic.c:470]          .. result --> 1, shard[13]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082443146   13598 timer_generic.c:393]          .. shard[14]: heap_empty=true\r\nD1108 22:22:12.082457979   13598 timer_generic.c:368]          .. shard[14]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082472898   13598 timer_generic.c:470]          .. result --> 1, shard[14]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082487983   13598 timer_generic.c:393]          .. shard[15]: heap_empty=true\r\nD1108 22:22:12.082502426   13598 timer_generic.c:368]          .. shard[15]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082538111   13598 timer_generic.c:470]          .. result --> 1, shard[15]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082554404   13598 timer_generic.c:393]          .. shard[16]: heap_empty=true\r\nD1108 22:22:12.082569011   13598 timer_generic.c:368]          .. shard[16]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082584395   13598 timer_generic.c:470]          .. result --> 1, shard[16]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082600018   13598 timer_generic.c:393]          .. shard[17]: heap_empty=true\r\nD1108 22:22:12.082614646   13598 timer_generic.c:368]          .. shard[17]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082629306   13598 timer_generic.c:470]          .. result --> 1, shard[17]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082644052   13598 timer_generic.c:393]          .. shard[18]: heap_empty=true\r\nD1108 22:22:12.082658465   13598 timer_generic.c:368]          .. shard[18]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082673328   13598 timer_generic.c:470]          .. result --> 1, shard[18]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082687068   13598 timer_generic.c:393]          .. shard[19]: heap_empty=true\r\nD1108 22:22:12.082700742   13598 timer_generic.c:368]          .. shard[19]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082714987   13598 timer_generic.c:470]          .. result --> 1, shard[19]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082728283   13598 timer_generic.c:393]          .. shard[20]: heap_empty=true\r\nD1108 22:22:12.082741926   13598 timer_generic.c:368]          .. shard[20]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082756835   13598 timer_generic.c:470]          .. result --> 1, shard[20]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082771376   13598 timer_generic.c:393]          .. shard[21]: heap_empty=true\r\nD1108 22:22:12.082785153   13598 timer_generic.c:368]          .. shard[21]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082799051   13598 timer_generic.c:470]          .. result --> 1, shard[21]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082812917   13598 timer_generic.c:393]          .. shard[22]: heap_empty=true\r\nD1108 22:22:12.082827194   13598 timer_generic.c:368]          .. shard[22]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082841286   13598 timer_generic.c:470]          .. result --> 1, shard[22]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082855067   13598 timer_generic.c:393]          .. shard[23]: heap_empty=true\r\nD1108 22:22:12.082868909   13598 timer_generic.c:368]          .. shard[23]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082882964   13598 timer_generic.c:470]          .. result --> 1, shard[23]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082897455   13598 timer_generic.c:393]          .. shard[24]: heap_empty=true\r\nD1108 22:22:12.082910626   13598 timer_generic.c:368]          .. shard[24]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082924504   13598 timer_generic.c:470]          .. result --> 1, shard[24]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082939111   13598 timer_generic.c:393]          .. shard[25]: heap_empty=true\r\nD1108 22:22:12.082953715   13598 timer_generic.c:368]          .. shard[25]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.082967154   13598 timer_generic.c:470]          .. result --> 1, shard[25]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.082980708   13598 timer_generic.c:393]          .. shard[26]: heap_empty=true\r\nD1108 22:22:12.082994197   13598 timer_generic.c:368]          .. shard[26]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083007288   13598 timer_generic.c:470]          .. result --> 1, shard[26]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083022578   13598 timer_generic.c:393]          .. shard[27]: heap_empty=true\r\nD1108 22:22:12.083036132   13598 timer_generic.c:368]          .. shard[27]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083050055   13598 timer_generic.c:470]          .. result --> 1, shard[27]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083064169   13598 timer_generic.c:393]          .. shard[28]: heap_empty=true\r\nD1108 22:22:12.083077030   13598 timer_generic.c:368]          .. shard[28]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083090773   13598 timer_generic.c:470]          .. result --> 1, shard[28]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083105020   13598 timer_generic.c:393]          .. shard[29]: heap_empty=true\r\nD1108 22:22:12.083118301   13598 timer_generic.c:368]          .. shard[29]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083133234   13598 timer_generic.c:470]          .. result --> 1, shard[29]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083147636   13598 timer_generic.c:393]          .. shard[30]: heap_empty=true\r\nD1108 22:22:12.083161539   13598 timer_generic.c:368]          .. shard[30]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083175347   13598 timer_generic.c:470]          .. result --> 1, shard[30]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083189231   13598 timer_generic.c:393]          .. shard[31]: heap_empty=true\r\nD1108 22:22:12.083202790   13598 timer_generic.c:368]          .. shard[31]->queue_deadline_cap --> 1010\r\nD1108 22:22:12.083217392   13598 timer_generic.c:470]          .. result --> 1, shard[31]->min_deadline 1 --> 1011, now=10\r\nD1108 22:22:12.083232886   13598 timer_generic.c:564]        TIMER CHECK END: r=1; next=6777.681259568 [1011]\r\nD1108 22:22:12.083248743   13598 timer_manager.c:181]        sleep for a 0.998518086 seconds\r\nI1108 22:22:12.082079797   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083297098   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083317735   13595 channel_create.c:88]        grpc_insecure_channel_create(target=10.5.0.5:50051, args=0x7fcf990fa458, reserved=(nil))\r\nD1108 22:22:12.083356566   13595 combiner.c:84]              C:0x2b98f20 create\r\nI1108 22:22:12.083414470   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.083432138   13595 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nI1108 22:22:12.085079846   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085110209   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085122232   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085140031   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085152765   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085164029   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085174147   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf9911d348)\r\nI1108 22:22:12.085189035   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085199422   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085210908   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085221519   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085230735   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7fcf9911d378)\r\nI1108 22:22:12.085245838   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085256476   13595 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nI1108 22:22:12.085276100   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085303520   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085321160   13595 init.c:168]                 grpc_init(void)\r\nI1108 22:22:12.085362139   13595 call.c:1780]                grpc_call_start_batch(call=0x2b9bd38, ops=0x2b9ca20, nops=6, tag=0x7fcf980c3320, reserved=(nil))\r\nI1108 22:22:12.085375297   13595 call.c:1426]                ops[0]: SEND_INITIAL_METADATA(nil)\r\nI1108 22:22:12.085385328   13595 call.c:1426]                ops[1]: SEND_MESSAGE ptr=0x2b98c10\r\nI1108 22:22:12.085393802   13595 call.c:1426]                ops[2]: SEND_CLOSE_FROM_CLIENT\r\nI1108 22:22:12.085402814   13595 call.c:1426]                ops[3]: RECV_INITIAL_METADATA ptr=0x7fcf9911d348\r\nI1108 22:22:12.085411791   13595 call.c:1426]                ops[4]: RECV_MESSAGE ptr=0x7fcf9d727d90\r\nI1108 22:22:12.085422998   13595 call.c:1426]                ops[5]: RECV_STATUS_ON_CLIENT metadata=0x7fcf9911d378 status=0x7fcf990f98a0 details=0x7fcf990f98a8\r\nI1108 22:22:12.085451645   13595 client_channel.c:1383]      OP[client-channel:0x2b9c3f0]:  SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.085463969   13595 client_channel.c:1421]      chand=0x2b98df0 calld=0x2b9c410: entering combiner\r\nD1108 22:22:12.085474306   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b9c6c0 last=1\r\nD1108 22:22:12.085484898   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\r\nD1108 22:22:12.085494641   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b9c6c0\r\nD1108 22:22:12.085504236   13595 client_channel.c:298]       chand=0x2b98df0: starting name resolution\r\nD1108 22:22:12.085552335   13595 client_channel.c:1108]      chand=0x2b98df0 calld=0x2b9c410: deferring pick pending resolver result\r\nD1108 22:22:12.085565301   13595 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nI1108 22:22:12.085577920   13595 init.c:173]                 grpc_shutdown(void)\r\nD1108 22:22:12.085581308   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b98a80 last=1\r\nI1108 22:22:12.085608938   13595 completion_queue.c:833]     grpc_completion_queue_next(cq=0x2b9b2a0, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 285604582, clock_type: 1 }, reserved=(nil))\r\nD1108 22:22:12.085614129   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085635250   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b98a80\r\nD1108 22:22:12.085655706   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b98e40 last=3\r\nI1108 22:22:12.085654726   13599 channel_connectivity.c:38]  grpc_channel_check_connectivity_state(channel=0x2b98d30, try_to_connect=0)\r\nD1108 22:22:12.085664751   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.085677411   13599 connectivity_state.c:84]    CONWATCH: 0x2b98e68 client_channel: get IDLE\r\nD1108 22:22:12.085687544   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085704218   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b98e40\r\nD1108 22:22:12.085712836   13597 client_channel.c:370]       chand=0x2b98df0: got resolver result: error=""No Error""\r\nD1108 22:22:12.085723549   13597 pick_first.c:687]           Pick First 0x7fcf94001470 created.\r\nI1108 22:22:12.085729416   13597 pick_first.c:335]           Pick First 0x7fcf94001470 received update with 1 addresses\r\nI1108 22:22:12.085776068   13597 pick_first.c:416]           Pick First 0x7fcf94001470 created subchannel 0x7fcf94001a70 for address uri ipv4:10.5.0.5:50051\r\nD1108 22:22:12.085789435   13597 client_channel.c:477]       chand=0x2b98df0: resolver result: lb_policy_name=""pick_first"" (changed), service_config=""(null)""\r\nD1108 22:22:12.085800177   13597 client_channel.c:559]       chand=0x2b98df0: initializing new LB policy\r\nD1108 22:22:12.085808176   13597 connectivity_state.c:96]    CONWATCH: 0x7fcf94001510 (null): get IDLE\r\nD1108 22:22:12.085815483   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b86840 last=3\r\nD1108 22:22:12.085822948   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from IDLE [cur=IDLE] notify=0x7fcf94001548\r\nD1108 22:22:12.085829578   13597 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to IDLE\r\nD1108 22:22:12.085837481   13597 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: IDLE --> IDLE [new_lb+resolver] error=(nil) ""No Error""\r\nD1108 22:22:12.085844512   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.085851633   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.085859308   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b86840\r\nD1108 22:22:12.085866019   13597 client_channel.c:1092]      chand=0x2b98df0 calld=0x2b9c410: resolver returned, doing pick\r\nD1108 22:22:12.085873073   13597 client_channel.c:940]       chand=0x2b98df0 calld=0x2b9c410: applying service config to call\r\nD1108 22:22:12.085880010   13597 client_channel.c:1178]      chand=0x2b98df0 calld=0x2b9c410: starting pick on lb_policy=0x7fcf94001470\r\nD1108 22:22:12.085889027   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001b40 subchannel: from IDLE [cur=IDLE] notify=0x2b9d118\r\nD1108 22:22:12.085898531   13597 connectivity_state.c:171]   SET: 0x7fcf94001b40 subchannel: IDLE --> CONNECTING [state_change] error=(nil) ""No Error""\r\nD1108 22:22:12.085906071   13597 connectivity_state.c:198]   NOTIFY: 0x7fcf94001b40 subchannel: 0x2b9d118\r\nD1108 22:22:12.086250713   13597 tcp_client_posix.c:321]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: asynchronously connecting fd 0x7fcf940024f0\r\nD1108 22:22:12.086270834   13597 timer_generic.c:248]        TIMER 0x7fcf94002650: SET 6796.685390789 [20016] now 6776.685761027 [15] call 0x7fcf94002678[0x7fcf9bc187f0]\r\nD1108 22:22:12.086281744   13597 timer_generic.c:281]          .. add to shard 22 with queue_deadline_cap=1010 => is_first_timer=false\r\nD1108 22:22:12.086296484   13597 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nD1108 22:22:12.086306677   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf940014b0 last=1\r\nD1108 22:22:12.086315346   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086323664   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf940014b0\r\nD1108 22:22:12.086332680   13597 pick_first.c:469]           Pick First 0x7fcf94001470 connectivity changed. Updating selected: 0; Updating subchannels: 0; Checking 0 index (1 total); State: 1; \r\nD1108 22:22:12.086342928   13597 connectivity_state.c:171]   SET: 0x7fcf94001510 (null): IDLE --> CONNECTING [connecting_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.086350812   13597 connectivity_state.c:198]   NOTIFY: 0x7fcf94001510 (null): 0x7fcf94001548\r\nD1108 22:22:12.086358411   13597 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf94001548 last=3\r\nD1108 22:22:12.086368003   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001b40 subchannel: from CONNECTING [cur=CONNECTING] notify=0x2b9d118\r\nD1108 22:22:12.086376276   13597 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.086406709   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086418206   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf94001548\r\nD1108 22:22:12.086426461   13597 client_channel.c:262]       chand=0x2b98df0: lb_policy=0x7fcf94001470 state changed to CONNECTING\r\nD1108 22:22:12.086434772   13597 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to CONNECTING\r\nD1108 22:22:12.086443589   13597 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: IDLE --> CONNECTING [lb_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.086451894   13597 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from CONNECTING [cur=CONNECTING] notify=0x7fcf94002838\r\nD1108 22:22:12.086459464   13597 combiner.c:284]             C:0x2b98f20 finish old_state=3\r\nD1108 22:22:12.086485488   13595 tcp_client_posix.c:142]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: on_writable: error=""No Error""\r\nD1108 22:22:12.086497308   13595 timer_generic.c:332]        TIMER 0x7fcf94002650: CANCEL pending=true\r\nI1108 22:22:12.086277793   13599 init.c:168]                 grpc_init(void)\r\nD1108 22:22:12.086525982   13595 combiner.c:84]              C:0x2b9d2e0 create\r\nI1108 22:22:12.086540307   13599 completion_queue.c:417]     grpc_completion_queue_create_internal(completion_type=0, polling_type=0)\r\nD1108 22:22:12.086549959   13595 tcp_client_posix.c:104]     CLIENT_CONNECT: ipv4:10.5.0.5:50051: on_alarm: error=""Cancelled""\r\nI1108 22:22:12.086589114   13599 channel_connectivity.c:209] grpc_channel_watch_connectivity_state(channel=0x2b98d30, last_observed_state=0, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 286559581, clock_type: 1 }, cq=0x7fcf88002400, tag=0x7fcf980c3250)\r\nD1108 22:22:12.086590347   13595 timer_generic.c:248]        TIMER 0x2b9d680: SET 6796.685390789 [20016] now 6776.686081383 [15] call 0x2b9d6a8[0x7fcf9bc05a40]\r\nD1108 22:22:12.086612688   13595 timer_generic.c:281]          .. add to shard 4 with queue_deadline_cap=1010 => is_first_timer=false\r\nD1108 22:22:12.086623861   13595 timer_generic.c:332]        TIMER 0x2b9d680: CANCEL pending=true\r\nD1108 22:22:12.086643083   13595 combiner.c:84]              C:0x2ba1ec0 create\r\nD1108 22:22:12.086721770   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state IDLE -> WRITING [initial_write]\r\nD1108 22:22:12.086732264   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=(nil)\r\nD1108 22:22:12.086737406   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b80cb0 last=1\r\nD1108 22:22:12.086746183   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> WRITING+MORE [init]\r\nD1108 22:22:12.086751054   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55330 last=1\r\nD1108 22:22:12.086756015   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea68 last=3\r\nD1108 22:22:12.086778884   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2996f30 last=5\r\nD1108 22:22:12.086785003   13595 connectivity_state.c:171]   SET: 0x7fcf94001b40 subchannel: CONNECTING --> READY [connected] error=(nil) ""No Error""\r\nD1108 22:22:12.086789792   13595 connectivity_state.c:198]   NOTIFY: 0x7fcf94001b40 subchannel: 0x2b9d118\r\nD1108 22:22:12.086795663   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf940014b0 last=1\r\nD1108 22:22:12.086801474   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086806300   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b80cb0\r\nD1108 22:22:12.086810824   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=0x2ba1ec0\r\nD1108 22:22:12.086815308   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=9\r\nD1108 22:22:12.086819936   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086824458   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea68\r\nD1108 22:22:12.086836182   13595 bdp_estimator.c:67]         bdp[ipv4:10.5.0.5:50051]:sched acc=0 est=65536\r\nD1108 22:22:12.086845850   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=7\r\nD1108 22:22:12.086851282   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086856095   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2996f30\r\nD1108 22:22:12.086861347   13595 connectivity_state.c:121]   CONWATCH: 0x2b9eb90 client_transport: from READY [cur=READY] notify=0x2ba2480\r\nD1108 22:22:12.086866047   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.086870657   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.086875410   13595 combiner.c:265]             C:0x2ba1ec0 execute_final[0] c=0x2b9e9f0\r\nD1108 22:22:12.086883899   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING+MORE -> WRITING [begin writing]\r\nD1108 22:22:12.086888621   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.086895325   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 50 52 49 20 2a 20 48 54 54 50 2f 32 2e 30 0d 0a 0d 0a 53 4d 0d 0a 0d 0a 'PRI * HTTP/2.0....SM....'\r\nD1108 22:22:12.086901946   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 24 04 00 00 00 00 00 00 02 00 00 00 00 00 03 00 00 00 00 00 04 00 00 ff ff 00 05 00 01 00 0f 00 06 00 00 20 00 fe 03 00 00 00 01 '..$.................................. .......'\r\nD1108 22:22:12.086954507   13595 tcp_posix.c:523]            write: ""No Error""\r\nD1108 22:22:12.086962935   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea40 last=1\r\nD1108 22:22:12.086968821   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086973561   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55330\r\nD1108 22:22:12.086978216   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nD1108 22:22:12.086982894   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.086987406   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf940014b0\r\nD1108 22:22:12.086992383   13595 pick_first.c:469]           Pick First 0x7fcf94001470 connectivity changed. Updating selected: 0; Updating subchannels: 0; Checking 0 index (1 total); State: 2; \r\nD1108 22:22:12.086998056   13595 connectivity_state.c:171]   SET: 0x7fcf94001510 (null): CONNECTING --> READY [connecting_ready] error=(nil) ""No Error""\r\nD1108 22:22:12.087002619   13595 connectivity_state.c:198]   NOTIFY: 0x7fcf94001510 (null): 0x7fcf94002838\r\nD1108 22:22:12.087006946   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf94002838 last=3\r\nI1108 22:22:12.087012223   13595 pick_first.c:573]           Pick First 0x7fcf94001470 selected subchannel 0x7fcf94001a70 (connected 0x2b9d880)\r\nI1108 22:22:12.087022177   13595 pick_first.c:586]           Servicing pending pick with selected subchannel 0x2b9d880\r\nD1108 22:22:12.087027065   13595 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x2b9c500 last=5\r\nD1108 22:22:12.087033177   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2996f30 last=3\r\nD1108 22:22:12.087037869   13595 combiner.c:284]             C:0x2b98f20 finish old_state=7\r\nD1108 22:22:12.087042673   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087047247   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x7fcf94002838\r\nD1108 22:22:12.087051825   13595 client_channel.c:262]       chand=0x2b98df0: lb_policy=0x7fcf94001470 state changed to READY\r\nD1108 22:22:12.087056600   13595 client_channel.c:248]       chand=0x2b98df0: setting connectivity state to READY\r\nD1108 22:22:12.087061387   13595 connectivity_state.c:171]   SET: 0x2b98e68 client_channel: CONNECTING --> READY [lb_changed] error=(nil) ""No Error""\r\nD1108 22:22:12.087066358   13595 connectivity_state.c:121]   CONWATCH: 0x7fcf94001510 (null): from READY [cur=READY] notify=0x2b83028\r\nD1108 22:22:12.087071052   13595 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.087075665   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087080096   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=0x2b9c500\r\nD1108 22:22:12.087084511   13595 client_channel.c:1160]      chand=0x2b98df0 calld=0x2b9c410: pick completed asynchronously\r\nD1108 22:22:12.087096753   13595 client_channel.c:983]       chand=0x2b98df0 calld=0x2b9c410: create subchannel_call=0x2ba2e48: error=""No Error""\r\nD1108 22:22:12.087102147   13595 client_channel.c:921]       chand=0x2b98df0 calld=0x2b9c410: sending 1 pending batches to subchannel_call=0x2ba2e48\r\nI1108 22:22:12.087115690   13595 http_client_filter.c:305]   OP[http-client:0x2ba2ec0]:  SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nI1108 22:22:12.087140656   13595 connected_channel.c:55]     OP[connected:0x2ba2ef0]:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.098483127   13595 chttp2_transport.c:1524]    perform_stream_op[s=0x2ba33c0]:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA\r\nD1108 22:22:12.098537808   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9c6c0 last=5\r\nD1108 22:22:12.098558022   13595 combiner.c:284]             C:0x2b98f20 finish old_state=5\r\nD1108 22:22:12.098575829   13595 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098588922   13595 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.098600039   13595 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.098621256   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098647880   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea40\r\nD1108 22:22:12.098662794   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> IDLE [finish writing]\r\nD1108 22:22:12.098679110   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=7\r\nD1108 22:22:12.098696322   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098710427   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2996f30\r\nD1108 22:22:12.098727604   13595 connectivity_state.c:121]   CONWATCH: 0x2b9eb90 client_transport: from READY [cur=READY] notify=0x7fcf940014b0\r\nD1108 22:22:12.098742052   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.098761473   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.098775803   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9c6c0\r\nD1108 22:22:12.098818995   13595 chttp2_transport.c:1263]    perform_stream_op_locked:  SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'} SEND_MESSAGE:flags=0x00000000:len=22 SEND_TRAILING_METADATA{} RECV_INITIAL_METADATA RECV_MESSAGE RECV_TRAILING_METADATA; on_complete = 0x2ba3050\r\nI1108 22:22:12.098846764   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :scheme: http\r\nI1108 22:22:12.098860479   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :method: POST\r\nI1108 22:22:12.098873366   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :path: /io.priceinsight.mt4manager.NewChartBasesService/ChartRequest\r\nI1108 22:22:12.098886778   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: :authority: 10.5.0.5:50051\r\nI1108 22:22:12.098903192   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: te: trailers\r\nI1108 22:22:12.098916576   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: content-type: application/grpc\r\nI1108 22:22:12.098929708   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: user-agent: grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)\r\nI1108 22:22:12.098943903   13595 chttp2_transport.c:1245]    HTTP:0:HDR:CLI: grpc-accept-encoding: identity,deflate,gzip\r\nD1108 22:22:12.098959459   13595 chttp2_transport.c:1047]    HTTP:CLI: Allocating new grpc_chttp2_stream 0x2ba33c0 to id 1\r\nD1108 22:22:12.098973626   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55358 last=1\r\nD1108 22:22:12.098989211   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state IDLE -> WRITING [new_stream]\r\nD1108 22:22:12.099003721   13595 combiner.c:332]             C:0x2ba1ec0 grpc_combiner_execute_finally c=0x2b9e9f0; ac=0x2ba1ec0\r\nD1108 22:22:12.099020631   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> WRITING+MORE [op.send_message]\r\nD1108 22:22:12.099039713   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=4 flags=0x0003 desc=op->on_complete err=""No Error""\r\nD1108 22:22:12.099053888   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.099068620   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.099082984   13595 combiner.c:265]             C:0x2ba1ec0 execute_final[0] c=0x2b9e9f0\r\nD1108 22:22:12.099099283   13595 writing.c:226]              W:0x2b9e940 CLIENT[1] im-(sent,send)=(0,1) announce=5\r\nD1108 22:22:12.099122359   13595 hpack_encoder.c:417]        Encode: ':path: /io.priceinsight.mt4manager.NewChartBasesService/ChartRequest', elem_interned=0 [2], k_interned=1, v_interned=0\r\nD1108 22:22:12.099150328   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING+MORE -> WRITING [begin writing]\r\nD1108 22:22:12.145577517   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.145613470   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 01 1b 01 04 00 00 00 01 40 07 '.........@.'\r\nD1108 22:22:12.145624954   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 73 63 68 65 6d 65 ':scheme'\r\nD1108 22:22:12.145634347   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 04 '.'\r\nD1108 22:22:12.145644461   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 68 74 74 70 'http'\r\nD1108 22:22:12.145653686   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 07 '@.'\r\nD1108 22:22:12.145663328   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 6d 65 74 68 6f 64 ':method'\r\nD1108 22:22:12.145672746   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 04 '.'\r\nD1108 22:22:12.145681996   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 50 4f 53 54 'POST'\r\nD1108 22:22:12.145691190   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 05 '..'\r\nD1108 22:22:12.145700476   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 70 61 74 68 ':path'\r\nD1108 22:22:12.145709865   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3d '='\r\nD1108 22:22:12.145721333   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 2f 69 6f 2e 70 72 69 63 65 69 6e 73 69 67 68 74 2e 6d 74 34 6d 61 6e 61 67 65 72 2e 4e 65 77 43 68 61 72 74 42 61 73 65 73 53 65 72 76 69 63 65 2f 43 68 61 72 74 52 65 71 75 65 73 74 '/io.priceinsight.mt4manager.NewChartBasesService/ChartRequest'\r\nD1108 22:22:12.145732446   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0a '@.'\r\nD1108 22:22:12.145742657   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a 61 75 74 68 6f 72 69 74 79 ':authority'\r\nD1108 22:22:12.145753414   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 0e '.'\r\nD1108 22:22:12.145763860   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 31 30 2e 35 2e 30 2e 35 3a 35 30 30 35 31 '10.5.0.5:50051'\r\nD1108 22:22:12.145774653   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 02 '@.'\r\nD1108 22:22:12.145783912   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 74 65 'te'\r\nD1108 22:22:12.145792516   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 08 '.'\r\nD1108 22:22:12.145800996   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 74 72 61 69 6c 65 72 73 'trailers'\r\nD1108 22:22:12.145809088   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0c '@.'\r\nD1108 22:22:12.145817390   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type'\r\nD1108 22:22:12.145825490   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 10 '.'\r\nD1108 22:22:12.145834735   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\r\nD1108 22:22:12.145843706   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 0a '@.'\r\nD1108 22:22:12.145852466   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 75 73 65 72 2d 61 67 65 6e 74 'user-agent'\r\nD1108 22:22:12.145860946   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 3a ':'\r\nD1108 22:22:12.145871465   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 36 2e 30 20 67 72 70 63 2d 63 2f 34 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 61 72 63 69 61 29 'grpc-python/1.6.0 grpc-c/4.0.0 (manylinux; chttp2; garcia)'\r\nD1108 22:22:12.145882841   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 40 14 '@.'\r\nD1108 22:22:12.145894246   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding'\r\nD1108 22:22:12.145903756   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 15 '.'\r\nD1108 22:22:12.145913168   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'\r\nD1108 22:22:12.145922849   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 04 08 00 00 00 00 01 00 00 00 05 00 00 '...............'\r\nD1108 22:22:12.145931720   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 1b 00 01 00 00 00 01 00 00 00 00 16 '............'\r\nD1108 22:22:12.145941144   13595 tcp_posix.c:491]            WRITE 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 0a 06 45 55 52 55 53 44 18 b0 ae 8f a5 05 20 b0 d2 b7 b5 05 28 01 '..EURUSD...... .....(.'\r\nD1108 22:22:12.098636805   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146275960   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146287524   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146298342   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146306187   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146313381   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146321638   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146329516   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146334268   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146339684   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146346433   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146353217   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146358036   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146364289   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146368812   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146374306   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146379902   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146388849   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146398320   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146407358   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146415399   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146423985   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146432349   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146440182   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146448723   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146457347   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146464391   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146471936   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146479370   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146486294   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146495012   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146502942   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146533421   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146544608   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146553047   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146560035   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146567893   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146576404   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146582931   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146591965   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146598799   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146604194   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146613489   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146621605   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146628090   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146636657   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146643826   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146650993   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146659436   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146667787   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146674656   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146682948   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146690694   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146697793   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146705920   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146715077   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146721764   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146729705   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146740182   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146745453   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146763194   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146798593   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146812791   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146825867   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146839386   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146853180   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146879311   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146897102   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146917539   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146933692   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146947889   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.146961504   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.146975836   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.146989854   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147002927   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147017208   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147031434   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147044908   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147059115   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147073896   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147086871   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147100857   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147115225   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147127987   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147140739   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147153092   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147164842   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147178883   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147192938   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147204738   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147217963   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147230838   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147245194   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147259974   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147274966   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147289306   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147302856   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147315485   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147327105   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147340017   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.147352637   13597 combiner.c:239]             C:0x2b98f20 maybe_finish_one n=(nil)\r\nD1108 22:22:12.147364582   13597 combiner.c:199]             C:0x2b98f20 queue_offload\r\nD1108 22:22:12.147377855   13597 combiner.c:221]             C:0x2b98f20 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.087314000   13599 combiner.c:161]             C:0x2b98f20 grpc_combiner_execute c=0x7fcf88001990 last=3\r\nI1108 22:22:12.197575213   13599 completion_queue.c:833]     grpc_completion_queue_next(cq=0x7fcf88002400, deadline=gpr_timespec { tv_sec: 1510132932, tv_nsec: 397562599, clock_type: 1 }, reserved=(nil))\r\nD1108 22:22:12.197657077   13599 resource_quota.c:776]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: alloc 8192; free_pool -> -8192\r\nD1108 22:22:12.197678372   13599 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55268 last=3\r\nD1108 22:22:12.146785999   13595 tcp_posix.c:523]            write: ""No Error""\r\nD1108 22:22:12.197707242   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea40 last=1\r\nD1108 22:22:12.197723416   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197730354   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55358\r\nD1108 22:22:12.197736908   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.197743170   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197748944   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55268\r\nD1108 22:22:12.197755425   13595 combiner.c:332]             C:0x2b9d2e0 grpc_combiner_execute_finally c=0x2af8278; ac=0x2b9d2e0\r\nD1108 22:22:12.197761273   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.197770991   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=1 exec_ctx_ready_to_finish=0 time_to_execute_final_list=1\r\nD1108 22:22:12.197780222   13595 combiner.c:265]             C:0x2b9d2e0 execute_final[0] c=0x2af8278\r\nD1108 22:22:12.197790415   13595 resource_quota.c:297]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: grant alloc 8192 bytes; rq_free_pool -> 9223372036854767615\r\nD1108 22:22:12.197799800   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nD1108 22:22:12.197827348   13595 tcp_posix.c:219]            read: error=""No Error""\r\nD1108 22:22:12.197847965   13595 tcp_posix.c:224]            READ 0x2b9d3b0 (peer=ipv4:10.5.0.5:50051): 00 00 18 04 00 00 00 00 00 00 04 00 00 ff ff 00 05 00 01 01 93 00 06 00 00 20 00 fe 03 00 00 00 01 00 00 00 04 01 00 00 00 00 00 00 66 01 05 00 00 00 01 40 07 3a 73 74 61 74 75 73 03 32 30 30 40 0c 63 6f 6e 74 65 6e 74 2d 74 79 70 65 10 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 00 0b 67 72 70 63 2d 73 74 61 74 75 73 02 31 33 00 0c 67 72 70 63 2d 6d 65 73 73 61 67 65 1b 44 69 64 20 6e 6f 74 20 72 65 61 64 20 65 6e 74 69 72 65 20 6d 65 73 73 61 67 65 '......................... ..................f......@.:status.200@.content-type.application/grpc..grpc-status.13..grpc-message.Did not read entire message'\r\nD1108 22:22:12.197863171   13595 combiner.c:161]             C:0x2ba1ec0 grpc_combiner_execute c=0x2b9ea68 last=3\r\nD1108 22:22:12.197873207   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197883763   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea40\r\nD1108 22:22:12.197894285   13595 chttp2_transport.c:841]     W:0x2b9e940 CLIENT state WRITING -> IDLE [finish writing]\r\nD1108 22:22:12.197906345   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=3 flags=0x0003 desc=send_initial_metadata_finished err=""No Error""\r\nD1108 22:22:12.197917133   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=2 flags=0x0003 desc=finish_write_cb err=""No Error""\r\nD1108 22:22:12.197927005   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=1 flags=0x0003 desc=send_trailing_metadata_finished err=""No Error""\r\nD1108 22:22:12.197941278   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=5\r\nD1108 22:22:12.197952038   13595 combiner.c:221]             C:0x2ba1ec0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=0 time_to_execute_final_list=0\r\nD1108 22:22:12.197960951   13595 combiner.c:239]             C:0x2ba1ec0 maybe_finish_one n=0x2b9ea68\r\nD1108 22:22:12.197974971   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting INITIAL_WINDOW_SIZE = 65535\r\nD1108 22:22:12.197985398   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting MAX_FRAME_SIZE = 65939\r\nD1108 22:22:12.197994709   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting MAX_HEADER_LIST_SIZE = 8192\r\nD1108 22:22:12.198003805   13595 frame_settings.c:215]       CHTTP2:CLI:ipv4:10.5.0.5:50051: got setting GRPC_ALLOW_TRUE_BINARY_METADATA = 1\r\nI1108 22:22:12.198015369   13595 parsing.c:621]              parsing Trailers-Only\r\nI1108 22:22:12.198032911   13595 parsing.c:496]              HTTP:1:TRL:CLI: :status: 32 30 30 '200'\r\nI1108 22:22:12.198049785   13595 parsing.c:496]              HTTP:1:TRL:CLI: content-type: 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\r\nD1108 22:22:12.198064301   13595 hpack_parser.c:656]         Decode: 'grpc-status: 13', elem_interned=0 [2], k_interned=1, v_interned=0\r\nI1108 22:22:12.198075098   13595 parsing.c:496]              HTTP:1:TRL:CLI: grpc-status: 31 33 '13'\r\nD1108 22:22:12.198086337   13595 hpack_parser.c:656]         Decode: 'grpc-message: Did not read entire message', elem_interned=0 [2], k_interned=1, v_interned=0\r\nI1108 22:22:12.198098334   13595 parsing.c:496]              HTTP:1:TRL:CLI: grpc-message: 44 69 64 20 6e 6f 74 20 72 65 61 64 20 65 6e 74 69 72 65 20 6d 65 73 73 61 67 65 'Did not read entire message'\r\nD1108 22:22:12.198119435   13595 chttp2_transport.c:1113]    complete_closure_step: 0x2ba3050 refs=0 flags=0x0003 desc=recv_trailing_metadata_finished err=""No Error""\r\nD1108 22:22:12.198140495   13595 call.c:695]                 get_final_status CLI\r\nD1108 22:22:12.198165446   13595 call.c:698]                   1: {""created"":""@1510132932.198137263"",""description"":""Error received from peer"",""file"":""src/core/lib/surface/call.c"",""file_line"":989,""grpc_message"":""Did not read entire message"",""grpc_status"":13}\r\nI1108 22:22:12.198180951   13595 completion_queue.c:620]     cq_end_op_for_next(exec_ctx=0x7ffd26cda6d0, cq=0x2b9b2a0, tag=0x7fcf980c3320, error=""No Error"", done=0x7fcf9bc29c90, done_arg=0x2b9c618, storage=0x2b9c620)\r\nD1108 22:22:12.198202981   13595 combiner.c:284]             C:0x2ba1ec0 finish old_state=3\r\nD1108 22:22:12.198214182   13595 resource_quota.c:776]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: alloc 8192; free_pool -> -8192\r\nD1108 22:22:12.198223860   13595 combiner.c:161]             C:0x2b9d2e0 grpc_combiner_execute c=0x2b55268 last=1\r\nD1108 22:22:12.198234417   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\r\nD1108 22:22:12.198243794   13595 combiner.c:239]             C:0x2b9d2e0 maybe_finish_one n=0x2b55268\r\nD1108 22:22:12.198252431   13595 combiner.c:332]             C:0x2b9d2e0 grpc_combiner_execute_finally c=0x2af8278; ac=0x2b9d2e0\r\nD1108 22:22:12.198261200   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=5\r\nD1108 22:22:12.198305253   13595 combiner.c:221]             C:0x2b9d2e0 grpc_combiner_continue_exec_ctx contended=0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=1\r\nD1108 22:22:12.198320349   13595 combiner.c:265]             C:0x2b9d2e0 execute_final[0] c=0x2af8278\r\nD1108 22:22:12.198335555   13595 resource_quota.c:297]       RQ anonymous_pool_2af8240 ipv4:10.5.0.5:50051: grant alloc 8192 bytes; rq_free_pool -> 9223372036854759423\r\nD1108 22:22:12.198352133   13595 combiner.c:284]             C:0x2b9d2e0 finish old_state=3\r\nI1108 22:22:12.198392592   13595 completion_queue.c:937]     RETURN_EVENT[0x2b9b2a0]: OP_COMPLETE: tag:0x7fcf980c3320 OK\r\nI1108 22:22:12.198432388   13595 metadata_array.c:27]        grpc_metadata_array_init(array=0x7ffd26cda5e0)\r\nI1108 22:22:12\r\n```\r\n### Anything else we should know about your project / environment?\r\n\r\nServer is written in c++","python\r\nchannel = grpc.insecure_channel(""10.5.0.5:50051"")\r\n\r\n    stub = NewChartBasesService_pb2_grpc.NewChartBasesServiceStub(channel)\r\n\r\n\r\n  \r\n\r\n    crrm = ChartRequestRequestMessage_pb2\r\n    request = crrm.ChartInfo__pb2.ChartInfo(symbol='HELLO', period=1, timesign=1, start=None, end=None)\r\n    response = stub.ChartRequest(request)\r\n\r\n\r\n    print(""printing results"")\r\n    for ri in response.rate_info:\r\n        print(ri)\r\n\r\n"
13049,"grpc + electron = Error: package.json does not exist at /package.json### What version of gRPC and what language are you using?\r\n tried 1.6.6 and 1.8.0-dev (master)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n node 8.4.0, python 2.17.13\r\n \r\n### What did you do?\r\nsimply\r\n`import grpc from 'grpc'` in code.\r\nCode run on electron app with two package.json config.\r\n \r\n### What did you expect to see?\r\n No crash\r\n \r\n### What did you see instead?\r\n`Error: package.json does not exist at /package.json`\r\n\r\nLooks like in this code from `grpc_extension.js`:\r\n\r\n`__dirname` is `/`, instead of current directory.\r\nCuriously if I run `__dirname` in the chrome devtools, I get the right path where the package.json is present.\r\n\r\n\r\n",lang/node,murgatroid99,"### What version of gRPC and what language are you using?\r\n tried 1.6.6 and 1.8.0-dev (master)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n linux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n node 8.4.0, python 2.17.13\r\n \r\n### What did you do?\r\nsimply\r\n`import grpc from 'grpc'` in code.\r\nCode run on electron app with two package.json config.\r\n \r\n### What did you expect to see?\r\n No crash\r\n \r\n### What did you see instead?\r\n`Error: package.json does not exist at /package.json`\r\n\r\nLooks like in this code from `grpc_extension.js`:\r\n```javascript\r\nvar binding_path = binary.find(path.resolve(path.join(__dirname, '../../../package.json')));\r\n```\r\n`__dirname` is `/`, instead of current directory.\r\nCuriously if I run `__dirname` in the chrome devtools, I get the right path where the package.json is present.\r\n\r\n\r\n","javascript\r\nvar binding_path = binary.find(path.resolve(path.join(__dirname, '../../../package.json')));\r\n"
12947,"I can't install grpcio..I'm new to gRPC. \r\nSo I'm studying gRPC step by step.\r\n\r\nFirst of all, I tried to install gRPC in my ubuntu 14.04 machine with pip (because I'm python user.)\r\n\r\nI typed the following command. \r\n\r\n```\r\n$ pip install grpcio\r\nCollecting grpcio\r\n  Downloading grpcio-1.6.3-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\r\n    95% |##############################  | 5.4MB 22kB/s eta 0:00:12\r\n```\r\n\r\nA few minutes ago, I received the following error message again and again each time I tried to enter the above command.\r\n\r\n\r\n\r\nSo I added an option the above command. Please see the following command.\r\n\r\n```\r\n$ pip install --default-timeout=100 grpcio\r\nCollecting grpcio\r\n  Downloading grpcio-1.6.3-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\r\n    100% |################################| 5.6MB 15kB/s \r\nCollecting futures>=2.2.0 (from grpcio)\r\n  Downloading futures-3.1.1-py2-none-any.whl\r\nCollecting enum34>=1.0.4 (from grpcio)\r\n  Downloading enum34-1.1.6-py2-none-any.whl\r\nCollecting protobuf>=3.3.0 (from grpcio)\r\n  Downloading protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (6.2MB)\r\n    50% |################                | 3.1MB 48bytes/s eta 17:38:51\r\nCollecting six>=1.5.2 (from grpcio)\r\n  Downloading six-1.11.0-py2.py3-none-any.whl\r\n```\r\n\r\nAnd this time, I received the following error message.\r\n\r\n```\r\nTHESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n    protobuf>=3.3.0 from https://pypi.python.org/packages/9c/e4/993661ca20c57307561a14180757a7a3cb30c0283f416ff3040dca7d91d3/protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=cdefe5c48a6ec2ef7e174209d0dd1e64 (from grpcio):\r\n        Expected md5 cdefe5c48a6ec2ef7e174209d0dd1e64\r\n             Got        2f246165d54d931a035157bc68c8a7eb\r\n```\r\n\r\nTherefore, I failed to install grpcio. \r\nHow can I resolve this problem?\r\n\r\nThank you for reading.\r\n",kind/question|lang/Python|disposition/requires reporter action,mehrdada,"I'm new to gRPC. \r\nSo I'm studying gRPC step by step.\r\n\r\nFirst of all, I tried to install gRPC in my ubuntu 14.04 machine with pip (because I'm python user.)\r\n\r\nI typed the following command. \r\n\r\n```\r\n$ pip install grpcio\r\nCollecting grpcio\r\n  Downloading grpcio-1.6.3-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\r\n    95% |##############################  | 5.4MB 22kB/s eta 0:00:12\r\n```\r\n\r\nA few minutes ago, I received the following error message again and again each time I tried to enter the above command.\r\n\r\n```python\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/download.py"", line 560, in resp_read\r\n    decode_content=False):\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 357, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 324, in read\r\n    flush_decoder = True\r\n  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 246, in _error_catcher\r\n    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\r\nReadTimeoutError: HTTPSConnectionPool(host='pypi.python.org', port=443): Read timed out.\r\n```\r\n\r\nSo I added an option the above command. Please see the following command.\r\n\r\n```\r\n$ pip install --default-timeout=100 grpcio\r\nCollecting grpcio\r\n  Downloading grpcio-1.6.3-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\r\n    100% |################################| 5.6MB 15kB/s \r\nCollecting futures>=2.2.0 (from grpcio)\r\n  Downloading futures-3.1.1-py2-none-any.whl\r\nCollecting enum34>=1.0.4 (from grpcio)\r\n  Downloading enum34-1.1.6-py2-none-any.whl\r\nCollecting protobuf>=3.3.0 (from grpcio)\r\n  Downloading protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (6.2MB)\r\n    50% |################                | 3.1MB 48bytes/s eta 17:38:51\r\nCollecting six>=1.5.2 (from grpcio)\r\n  Downloading six-1.11.0-py2.py3-none-any.whl\r\n```\r\n\r\nAnd this time, I received the following error message.\r\n\r\n```\r\nTHESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\r\n    protobuf>=3.3.0 from https://pypi.python.org/packages/9c/e4/993661ca20c57307561a14180757a7a3cb30c0283f416ff3040dca7d91d3/protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl#md5=cdefe5c48a6ec2ef7e174209d0dd1e64 (from grpcio):\r\n        Expected md5 cdefe5c48a6ec2ef7e174209d0dd1e64\r\n             Got        2f246165d54d931a035157bc68c8a7eb\r\n```\r\n\r\nTherefore, I failed to install grpcio. \r\nHow can I resolve this problem?\r\n\r\nThank you for reading.\r\n","python\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/download.py"", line 560, in resp_read\r\n    decode_content=False):\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 357, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 324, in read\r\n    flush_decoder = True\r\n  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File ""/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/packages/urllib3/response.py"", line 246, in _error_catcher\r\n    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\r\nReadTimeoutError: HTTPSConnectionPool(host='pypi.python.org', port=443): Read timed out.\r\n"
12579,"Unable to run app locally with dev_appserver.py due to ""ImportError: No module named grpc._cython.cygrpc""Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n  \r\n### What version of gRPC and what language are you using?\r\n \r\n 1.6.0\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOSX 10.11.6 \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython:\r\n2.7.13 :: Continuum Analytics, Inc.\r\n\r\nGCC:\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n \r\n### What did you do?\r\n\r\nRunning `dev_appserver.py app.yaml`.\r\n\r\nCurrent `app.yaml` file:\r\n\r\n```yaml\r\nruntime: python27\r\nentrypoint: gunicorn -b :$PORT main:app\r\nservice: data-api-service\r\n\r\nthreadsafe: false\r\n\r\n# [START handlers]\r\nhandlers:\r\n- url: /.*\r\n  script: main\r\n# [END handlers]\r\n\r\nresources:\r\n  cpu: 1.0\r\n  memory_gb: 2.0\r\n  disk_size_gb: 10\r\n\r\nautomatic_scaling:\r\n  min_num_instances: 3\r\n  max_num_instances: 13\r\n  cool_down_period_sec: 120 # default value\r\n  cpu_utilization:\r\n    target_utilization: 0.5\r\n\r\nenv_variables:\r\n    GOOGLE_APPLICATION_CREDENTIALS: ""./cred.json""\r\n    API_MODE: 'PROD'\r\n```\r\n\r\nCurrent `appengine_config.py`:\r\n\r\n\r\nCurrent `requirements.txt`:\r\n```\r\nFlask==0.11.1\r\nflask-restplus==0.9.2\r\nprotobuf==3.0.0\r\ngrpcio==1.6.0\r\ngcloud==0.18.3\r\ngoogle-api-python-client==1.6.3\r\ngunicorn==19.6.0\r\npython-dateutil==2.5.3\r\npython-dotenv==0.6.0\r\nuniversal-analytics-python==0.2.4\r\ngoogleapis_common_protos==1.5.2\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nThe server does start. The issue occurs when I actually try to visit `localhost:8080`.\r\n \r\n### What did you see instead?\r\n\r\nConsole output:\r\n```\r\n/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/dotenv/main.py:24: UserWarning: Not loading  - it doesn't exist.\r\n  warnings.warn(""Not loading %s - it doesn't exist."" % dotenv_path)\r\nERROR    2017-09-14 19:03:40,527 wsgi.py:263]\r\nTraceback (most recent call last):\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 240, in Handle\r\n    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler\r\n    handler, path, err = LoadObject(self._handler)\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 85, in LoadObject\r\n    obj = __import__(path[0])\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/main.py"", line 14, in <module>\r\n    from mlab_api.endpoints.locations import locations_ns\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/endpoints/locations.py"", line 10, in <module>\r\n    from mlab_api.data.data import LOCATION_DATA as DATA\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/data.py"", line 9, in <module>\r\n    from mlab_api.data.location_data import LocationData\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/location_data.py"", line 5, in <module>\r\n    from mlab_api.data.base_data import Data\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/base_data.py"", line 10, in <module>\r\n    import mlab_api.data.bigtable_utils as bt\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/bigtable_utils.py"", line 8, in <module>\r\n    from gcloud import bigtable\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/gcloud/bigtable/__init__.py"", line 18, in <module>\r\n    from gcloud.bigtable.client import Client\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/gcloud/bigtable/client.py"", line 32, in <module>\r\n    from grpc.beta import implementations\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/grpc/__init__.py"", line 22, in <module>\r\n    from grpc._cython import cygrpc as _cygrpc\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/tools/devappserver2/python/runtime/sandbox.py"", line 1091, in load_module\r\n    raise ImportError('No module named %s' % fullname)\r\nImportError: No module named grpc._cython.cygrpc\r\nINFO     2017-09-14 19:03:40,555 module.py:821] data-api: ""GET / HTTP/1.1"" 500 -\r\n```\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nWe have been previously running this as a standalone python flask app. We'd like to use the local app engine dev setup to ensure our configuration is correct. Can you assist in debugging this?\r\n\r\nAdditional things I've tried:\r\n\r\n1. Downgrading versions of grpcio. Went as far as 0.13.1\r\n2. Tried to add the flag `GRPC_BUILD_WITH_CYTHON=0|1` and have attempted both builds.\r\n\r\nI followed the threads in issue #5280, but wasn't able to identify anything that would be of help. Linking it here since it appears to be related.\r\n\r\nThank you for your help.\r\n",lang/Python|platform/macOS,kpayson64,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n  \r\n### What version of gRPC and what language are you using?\r\n \r\n 1.6.0\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nOSX 10.11.6 \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\nPython:\r\n2.7.13 :: Continuum Analytics, Inc.\r\n\r\nGCC:\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n \r\n### What did you do?\r\n\r\nRunning `dev_appserver.py app.yaml`.\r\n\r\nCurrent `app.yaml` file:\r\n\r\n```yaml\r\nruntime: python27\r\nentrypoint: gunicorn -b :$PORT main:app\r\nservice: data-api-service\r\n\r\nthreadsafe: false\r\n\r\n# [START handlers]\r\nhandlers:\r\n- url: /.*\r\n  script: main\r\n# [END handlers]\r\n\r\nresources:\r\n  cpu: 1.0\r\n  memory_gb: 2.0\r\n  disk_size_gb: 10\r\n\r\nautomatic_scaling:\r\n  min_num_instances: 3\r\n  max_num_instances: 13\r\n  cool_down_period_sec: 120 # default value\r\n  cpu_utilization:\r\n    target_utilization: 0.5\r\n\r\nenv_variables:\r\n    GOOGLE_APPLICATION_CREDENTIALS: ""./cred.json""\r\n    API_MODE: 'PROD'\r\n```\r\n\r\nCurrent `appengine_config.py`:\r\n```python\r\nfrom google.appengine.ext import vendor\r\n# Add any libraries installed in the ""lib"" folder.\r\nvendor.add('lib')\r\n```\r\n\r\nCurrent `requirements.txt`:\r\n```\r\nFlask==0.11.1\r\nflask-restplus==0.9.2\r\nprotobuf==3.0.0\r\ngrpcio==1.6.0\r\ngcloud==0.18.3\r\ngoogle-api-python-client==1.6.3\r\ngunicorn==19.6.0\r\npython-dateutil==2.5.3\r\npython-dotenv==0.6.0\r\nuniversal-analytics-python==0.2.4\r\ngoogleapis_common_protos==1.5.2\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nThe server does start. The issue occurs when I actually try to visit `localhost:8080`.\r\n \r\n### What did you see instead?\r\n\r\nConsole output:\r\n```\r\n/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/dotenv/main.py:24: UserWarning: Not loading  - it doesn't exist.\r\n  warnings.warn(""Not loading %s - it doesn't exist."" % dotenv_path)\r\nERROR    2017-09-14 19:03:40,527 wsgi.py:263]\r\nTraceback (most recent call last):\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 240, in Handle\r\n    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler\r\n    handler, path, err = LoadObject(self._handler)\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 85, in LoadObject\r\n    obj = __import__(path[0])\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/main.py"", line 14, in <module>\r\n    from mlab_api.endpoints.locations import locations_ns\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/endpoints/locations.py"", line 10, in <module>\r\n    from mlab_api.data.data import LOCATION_DATA as DATA\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/data.py"", line 9, in <module>\r\n    from mlab_api.data.location_data import LocationData\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/location_data.py"", line 5, in <module>\r\n    from mlab_api.data.base_data import Data\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/base_data.py"", line 10, in <module>\r\n    import mlab_api.data.bigtable_utils as bt\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/mlab_api/data/bigtable_utils.py"", line 8, in <module>\r\n    from gcloud import bigtable\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/gcloud/bigtable/__init__.py"", line 18, in <module>\r\n    from gcloud.bigtable.client import Client\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/gcloud/bigtable/client.py"", line 32, in <module>\r\n    from grpc.beta import implementations\r\n  File ""/Users/iros/dev/opensource/mlab/mlab-vis-api/lib/grpc/__init__.py"", line 22, in <module>\r\n    from grpc._cython import cygrpc as _cygrpc\r\n  File ""/Users/iros/dev/tools/google-cloud-sdk/platform/google_appengine/google/appengine/tools/devappserver2/python/runtime/sandbox.py"", line 1091, in load_module\r\n    raise ImportError('No module named %s' % fullname)\r\nImportError: No module named grpc._cython.cygrpc\r\nINFO     2017-09-14 19:03:40,555 module.py:821] data-api: ""GET / HTTP/1.1"" 500 -\r\n```\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\nWe have been previously running this as a standalone python flask app. We'd like to use the local app engine dev setup to ensure our configuration is correct. Can you assist in debugging this?\r\n\r\nAdditional things I've tried:\r\n\r\n1. Downgrading versions of grpcio. Went as far as 0.13.1\r\n2. Tried to add the flag `GRPC_BUILD_WITH_CYTHON=0|1` and have attempted both builds.\r\n\r\nI followed the threads in issue #5280, but wasn't able to identify anything that would be of help. Linking it here since it appears to be related.\r\n\r\nThank you for your help.\r\n","python\r\nfrom google.appengine.ext import vendor\r\n# Add any libraries installed in the ""lib"" folder.\r\nvendor.add('lib')\r\n"
12506,"Call dropped by load balancing policy### What version of gRPC and what language are you using?\r\ngRPC 1.6.0 installed by pip\r\n### What operating system (Linux, Windows, \u2026) and version?\r\nCentOS Linux release 7.3.1611 (Core)\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 3.4.5 (default, Nov  9 2016, 16:24:59)\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n \r\n### What did you do?\r\nI have a series of py.test suites that check GRPC API of a particular service. Channel and client for a service is periodically recreated in a such manner:\r\n\r\nUsing:\r\n\r\nRequests to the service are performed in a default manner (according to the grpcio documentation). There are a lot of tests within a single testing session, and after a while, requests start to fail with a strange error: ""Call dropped by load balancing policy"". This problem is floating, so the moment when error start to occur is changes from time to time.\r\n \r\n### What did you expect to see?\r\n*Successful response from the server*\r\n \r\n### What did you see instead?\r\n\r\nThe thing that confuses me a lot is that every ```GRPC_ERROR_NONE``` is treated as load balancing error in GRPC core:\r\n```\r\n    calld->error = error == GRPC_ERROR_NONE\r\n                       ? GRPC_ERROR_CREATE_FROM_STATIC_STRING(\r\n                             ""Call dropped by load balancing policy"")\r\n                       : GRPC_ERROR_CREATE_REFERENCING_FROM_STATIC_STRING(\r\n                             ""Failed to create subchannel"", &error, 1);\r\n```\r\nSee https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/client_channel.c#L1027",area/client channel,dgquintas,"### What version of gRPC and what language are you using?\r\ngRPC 1.6.0 installed by pip\r\n### What operating system (Linux, Windows, \u2026) and version?\r\nCentOS Linux release 7.3.1611 (Core)\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 3.4.5 (default, Nov  9 2016, 16:24:59)\r\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\n \r\n### What did you do?\r\nI have a series of py.test suites that check GRPC API of a particular service. Channel and client for a service is periodically recreated in a such manner:\r\n```python\r\nclass DummyGRPCChannel(object):\r\n    def __init__(self, host, port, root_certificates=None, private_key=None, certificate_chain=None, logger=logging):\r\n        self.host = host\r\n        self.port = port\r\n        self.endpoint = join_host_port(self.host, self.port)\r\n        self.credentials = grpc.ssl_channel_credentials(root_certificates, private_key, certificate_chain)\r\n        self.channel = self._connect(logger=logger)\r\n        self.stubs = dict()\r\n\r\n    def _create_channel(self):\r\n        raise Exception('Not implemented')\r\n\r\n    def _connect(self, logger=logging):\r\n        logger.info('Trying to connect to {}'.format(self.endpoint))\r\n        with step('[PRECONDITION] Waiting for grpc channel at {}'.format(self.endpoint)):\r\n            wait_for_connection(host=self.host, port=self.port)\r\n        return self._create_channel()\r\n\r\n    def create_stub(self, stub_cls):\r\n        _stub = stub_cls(self.channel)\r\n        _stub_name = stub_cls.__name__\r\n        if _stub_name not in self.stubs:\r\n            self.stubs[_stub_name] = list()\r\n        self.stubs[_stub_name].append(_stub)\r\n        return _stub\r\n\r\nclass GRPCChannel(DummyGRPCChannel):\r\n    def _create_channel(self):\r\n        channel = grpc.insecure_channel(\r\n            self.endpoint,\r\n            options=[('grpc.max_message_length', Constants.MAX_MESSAGE_LEN), ('grpc.min_reconnect_backoff_ms', 100)])\r\n        return channel\r\n\r\nclass DummyEndpoint(object):\r\n    def __init__(self, host, port, use_tls=False, tls_creds=None):\r\n        self.host = host\r\n        self.port = port\r\n        self._channel = None\r\n        self.clients = dict()\r\n        self.use_tls = use_tls\r\n        self.tls_creds = tls_creds\r\n\r\nclass Endpoint(DummyEndpoint):\r\n    def _connect(self, logger=logging):\r\n        if self.use_tls:\r\n            self._channel = SSLGRPCChannel(self.host, self.port, logger=logger, **self.tls_creds)\r\n        else:\r\n            self._channel = GRPCChannel(self.host, self.port, logger=logger)\r\n    # Method for create client\r\n    def create_client(self, stub, name=None, logger=logging):\r\n        if self._channel is None:\r\n            self._connect(logger=logger)\r\n        name = name if name is not None else rand_str()\r\n        with step('Creating {} client'.format(stub.__name__)):\r\n            _client = self._channel.create_stub(stub)\r\n            self.clients[name] = _client\r\n            return _client\r\n```\r\nUsing:\r\n```python\r\nfrom component import component_pb2\r\n\r\nservice = Endpoint(HOST, PORT, False)\r\nclient = service.create_client(component_pb2.ComponentStub, name='component', logger=logger)\r\nclient.DoSomething(component_pb2.DoSomethingRequest())\r\n```\r\nRequests to the service are performed in a default manner (according to the grpcio documentation). There are a lot of tests within a single testing session, and after a while, requests start to fail with a strange error: ""Call dropped by load balancing policy"". This problem is floating, so the moment when error start to occur is changes from time to time.\r\n \r\n### What did you expect to see?\r\n*Successful response from the server*\r\n \r\n### What did you see instead?\r\n```python\r\nstate = <grpc._channel._RPCState object at 0x7fd982bfbda0>, call = <grpc._cython.cygrpc.Call object at 0x7fd982bfb9e8>, with_call = False, deadline = None\r\n    def _end_unary_response_blocking(state, call, with_call, deadline):\r\n        if state.code is grpc.StatusCode.OK:\r\n            if with_call:\r\n                rendezvous = _Rendezvous(state, call, None, deadline)\r\n                return state.response, rendezvous\r\n            else:\r\n                return state.response\r\n        else:\r\n>           raise _Rendezvous(state, None, None, deadline)\r\nE           grpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.UNKNOWN, Call dropped by load balancing policy)>\r\n/usr/lib64/python3.4/site-packages/grpc/_channel.py:440: _Rendezvous\r\n```\r\nThe thing that confuses me a lot is that every ```GRPC_ERROR_NONE``` is treated as load balancing error in GRPC core:\r\n```\r\n    calld->error = error == GRPC_ERROR_NONE\r\n                       ? GRPC_ERROR_CREATE_FROM_STATIC_STRING(\r\n                             ""Call dropped by load balancing policy"")\r\n                       : GRPC_ERROR_CREATE_REFERENCING_FROM_STATIC_STRING(\r\n                             ""Failed to create subchannel"", &error, 1);\r\n```\r\nSee https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/client_channel.c#L1027","python\r\nclass DummyGRPCChannel(object):\r\n    def __init__(self, host, port, root_certificates=None, private_key=None, certificate_chain=None, logger=logging):\r\n        self.host = host\r\n        self.port = port\r\n        self.endpoint = join_host_port(self.host, self.port)\r\n        self.credentials = grpc.ssl_channel_credentials(root_certificates, private_key, certificate_chain)\r\n        self.channel = self._connect(logger=logger)\r\n        self.stubs = dict()\r\n\r\n    def _create_channel(self):\r\n        raise Exception('Not implemented')\r\n\r\n    def _connect(self, logger=logging):\r\n        logger.info('Trying to connect to {}'.format(self.endpoint))\r\n        with step('[PRECONDITION] Waiting for grpc channel at {}'.format(self.endpoint)):\r\n            wait_for_connection(host=self.host, port=self.port)\r\n        return self._create_channel()\r\n\r\n    def create_stub(self, stub_cls):\r\n        _stub = stub_cls(self.channel)\r\n        _stub_name = stub_cls.__name__\r\n        if _stub_name not in self.stubs:\r\n            self.stubs[_stub_name] = list()\r\n        self.stubs[_stub_name].append(_stub)\r\n        return _stub\r\n\r\nclass GRPCChannel(DummyGRPCChannel):\r\n    def _create_channel(self):\r\n        channel = grpc.insecure_channel(\r\n            self.endpoint,\r\n            options=[('grpc.max_message_length', Constants.MAX_MESSAGE_LEN), ('grpc.min_reconnect_backoff_ms', 100)])\r\n        return channel\r\n\r\nclass DummyEndpoint(object):\r\n    def __init__(self, host, port, use_tls=False, tls_creds=None):\r\n        self.host = host\r\n        self.port = port\r\n        self._channel = None\r\n        self.clients = dict()\r\n        self.use_tls = use_tls\r\n        self.tls_creds = tls_creds\r\n\r\nclass Endpoint(DummyEndpoint):\r\n    def _connect(self, logger=logging):\r\n        if self.use_tls:\r\n            self._channel = SSLGRPCChannel(self.host, self.port, logger=logger, **self.tls_creds)\r\n        else:\r\n            self._channel = GRPCChannel(self.host, self.port, logger=logger)\r\n    # Method for create client\r\n    def create_client(self, stub, name=None, logger=logging):\r\n        if self._channel is None:\r\n            self._connect(logger=logger)\r\n        name = name if name is not None else rand_str()\r\n        with step('Creating {} client'.format(stub.__name__)):\r\n            _client = self._channel.create_stub(stub)\r\n            self.clients[name] = _client\r\n            return _client\r\n"
12425,"C# native memory leak on Grpc.Core.RpcException throw[Server - C# with gRPC v1.3.6, on Windows]\r\n[Client - Node.JS with gRPC v1.3.8, on Linux]\r\n\r\nScenario description:\r\nThe client opens many gRPC streams with the C# server. On internal errors, the C# server closes the relevant streams by throwing `Grpc.Core.RpcException`, passing the exception description over the `status` field, using encoded base64 stream (due to the lack of ability to pass complex exception data over the `RpcException`):\r\n\r\n\r\n\r\nWhile analyzing the virtual memory of the C# server process, after running for a long time (handling hundreds of thousands of exceptions) I found that many **GBs** (!!) of **native heap buffers** are leaked - and all contain the strings that are indicative to the internal `RpcException` (""client exception!XXXXXX"") - meaning the `RpcException`'s `status` field is leaked.\r\nThose buffers are **not** part of the GC heap - meaning they are native memory segments, and not managed memory.\r\n\r\n@jtattermusch is it related to #11434 ?\r\n\r\nThanks!\r\n",lang/C#,jtattermusch,"[Server - C# with gRPC v1.3.6, on Windows]\r\n[Client - Node.JS with gRPC v1.3.8, on Linux]\r\n\r\nScenario description:\r\nThe client opens many gRPC streams with the C# server. On internal errors, the C# server closes the relevant streams by throwing `Grpc.Core.RpcException`, passing the exception description over the `status` field, using encoded base64 stream (due to the lack of ability to pass complex exception data over the `RpcException`):\r\n\r\n```C#\r\npublic static RpcException GenerateRpcException(MyInternalException ex)\r\n{\r\n    // MyInternalException is a protobuf message - so it is serialized using protobuf's ToByteString()\r\n    var exceptioninfo_base64 = ex.Request.ToByteString().ToBase64();\r\n    var exception_magic = ""client exception"";\r\n    var exception_details = string.Join(""|"", exception_magic, exceptioninfo_base64);\r\n    var status = new Grpc.Core.Status(StatusCode.Unknown, exception_details);\r\n    return new Grpc.Core.RpcException(status);\r\n}\r\n```\r\n\r\nWhile analyzing the virtual memory of the C# server process, after running for a long time (handling hundreds of thousands of exceptions) I found that many **GBs** (!!) of **native heap buffers** are leaked - and all contain the strings that are indicative to the internal `RpcException` (""client exception!XXXXXX"") - meaning the `RpcException`'s `status` field is leaked.\r\nThose buffers are **not** part of the GC heap - meaning they are native memory segments, and not managed memory.\r\n\r\n@jtattermusch is it related to #11434 ?\r\n\r\nThanks!\r\n","C#\r\npublic static RpcException GenerateRpcException(MyInternalException ex)\r\n{\r\n    // MyInternalException is a protobuf message - so it is serialized using protobuf's ToByteString()\r\n    var exceptioninfo_base64 = ex.Request.ToByteString().ToBase64();\r\n    var exception_magic = ""client exception"";\r\n    var exception_details = string.Join(""|"", exception_magic, exceptioninfo_base64);\r\n    var status = new Grpc.Core.Status(StatusCode.Unknown, exception_details);\r\n    return new Grpc.Core.RpcException(status);\r\n}\r\n"
12409,"Critical: gRPC async server crashed on gpr_error_unrefPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes, it is a crash.\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\ngRPC 1.4.2, C++.\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n Linux 3.10, aka. Redhat 7u2, x86_64.\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nGCC 6.2.0.\r\n \r\n### What did you do?\r\n\r\nThere are several async servers utilizing gRPC, and there are internal async requests among each other, via the same group of `CompletionQueue`s. At heavy loads, the internal async requests might timeout.\r\n \r\n### What did you expect to see?\r\n \r\nServers works.\r\n \r\n### What did you see instead?\r\n \r\nServers crashed.\r\n\r\nHere is the backtrace:\r\n\r\n```\r\n#0  0x00002ac9ffc10de7 in abort () from /lib64/libc.so.6\r\n#1  0x00000000010e259b in gpr_unref ()\r\n#2  0x00000000010c15d0 in grpc_error_unref ()\r\n#3  0x00000000010cb58a in ?? ()\r\n#4  0x00000000010c2ad4 in grpc_exec_ctx_flush ()\r\n#5  0x00000000010edb7f in ?? ()\r\n#6  0x00000000010d11bd in ?? ()\r\n#7  0x00000000010a8cf8 in grpc::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) ()\r\n#9  0x0000000000a3edc9 in Next (ok=0x2aca203ff037, tag=0x2aca203ff038, this=0x2ad45f1411b0) at completion_queue.h:151\r\n```\r\n\r\n`??` is due to missing of debuginfo. \r\nFrom disassembling, I found that `grpc_error_unref` was called from `destroy_call`:\r\n\r\n\r\n\r\nHere is the corresponding assembly on frame `#3`:\r\n```\r\n...\r\n   0x00000000010cb51a:  callq  0x10d0a60 <grpc_cq_internal_unref>\r\n   0x00000000010cb51f:  lea    -0x3d6(%rip),%rsi        # 0x10cb150\r\n   0x00000000010cb526:  lea    0x488(%rbx),%rdx\r\n   0x00000000010cb52d:  xor    %ecx,%ecx\r\n   0x00000000010cb52f:  mov    %rbx,%rdi\r\n   0x00000000010cb532:  lea    0x420(%rbx),%rbp\r\n   0x00000000010cb539:  lea    0x448(%rbx),%r13\r\n   0x00000000010cb540:  callq  0x10cb260\r\n   0x00000000010cb545:  xor    %edi,%edi\r\n   0x00000000010cb547:  xor    %r14d,%r14d\r\n   0x00000000010cb54a:  callq  0x10b7a70 <gpr_now>\r\n   0x00000000010cb54f:  mov    0x38(%rbx),%rcx\r\n   0x00000000010cb553:  mov    %rdx,%rsi\r\n   0x00000000010cb556:  mov    0x30(%rbx),%rdx\r\n   0x00000000010cb55a:  mov    %rax,%rdi\r\n   0x00000000010cb55d:  callq  0x10b7620 <gpr_time_sub>\r\n   0x00000000010cb562:  mov    %rax,0x478(%rbx)\r\n   0x00000000010cb569:  mov    %rdx,0x480(%rbx)\r\n   0x00000000010cb570:  mov    0x0(%rbp),%rax\r\n   0x00000000010cb574:  mov    %rax,%rdi\r\n   0x00000000010cb577:  and    $0xfffffffffffffffe,%rdi\r\n   0x00000000010cb57b:  test   $0x1,%al\r\n   0x00000000010cb57d:  cmove  %r14,%rdi\r\n   0x00000000010cb581:  add    $0x8,%rbp\r\n==>0x00000000010cb585:  callq  0x10c15a0 <grpc_error_unref>   # crashed here\r\n   0x00000000010cb58a:  cmp    %rbp,%r13\r\n   0x00000000010cb58d:  jne    0x10cb570\r\n   0x00000000010cb58f:  lea    0xc9459a(%rip),%rax        # 0x1d5fb30 <grpc_schedule_on_exec_ctx>\r\n   0x00000000010cb596:  lea    0x628(%rbx),%rdi\r\n   0x00000000010cb59d:  lea    -0x434(%rip),%rsi        # 0x10cb170\r\n   0x00000000010cb5a4:  mov    %rbx,%rdx\r\n   0x00000000010cb5a7:  mov    (%rax),%rcx\r\n   0x00000000010cb5aa:  callq  0x10e4380 <grpc_closure_init>\r\n   0x00000000010cb5af:  lea    0x668(%rbx),%rsi\r\n...\r\n```\r\n\r\n It seems a reference counting issue, cause' the target counter is being `-1` after unref.\r\n",lang/c++,vjpai,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes, it is a crash.\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\ngRPC 1.4.2, C++.\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n Linux 3.10, aka. Redhat 7u2, x86_64.\r\n\r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nGCC 6.2.0.\r\n \r\n### What did you do?\r\n\r\nThere are several async servers utilizing gRPC, and there are internal async requests among each other, via the same group of `CompletionQueue`s. At heavy loads, the internal async requests might timeout.\r\n \r\n### What did you expect to see?\r\n \r\nServers works.\r\n \r\n### What did you see instead?\r\n \r\nServers crashed.\r\n\r\nHere is the backtrace:\r\n\r\n```\r\n#0  0x00002ac9ffc10de7 in abort () from /lib64/libc.so.6\r\n#1  0x00000000010e259b in gpr_unref ()\r\n#2  0x00000000010c15d0 in grpc_error_unref ()\r\n#3  0x00000000010cb58a in ?? ()\r\n#4  0x00000000010c2ad4 in grpc_exec_ctx_flush ()\r\n#5  0x00000000010edb7f in ?? ()\r\n#6  0x00000000010d11bd in ?? ()\r\n#7  0x00000000010a8cf8 in grpc::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) ()\r\n#9  0x0000000000a3edc9 in Next (ok=0x2aca203ff037, tag=0x2aca203ff038, this=0x2ad45f1411b0) at completion_queue.h:151\r\n```\r\n\r\n`??` is due to missing of debuginfo. \r\nFrom disassembling, I found that `grpc_error_unref` was called from `destroy_call`:\r\n\r\n```c\r\nstatic void destroy_call(grpc_exec_ctx *exec_ctx, void *call,\r\n                         grpc_error *error) {\r\n...\r\n...\r\n...\r\n  if (c->cq) {\r\n    GRPC_CQ_INTERNAL_UNREF(exec_ctx, c->cq, ""bind"");\r\n  }\r\n\r\n  get_final_status(call, set_status_value_directly, &c->final_info.final_status,\r\n                   NULL);\r\n  c->final_info.stats.latency =\r\n      gpr_time_sub(gpr_now(GPR_CLOCK_MONOTONIC), c->start_time);\r\n\r\n  for (i = 0; i < STATUS_SOURCE_COUNT; i++) {\r\n    GRPC_ERROR_UNREF(\r\n        unpack_received_status(gpr_atm_acq_load(&c->status[i])).error);   // crashed here\r\n  }\r\n\r\n  grpc_call_stack_destroy(exec_ctx, CALL_STACK_FROM_CALL(c), &c->final_info,\r\n                          grpc_closure_init(&c->release_call, release_call, c,\r\n                                            grpc_schedule_on_exec_ctx));\r\n  GPR_TIMER_END(""destroy_call"", 0);\r\n}\r\n```\r\n\r\nHere is the corresponding assembly on frame `#3`:\r\n```\r\n...\r\n   0x00000000010cb51a:  callq  0x10d0a60 <grpc_cq_internal_unref>\r\n   0x00000000010cb51f:  lea    -0x3d6(%rip),%rsi        # 0x10cb150\r\n   0x00000000010cb526:  lea    0x488(%rbx),%rdx\r\n   0x00000000010cb52d:  xor    %ecx,%ecx\r\n   0x00000000010cb52f:  mov    %rbx,%rdi\r\n   0x00000000010cb532:  lea    0x420(%rbx),%rbp\r\n   0x00000000010cb539:  lea    0x448(%rbx),%r13\r\n   0x00000000010cb540:  callq  0x10cb260\r\n   0x00000000010cb545:  xor    %edi,%edi\r\n   0x00000000010cb547:  xor    %r14d,%r14d\r\n   0x00000000010cb54a:  callq  0x10b7a70 <gpr_now>\r\n   0x00000000010cb54f:  mov    0x38(%rbx),%rcx\r\n   0x00000000010cb553:  mov    %rdx,%rsi\r\n   0x00000000010cb556:  mov    0x30(%rbx),%rdx\r\n   0x00000000010cb55a:  mov    %rax,%rdi\r\n   0x00000000010cb55d:  callq  0x10b7620 <gpr_time_sub>\r\n   0x00000000010cb562:  mov    %rax,0x478(%rbx)\r\n   0x00000000010cb569:  mov    %rdx,0x480(%rbx)\r\n   0x00000000010cb570:  mov    0x0(%rbp),%rax\r\n   0x00000000010cb574:  mov    %rax,%rdi\r\n   0x00000000010cb577:  and    $0xfffffffffffffffe,%rdi\r\n   0x00000000010cb57b:  test   $0x1,%al\r\n   0x00000000010cb57d:  cmove  %r14,%rdi\r\n   0x00000000010cb581:  add    $0x8,%rbp\r\n==>0x00000000010cb585:  callq  0x10c15a0 <grpc_error_unref>   # crashed here\r\n   0x00000000010cb58a:  cmp    %rbp,%r13\r\n   0x00000000010cb58d:  jne    0x10cb570\r\n   0x00000000010cb58f:  lea    0xc9459a(%rip),%rax        # 0x1d5fb30 <grpc_schedule_on_exec_ctx>\r\n   0x00000000010cb596:  lea    0x628(%rbx),%rdi\r\n   0x00000000010cb59d:  lea    -0x434(%rip),%rsi        # 0x10cb170\r\n   0x00000000010cb5a4:  mov    %rbx,%rdx\r\n   0x00000000010cb5a7:  mov    (%rax),%rcx\r\n   0x00000000010cb5aa:  callq  0x10e4380 <grpc_closure_init>\r\n   0x00000000010cb5af:  lea    0x668(%rbx),%rsi\r\n...\r\n```\r\n\r\n It seems a reference counting issue, cause' the target counter is being `-1` after unref.\r\n","c\r\nstatic void destroy_call(grpc_exec_ctx *exec_ctx, void *call,\r\n                         grpc_error *error) {\r\n...\r\n...\r\n...\r\n  if (c->cq) {\r\n    GRPC_CQ_INTERNAL_UNREF(exec_ctx, c->cq, ""bind"");\r\n  }\r\n\r\n  get_final_status(call, set_status_value_directly, &c->final_info.final_status,\r\n                   NULL);\r\n  c->final_info.stats.latency =\r\n      gpr_time_sub(gpr_now(GPR_CLOCK_MONOTONIC), c->start_time);\r\n\r\n  for (i = 0; i < STATUS_SOURCE_COUNT; i++) {\r\n    GRPC_ERROR_UNREF(\r\n        unpack_received_status(gpr_atm_acq_load(&c->status[i])).error);   // crashed here\r\n  }\r\n\r\n  grpc_call_stack_destroy(exec_ctx, CALL_STACK_FROM_CALL(c), &c->final_info,\r\n                          grpc_closure_init(&c->release_call, release_call, c,\r\n                                            grpc_schedule_on_exec_ctx));\r\n  GPR_TIMER_END(""destroy_call"", 0);\r\n}\r\n"
12355,"C#: Support CancellationToken in IAsyncStreamReader.MoveNext()Using Grpc.Core 1.4.1\r\n\r\nMoveNext does not support cancellation, even though cancellation tokens are accepted as a parameter, which makes it more difficult than necessary to gracefully exit a streaming call on server shutdown:\r\n\r\n\r\n\r\nIn order to handle cancellation, I need a wrapper like:\r\n\r\n\r\n",lang/C#,jtattermusch,"Using Grpc.Core 1.4.1\r\n\r\nMoveNext does not support cancellation, even though cancellation tokens are accepted as a parameter, which makes it more difficult than necessary to gracefully exit a streaming call on server shutdown:\r\n\r\n```c#\r\npublic override async Task EventStream(IAsyncStreamReader<StreamingMessage> requestStream, IServerStreamWriter<StreamingMessage> responseStream, ServerCallContext context)\r\n{\r\n  // immediately throws InvalidOperationException 'Cancellation of individual reads is not supported.'\r\n  while(await requestStream.MoveNext(context.CancellationToken))) {\r\n  ...\r\n  } \r\n}\r\n```\r\n\r\nIn order to handle cancellation, I need a wrapper like:\r\n```c#\r\nvar cancelSource = new TaskCompletionSource<bool>();\r\ncontext.CancellationToken.Register(() => cancelSource.SetResult(false));\r\n\r\nFunc<Task<bool>> messageAvailable = async () => {\r\n  var requestTask = requestStream.MoveNext(CancellationToken.None);\r\n  var completed = await Task.WhenAny(cancelSource.Task, requestTask);\r\n  return completed.Result;\r\n}\r\n\r\nwhile(await messageAvailable()) {\r\n...\r\n}\r\n// ensure not leaking tasks\r\ncancelSource.TrySetResult(false);\r\n```\r\n\r\n","c#\r\npublic override async Task EventStream(IAsyncStreamReader<StreamingMessage> requestStream, IServerStreamWriter<StreamingMessage> responseStream, ServerCallContext context)\r\n{\r\n  // immediately throws InvalidOperationException 'Cancellation of individual reads is not supported.'\r\n  while(await requestStream.MoveNext(context.CancellationToken))) {\r\n  ...\r\n  } \r\n}\r\n"
12332,"Python GRPC call hangs if retried after `_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)` raisedPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\nYes. \r\n\r\n### What version of gRPC and what language are you using?\r\ngrpcio 1.3.5 (Python) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nUbuntu 16.04 \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 2.7.12 \r\n \r\n### What did you do?\r\nI have two services, a message generator and a processor, that run in the same Kubernetes pod.\r\n\r\nOn the generator side, there's code that looks almost identical to the following:\r\n\r\n\r\nIf the generator calls `queue_message_for_processing` before the processor starts listening, GRPC will raise `_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)`.\r\n\r\nThe decorator `_grpc_retriable` will then retry the call.\r\n\r\n### What did you expect to see?\r\nThe retried call should have worked once the generator was initialised.\r\n \r\n### What did you see instead?\r\n The call to `queue_message_for_processing` now *hangs*.\r\n\r\nThe stack trace looks like this:\r\n```\r\n(gdb) py-bt\r\nTraceback (most recent call first):\r\n  File ""/usr/local/lib/python2.7/dist-packages/grpc/_channel.py"", line 500, in _blocking\r\n    _handle_event(completion_queue.poll(), state,\r\n  File ""/usr/local/lib/python2.7/dist-packages/grpc/_channel.py"", line 506, in __call__\r\n    credentials)\r\n  File ""/mycode/processor_proxy.py"", line 40, in queue_message_for_processing\r\n    self._stub.queueMessageForProcessing(data)\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",lang/Python|disposition/requires reporter action,mehrdada,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\nYes. \r\n\r\n### What version of gRPC and what language are you using?\r\ngrpcio 1.3.5 (Python) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nUbuntu 16.04 \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nPython 2.7.12 \r\n \r\n### What did you do?\r\nI have two services, a message generator and a processor, that run in the same Kubernetes pod.\r\n\r\nOn the generator side, there's code that looks almost identical to the following:\r\n```python\r\ndef _grpc_retriable(f):\r\n    @wraps(f)\r\n    def wrapper(*args, **kwargs):\r\n        delay = 1\r\n        while True:\r\n            try:\r\n                return f(*args, **kwargs)\r\n            except grpc.RpcError as e:\r\n                reraise = True\r\n\r\n                # Retry if we should\r\n                if hasattr(e, 'code'):\r\n                    code = e.code()\r\n                    if code == grpc.StatusCode.UNAVAILABLE:\r\n                        if delay > 30:  # give up and re-raise\r\n                            my_logger.error(""Got GRPC retriable exception too many times, giving up: %s"" % repr(e))\r\n                        else:\r\n                            my_logger.info(""Got GRPC retriable exception, will retry in %ds: %s"" % (delay, repr(e)))\r\n                            time.sleep(delay)\r\n                            delay += delay\r\n                            reraise = False\r\n\r\n                if reraise:\r\n                    raise\r\n\r\n    return wrapper\r\n\r\n\r\nclass MyGRPCProxy(object):\r\n    def __init__(self):\r\n        max_message_size = my_global_cfg['MAX_MESSAGE_SIZE']\r\n        server_address = my_global_cfg['SERVER_ADDRESS']\r\n\r\n        channel = grpc.insecure_channel(server_address,\r\n                                        options=[('grpc.max_send_message_length', max_message_size)])\r\n        self._stub = processor_pb2_grpc.ProcessorStub(channel)\r\n\r\n    @_grpc_retriable\r\n    def queue_message_for_processing(self, request_id, message_data):\r\n        data = processor_pb2.MessageData(requestId=request_id,\r\n                                         messageData=message_data)\r\n        self._stub.queueMessageForProcessing(data)\r\n```\r\n\r\nIf the generator calls `queue_message_for_processing` before the processor starts listening, GRPC will raise `_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Connect Failed)`.\r\n\r\nThe decorator `_grpc_retriable` will then retry the call.\r\n\r\n### What did you expect to see?\r\nThe retried call should have worked once the generator was initialised.\r\n \r\n### What did you see instead?\r\n The call to `queue_message_for_processing` now *hangs*.\r\n\r\nThe stack trace looks like this:\r\n```\r\n(gdb) py-bt\r\nTraceback (most recent call first):\r\n  File ""/usr/local/lib/python2.7/dist-packages/grpc/_channel.py"", line 500, in _blocking\r\n    _handle_event(completion_queue.poll(), state,\r\n  File ""/usr/local/lib/python2.7/dist-packages/grpc/_channel.py"", line 506, in __call__\r\n    credentials)\r\n  File ""/mycode/processor_proxy.py"", line 40, in queue_message_for_processing\r\n    self._stub.queueMessageForProcessing(data)\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","python\r\ndef _grpc_retriable(f):\r\n    @wraps(f)\r\n    def wrapper(*args, **kwargs):\r\n        delay = 1\r\n        while True:\r\n            try:\r\n                return f(*args, **kwargs)\r\n            except grpc.RpcError as e:\r\n                reraise = True\r\n\r\n                # Retry if we should\r\n                if hasattr(e, 'code'):\r\n                    code = e.code()\r\n                    if code == grpc.StatusCode.UNAVAILABLE:\r\n                        if delay > 30:  # give up and re-raise\r\n                            my_logger.error(""Got GRPC retriable exception too many times, giving up: %s"" % repr(e))\r\n                        else:\r\n                            my_logger.info(""Got GRPC retriable exception, will retry in %ds: %s"" % (delay, repr(e)))\r\n                            time.sleep(delay)\r\n                            delay += delay\r\n                            reraise = False\r\n\r\n                if reraise:\r\n                    raise\r\n\r\n    return wrapper\r\n\r\n\r\nclass MyGRPCProxy(object):\r\n    def __init__(self):\r\n        max_message_size = my_global_cfg['MAX_MESSAGE_SIZE']\r\n        server_address = my_global_cfg['SERVER_ADDRESS']\r\n\r\n        channel = grpc.insecure_channel(server_address,\r\n                                        options=[('grpc.max_send_message_length', max_message_size)])\r\n        self._stub = processor_pb2_grpc.ProcessorStub(channel)\r\n\r\n    @_grpc_retriable\r\n    def queue_message_for_processing(self, request_id, message_data):\r\n        data = processor_pb2.MessageData(requestId=request_id,\r\n                                         messageData=message_data)\r\n        self._stub.queueMessageForProcessing(data)\r\n"
12330,"grpclb in C core doesn't seem to work as expected.Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n Yes\r\n\r\n### What version of gRPC and what language are you using? \r\n gRPC version: v1.6.0 (I also tested *master*)\r\n gRPC-go version: v1.5.2\r\n language: client (C++), server (Go), grpclb (Go)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n macOS Sierra 10.12\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n C++:\r\n    Apple LLVM version 8.1.0 (clang-802.0.38)\r\n    Target: x86_64-apple-darwin16.0.0\r\n    Thread model: posix\r\n Go:\r\n    1.9\r\n Environment variables:\r\n    GRPC_DNS_RESOLVER=ares\r\n    GRPC_VERBOSITY=DEBUG\r\n    GRPC_TRACE=glb\r\n\r\n### What did you do?\r\n\r\n0. Simply modify [grpc/example/cpp/helloworld/greeter_client.cc](https://github.com/grpc/grpc/blob/master/examples/cpp/helloworld/greeter_client.cc)\r\n\r\n\r\n1. Setup DNS and Load Balancer:\r\n\r\nFollowing the [grpclb in dns proposal](https://github.com/grpc/proposal/blob/master/A5-grpclb-in-dns.md), I setup a DNS service with [SkyDNS](https://github.com/skynetservices/skydns), and a simple service discovery with [etcd](https://github.com/coreos/etcd)\r\n\r\nURL for test server and LB are:\r\na) example.52tt.local, which is a backend server implements [examples/protos/helloworld.proto](https://github.com/grpc/grpc/blob/master/examples/protos/helloworld.proto)\r\nb) lb.52tt.local, which is a gRPC load balancer implements [grpc/lb/v1/load_balancer.proto](https://github.com/grpc/grpc/blob/master/src/proto/grpc/lb/v1/load_balancer.proto).\r\n\r\nLB accepts *BalanceLoad* request, resolves name to backend addresses (from etcd, watches the updates, and reply a server list to client (In my testcase, I send server list to client every 15 seconds).\r\n\r\n\r\n\r\n> Note: the LB always send all registered backend addresses to client.\r\n\r\n\r\nAnd refer to the source code in [grpclb.c](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c), the client will create subchannels to each backends and pickup one from them with *RoundRobin* policy to call each RPC.\r\n\r\nBelow is the my skydns config and the result of dns lookup:\r\n> \uf8ff ~/ etcdctl  get --prefix /skydns\r\n```\r\n/skydns/local/52tt/example/_tcp/_grpclb\r\n{""host"":""lb.52tt.local"",""port"":9999}\r\n/skydns/local/52tt/lb\r\n{""host"": ""192.168.8.80"", ""port"": 9999}\r\n```\r\n> \uf8ff ~/ dig SRV _grpclb._tcp.example.52tt.local\r\n```\r\n; <<>> DiG 9.8.3-P1 <<>> SRV _grpclb._tcp.example.52tt.local\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 53641\r\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; QUESTION SECTION:\r\n;_grpclb._tcp.example.52tt.local. IN\tSRV\r\n\r\n;; ANSWER SECTION:\r\n_grpclb._tcp.example.52tt.local. 3600 IN SRV\t10 100 9999 lb.52tt.local.\r\n\r\n;; ADDITIONAL SECTION:\r\nlb.52tt.local.\t\t3600\tIN\tA\t192.168.8.80\r\n```\r\n2. Start 3 backends;\r\n\r\nThen I start 3 *example server*s and register them to etcd.\r\n```\r\nA: 192.168.8.80:50051\r\nB: 192.168.8.80:50052\r\nC: 192.168.8.80:50053\r\n```\r\n\r\n> \uf8ff ~/ etcdctl get --prefix example.52tt.local\r\n```\r\nexample.52tt.local/192.168.8.80:50051\r\n{""Op"":0,""Addr"":""192.168.8.80:50051"",""Metadata"":null}\r\nexample.52tt.local/192.168.8.80:50052\r\n{""Op"":0,""Addr"":""192.168.8.80:50052"",""Metadata"":null}\r\nexample.52tt.local/192.168.8.80:50053\r\n{""Op"":0,""Addr"":""192.168.8.80:50053"",""Metadata"":null}\r\n```\r\n\r\nLB is now able to resolve *example.52tt.local* to 3 backend addresses, and send them to client periodically.\r\n\r\n3. Run an instance of greeter_client;\r\nIn this situation, grpclb and RR work as expected, all the are requests well-distributed to all three backends.\r\n\r\n```\r\nI0830 17:23:41.232431000 123145311817728 grpclb.c:1449]                Query for backends (grpclb: 0x7fdad3500980, lb_channel: 0x7fdad3501c00, lb_call: 0x7fdad5000b58)\r\nI0830 17:23:41.232972000 123145311817728 grpclb.c:1560]                received initial LB response message; client load reporting interval = 60.000000000 sec\r\nI0830 17:23:41.233001000 123145311817728 grpclb.c:1585]                Serverlist with 3 servers received\r\nI0830 17:23:41.233008000 123145311817728 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 17:23:41.233013000 123145311817728 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50053\r\nI0830 17:23:41.233016000 123145311817728 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50052\r\nI0830 17:23:41.233093000 123145311817728 grpclb.c:588]                 Setting grpclb's state to IDLE from new RR policy 0x7fdad3600630 state.\r\n```\r\n\r\n4. Start a new backend *D*: 192.168.8.80:50054;\r\n5. Restart backend *C*;\r\n6. Restart backend *B*;\r\n7. Restart backend *A*.\r\n\r\n### What did you expect to see?\r\n \r\n4. Start a new backend *D*: 192.168.8.80:50054\r\nNew backend will register to LB, and LB send new server list to client.\r\nClient creates new RR policy, and pending 1/4 requets should be sent to backend *D*.\r\n\r\n5. Restart backend *C*;\r\nAfter *C* stops, clients should send requests to A, B, D (1/3 requests to each).\r\nAfter *C* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n6. Restart backend *B*;\r\nAfter *B* stops, clients should send requests to A, C, D (1/3 requests to each).\r\nAfter *B* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n7. Restart backend *A*.\r\nAfter *A* stops, clients should send requests to B, C, D  (1/3 requests to each).\r\nAfter *B* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n### What did you see instead?\r\n\r\n4. Start a new backend *D*: 192.168.8.80:50054\r\nLB receive the update from etcd, and send new server list (including A,B,C,D) to the client,\r\nBUT **No request is sent to *D*.**\r\nIndeed, [the grpclb policy is expected to keep listening the change of server list.](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1632)\r\n\r\nBut according the trace logs, I didn't see *glb_policy* receive any server list responses.\r\n\r\n5. Restart backend *C*.\r\nOnce I stop *C*, one RPC fails with code 14: Endpoint read failed, but all the pending requests are well-distributed to the other *A* and *B*. \r\nBUT after I restart *C*, **No request is sent to *C*.**\r\n\r\n6. Restart backend *B*:\r\nOnce I stop *B*, one RPC fails with code 14: Endpoint read failed, but all the pending requests are all sent to *A*.\r\nBUT after I restart *B*, **No request is sent to *B*.**\r\n\r\n**By now, there are 4 backends (A, B, C, D) running and well registered, but only backend *A* receives all the requests.**\r\n\r\n7. Restart backend *A*:\r\nOnce I restart *A*,  one RPC fails with code 14: Endpoint read failed, the RR policy becoming unavailable and client suddenly receives all the *BalanceLoadResponse* with *ServerList* sent from LB server, and then all the 4 backends begin to handle request as expected.\r\n```\r\nD0830 18:01:10.169154000 123145534410752 grpclb.c:1212]                No RR policy in grpclb instance 0x7fc16df00910. Adding to grpclb's pending picks\r\nI0830 18:01:10.169265000 123145534410752 grpclb.c:1585]                Serverlist with 3 servers received\r\nI0830 18:01:10.169274000 123145534410752 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 18:01:10.169279000 123145534410752 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50052\r\nI0830 18:01:10.169282000 123145534410752 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50053\r\nI0830 18:01:10.169286000 123145534410752 grpclb.c:1601]                Incoming server list identical to current, ignoring.\r\nI0830 18:01:10.169298000 123145534410752 grpclb.c:1585]                Serverlist with 4 servers received\r\nI0830 18:01:10.169303000 123145534410752 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 18:01:10.169309000 123145534410752 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50052\r\nI0830 18:01:10.169313000 123145534410752 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50053\r\nI0830 18:01:10.169338000 123145534410752 grpclb.c:1592]                Serverlist[3]: [::ffff:192.168.8.80]:50054\r\nI0830 18:01:10.169458000 123145534410752 grpclb.c:588]                 Setting grpclb's state to IDLE from new RR policy 0x7fc16dd01fb0 state.\r\nI0830 18:01:10.169630000 123145534410752 grpclb.c:753]                 Pending pick about to (async) PICK from 0x7fc16dd01fb0\r\nD0830 18:01:10.169638000 123145534410752 grpclb.c:792]                 Created new Round Robin policy (0x7fc16dd01fb0)\r\nI0830 18:01:10.169650000 123145534410752 grpclb.c:588]                 Setting grpclb's state to CONNECTING from new RR policy 0x7fc16dd01fb0 state.\r\n............\r\n```\r\n \r\nSo, as I have observed, all the new started backends (including restart) can be properly registered to LB, but the client (glb_policy) **can not** get the new server list **UNTIL** I stop the last backend.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n",area/client channel,dgquintas,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n Yes\r\n\r\n### What version of gRPC and what language are you using? \r\n gRPC version: v1.6.0 (I also tested *master*)\r\n gRPC-go version: v1.5.2\r\n language: client (C++), server (Go), grpclb (Go)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n macOS Sierra 10.12\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n C++:\r\n    Apple LLVM version 8.1.0 (clang-802.0.38)\r\n    Target: x86_64-apple-darwin16.0.0\r\n    Thread model: posix\r\n Go:\r\n    1.9\r\n Environment variables:\r\n    GRPC_DNS_RESOLVER=ares\r\n    GRPC_VERBOSITY=DEBUG\r\n    GRPC_TRACE=glb\r\n\r\n### What did you do?\r\n\r\n0. Simply modify [grpc/example/cpp/helloworld/greeter_client.cc](https://github.com/grpc/grpc/blob/master/examples/cpp/helloworld/greeter_client.cc)\r\n```cpp\r\nint main(int argc, char** argv) {\r\n  grpc::ChannelArguments arg;\r\n  arg.SetLoadBalancingPolicyName(""grpclb"");\r\n  const char *target = ""example.52tt.local"";\r\n  GreeterClient greeter(grpc::CreateCustomChannel(\r\n    target, grpc::InsecureChannelCredentials(), arg));\r\n\r\n  std::mutex m;\r\n  int i = 0;\r\n  auto block = [&] () {\r\n    i++;\r\n    char name[128] = { 0 };\r\n    snprintf(name, sizeof(name), ""world %d"", i);\r\n    std::string user(name);\r\n    std::string reply = greeter.SayHello(user);\r\n\r\n    std::lock_guard<std::mutex> guard(m);\r\n    std::cout << ""Greeter received: "" << reply << std::endl;\r\n  };\r\n\r\n  while (true) {\r\n    std::thread t1(block);\r\n    t1.join();\r\n\r\n    sleep(3);\r\n  }\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n1. Setup DNS and Load Balancer:\r\n\r\nFollowing the [grpclb in dns proposal](https://github.com/grpc/proposal/blob/master/A5-grpclb-in-dns.md), I setup a DNS service with [SkyDNS](https://github.com/skynetservices/skydns), and a simple service discovery with [etcd](https://github.com/coreos/etcd)\r\n\r\nURL for test server and LB are:\r\na) example.52tt.local, which is a backend server implements [examples/protos/helloworld.proto](https://github.com/grpc/grpc/blob/master/examples/protos/helloworld.proto)\r\nb) lb.52tt.local, which is a gRPC load balancer implements [grpc/lb/v1/load_balancer.proto](https://github.com/grpc/grpc/blob/master/src/proto/grpc/lb/v1/load_balancer.proto).\r\n\r\nLB accepts *BalanceLoad* request, resolves name to backend addresses (from etcd, watches the updates, and reply a server list to client (In my testcase, I send server list to client every 15 seconds).\r\n\r\n```go\r\nfunc (b *balancer) BalanceLoad(s lbpb.LoadBalancer_BalanceLoadServer) error {\r\n\treq, err := s.Recv()\r\n\tif err != nil {\r\n\t\tlog.Errorln(""[balancer] Unable to recv InitialRequest:"", err)\r\n\t\treturn err\r\n\t}\r\n\r\n\tinitReq := req.GetInitialRequest()\r\n\tif initReq == nil {\r\n\t\tlog.Errorln(""[balancer] Bad stream, first request must be InitialRequest"")\r\n\t\treturn nil\r\n\t}\r\n\r\n\ttarget := initReq.GetName()\r\n\tresp := &lbpb.LoadBalanceResponse{\r\n\t\tLoadBalanceResponseType: &lbpb.LoadBalanceResponse_InitialResponse{\r\n\t\t\tInitialResponse: &lbpb.InitialLoadBalanceResponse{\r\n\t\t\t\tLoadBalancerDelegate: """",\r\n\t\t\t\tClientStatsReportInterval: &lbpb.Duration{\r\n\t\t\t\t\tSeconds: int64(b.statsDura.Seconds()),\r\n\t\t\t\t\tNanos:   int32(b.statsDura.Nanoseconds() - int64(b.statsDura.Seconds())*1e9),\r\n\t\t\t\t},\r\n\t\t\t},\r\n\t\t},\r\n\t}\r\n\r\n\tif err = s.Send(resp); err != nil {\r\n\t\tlog.Errorf(""[balancer] Unable to send InitialResponse: %v, err %v"", resp, err)\r\n\t\treturn err\r\n\t}\r\n\r\n\tsw, err := b.resolve(target)\r\n\tif err != nil {\r\n\t\tlog.Errorf(""[balancer] Unable to send InitialResponse: %v, err %v"", resp, err)\r\n\t\treturn err\r\n\t}\r\n\r\n\t// client stats report\r\n\tgo b.handleClientStatsReports(s)\r\n\r\n\tif sls := sw.copyServerList(); len(sls) > 0 {\r\n\t\terr := b.sendServerList(s, sls)\r\n\t\tif err != nil {\r\n\t\t\tlog.Errorf(""[balancer] Unable to send server list: %v"", err)\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tlog.Debugf(""[balancer] Sent server list, target: %s, sls: %v"", target, sls)\r\n\t} else {\r\n\t\tlog.Warnf(""[balancer] target %s, no available endpoints"", target)\r\n\t}\r\n\r\n\tticker := time.NewTicker(time.Second * 15)\r\nLoop:\r\n\tfor {\r\n\t\tselect {\r\n\t\tcase <-s.Context().Done():\r\n\t\t\tlog.Debugf(""[balancer] Context done: %v"", s.Context().Err())\r\n\t\t\tbreak Loop\r\n\t\tcase <-ticker.C:\r\n\t\t\tif sls := sw.copyServerList(); len(sls) > 0 {\r\n\t\t\t\terr := b.sendServerList(s, sls)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tlog.Errorf(""[balancer] Failed to send server list: %v"", err)\r\n\t\t\t\t\treturn err\r\n\t\t\t\t}\r\n\t\t\t\tlog.Debugf(""[balancer] Sent server list, target %s, sls: %v"", target, sls)\r\n\t\t\t} else {\r\n\t\t\t\tlog.Warnf(""[balancer] target %s, no available endpoints"", target)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n```\r\n\r\n> Note: the LB always send all registered backend addresses to client.\r\n\r\n\r\nAnd refer to the source code in [grpclb.c](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c), the client will create subchannels to each backends and pickup one from them with *RoundRobin* policy to call each RPC.\r\n\r\nBelow is the my skydns config and the result of dns lookup:\r\n> \uf8ff ~/ etcdctl  get --prefix /skydns\r\n```\r\n/skydns/local/52tt/example/_tcp/_grpclb\r\n{""host"":""lb.52tt.local"",""port"":9999}\r\n/skydns/local/52tt/lb\r\n{""host"": ""192.168.8.80"", ""port"": 9999}\r\n```\r\n> \uf8ff ~/ dig SRV _grpclb._tcp.example.52tt.local\r\n```\r\n; <<>> DiG 9.8.3-P1 <<>> SRV _grpclb._tcp.example.52tt.local\r\n;; global options: +cmd\r\n;; Got answer:\r\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 53641\r\n;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\r\n\r\n;; QUESTION SECTION:\r\n;_grpclb._tcp.example.52tt.local. IN\tSRV\r\n\r\n;; ANSWER SECTION:\r\n_grpclb._tcp.example.52tt.local. 3600 IN SRV\t10 100 9999 lb.52tt.local.\r\n\r\n;; ADDITIONAL SECTION:\r\nlb.52tt.local.\t\t3600\tIN\tA\t192.168.8.80\r\n```\r\n2. Start 3 backends;\r\n\r\nThen I start 3 *example server*s and register them to etcd.\r\n```\r\nA: 192.168.8.80:50051\r\nB: 192.168.8.80:50052\r\nC: 192.168.8.80:50053\r\n```\r\n\r\n> \uf8ff ~/ etcdctl get --prefix example.52tt.local\r\n```\r\nexample.52tt.local/192.168.8.80:50051\r\n{""Op"":0,""Addr"":""192.168.8.80:50051"",""Metadata"":null}\r\nexample.52tt.local/192.168.8.80:50052\r\n{""Op"":0,""Addr"":""192.168.8.80:50052"",""Metadata"":null}\r\nexample.52tt.local/192.168.8.80:50053\r\n{""Op"":0,""Addr"":""192.168.8.80:50053"",""Metadata"":null}\r\n```\r\n\r\nLB is now able to resolve *example.52tt.local* to 3 backend addresses, and send them to client periodically.\r\n\r\n3. Run an instance of greeter_client;\r\nIn this situation, grpclb and RR work as expected, all the are requests well-distributed to all three backends.\r\n\r\n```\r\nI0830 17:23:41.232431000 123145311817728 grpclb.c:1449]                Query for backends (grpclb: 0x7fdad3500980, lb_channel: 0x7fdad3501c00, lb_call: 0x7fdad5000b58)\r\nI0830 17:23:41.232972000 123145311817728 grpclb.c:1560]                received initial LB response message; client load reporting interval = 60.000000000 sec\r\nI0830 17:23:41.233001000 123145311817728 grpclb.c:1585]                Serverlist with 3 servers received\r\nI0830 17:23:41.233008000 123145311817728 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 17:23:41.233013000 123145311817728 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50053\r\nI0830 17:23:41.233016000 123145311817728 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50052\r\nI0830 17:23:41.233093000 123145311817728 grpclb.c:588]                 Setting grpclb's state to IDLE from new RR policy 0x7fdad3600630 state.\r\n```\r\n\r\n4. Start a new backend *D*: 192.168.8.80:50054;\r\n5. Restart backend *C*;\r\n6. Restart backend *B*;\r\n7. Restart backend *A*.\r\n\r\n### What did you expect to see?\r\n \r\n4. Start a new backend *D*: 192.168.8.80:50054\r\nNew backend will register to LB, and LB send new server list to client.\r\nClient creates new RR policy, and pending 1/4 requets should be sent to backend *D*.\r\n\r\n5. Restart backend *C*;\r\nAfter *C* stops, clients should send requests to A, B, D (1/3 requests to each).\r\nAfter *C* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n6. Restart backend *B*;\r\nAfter *B* stops, clients should send requests to A, C, D (1/3 requests to each).\r\nAfter *B* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n7. Restart backend *A*.\r\nAfter *A* stops, clients should send requests to B, C, D  (1/3 requests to each).\r\nAfter *B* restarts, clients should send requests to A, B, C, D (1/4 requests to each).\r\n\r\n### What did you see instead?\r\n\r\n4. Start a new backend *D*: 192.168.8.80:50054\r\nLB receive the update from etcd, and send new server list (including A,B,C,D) to the client,\r\nBUT **No request is sent to *D*.**\r\nIndeed, [the grpclb policy is expected to keep listening the change of server list.](https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.c#L1632)\r\n```C\r\nif (!glb_policy->shutting_down) {\r\n      /* keep listening for serverlist updates */\r\n      op->op = GRPC_OP_RECV_MESSAGE;\r\n      op->data.recv_message.recv_message = &glb_policy->lb_response_payload;\r\n      op->flags = 0;\r\n      op->reserved = NULL;\r\n      op++;\r\n      /* reuse the ""lb_on_response_received_locked"" weak ref taken in\r\n       * query_for_backends_locked() */\r\n      const grpc_call_error call_error = grpc_call_start_batch_and_execute(\r\n          exec_ctx, glb_policy->lb_call, ops, (size_t)(op - ops),\r\n          &glb_policy->lb_on_response_received); /* loop */\r\n      GPR_ASSERT(GRPC_CALL_OK == call_error);\r\n    }\r\n```\r\nBut according the trace logs, I didn't see *glb_policy* receive any server list responses.\r\n\r\n5. Restart backend *C*.\r\nOnce I stop *C*, one RPC fails with code 14: Endpoint read failed, but all the pending requests are well-distributed to the other *A* and *B*. \r\nBUT after I restart *C*, **No request is sent to *C*.**\r\n\r\n6. Restart backend *B*:\r\nOnce I stop *B*, one RPC fails with code 14: Endpoint read failed, but all the pending requests are all sent to *A*.\r\nBUT after I restart *B*, **No request is sent to *B*.**\r\n\r\n**By now, there are 4 backends (A, B, C, D) running and well registered, but only backend *A* receives all the requests.**\r\n\r\n7. Restart backend *A*:\r\nOnce I restart *A*,  one RPC fails with code 14: Endpoint read failed, the RR policy becoming unavailable and client suddenly receives all the *BalanceLoadResponse* with *ServerList* sent from LB server, and then all the 4 backends begin to handle request as expected.\r\n```\r\nD0830 18:01:10.169154000 123145534410752 grpclb.c:1212]                No RR policy in grpclb instance 0x7fc16df00910. Adding to grpclb's pending picks\r\nI0830 18:01:10.169265000 123145534410752 grpclb.c:1585]                Serverlist with 3 servers received\r\nI0830 18:01:10.169274000 123145534410752 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 18:01:10.169279000 123145534410752 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50052\r\nI0830 18:01:10.169282000 123145534410752 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50053\r\nI0830 18:01:10.169286000 123145534410752 grpclb.c:1601]                Incoming server list identical to current, ignoring.\r\nI0830 18:01:10.169298000 123145534410752 grpclb.c:1585]                Serverlist with 4 servers received\r\nI0830 18:01:10.169303000 123145534410752 grpclb.c:1592]                Serverlist[0]: [::ffff:192.168.8.80]:50051\r\nI0830 18:01:10.169309000 123145534410752 grpclb.c:1592]                Serverlist[1]: [::ffff:192.168.8.80]:50052\r\nI0830 18:01:10.169313000 123145534410752 grpclb.c:1592]                Serverlist[2]: [::ffff:192.168.8.80]:50053\r\nI0830 18:01:10.169338000 123145534410752 grpclb.c:1592]                Serverlist[3]: [::ffff:192.168.8.80]:50054\r\nI0830 18:01:10.169458000 123145534410752 grpclb.c:588]                 Setting grpclb's state to IDLE from new RR policy 0x7fc16dd01fb0 state.\r\nI0830 18:01:10.169630000 123145534410752 grpclb.c:753]                 Pending pick about to (async) PICK from 0x7fc16dd01fb0\r\nD0830 18:01:10.169638000 123145534410752 grpclb.c:792]                 Created new Round Robin policy (0x7fc16dd01fb0)\r\nI0830 18:01:10.169650000 123145534410752 grpclb.c:588]                 Setting grpclb's state to CONNECTING from new RR policy 0x7fc16dd01fb0 state.\r\n............\r\n```\r\n \r\nSo, as I have observed, all the new started backends (including restart) can be properly registered to LB, but the client (glb_policy) **can not** get the new server list **UNTIL** I stop the last backend.\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n","cpp\r\nint main(int argc, char** argv) {\r\n  grpc::ChannelArguments arg;\r\n  arg.SetLoadBalancingPolicyName(""grpclb"");\r\n  const char *target = ""example.52tt.local"";\r\n  GreeterClient greeter(grpc::CreateCustomChannel(\r\n    target, grpc::InsecureChannelCredentials(), arg));\r\n\r\n  std::mutex m;\r\n  int i = 0;\r\n  auto block = [&] () {\r\n    i++;\r\n    char name[128] = { 0 };\r\n    snprintf(name, sizeof(name), ""world %d"", i);\r\n    std::string user(name);\r\n    std::string reply = greeter.SayHello(user);\r\n\r\n    std::lock_guard<std::mutex> guard(m);\r\n    std::cout << ""Greeter received: "" << reply << std::endl;\r\n  };\r\n\r\n  while (true) {\r\n    std::thread t1(block);\r\n    t1.join();\r\n\r\n    sleep(3);\r\n  }\r\n\r\n  return 0;\r\n}\r\n"
12250,Handshake read failed security_handshaker.c:126 (php client)Hey..\r\nTrying to communicate with my service with php client (same service works for the golang client)\r\nCan't make it work.. already tried exporting the cipher suites \r\n```\r\nexport GRPC_SSL_CIPHER_SUITES=ECDHE-ECDSA-AES256-GCM-SHA384\r\n```\r\n\r\n\r\n\r\n\r\n\r\n,lang/php|priority/P1,jboeuf|stanley-cheung|ZhouyihaiDing,"Hey..\r\nTrying to communicate with my service with php client (same service works for the golang client)\r\nCan't make it work.. already tried exporting the cipher suites \r\n```\r\nexport GRPC_SSL_CIPHER_SUITES=ECDHE-ECDSA-AES256-GCM-SHA384\r\n```\r\n\r\n\r\n```php\r\nuire dirname(__FILE__).'/vendor/autoload.php';\r\n $client = new \\Productpb\\ProductServiceClient('localhost:3051', [\r\n     'credentials' => Grpc\\ChannelCredentials::createSsl(file_get_contents(dirname(__FILE__).'/client.com.pem'))\r\n ]);\r\n$request = new Productpb\\GetRequest();\r\n$request->SetId(""123"");\r\nvar_dump($client->Get($request)->wait());\r\n```\r\n\r\n```bash\r\nI0822 15:05:47.946677000    2059 ev_epollsig_linux.c:90]     epoll engine will be using signal: 40\r\nD0822 15:05:47.947273600    2059 ev_posix.c:121]             Using polling engine: epollsig\r\nD0822 15:05:47.947675000    2059 dns_resolver.c:316]         Using native dns resolver\r\nI0822 15:05:48.041374600    2061 ssl_transport_security.c:184]      HANDSHAKE START -  before connect initialization  - CINIT\r\nI0822 15:05:48.041807600    2061 ssl_transport_security.c:184]                 LOOP -  before connect initialization  - CINIT\r\nI0822 15:05:48.042218500    2061 ssl_transport_security.c:184]                 LOOP -     SSLv3 write client hello A  - 3WCH_A\r\nI0822 15:05:48.042570400    2061 ssl_transport_security.c:184]                 LOOP -               SSLv3 flush data  - 3FLUSH\r\nD0822 15:06:08.043073400    2060 security_handshaker.c:126]  Security handshake failed: {""created"":""@1503403568.042903200"",""description"":""Handshake read failed"",""file"":""/tmp/pear/temp/grpc/src/core/lib/security/transport/security_handshaker.c"",""file_line"":304,""referenced_errors"":[{""created"":""@1503403568.042655500"",""description"":""FD Shutdown"",""file"":""/tmp/pear/temp/grpc/src/core/lib/iomgr/lockfree_event.c"",""file_line"":197,""referenced_errors"":[{""created"":""@1503403568.042643600"",""description"":""Handshake timed out"",""file"":""/tmp/pear/temp/grpc/src/core/lib/channel/handshaker.c"",""file_line"":241}]}]}\r\nI0822 15:06:08.050053700    2060 subchannel.c:707]           Connect failed: {""created"":""@1503403568.042903200"",""description"":""Handshake read failed"",""file"":""/tmp/pear/temp/grpc/src/core/lib/security/transport/security_handshaker.c"",""file_line"":304,""referenced_errors"":[{""created"":""@1503403568.042655500"",""description"":""FD Shutdown"",""file"":""/tmp/pear/temp/grpc/src/core/lib/iomgr/lockfree_event.c"",""file_line"":197,""referenced_errors"":[{""created"":""@1503403568.042643600"",""description"":""Handshake timed out"",""file"":""/tmp/pear/temp/grpc/src/core/lib/channel/handshaker.c"",""file_line"":241}]}]}\r\nI0822 15:06:08.050647900    2060 subchannel.c:501]           Retry immediately\r\nI0822 15:06:08.051041400    2060 subchannel.c:453]           Failed to connect to channel, retrying\r\nI0822 15:06:08.052902600    2060 ssl_transport_security.c:184]      HANDSHAKE START -  before connect initialization  - CINIT\r\nI0822 15:06:08.054668900    2060 ssl_transport_security.c:184]                 LOOP -  before connect initialization  - CINIT\r\nI0822 15:06:08.055249200    2060 ssl_transport_security.c:184]                 LOOP -     SSLv3 write client hello A  - 3WCH_A\r\nI0822 15:06:08.056089100    2060 ssl_transport_security.c:184]                 LOOP -               SSLv3 flush data  - 3FLUSH\r\narray(2) {\r\n  [0]=>\r\n  NULL\r\n  [1]=>\r\n  object(stdClass)#376 (3) {\r\n    [""metadata""]=>\r\n    array(0) {\r\n    }\r\n    [""code""]=>\r\n    int(14)\r\n    [""details""]=>\r\n    string(14) ""Connect Failed""\r\n  }\r\n}\r\nD0822 15:06:08.094558300    2059 security_handshaker.c:126]  Security handshake failed: {""created"":""@1503403568.094424700"",""description"":""Handshake read failed"",""file"":""/tmp/pear/temp/grpc/src/core/lib/security/transport/security_handshaker.c"",""file_line"":304,""referenced_errors"":[{""created"":""@1503403568.094149800"",""description"":""FD Shutdown"",""file"":""/tmp/pear/temp/grpc/src/core/lib/iomgr/lockfree_event.c"",""file_line"":197,""referenced_errors"":[{""created"":""@1503403568.094134800"",""description"":""Subchannel disconnected"",""file"":""/tmp/pear/temp/grpc/src/core/ext/filters/client_channel/subchannel.c"",""file_line"":274}]}]}\r\n```\r\n","php\r\nuire dirname(__FILE__).'/vendor/autoload.php';\r\n $client = new \\Productpb\\ProductServiceClient('localhost:3051', [\r\n     'credentials' => Grpc\\ChannelCredentials::createSsl(file_get_contents(dirname(__FILE__).'/client.com.pem'))\r\n ]);\r\n$request = new Productpb\\GetRequest();\r\n$request->SetId(""123"");\r\nvar_dump($client->Get($request)->wait());\r\n"
12232,"Unknown constexpr on Visual Studio 2013Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\nYes\r\n \r\n### What version of gRPC and what language are you using?\r\nmaster(2017-08-20) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nWindows 7, Windows 10\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nVisual Studio 2013 (Win7), Visual Studio 2015 (Win10)\r\n \r\n### What did you do?\r\nI try to build grpc with both versions of Visual Studio.\r\n \r\n### What did you expect to see?\r\nBuild ok\r\n \r\n### What did you see instead?\r\nVisual Studio 2015 is working well, but 2013 is not working:\r\n\r\n```\r\n1>D:\\Build\\gRPC\\src\\include\\grpc++/security/credentials.h(136): error C2144: Syntaxfehler: 'long' sollte auf ';' folgen\r\n1>D:\\Build\\gRPC\\src\\include\\grpc++/security/credentials.h(136): error C4430: Fehlender Typspezifizierer - int wird angenommen. Hinweis: ""default-int"" wird von C++ nicht unterst\xfctzt.\r\n```\r\n\r\nAnd\r\n```\r\n10>D:\\Build\\gRPC\\build\\gens\\src/proto/grpc/reflection/v1alpha/reflection.grpc.pb.h(50): error C2144: Syntaxfehler: 'const char' sollte auf ';' folgen\r\n10>D:\\Build\\gRPC\\build\\gens\\src/proto/grpc/reflection/v1alpha/reflection.grpc.pb.h(50): error C4430: Fehlender Typspezifizierer - int wird angenommen. Hinweis: ""default-int"" wird von C++ nicht unterst\xfctzt.\r\n```\r\n\r\n \r\n### Anything else we should know about your project / environment?\r\nThe problem is ``constexpr``, which is unknown on VS below 2015. I've made the following fix:\r\n```patch\r\ndiff --git a/include/grpc++/impl/codegen/config.h b/include/grpc++/impl/codegen/config.h\r\nindex b5ac9a752e..9e70ed74bd 100644\r\n--- a/include/grpc++/impl/codegen/config.h\r\n+++ b/include/grpc++/impl/codegen/config.h\r\n@@ -38,4 +38,10 @@ using std::to_string;\r\n\r\n }  // namespace grpc\r\n\r\n+#if defined(_MSC_VER)\r\n+#if _MSC_VER < 1900 && !defined(constexpr)\r\n+#define constexpr const\r\n+#endif\r\n+#endif\r\n+\r\n #endif  // GRPCXX_IMPL_CODEGEN_CONFIG_H\r\n```\r\n",platform/Windows,jtattermusch,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\nYes\r\n \r\n### What version of gRPC and what language are you using?\r\nmaster(2017-08-20) \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\nWindows 7, Windows 10\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\nVisual Studio 2013 (Win7), Visual Studio 2015 (Win10)\r\n \r\n### What did you do?\r\nI try to build grpc with both versions of Visual Studio.\r\n \r\n### What did you expect to see?\r\nBuild ok\r\n \r\n### What did you see instead?\r\nVisual Studio 2015 is working well, but 2013 is not working:\r\n\r\n```\r\n1>D:\\Build\\gRPC\\src\\include\\grpc++/security/credentials.h(136): error C2144: Syntaxfehler: 'long' sollte auf ';' folgen\r\n1>D:\\Build\\gRPC\\src\\include\\grpc++/security/credentials.h(136): error C4430: Fehlender Typspezifizierer - int wird angenommen. Hinweis: ""default-int"" wird von C++ nicht unterst\xfctzt.\r\n```\r\n```C++\r\n/// Constant for maximum auth token lifetime.\r\nconstexpr long kMaxAuthTokenLifetimeSecs = 3600;\r\n```\r\nAnd\r\n```\r\n10>D:\\Build\\gRPC\\build\\gens\\src/proto/grpc/reflection/v1alpha/reflection.grpc.pb.h(50): error C2144: Syntaxfehler: 'const char' sollte auf ';' folgen\r\n10>D:\\Build\\gRPC\\build\\gens\\src/proto/grpc/reflection/v1alpha/reflection.grpc.pb.h(50): error C4430: Fehlender Typspezifizierer - int wird angenommen. Hinweis: ""default-int"" wird von C++ nicht unterst\xfctzt.\r\n```\r\n```C++\r\n  static constexpr char const* service_full_name() {\r\n```\r\n \r\n### Anything else we should know about your project / environment?\r\nThe problem is ``constexpr``, which is unknown on VS below 2015. I've made the following fix:\r\n```patch\r\ndiff --git a/include/grpc++/impl/codegen/config.h b/include/grpc++/impl/codegen/config.h\r\nindex b5ac9a752e..9e70ed74bd 100644\r\n--- a/include/grpc++/impl/codegen/config.h\r\n+++ b/include/grpc++/impl/codegen/config.h\r\n@@ -38,4 +38,10 @@ using std::to_string;\r\n\r\n }  // namespace grpc\r\n\r\n+#if defined(_MSC_VER)\r\n+#if _MSC_VER < 1900 && !defined(constexpr)\r\n+#define constexpr const\r\n+#endif\r\n+#endif\r\n+\r\n #endif  // GRPCXX_IMPL_CODEGEN_CONFIG_H\r\n```\r\n",C++\r\n/// Constant for maximum auth token lifetime.\r\nconstexpr long kMaxAuthTokenLifetimeSecs = 3600;\r\n
12175,"PHP cannot make concurrent callsWhen making Unary method calls using PHP gRPC, it is not possible to start a call without also blocking on it, even though the UnaryCall object provides separate `start()` and `wait()` methods.\r\n\r\nFor example, the following Speech code:\r\n\r\nproduces this output:\r\n```\r\n    Time to call: 1.3245630264282\r\n    Time to wait: 0.0020620822906494\r\n```\r\n(see full code here: https://gist.github.com/michaelbausor/d3baa9db9e981949b0eded149a515e23)\r\nThe expected output would be that `Time to call` would be only the time required for gRPC to start the call, and that `Time to wait` would be the much longer time required for the Speech API to process the request.\r\n\r\nCurrently, this makes it impossible to start multiple calls in parallel. For example, this code:\r\n\r\nwill always execute N requests in series, and thus take time equal to `N * average_request_length`. The expected behaviour would be that this would take time approximately equal to the time taken by the longest request.\r\n\r\nThis seems to be caused because currently the UnaryCall object sends the `OP_RECV_INITIAL_METADATA` code in the `start()` method. (See https://github.com/grpc/grpc/blob/master/src/php/lib/Grpc/UnaryCall.php#L45). This causes calls to the `start()` method to block.\r\n\r\nMoving the `OP_RECV_INITIAL_METADATA` code from the `start()` method to the `wait()` method seems to resolve this issue.\r\n\r\ncc @stanley-cheung @dwsupplee @jdpedrie @bshaffer",lang/php,stanley-cheung,"When making Unary method calls using PHP gRPC, it is not possible to start a call without also blocking on it, even though the UnaryCall object provides separate `start()` and `wait()` methods.\r\n\r\nFor example, the following Speech code:\r\n```php\r\n    $startTime = microtime(true);\r\n    $call = $stub->Recognize($request, [], ['call_credentials_callback' => $tokenCallback]);\r\n    $callTime = microtime(true);\r\n    $result = $call->wait();\r\n    $endTime = microtime(true);\r\n    echo ""Time to call: "" . ($callTime - $startTime) . PHP_EOL;\r\n    echo ""Time to wait: "" . ($endTime - $callTime) . PHP_EOL;\r\n```\r\nproduces this output:\r\n```\r\n    Time to call: 1.3245630264282\r\n    Time to wait: 0.0020620822906494\r\n```\r\n(see full code here: https://gist.github.com/michaelbausor/d3baa9db9e981949b0eded149a515e23)\r\nThe expected output would be that `Time to call` would be only the time required for gRPC to start the call, and that `Time to wait` would be the much longer time required for the Speech API to process the request.\r\n\r\nCurrently, this makes it impossible to start multiple calls in parallel. For example, this code:\r\n```php\r\n$requests = [*Array of N speech recognize requests*]\r\n$calls = []\r\nforeach ($requests as $request) {\r\n  $calls[] = $stub->Recognize($request, [], ['call_credentials_callback' => $tokenCallback]);\r\n}\r\nforeach ($calls as $call) {\r\n  $call->wait();\r\n}\r\n```\r\nwill always execute N requests in series, and thus take time equal to `N * average_request_length`. The expected behaviour would be that this would take time approximately equal to the time taken by the longest request.\r\n\r\nThis seems to be caused because currently the UnaryCall object sends the `OP_RECV_INITIAL_METADATA` code in the `start()` method. (See https://github.com/grpc/grpc/blob/master/src/php/lib/Grpc/UnaryCall.php#L45). This causes calls to the `start()` method to block.\r\n\r\nMoving the `OP_RECV_INITIAL_METADATA` code from the `start()` method to the `wait()` method seems to resolve this issue.\r\n\r\ncc @stanley-cheung @dwsupplee @jdpedrie @bshaffer","php\r\n    $startTime = microtime(true);\r\n    $call = $stub->Recognize($request, [], ['call_credentials_callback' => $tokenCallback]);\r\n    $callTime = microtime(true);\r\n    $result = $call->wait();\r\n    $endTime = microtime(true);\r\n    echo ""Time to call: "" . ($callTime - $startTime) . PHP_EOL;\r\n    echo ""Time to wait: "" . ($endTime - $callTime) . PHP_EOL;\r\n"
11966,"flow control bug in python grpcio >= 1.4.0### Description\r\n\r\nThere seems to be a flow control bug in the python client as of `1.4.0`. \r\n\r\nBefore `1.4.0` (`1.3.5` for example) a slow client of a streaming method would cause back pressure that was only relieved when the client consumed enough messages to allow room for another block of messages from the server. Just what we'd expect.\r\n\r\nAs of `1.4.0` the back pressure seems to be relieved whenever the client consumes ***any*** message. That results in the server sending new blocks of messages much more frequently than it should. The server gets way ahead of the client and the client memory use grows continuously until it crashes. In fact, the faster the client consumes messages the faster it's memory usage grows (so long as the server is still producing faster than the client is consuming.)\r\n\r\nI also found the bug on the `1.4.x` and `master` branches as of now. I'm assuming it's a client side bug because varying the version or language (I tried go) on the server side doesn't seem to make a difference.\r\n\r\nOS: Fedora 25\r\nPython: 2.7.13\r\n\r\n### Example Code\r\n\r\nThese examples use `route_guide.proto` and the associated generated python code in the `examples` directory of this repo.\r\n\r\nserver.py:\r\n\r\n\r\nclient.py:\r\n\r\n\r\n### 1.3.5 Output (Working)\r\n\r\n```\r\n$ python client.py\r\n1501214899: 0\r\n1501214909: 1\r\n1501214919: 2\r\n1501214929: 3\r\n1501214939: 4\r\n1501214949: 5\r\n1501214959: 6\r\n```\r\n```\r\n$ python server.py\r\n1501214899: 0\r\n1501214899: 1\r\n1501214899: 2\r\n1501214899: 3\r\n1501214929: 4\r\n1501214929: 5\r\n1501214929: 6\r\n1501214959: 7\r\n1501214959: 8\r\n1501214959: 9\r\n1501214959: 10\r\n\r\n```\r\n\r\n### 1.4.0 Output (Broken)\r\n\r\n```\r\n$ python client.py\r\n1501214672: 0\r\n1501214682: 1\r\n1501214692: 2\r\n1501214702: 3\r\n1501214712: 4\r\n1501214722: 5\r\n1501214732: 6\r\n```\r\n```\r\n$ python server.py\r\n1501214672: 0\r\n1501214672: 1\r\n1501214672: 2\r\n1501214672: 3\r\n1501214682: 4\r\n1501214682: 5\r\n1501214682: 6\r\n1501214692: 7\r\n1501214692: 8\r\n1501214692: 9\r\n1501214702: 10\r\n1501214702: 11\r\n1501214702: 12\r\n1501214702: 13\r\n1501214712: 14\r\n1501214712: 15\r\n1501214712: 16\r\n1501214712: 17\r\n1501214722: 18\r\n1501214722: 19\r\n1501214722: 20\r\n1501214722: 21\r\n1501214722: 22\r\n1501214732: 23\r\n1501214732: 24\r\n1501214732: 25\r\n1501214732: 26\r\n1501214732: 27\r\n\r\n```\r\n\r\n\r\n",kind/bug|area/core|lang/Python|priority/P1|disposition/requires reporter action,muxi|ncteisen|kpayson64,"### Description\r\n\r\nThere seems to be a flow control bug in the python client as of `1.4.0`. \r\n\r\nBefore `1.4.0` (`1.3.5` for example) a slow client of a streaming method would cause back pressure that was only relieved when the client consumed enough messages to allow room for another block of messages from the server. Just what we'd expect.\r\n\r\nAs of `1.4.0` the back pressure seems to be relieved whenever the client consumes ***any*** message. That results in the server sending new blocks of messages much more frequently than it should. The server gets way ahead of the client and the client memory use grows continuously until it crashes. In fact, the faster the client consumes messages the faster it's memory usage grows (so long as the server is still producing faster than the client is consuming.)\r\n\r\nI also found the bug on the `1.4.x` and `master` branches as of now. I'm assuming it's a client side bug because varying the version or language (I tried go) on the server side doesn't seem to make a difference.\r\n\r\nOS: Fedora 25\r\nPython: 2.7.13\r\n\r\n### Example Code\r\n\r\nThese examples use `route_guide.proto` and the associated generated python code in the `examples` directory of this repo.\r\n\r\nserver.py:\r\n```python\r\nfrom __future__ import print_function\r\nfrom concurrent import futures\r\nimport time\r\nimport grpc\r\nimport route_guide_pb2\r\nimport route_guide_pb2_grpc\r\n\r\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\r\n\r\nclass RouteGuideServicer(route_guide_pb2_grpc.RouteGuideServicer):\r\n\r\n    def RouteChat(self, request_iterator, context):\r\n        i = 0\r\n        while True:\r\n            print(""%d: %d"" % (time.time(), i))\r\n            yield route_guide_pb2.RouteNote(\r\n                message = ""X"" * (32*1024),\r\n            )\r\n            i += 1\r\n\r\ndef serve():\r\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    route_guide_pb2_grpc.add_RouteGuideServicer_to_server(\r\n            RouteGuideServicer(), server)\r\n    server.add_insecure_port('[::]:50051')\r\n    server.start()\r\n    try:\r\n        while True:\r\n            time.sleep(_ONE_DAY_IN_SECONDS)\r\n    except KeyboardInterrupt:\r\n        server.stop(0)\r\n\r\nif __name__ == '__main__':\r\n    serve()\r\n```\r\n\r\nclient.py:\r\n```python\r\nfrom __future__ import print_function\r\nimport time\r\nimport grpc\r\nimport route_guide_pb2\r\nimport route_guide_pb2_grpc\r\n\r\ndef guide_route_chat(stub):\r\n    i = 0\r\n    for response in stub.RouteChat(iter(())):\r\n        print(""%d: %d"" % (time.time(), i))\r\n        time.sleep(10)\r\n        i += 1\r\n\r\ndef run():\r\n    channel = grpc.insecure_channel('localhost:50051')\r\n    stub = route_guide_pb2_grpc.RouteGuideStub(channel)\r\n    guide_route_chat(stub)\r\n\r\nif __name__ == '__main__':\r\n    run()\r\n```\r\n\r\n### 1.3.5 Output (Working)\r\n\r\n```\r\n$ python client.py\r\n1501214899: 0\r\n1501214909: 1\r\n1501214919: 2\r\n1501214929: 3\r\n1501214939: 4\r\n1501214949: 5\r\n1501214959: 6\r\n```\r\n```\r\n$ python server.py\r\n1501214899: 0\r\n1501214899: 1\r\n1501214899: 2\r\n1501214899: 3\r\n1501214929: 4\r\n1501214929: 5\r\n1501214929: 6\r\n1501214959: 7\r\n1501214959: 8\r\n1501214959: 9\r\n1501214959: 10\r\n\r\n```\r\n\r\n### 1.4.0 Output (Broken)\r\n\r\n```\r\n$ python client.py\r\n1501214672: 0\r\n1501214682: 1\r\n1501214692: 2\r\n1501214702: 3\r\n1501214712: 4\r\n1501214722: 5\r\n1501214732: 6\r\n```\r\n```\r\n$ python server.py\r\n1501214672: 0\r\n1501214672: 1\r\n1501214672: 2\r\n1501214672: 3\r\n1501214682: 4\r\n1501214682: 5\r\n1501214682: 6\r\n1501214692: 7\r\n1501214692: 8\r\n1501214692: 9\r\n1501214702: 10\r\n1501214702: 11\r\n1501214702: 12\r\n1501214702: 13\r\n1501214712: 14\r\n1501214712: 15\r\n1501214712: 16\r\n1501214712: 17\r\n1501214722: 18\r\n1501214722: 19\r\n1501214722: 20\r\n1501214722: 21\r\n1501214722: 22\r\n1501214732: 23\r\n1501214732: 24\r\n1501214732: 25\r\n1501214732: 26\r\n1501214732: 27\r\n\r\n```\r\n\r\n\r\n","python\r\nfrom __future__ import print_function\r\nfrom concurrent import futures\r\nimport time\r\nimport grpc\r\nimport route_guide_pb2\r\nimport route_guide_pb2_grpc\r\n\r\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\r\n\r\nclass RouteGuideServicer(route_guide_pb2_grpc.RouteGuideServicer):\r\n\r\n    def RouteChat(self, request_iterator, context):\r\n        i = 0\r\n        while True:\r\n            print(""%d: %d"" % (time.time(), i))\r\n            yield route_guide_pb2.RouteNote(\r\n                message = ""X"" * (32*1024),\r\n            )\r\n            i += 1\r\n\r\ndef serve():\r\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\r\n    route_guide_pb2_grpc.add_RouteGuideServicer_to_server(\r\n            RouteGuideServicer(), server)\r\n    server.add_insecure_port('[::]:50051')\r\n    server.start()\r\n    try:\r\n        while True:\r\n            time.sleep(_ONE_DAY_IN_SECONDS)\r\n    except KeyboardInterrupt:\r\n        server.stop(0)\r\n\r\nif __name__ == '__main__':\r\n    serve()\r\n"
11903,C-Ares: Remove Internal HeaderThe `nameser.h` header is a [C-Ares internal header](https://github.com/c-ares/c-ares/issues/131) which is not installed with C-Ares.\r\n\r\nThis fixes build errors with externally installed C-Ares (1.13) with\r\n\r\n\r\nRemoves a the header defining two [IANA controlled numbers](https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml) to do a SRV lookup.,cla: yes,y-zeng,The `nameser.h` header is a [C-Ares internal header](https://github.com/c-ares/c-ares/issues/131) which is not installed with C-Ares.\r\n\r\nThis fixes build errors with externally installed C-Ares (1.13) with\r\n```bash\r\n  cmake -DgRPC_CARES_PROVIDER=package ..\r\n```\r\n\r\nRemoves a the header defining two [IANA controlled numbers](https://www.iana.org/assignments/dns-parameters/dns-parameters.xhtml) to do a SRV lookup.,bash\r\n  cmake -DgRPC_CARES_PROVIDER=package ..\r\n
11887,"Memory usage of the server increases when running the greeter_server.py exampleHi all,\r\nI am new to grpc. Then I just runned the demo of helloword server, the resident memory usage of server growed and growed. \r\nI tested it on Linux, installed grpc by pip, `grpcio==1.4.0, grpcio-tools==1.4.0`, both python 2.7.12 and 3.6.1.\r\n\r\nI just add one line in [greeter_client.py](https://github.com/grpc/grpc/blob/master/examples/python/helloworld/greeter_client.py), then the codes just like below:\r\n\r\n\r\nthe `ps aux` result:\r\n\r\n![20170720203115](https://user-images.githubusercontent.com/6399856/28417768-e8171cd4-6d8b-11e7-9cf1-817c33641f63.png)\r\n\r\nI would appreciate any further information on this, thanks.",kind/bug|lang/Python|priority/P1,nathanielmanistaatgoogle,"Hi all,\r\nI am new to grpc. Then I just runned the demo of helloword server, the resident memory usage of server growed and growed. \r\nI tested it on Linux, installed grpc by pip, `grpcio==1.4.0, grpcio-tools==1.4.0`, both python 2.7.12 and 3.6.1.\r\n\r\nI just add one line in [greeter_client.py](https://github.com/grpc/grpc/blob/master/examples/python/helloworld/greeter_client.py), then the codes just like below:\r\n```python\r\ndef run():\r\n  channel = grpc.insecure_channel('localhost:50051')\r\n  stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n  for i in range(100000):\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n    print(""Greeter client received: "" + response.message)\r\n```\r\n\r\nthe `ps aux` result:\r\n\r\n![20170720203115](https://user-images.githubusercontent.com/6399856/28417768-e8171cd4-6d8b-11e7-9cf1-817c33641f63.png)\r\n\r\nI would appreciate any further information on this, thanks.","python\r\ndef run():\r\n  channel = grpc.insecure_channel('localhost:50051')\r\n  stub = helloworld_pb2_grpc.GreeterStub(channel)\r\n  for i in range(100000):\r\n    response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'))\r\n    print(""Greeter client received: "" + response.message)\r\n"
11776,"Ruby Concurrent RequestsHello, we are unable to make more than 4 concurrent requests using the Ruby client. When more than 4 concurrent requests are made the client seems to lock up until the requests time out. We are currently in an effort to add more concurrency in the Google Cloud Pub/Sub Ruby library, and while this seems to be working well while making concurrent streaming API calls, we are seeing the grpc client lock up while concurrently calling non-streaming APIs under moderate load.\r\n\r\nTo illustrate, I have included a ruby script that attempts to benchmark the performance differences between making sequential and concurrent requests. The default number of requests is 4, but you can pass in an argument to change the number of requests. This script will lock up and eventually time out when passing in a number larger than 4. Here is the output when running 4 and 6 concurrent requests:\r\n\r\n```\r\n$ ruby benchmark_gapic.rb 4\r\n       user     system      total        real\r\nsequential#<Google::Pubsub::V1::PublishResponse:0x007faed3afb160>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3ad8660>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3aa9bd0>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3a7b118>\r\n  0.390000   0.500000   0.890000 ( 16.913990)\r\nconcurrent#<Google::Pubsub::V1::PublishResponse:0x007faed1845b20>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed18a7898>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed19372b8>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed1955178>\r\n  0.390000   0.540000   0.930000 ( 11.678385)\r\n\r\n$ ruby benchmark_gapic.rb 6\r\n       user     system      total        real\r\nsequential#<Google::Pubsub::V1::PublishResponse:0x007ffd71951f48>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd71923418>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd719009b8>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd718d1f28>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd6f847488>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd6f8979d8>\r\n  0.570000   0.700000   1.270000 ( 18.899312)\r\nconcurrent/Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:357:in `rescue in block (2 levels) in retryable': GaxError Retry total timeout exceeded with exception, caused by 4:Deadline Exceeded (Google::Gax::RetryError)\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:343:in `block (2 levels) in retryable'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:342:in `loop'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:342:in `block in retryable'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:262:in `block in catch_errors'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:224:in `block in create_api_call'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:250:in `block in create_api_call'\r\n\tfrom /Users/blowmage/google/codez/gcloud-ruby/google-cloud-pubsub/lib/google/cloud/pubsub/v1/publisher_client.rb:326:in `publish'\r\n\tfrom benchmark_gapic.rb:56:in `block (4 levels) in <main>'\r\n```\r\n\r\nHere is the script:\r\n\r\n\r\n\r\n\r\nThis script is using the GAPIC layer to make the GRPC calls. I can rewrite it to make the calls on the GRPC stub directly, but the GAPIC layer doesn't look to be doing anything to prevent concurrent calls.",lang/ruby,apolcyn,"Hello, we are unable to make more than 4 concurrent requests using the Ruby client. When more than 4 concurrent requests are made the client seems to lock up until the requests time out. We are currently in an effort to add more concurrency in the Google Cloud Pub/Sub Ruby library, and while this seems to be working well while making concurrent streaming API calls, we are seeing the grpc client lock up while concurrently calling non-streaming APIs under moderate load.\r\n\r\nTo illustrate, I have included a ruby script that attempts to benchmark the performance differences between making sequential and concurrent requests. The default number of requests is 4, but you can pass in an argument to change the number of requests. This script will lock up and eventually time out when passing in a number larger than 4. Here is the output when running 4 and 6 concurrent requests:\r\n\r\n```\r\n$ ruby benchmark_gapic.rb 4\r\n       user     system      total        real\r\nsequential#<Google::Pubsub::V1::PublishResponse:0x007faed3afb160>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3ad8660>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3aa9bd0>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed3a7b118>\r\n  0.390000   0.500000   0.890000 ( 16.913990)\r\nconcurrent#<Google::Pubsub::V1::PublishResponse:0x007faed1845b20>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed18a7898>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed19372b8>\r\n#<Google::Pubsub::V1::PublishResponse:0x007faed1955178>\r\n  0.390000   0.540000   0.930000 ( 11.678385)\r\n\r\n$ ruby benchmark_gapic.rb 6\r\n       user     system      total        real\r\nsequential#<Google::Pubsub::V1::PublishResponse:0x007ffd71951f48>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd71923418>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd719009b8>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd718d1f28>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd6f847488>\r\n#<Google::Pubsub::V1::PublishResponse:0x007ffd6f8979d8>\r\n  0.570000   0.700000   1.270000 ( 18.899312)\r\nconcurrent/Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:357:in `rescue in block (2 levels) in retryable': GaxError Retry total timeout exceeded with exception, caused by 4:Deadline Exceeded (Google::Gax::RetryError)\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:343:in `block (2 levels) in retryable'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:342:in `loop'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:342:in `block in retryable'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:262:in `block in catch_errors'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:224:in `block in create_api_call'\r\n\tfrom /Users/blowmage/.gem/repos/gcloud-ruby/gems/google-gax-0.8.4/lib/google/gax/api_callable.rb:250:in `block in create_api_call'\r\n\tfrom /Users/blowmage/google/codez/gcloud-ruby/google-cloud-pubsub/lib/google/cloud/pubsub/v1/publisher_client.rb:326:in `publish'\r\n\tfrom benchmark_gapic.rb:56:in `block (4 levels) in <main>'\r\n```\r\n\r\nHere is the script:\r\n\r\n\r\n```ruby\r\n$stdout.sync = true\r\nThread.abort_on_exception = true\r\n\r\nrequire ""google/cloud/pubsub/v1""\r\nrequire ""grpc""\r\nrequire ""googleauth""\r\nrequire ""securerandom""\r\nrequire ""benchmark""\r\n\r\n# This will benchmark publishing 1 million 4KB messages\r\n# using the previous batch method, and the new async publisher\r\n\r\nproject = ENV[""GOOGLE_CLOUD_PROJECT""] || ""<ENTER-PROJECT-ID-HERE>""\r\nkeyfile = ENV[""GOOGLE_CLOUD_KEYFILE""] || ""<ENTER-KEYFILE-PATH-HERE>""\r\n\r\nhost = Google::Cloud::Pubsub::V1::PublisherClient::SERVICE_ADDRESS\r\nchan_args = { ""grpc.max_send_message_length""    => -1,\r\n              ""grpc.max_receive_message_length"" => -1 }\r\nauth = Google::Auth::ServiceAccountCredentials.make_creds \\\r\n  json_key_io: File.open(keyfile, mode: ""r""),\r\n  scope: Google::Cloud::Pubsub::V1::PublisherClient::ALL_SCOPES\r\nchan_creds = GRPC::Core::ChannelCredentials.new.compose \\\r\n  GRPC::Core::CallCredentials.new(auth.updater_proc)\r\nchannel = GRPC::Core::Channel.new host, chan_args, chan_creds\r\npub_client = Google::Cloud::Pubsub::V1::PublisherClient.new channel: channel\r\n\r\ntopic_name = ""loadtest""\r\ntopic_path = Google::Cloud::Pubsub::V1::PublisherClient.topic_path project, topic_name\r\ntopic_grpc = begin\r\n               pub_client.get_topic topic_path\r\n             rescue Google::Gax::GaxError => e\r\n               raise e unless e.cause.code == 5\r\n               pub_client.create_topic topic_path\r\n             end\r\n\r\nmsg_data = SecureRandom.random_bytes 4*1024\r\nmessages = 1000.times.map { Google::Pubsub::V1::PubsubMessage.new data: msg_data }\r\n\r\nn = (ARGV.first || 4).to_i\r\n\r\nBenchmark.bm do |bm|\r\n  bm.report ""sequential"" do\r\n    # runs the publish requests synchronously\r\n    responses = n.times.map do\r\n      pub_response = pub_client.publish topic_path, messages\r\n      puts pub_response # uncomment to see when recieved\r\n      pub_response\r\n    end\r\n    # puts responses.inspect\r\n  end\r\n\r\n  bm.report ""concurrent"" do\r\n    # runs the publish requests asynchronously\r\n    threads = n.times.map do\r\n      Thread.new do\r\n        pub_response = pub_client.publish topic_path, messages\r\n        puts pub_response # uncomment to see when recieved\r\n        pub_response\r\n      end\r\n    end\r\n    responses = threads.map &:value\r\n    # puts responses.inspect\r\n  end\r\nend\r\n```\r\n\r\nThis script is using the GAPIC layer to make the GRPC calls. I can rewrite it to make the calls on the GRPC stub directly, but the GAPIC layer doesn't look to be doing anything to prevent concurrent calls.","ruby\r\n$stdout.sync = true\r\nThread.abort_on_exception = true\r\n\r\nrequire ""google/cloud/pubsub/v1""\r\nrequire ""grpc""\r\nrequire ""googleauth""\r\nrequire ""securerandom""\r\nrequire ""benchmark""\r\n\r\n# This will benchmark publishing 1 million 4KB messages\r\n# using the previous batch method, and the new async publisher\r\n\r\nproject = ENV[""GOOGLE_CLOUD_PROJECT""] || ""<ENTER-PROJECT-ID-HERE>""\r\nkeyfile = ENV[""GOOGLE_CLOUD_KEYFILE""] || ""<ENTER-KEYFILE-PATH-HERE>""\r\n\r\nhost = Google::Cloud::Pubsub::V1::PublisherClient::SERVICE_ADDRESS\r\nchan_args = { ""grpc.max_send_message_length""    => -1,\r\n              ""grpc.max_receive_message_length"" => -1 }\r\nauth = Google::Auth::ServiceAccountCredentials.make_creds \\\r\n  json_key_io: File.open(keyfile, mode: ""r""),\r\n  scope: Google::Cloud::Pubsub::V1::PublisherClient::ALL_SCOPES\r\nchan_creds = GRPC::Core::ChannelCredentials.new.compose \\\r\n  GRPC::Core::CallCredentials.new(auth.updater_proc)\r\nchannel = GRPC::Core::Channel.new host, chan_args, chan_creds\r\npub_client = Google::Cloud::Pubsub::V1::PublisherClient.new channel: channel\r\n\r\ntopic_name = ""loadtest""\r\ntopic_path = Google::Cloud::Pubsub::V1::PublisherClient.topic_path project, topic_name\r\ntopic_grpc = begin\r\n               pub_client.get_topic topic_path\r\n             rescue Google::Gax::GaxError => e\r\n               raise e unless e.cause.code == 5\r\n               pub_client.create_topic topic_path\r\n             end\r\n\r\nmsg_data = SecureRandom.random_bytes 4*1024\r\nmessages = 1000.times.map { Google::Pubsub::V1::PubsubMessage.new data: msg_data }\r\n\r\nn = (ARGV.first || 4).to_i\r\n\r\nBenchmark.bm do |bm|\r\n  bm.report ""sequential"" do\r\n    # runs the publish requests synchronously\r\n    responses = n.times.map do\r\n      pub_response = pub_client.publish topic_path, messages\r\n      puts pub_response # uncomment to see when recieved\r\n      pub_response\r\n    end\r\n    # puts responses.inspect\r\n  end\r\n\r\n  bm.report ""concurrent"" do\r\n    # runs the publish requests asynchronously\r\n    threads = n.times.map do\r\n      Thread.new do\r\n        pub_response = pub_client.publish topic_path, messages\r\n        puts pub_response # uncomment to see when recieved\r\n        pub_response\r\n      end\r\n    end\r\n    responses = threads.map &:value\r\n    # puts responses.inspect\r\n  end\r\nend\r\n"
11679,"Bad behaviour with multiple threadsI'm using a C++ grpc sync server, and am currently testing it's behavior with multiple threads/under heavy load.  Unfortunately it doesn't degrade very gracefully when it gets hit by a lot of requests.\r\n\r\nAll tests were done with gRPC 1.4.1, under linux (Using both CentOS 7.3 and RHEL 6.8).\r\n\r\nWhen making a lot of requests from different processes, I found that the grpc server can run out of file descriptors.  Once that happens instead of rejecting requests, the server just locks up and handles no more additional requests.  Ideally I think the server would either wait until the file descriptors can be free'd up, or return an error to the caller.\r\n\r\nThe below diff fixes this, but is clearly less than ideal, as it just makes the server loop while there are no file descriptors available:\r\n\r\n\r\n\r\nI'd by happy to try and submit a pull request with some guidance on the best way to fix this issue.\r\n\r\nThe other issue I noticed is when having a client that uses multiple threads to connect to the server.  As the thread pool scales dynamically, it seems that it just scales up without any limits.  As soon as the ulimit  for the number of processes is reached, the grpc server will crash.  There currently seems no way to limit the number of threads.  I haven't found a way to try and fix that yet.  Any suggestions are welcome.\r\n\r\nI tried playing around `grpc::ResourceQuota`, and the `NUM_CQS`, `MIN_POLLERS` and `MAX_POLLERS` settings, but none of them seemed to make any difference.\r\n\r\nWhile I don't think this will be a problem in normal operation, it could still be problematic with rogue clients, that can DoS the grpc server.\r\n\r\nDo you have any suggestions on how to alleviate the issue?",disposition/to close,sreecha|ctiller,"I'm using a C++ grpc sync server, and am currently testing it's behavior with multiple threads/under heavy load.  Unfortunately it doesn't degrade very gracefully when it gets hit by a lot of requests.\r\n\r\nAll tests were done with gRPC 1.4.1, under linux (Using both CentOS 7.3 and RHEL 6.8).\r\n\r\nWhen making a lot of requests from different processes, I found that the grpc server can run out of file descriptors.  Once that happens instead of rejecting requests, the server just locks up and handles no more additional requests.  Ideally I think the server would either wait until the file descriptors can be free'd up, or return an error to the caller.\r\n\r\nThe below diff fixes this, but is clearly less than ideal, as it just makes the server loop while there are no file descriptors available:\r\n\r\n```c++\r\ndiff --git a/src/core/lib/iomgr/tcp_server_posix.c b/src/core/lib/iomgr/tcp_server_posix.c\r\nindex 08997b5e2b..c0e54e2ea2 100644\r\n--- a/src/core/lib/iomgr/tcp_server_posix.c\r\n+++ b/src/core/lib/iomgr/tcp_server_posix.c\r\n@@ -239,6 +239,10 @@ static void on_read(grpc_exec_ctx *exec_ctx, void *arg, grpc_error *err) {\r\n         case EAGAIN:\r\n           grpc_fd_notify_on_read(exec_ctx, sp->emfd, &sp->read_closure);\r\n           return;\r\n+        case EMFILE:\r\n+        case ENFILE:\r\n+          gpr_log(GPR_ERROR, ""Too many open file descriptors: %s"", strerror(errno));\r\n+          continue;\r\n         default:\r\n           gpr_mu_lock(&sp->server->mu);\r\n           if (!sp->server->shutdown_listeners) {\r\n```\r\n\r\nI'd by happy to try and submit a pull request with some guidance on the best way to fix this issue.\r\n\r\nThe other issue I noticed is when having a client that uses multiple threads to connect to the server.  As the thread pool scales dynamically, it seems that it just scales up without any limits.  As soon as the ulimit  for the number of processes is reached, the grpc server will crash.  There currently seems no way to limit the number of threads.  I haven't found a way to try and fix that yet.  Any suggestions are welcome.\r\n\r\nI tried playing around `grpc::ResourceQuota`, and the `NUM_CQS`, `MIN_POLLERS` and `MAX_POLLERS` settings, but none of them seemed to make any difference.\r\n\r\nWhile I don't think this will be a problem in normal operation, it could still be problematic with rogue clients, that can DoS the grpc server.\r\n\r\nDo you have any suggestions on how to alleviate the issue?","c++\r\ndiff --git a/src/core/lib/iomgr/tcp_server_posix.c b/src/core/lib/iomgr/tcp_server_posix.c\r\nindex 08997b5e2b..c0e54e2ea2 100644\r\n--- a/src/core/lib/iomgr/tcp_server_posix.c\r\n+++ b/src/core/lib/iomgr/tcp_server_posix.c\r\n@@ -239,6 +239,10 @@ static void on_read(grpc_exec_ctx *exec_ctx, void *arg, grpc_error *err) {\r\n         case EAGAIN:\r\n           grpc_fd_notify_on_read(exec_ctx, sp->emfd, &sp->read_closure);\r\n           return;\r\n+        case EMFILE:\r\n+        case ENFILE:\r\n+          gpr_log(GPR_ERROR, ""Too many open file descriptors: %s"", strerror(errno));\r\n+          continue;\r\n         default:\r\n           gpr_mu_lock(&sp->server->mu);\r\n           if (!sp->server->shutdown_listeners) {\r\n"
11657,Add protobuf DESCRIPTOR objects as attributes of generated classes and methodsThe generated `_pb2_grpc.py` code should look something like\r\n\r\nso that\r\n\r\nworks and is useful code.,kind/enhancement|lang/Python|area/protoc plugins,nathanielmanistaatgoogle,"The generated `_pb2_grpc.py` code should look something like\r\n```python\r\nclass MyServiceStub(object):\r\n  DESCRIPTOR = my_service_pb2.DESCRIPTOR.services_by_name['MyService']\r\n\r\n  def __init__(self, channel):\r\n    self.MyMethod = channel.unary_unary(\r\n        '/my_package.MyService/MyMethod',\r\n       request_serializer=my_messages__pb2.MyMethodRequest.SerializeToString,\r\n       response_deserializer=my_messages__pb2.MyMethodResponse.FromString,\r\n       descriptor=my_service_pb2.DESCRIPTOR.services_by_name['MyService'].methods_by_name['MyMethod'],\r\n      )\r\n```\r\nso that\r\n```python\r\nmy_stub = my_service_pb2_grpc.MyServiceStub(my_channel)\r\nmy_service_descriptor = my_stub.DESCRIPTOR\r\nmy_method_descriptor = my_stub.MyMethod.DESCRIPTOR\r\n```\r\nworks and is useful code.","python\r\nclass MyServiceStub(object):\r\n  DESCRIPTOR = my_service_pb2.DESCRIPTOR.services_by_name['MyService']\r\n\r\n  def __init__(self, channel):\r\n    self.MyMethod = channel.unary_unary(\r\n        '/my_package.MyService/MyMethod',\r\n       request_serializer=my_messages__pb2.MyMethodRequest.SerializeToString,\r\n       response_deserializer=my_messages__pb2.MyMethodResponse.FromString,\r\n       descriptor=my_service_pb2.DESCRIPTOR.services_by_name['MyService'].methods_by_name['MyMethod'],\r\n      )\r\n"
11653,"grpc.AuthMetadataContext should use @abstractproperty for its propertiesPlease answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nHEAD @ 8576275\r\n \r\n### What did you do?\r\n\r\nWhen mocking gRPC items during unit tests, it's common to use `mock.create_autospec`:\r\n\r\n\r\n \r\n### What did you expect to see?\r\n \r\nI expected mock to pick up the documented attributes on `grpc.AuthMetadataContext`, namely `service_url` and `method_name`.\r\n \r\n### What did you see instead?\r\n \r\nMock doesn't think they exist because, well, they don't. They only exist in [documentation](https://github.com/grpc/grpc/blob/85762752c4a12390f6815eb7ddd3833aa3bfa223/src/python/grpcio/grpc/__init__.py#L376).\r\n\r\nIf you were to define them on the class as abstract properties then mock would be able to properly spec the class:\r\n\r\n\r\n\r\n",kind/enhancement|lang/Python|disposition/FOR DISCUSSION|area/api|priority/P3|disposition/stale,nathanielmanistaatgoogle,"Please answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n \r\nHEAD @ 8576275\r\n \r\n### What did you do?\r\n\r\nWhen mocking gRPC items during unit tests, it's common to use `mock.create_autospec`:\r\n\r\n```python\r\ncontext = mock.create_autospec(grpc.AuthMetadataContext, instance=True)\r\n```\r\n \r\n### What did you expect to see?\r\n \r\nI expected mock to pick up the documented attributes on `grpc.AuthMetadataContext`, namely `service_url` and `method_name`.\r\n \r\n### What did you see instead?\r\n \r\nMock doesn't think they exist because, well, they don't. They only exist in [documentation](https://github.com/grpc/grpc/blob/85762752c4a12390f6815eb7ddd3833aa3bfa223/src/python/grpcio/grpc/__init__.py#L376).\r\n\r\nIf you were to define them on the class as abstract properties then mock would be able to properly spec the class:\r\n\r\n```python\r\n@abc.abstractproperty\r\ndef service_url(self):\r\n    ...\r\n```\r\n\r\n","python\r\ncontext = mock.create_autospec(grpc.AuthMetadataContext, instance=True)\r\n"
11538,"grpc::Alarm copy and move constructorsPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\n All\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nAny \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nC++\r\n \r\n### What did you do?\r\n\r\n`grpc::Alarm` should not be copy-constructible, but the copy constructor is not deleted\r\n`grpc::Alarm` should be move-constructible, and that needs to be made explicit and tested\r\n\r\nThis causes a crash on \r\n\r\n\r\n\r\n### What did you expect to see?\r\n\r\nNo crash\r\n \r\n### What did you see instead?\r\n \r\nA crash\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",kind/bug|lang/c++|kind/internal cleanup,dgquintas,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nYes\r\n \r\n### What version of gRPC and what language are you using?\r\n \r\n All\r\n\r\n### What operating system (Linux, Windows, \u2026) and version?\r\n\r\nAny \r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nC++\r\n \r\n### What did you do?\r\n\r\n`grpc::Alarm` should not be copy-constructible, but the copy constructor is not deleted\r\n`grpc::Alarm` should be move-constructible, and that needs to be made explicit and tested\r\n\r\nThis causes a crash on \r\n\r\n```cpp\r\n ::grpc::CompletionQueue completion_queue;\r\n  std::vector<grpc::Alarm> alarms;\r\n  for (int i = 0; i < 10; ++i) {\r\n    printf(""alarm %d\\n"", i);\r\n    // Crashes when i==4\r\n    alarms.emplace_back(&completion_queue, \r\n                        std::chrono::system_clock::now(),\r\n                        reinterpret_cast<void*>(i));\r\n```\r\n\r\n### What did you expect to see?\r\n\r\nNo crash\r\n \r\n### What did you see instead?\r\n \r\nA crash\r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n","cpp\r\n ::grpc::CompletionQueue completion_queue;\r\n  std::vector<grpc::Alarm> alarms;\r\n  for (int i = 0; i < 10; ++i) {\r\n    printf(""alarm %d\\n"", i);\r\n    // Crashes when i==4\r\n    alarms.emplace_back(&completion_queue, \r\n                        std::chrono::system_clock::now(),\r\n                        reinterpret_cast<void*>(i));\r\n"
11523,"Connection Failed on node after some successful calls.`Connection failed` error event from server stream call. this is happening only after some successful calls. \r\n\r\n\r\ngrpc version: 1.3.8\r\nOS: centos  (AWS cloud)\r\nPlatform: NODE JS.\r\n\r\n**Code from rpc-client.js file**\r\n``` javascript\r\nimport grpc from 'grpc'\r\n\r\nconst ckService = grpc.load(PROTO_PATH).external.protocol\r\nconst client = new ckService.CkPublic(url, grpc.credentials.createInsecure())\r\n\r\nfunction executeCommand(request) {\r\n  return client.executeCommand(request)\r\n}\r\n\r\nexport default { executeCommand }\r\n```\r\n**Code from action.js**\r\n``` javascript\r\nimport ckClient from './rpc-client.js'\r\nconst stream = ckClient.executeCommand(request)\r\nstream.on('error', function(err) {\r\n    console.error(err)\r\n})\r\n\r\nstream.on('data', function(chunk) { \r\n})\r\nstream.on('status', function(rpcStatus) {\r\n    console.log(`${request.rpcRequest.action} call status : ${rpcStatus.details}`)\r\n})\r\n```",lang/node|disposition/to close|area/client channel,dgquintas|murgatroid99,"`Connection failed` error event from server stream call. this is happening only after some successful calls. \r\n\r\n```javascript\r\n$ commit call status : Connect Failed\r\n\r\n$ { Error: Connect Failed\r\n    at ClientReadableStream._emitStatusIfDone (/rapid/node_modules/grpc/src/node/src/client.js:209:19)\r\n    at ClientReadableStream._readsDone (/rapid/node_modules/grpc/src/node/src/client.js:177:8)\r\n    at readCallback (/rapid/node_modules/grpc/src/node/src/client.js:237:12) code: 14, metadata: Metadata { _internal_repr: {} } }\r\n```\r\ngrpc version: 1.3.8\r\nOS: centos  (AWS cloud)\r\nPlatform: NODE JS.\r\n\r\n**Code from rpc-client.js file**\r\n``` javascript\r\nimport grpc from 'grpc'\r\n\r\nconst ckService = grpc.load(PROTO_PATH).external.protocol\r\nconst client = new ckService.CkPublic(url, grpc.credentials.createInsecure())\r\n\r\nfunction executeCommand(request) {\r\n  return client.executeCommand(request)\r\n}\r\n\r\nexport default { executeCommand }\r\n```\r\n**Code from action.js**\r\n``` javascript\r\nimport ckClient from './rpc-client.js'\r\nconst stream = ckClient.executeCommand(request)\r\nstream.on('error', function(err) {\r\n    console.error(err)\r\n})\r\n\r\nstream.on('data', function(chunk) { \r\n})\r\nstream.on('status', function(rpcStatus) {\r\n    console.log(`${request.rpcRequest.action} call status : ${rpcStatus.details}`)\r\n})\r\n```","javascript\r\n$ commit call status : Connect Failed\r\n\r\n$ { Error: Connect Failed\r\n    at ClientReadableStream._emitStatusIfDone (/rapid/node_modules/grpc/src/node/src/client.js:209:19)\r\n    at ClientReadableStream._readsDone (/rapid/node_modules/grpc/src/node/src/client.js:177:8)\r\n    at readCallback (/rapid/node_modules/grpc/src/node/src/client.js:237:12) code: 14, metadata: Metadata { _internal_repr: {} } }\r\n"
11451,"X509_STORE_add_cert errors need to be decoded and handled### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n\r\n### What version of gRPC and what language are you using?\r\n \r\n```\r\n> GRPC::VERSION\r\n=> ""1.2.5""\r\n```\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nRuby\r\n\r\n```\r\n> RUBY_VERSION\r\n=> ""2.4.0""\r\n```\r\n \r\n### What did you do?\r\n\r\nI pointed GRPC at a CA bundle that included a duplicate certificate and then called a method on my stub:\r\n\r\n\r\n\r\n### What did you expect to see?\r\n \r\nI would expect an exception not to be raised.\r\n \r\n### What did you see instead?\r\n \r\nWhen calling a method on my stub I get a `GRPC::DeadlineExceeded` error, along with the following debugging output:\r\n\r\n```\r\nE0609 11:07:14.082544835   29191 ssl_transport_security.c:602] Could not add root certificate to ssl context.\r\nE0609 11:07:14.082587138   29191 ssl_transport_security.c:1348] Cannot load server root certificates.\r\nE0609 11:07:14.082924913   29191 security_connector.c:837]   Handshaker factory creation failed with TSI_INTERNAL_ERROR.\r\nE0609 11:07:14.082939716   29191 secure_channel_create.c:127] Failed to create secure subchannel for secure name 'earthsmoke.service.cp1.consul:8080'\r\nE0609 11:07:14.082947490   29191 secure_channel_create.c:158] Failed to create subchannel arguments during subchannel creation.\r\nE0609 11:07:14.086947973   29191 ssl_transport_security.c:602] Could not add root certificate to ssl context.\r\nE0609 11:07:14.086965201   29191 ssl_transport_security.c:1348] Cannot load server root certificates.\r\nE0609 11:07:14.087280224   29191 security_connector.c:837]   Handshaker factory creation failed with TSI_INTERNAL_ERROR.\r\nE0609 11:07:14.087291073   29191 secure_channel_create.c:127] Failed to create secure subchannel for secure name 'earthsmoke.service.cp1.consul:8080'\r\nE0609 11:07:14.087298127   29191 secure_channel_create.c:158] Failed to create subchannel arguments during subchannel creation.\r\n```\r\n\r\nI tracked the issue down to the fact that my CA bundle contains one certificate twice. \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope.",kind/bug|lang/core|priority/P2,ZhenLian,"### Should this be an issue in the gRPC issue tracker?\r\n\r\nYes.\r\n\r\n### What version of gRPC and what language are you using?\r\n \r\n```\r\n> GRPC::VERSION\r\n=> ""1.2.5""\r\n```\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\nLinux\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\nRuby\r\n\r\n```\r\n> RUBY_VERSION\r\n=> ""2.4.0""\r\n```\r\n \r\n### What did you do?\r\n\r\nI pointed GRPC at a CA bundle that included a duplicate certificate and then called a method on my stub:\r\n\r\n```ruby\r\ncreds = GRPC::Core::ChannelCredentials.new(File.read(ca_file))\r\nstub = MyService::Stub.new(server, creds)\r\nstub.some_method\r\n```\r\n\r\n### What did you expect to see?\r\n \r\nI would expect an exception not to be raised.\r\n \r\n### What did you see instead?\r\n \r\nWhen calling a method on my stub I get a `GRPC::DeadlineExceeded` error, along with the following debugging output:\r\n\r\n```\r\nE0609 11:07:14.082544835   29191 ssl_transport_security.c:602] Could not add root certificate to ssl context.\r\nE0609 11:07:14.082587138   29191 ssl_transport_security.c:1348] Cannot load server root certificates.\r\nE0609 11:07:14.082924913   29191 security_connector.c:837]   Handshaker factory creation failed with TSI_INTERNAL_ERROR.\r\nE0609 11:07:14.082939716   29191 secure_channel_create.c:127] Failed to create secure subchannel for secure name 'earthsmoke.service.cp1.consul:8080'\r\nE0609 11:07:14.082947490   29191 secure_channel_create.c:158] Failed to create subchannel arguments during subchannel creation.\r\nE0609 11:07:14.086947973   29191 ssl_transport_security.c:602] Could not add root certificate to ssl context.\r\nE0609 11:07:14.086965201   29191 ssl_transport_security.c:1348] Cannot load server root certificates.\r\nE0609 11:07:14.087280224   29191 security_connector.c:837]   Handshaker factory creation failed with TSI_INTERNAL_ERROR.\r\nE0609 11:07:14.087291073   29191 secure_channel_create.c:127] Failed to create secure subchannel for secure name 'earthsmoke.service.cp1.consul:8080'\r\nE0609 11:07:14.087298127   29191 secure_channel_create.c:158] Failed to create subchannel arguments during subchannel creation.\r\n```\r\n\r\nI tracked the issue down to the fact that my CA bundle contains one certificate twice. \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\nNope.","ruby\r\ncreds = GRPC::Core::ChannelCredentials.new(File.read(ca_file))\r\nstub = MyService::Stub.new(server, creds)\r\nstub.some_method\r\n"
11345,"Ruby deadlock### What version of gRPC and what language are you using?\r\n\r\n* v1.3.4 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n* Linux (Ubuntu)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n* ruby 2.3\r\n \r\n### What did you do?\r\n\r\nGRPC has been used within a unicorn (without preload_app).\r\n\r\n\r\n \r\n### What did you see instead?\r\n \r\n```\r\nE0531 09:11:03.277169204 2657243 ev_epoll_linux.c:1757]      add_poll_object: {""created"":""@1496214663.277114126"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":280,""referenced_errors"":[{""created"":""@1496214663.277110716"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":557,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\n```\r\n\r\n```\r\nE0531 09:12:38.959508831 2657217 ev_epoll_linux.c:1607]      pollset_work: {""created"":""@1496214758.959482077"",""description"":""pollset_work_and_unlock"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":280,""referenced_errors"":[{""created"":""@1496214758.959480505"",""description"":""OS Error"",""errno"":22,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":1438,""os_error"":""Invalid argument"",""syscall"":""epoll_wait() epoll fd: 20 failed with error: 22 (Invalid argument)""}]}\r\n```\r\n\r\n",lang/ruby,apolcyn,"### What version of gRPC and what language are you using?\r\n\r\n* v1.3.4 \r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n* Linux (Ubuntu)\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n\r\n* ruby 2.3\r\n \r\n### What did you do?\r\n\r\nGRPC has been used within a unicorn (without preload_app).\r\n\r\n```ruby\r\nloop do\r\n  stub = Etcdserverpb::KV::Stub.new(host, :this_channel_is_insecure)\r\n  res = stub.range(Etcdserverpb::RangeRequest.new(key: key)).kvs[0]\r\n  stub.instance_variable_get(""@ch"").close\r\n  break if res and res.value == ""1"" # It will take about minutes\r\n  sleep 1\r\nend\r\n```\r\n \r\n### What did you see instead?\r\n \r\n```\r\nE0531 09:11:03.277169204 2657243 ev_epoll_linux.c:1757]      add_poll_object: {""created"":""@1496214663.277114126"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":280,""referenced_errors"":[{""created"":""@1496214663.277110716"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":557,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\n```\r\n\r\n```\r\nE0531 09:12:38.959508831 2657217 ev_epoll_linux.c:1607]      pollset_work: {""created"":""@1496214758.959482077"",""description"":""pollset_work_and_unlock"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":280,""referenced_errors"":[{""created"":""@1496214758.959480505"",""description"":""OS Error"",""errno"":22,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":1438,""os_error"":""Invalid argument"",""syscall"":""epoll_wait() epoll fd: 20 failed with error: 22 (Invalid argument)""}]}\r\n```\r\n\r\n","ruby\r\nloop do\r\n  stub = Etcdserverpb::KV::Stub.new(host, :this_channel_is_insecure)\r\n  res = stub.range(Etcdserverpb::RangeRequest.new(key: key)).kvs[0]\r\n  stub.instance_variable_get(""@ch"").close\r\n  break if res and res.value == ""1"" # It will take about minutes\r\n  sleep 1\r\nend\r\n"
11287,"[node] options.host is not workPlease answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nCreate new issues for bugs and feature requests. An issue needs to be actionable. General gRPC discussions and usage questions belong to:\r\n- [grpc.io mailing list](https://groups.google.com/forum/#!forum/grpc-io)\r\n- [StackOverflow, with `grpc` tag](http://stackoverflow.com/questions/tagged/grpc)\r\n \r\n*Please don't double post your questions in more locations, we are monitoring both channels and the time spent de-duplicating questions can is better spent answering more user questions.*\r\n \r\n### What version of gRPC and what language are you using?\r\n- nodejs\r\n- `""grpc"": ""^1.3.2""`\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n mac\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n \r\n### What did you do?\r\nIf possible, provide a recipe for reproducing the error. Try being specific and include code snippets if helpful.\r\n\r\nsee https://github.com/atian25/grpc-host\r\n\r\n```js\r\nconst grpc = require('grpc');\r\nconst proto = grpc.load('./test.proto').example;\r\nconst host = 'localhost:50052';\r\n\r\nconst server = new grpc.Server();\r\nserver.addService(proto.Test.service, {\r\n  echo(call, callback) {\r\n    callback(null, { msg: 'server_' + call.request.id });\r\n  }\r\n});\r\nserver.bind(host, grpc.ServerCredentials.createInsecure());\r\nserver.start();\r\nconsole.log('grpc server start at %s', host);\r\n\r\nconst client = new proto.Test('localhost:50052', grpc.credentials.createInsecure());\r\n\r\nclient.echo({ id: 1}, function(err, response) {\r\n  console.log('[client1] 50052 recieved:', response);\r\n});\r\n\r\n// it connected!\r\nclient.echo({ id: 1 }, undefined, { host: 'localhost:50051', deadline: 10000 }, function (err, response) {\r\n  console.log('[client1] 50051 recieved:', response);\r\n});\r\n\r\n\r\nconst client2 = new proto.Test('localhost:50051', grpc.credentials.createInsecure());\r\nclient2.echo({ id: 1 }, function (err, response) {\r\n  console.log('[client2] 50051 recieved:', response, err && err.message);\r\n});\r\n\r\n// it connect fail!\r\nclient2.echo({ id: 1 }, undefined, { host: 'localhost:50052', deadline: 20000 }, function (err, response) {\r\n  console.log('[client2] 50052 recieved:', response, err && err.message);\r\n});\r\n```\r\n \r\n### What did you expect to see?\r\n \r\n`client2.echo({ id: 1 }, undefined, { host: 'localhost:50052' })` should connect to server.\r\n \r\n### What did you see instead?\r\n\r\ncustom `options.host` is not work.\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n \r\n\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n",lang/node,murgatroid99,"Please answer these questions before submitting your issue. \r\n \r\n### Should this be an issue in the gRPC issue tracker?\r\n \r\nCreate new issues for bugs and feature requests. An issue needs to be actionable. General gRPC discussions and usage questions belong to:\r\n- [grpc.io mailing list](https://groups.google.com/forum/#!forum/grpc-io)\r\n- [StackOverflow, with `grpc` tag](http://stackoverflow.com/questions/tagged/grpc)\r\n \r\n*Please don't double post your questions in more locations, we are monitoring both channels and the time spent de-duplicating questions can is better spent answering more user questions.*\r\n \r\n### What version of gRPC and what language are you using?\r\n- nodejs\r\n- `""grpc"": ""^1.3.2""`\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n mac\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n \r\n### What did you do?\r\nIf possible, provide a recipe for reproducing the error. Try being specific and include code snippets if helpful.\r\n\r\nsee https://github.com/atian25/grpc-host\r\n\r\n```js\r\nconst grpc = require('grpc');\r\nconst proto = grpc.load('./test.proto').example;\r\nconst host = 'localhost:50052';\r\n\r\nconst server = new grpc.Server();\r\nserver.addService(proto.Test.service, {\r\n  echo(call, callback) {\r\n    callback(null, { msg: 'server_' + call.request.id });\r\n  }\r\n});\r\nserver.bind(host, grpc.ServerCredentials.createInsecure());\r\nserver.start();\r\nconsole.log('grpc server start at %s', host);\r\n\r\nconst client = new proto.Test('localhost:50052', grpc.credentials.createInsecure());\r\n\r\nclient.echo({ id: 1}, function(err, response) {\r\n  console.log('[client1] 50052 recieved:', response);\r\n});\r\n\r\n// it connected!\r\nclient.echo({ id: 1 }, undefined, { host: 'localhost:50051', deadline: 10000 }, function (err, response) {\r\n  console.log('[client1] 50051 recieved:', response);\r\n});\r\n\r\n\r\nconst client2 = new proto.Test('localhost:50051', grpc.credentials.createInsecure());\r\nclient2.echo({ id: 1 }, function (err, response) {\r\n  console.log('[client2] 50051 recieved:', response, err && err.message);\r\n});\r\n\r\n// it connect fail!\r\nclient2.echo({ id: 1 }, undefined, { host: 'localhost:50052', deadline: 20000 }, function (err, response) {\r\n  console.log('[client2] 50052 recieved:', response, err && err.message);\r\n});\r\n```\r\n \r\n### What did you expect to see?\r\n \r\n`client2.echo({ id: 1 }, undefined, { host: 'localhost:50052' })` should connect to server.\r\n \r\n### What did you see instead?\r\n\r\ncustom `options.host` is not work.\r\n \r\nMake sure you include information that can help us debug (full error message, exception listing, stack trace, logs).\r\n \r\n```bash\r\n\u279c  grpc-host node ""/Users/tz/Workspaces/eggjs/test/grpc-host/index.js""\r\ngrpc server start at localhost:50052\r\n### log at client.js makeUnaryRequest`: args.options =  undefined\r\n### log at client.js makeUnaryRequest`: args.options =  { host: 'localhost:50051', deadline: 10000 }\r\n### log at client.js makeUnaryRequest`: args.options =  undefined\r\n### log at client.js makeUnaryRequest`: args.options =  { host: 'localhost:50052', deadline: 20000 }\r\n[client1] 50051 recieved: undefined\r\n[client2] 50052 recieved: undefined Deadline Exceeded\r\n[client2] 50051 recieved: undefined Connect Failed\r\n[client1] 50052 recieved: { msg: 'server_1' }\r\n```\r\n\r\n### Anything else we should know about your project / environment?\r\n\r\n","bash\r\n\u279c  grpc-host node ""/Users/tz/Workspaces/eggjs/test/grpc-host/index.js""\r\ngrpc server start at localhost:50052\r\n### log at client.js makeUnaryRequest`: args.options =  undefined\r\n### log at client.js makeUnaryRequest`: args.options =  { host: 'localhost:50051', deadline: 10000 }\r\n### log at client.js makeUnaryRequest`: args.options =  undefined\r\n### log at client.js makeUnaryRequest`: args.options =  { host: 'localhost:50052', deadline: 20000 }\r\n[client1] 50051 recieved: undefined\r\n[client2] 50052 recieved: undefined Deadline Exceeded\r\n[client2] 50051 recieved: undefined Connect Failed\r\n[client1] 50052 recieved: { msg: 'server_1' }\r\n"
11279,"Transitive dependencies are broken in Grpc C# (new-style csproj projects targeting net45+)Something about the packaging of gRPC - in particular the native component - looks like it's changed in 1.3.0, which causes the native library to not be found.\r\n\r\nSample csproj:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>net452</TargetFramework>\r\n  </PropertyGroup>  \r\n  <ItemGroup>\r\n    <PackageReference Include=""Grpc.Auth"" Version=""1.3.0"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nAnd program:\r\n\r\n\r\n\r\n(It doesn't get much more minimal than that :)\r\n\r\nRun `dotnet restore` then `dotnet run`, and you'll get an exception with a failure to find the native DLL. The exception isn't important - but the fact that the native DLL isn't under `bin\\Debug\\net452` is.\r\n\r\nNow, this can be fixed by *either* of:\r\n\r\n- Adding a *direct* package dependency to `Grpc.Core` version 1.3.0\r\n- Changing `Grpc.Auth` to version 1.2.0.\r\n\r\nPresumably there's some change in the packaging (I suspect of Grpc.Core) between 1.2.0 and 1.3.0 that makes this indirect dependency break.\r\n\r\nI'd urge this to be treated as a high-priority bug.",kind/bug|lang/C#|priority/P3,jtattermusch,"Something about the packaging of gRPC - in particular the native component - looks like it's changed in 1.3.0, which causes the native library to not be found.\r\n\r\nSample csproj:\r\n\r\n```xml\r\n<Project Sdk=""Microsoft.NET.Sdk"">\r\n  <PropertyGroup>\r\n    <OutputType>Exe</OutputType>\r\n    <TargetFramework>net452</TargetFramework>\r\n  </PropertyGroup>  \r\n  <ItemGroup>\r\n    <PackageReference Include=""Grpc.Auth"" Version=""1.3.0"" />\r\n  </ItemGroup>\r\n</Project>\r\n```\r\n\r\nAnd program:\r\n\r\n```csharp\r\npublic class Program\r\n{\r\n    static void Main()\r\n    {\r\n        // Just force the native methods to get loaded\r\n        var server = new Grpc.Core.Server();\r\n    }\r\n}\r\n```\r\n\r\n(It doesn't get much more minimal than that :)\r\n\r\nRun `dotnet restore` then `dotnet run`, and you'll get an exception with a failure to find the native DLL. The exception isn't important - but the fact that the native DLL isn't under `bin\\Debug\\net452` is.\r\n\r\nNow, this can be fixed by *either* of:\r\n\r\n- Adding a *direct* package dependency to `Grpc.Core` version 1.3.0\r\n- Changing `Grpc.Auth` to version 1.2.0.\r\n\r\nPresumably there's some change in the packaging (I suspect of Grpc.Core) between 1.2.0 and 1.3.0 that makes this indirect dependency break.\r\n\r\nI'd urge this to be treated as a high-priority bug.",csharp\r\npublic class Program\r\n{\r\n    static void Main()\r\n    {\r\n        // Just force the native methods to get loaded\r\n        var server = new Grpc.Core.Server();\r\n    }\r\n}\r\n
11277,"how to change backoff parametersIf my grpc server is down, and my client sends a series of 3 unary call, the first unary call fails instantly , the second fails only after 20 more seconds, and the third after another 20 more seconds.\r\n\r\nIs this behavior the ""[backoff behavior](https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md)"" described [on this page](https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md) ?\r\n\r\n## How to alter the backoff parameters from python client?\r\n\r\nI tried these with no effect:\r\n\r\n\r\nMany thanks!\r\n\r\nnote: ubuntu 16.04, python 3.6, grpcio 1.3.0",kind/question|lang/Python|priority/P2,dgquintas,"If my grpc server is down, and my client sends a series of 3 unary call, the first unary call fails instantly , the second fails only after 20 more seconds, and the third after another 20 more seconds.\r\n\r\nIs this behavior the ""[backoff behavior](https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md)"" described [on this page](https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md) ?\r\n\r\n## How to alter the backoff parameters from python client?\r\n\r\nI tried these with no effect:\r\n```python\r\n    channel_args = (\r\n        ('grpc.initial_reconnect_backoff_ms', 10000),\r\n        ('grpc.max_reconnect_backoff_ms', 1000),\r\n    )\r\n    secure_channel = grpc.secure_channel(\r\n        server_url,\r\n        channel_creds,\r\n        options=channel_args,\r\n    )\r\n```\r\n\r\nMany thanks!\r\n\r\nnote: ubuntu 16.04, python 3.6, grpcio 1.3.0","python\r\n    channel_args = (\r\n        ('grpc.initial_reconnect_backoff_ms', 10000),\r\n        ('grpc.max_reconnect_backoff_ms', 1000),\r\n    )\r\n    secure_channel = grpc.secure_channel(\r\n        server_url,\r\n        channel_creds,\r\n        options=channel_args,\r\n    )\r\n"
11240,"string representation of proto message incorrectly displays UTF8 chinese chararactersHello,\r\n\r\nHere is what I see when printing a proto object received through grpc having UTF8 string as one of the fields:\r\n\r\n### I'd expect to see that\r\n\r\n\r\nI suppose the ```__print__``` and ```__repr__``` methods should be adjusted...?\r\n\r\nThank you for your help,\r\n\r\nnote: Ubuntu 16.04, python gRPC@1.3.0",kind/bug|lang/Python|disposition/requires reporter action,nathanielmanistaatgoogle,"Hello,\r\n\r\nHere is what I see when printing a proto object received through grpc having UTF8 string as one of the fields:\r\n```python\r\nfield1: ""...'""\r\noerrmsg: ""\\345\\275\\223\\345\\211\\215\\347\\212\\266\\346\\200\\201\\347\\246\\201\\346\\255\\242\\346\\255\\244\\351\\241\\271\\346\\223\\215\\344\\275\\234""\r\nfield3: ""...""\r\n```\r\n### I'd expect to see that\r\n```python\r\nfield1: ""...'""\r\noerrmsg: ""\u5f53\u524d\u72b6\u6001\u7981\u6b62\u6b64\u9879\u64cd\u4f5c""\r\nfield3: ""...""\r\n```\r\n\r\nI suppose the ```__print__``` and ```__repr__``` methods should be adjusted...?\r\n\r\nThank you for your help,\r\n\r\nnote: Ubuntu 16.04, python gRPC@1.3.0","python\r\nfield1: ""...'""\r\noerrmsg: ""\\345\\275\\223\\345\\211\\215\\347\\212\\266\\346\\200\\201\\347\\246\\201\\346\\255\\242\\346\\255\\244\\351\\241\\271\\346\\223\\215\\344\\275\\234""\r\nfield3: ""...""\r\n"
11213,"FAILED: objc_macos_opt_native.objc-examples-buildFailed in master:\r\nhttps://grpc-testing.appspot.com/job/gRPC_master_macos/2396/testReport/junit/(root)/objc_macos_opt_native/objc_examples_build/\r\n```\r\nError Message\r\n\r\nFailure\r\nStacktrace\r\n\r\n Compiling RouteGuide-dummy.m\r\n Compiling RouteGuide.pbobjc.m\r\n Compiling RouteGuide.pbrpc.m\r\n Building library libRouteGuide.a\r\n Copying RouteGuide.pbrpc.h\r\n Copying RouteGuide.pbobjc.h\r\n Building Pods/Pods-RouteGuideClient [Debug]\r\n Check Dependencies\r\n Compiling Pods-RouteGuideClient-dummy.m\r\n Building library libPods-RouteGuideClient.a\r\n Building RouteGuideClient/RouteGuideClient [Debug]\r\n Check Dependencies\r\n Running script '[CP] Check Pods Manifest.lock'\r\n Compiling main.m\r\n Compiling AppDelegate.m\r\n Linking RouteGuideClient\r\n Copying route_guide_db.json\r\n Compiling Main.storyboard\r\n Processing Info.plist\r\n Generating 'RouteGuideClient.app.dSYM'\r\n Running script '[CP] Copy Pods Resources'\r\n Running script '[CP] Embed Pods Frameworks'\r\n Touching RouteGuideClient.app\r\n Build Succeeded\r\n\r\necho ""TIME:  $(date)""\r\ndate\r\nTIME:  Tue May 16 21:32:02 PDT 2017\r\nSCHEME=AuthSample                               \\\r\n  EXAMPLE_PATH=examples/objective-c/auth_sample \\\r\n  ./build_one_example.sh\r\n\r\n# Params:\r\n# EXAMPLE_PATH - directory of the example\r\n# SCHEME - scheme of the example, used by xcodebuild\r\n\r\n# CocoaPods requires the terminal to be using UTF-8 encoding.\r\nexport LANG=en_US.UTF-8\r\n\r\ncd `dirname $0`/../../..\r\ndirname $0\r\n\r\ncd $EXAMPLE_PATH\r\n\r\n# clean the directory\r\nrm -rf Pods\r\nrm -rf $SCHEME.xcworkspace\r\nrm -f Podfile.lock\r\n\r\npod install\r\nAnalyzing dependencies\r\nFetching podspec for `AuthTestService` from `.`\r\nDownloading dependencies\r\nInstalling !ProtoCompiler (3.2.0)\r\nInstalling !ProtoCompiler-gRPCPlugin (1.3.0)\r\nInstalling AuthTestService (0.0.1)\r\nInstalling BoringSSL (8.2)\r\n\r\n MARKDOWN TEMPLATE \r\n\r\n### Command\r\n\r\n```\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod install\r\n```\r\n\r\n### Report\r\n\r\n* What did you do?\r\n\r\n* What did you expect to happen?\r\n\r\n* What happened instead?\r\n\r\n\r\n### Stack\r\n\r\n```\r\n   CocoaPods : 1.2.0\r\n        Ruby : ruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-darwin14]\r\n    RubyGems : 2.6.10\r\n        Host : Mac OS X 10.10.5 (14F2315)\r\n       Xcode : 7.2.1 (7C1002)\r\n         Git : git version 2.5.4 (Apple Git-61)\r\nRuby lib dir : /usr/local/rvm/rubies/ruby-2.4.0/lib\r\nRepositories : master - https://github.com/CocoaPods/Specs.git @ f0acffea804ddc155099c1239659faa81738cb37\r\n```\r\n\r\n### Plugins\r\n\r\n```\r\ncocoapods-deintegrate : 1.0.1\r\ncocoapods-plugins     : 1.0.0\r\ncocoapods-search      : 1.0.0\r\ncocoapods-stats       : 1.0.0\r\ncocoapods-trunk       : 1.1.2\r\ncocoapods-try         : 1.1.0\r\n```\r\n\r\n### Podfile\r\n\r\n\r\n\r\n### Error\r\n\r\n```\r\nErrno::ENOENT - No such file or directory @ rb_file_s_lstat - /Users/jenkins/Library/Caches/CocoaPods/Pods/Release/BoringSSL/8.2-4135a/crypto/bytestring/internal.h\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1220:in `lstat'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1220:in `lstat'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1247:in `copy'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:409:in `block in copy_entry'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1381:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:406:in `copy_entry'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:384:in `block in cp_r'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1454:in `block in fu_each_src_dest'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1470:in `fu_each_src_dest0'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1452:in `fu_each_src_dest'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:383:in `cp_r'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/downloader.rb:54:in `block in download'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:142:in `message'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/downloader.rb:52:in `download'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer/pod_source_installer.rb:120:in `download_source'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer/pod_source_installer.rb:60:in `install!'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:355:in `install_source_of_pod'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:322:in `block (2 levels) in install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:85:in `titled_section'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:321:in `block in install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:313:in `each'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:313:in `install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:159:in `block in download_dependencies'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:64:in `section'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:157:in `download_dependencies'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:111:in `install!'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/command/install.rb:37:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/claide-1.0.1/lib/claide/command.rb:334:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/command.rb:52:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/bin/pod:55:in `<top (required)>'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod:22:in `load'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod:22:in `<main>'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/ruby_executable_hooks:15:in `eval'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/ruby_executable_hooks:15:in `<main>'\r\n```\r\n\r\n TEMPLATE END \r\n\r\n[!] Oh no, an error occurred.\r\n\r\nSearch for existing GitHub issues similar to yours:\r\nhttps://github.com/CocoaPods/CocoaPods/search?q=No+such+file+or+directory+%40+rb_file_s_lstat+-+%2FUsers%2Fjenkins%2FLibrary%2FCaches%2FCocoaPods%2FPods%2FRelease%2FBoringSSL%2F8.2-4135a%2Fcrypto%2Fbytestring%2Finternal.h&type=Issues\r\n\r\nIf none exists, create a ticket, with the template displayed above, on:\r\nhttps://github.com/CocoaPods/CocoaPods/issues/new\r\n\r\nBe sure to first read the contributing guide for details on how to properly submit a ticket:\r\nhttps://github.com/CocoaPods/CocoaPods/blob/master/CONTRIBUTING.md\r\n\r\nDon't forget to anonymize any private data!\r\n\r\nLooking for related issues on cocoapods/cocoapods...\r\nFound no similar issues. To create a new issue, please visit:\r\nhttps://github.com/cocoapods/cocoapods/issues/new\r\necho ""EXIT TIME:  $(date)""\r\ndate\r\nEXIT TIME:  Tue May 16 21:32:29 PDT 2017\r\nreal       237.81\r\nuser       134.62\r\nsys         39.89\r\n```",priority/P1|lang/ObjC|disposition/BUILDNURSE,muxi,"Failed in master:\r\nhttps://grpc-testing.appspot.com/job/gRPC_master_macos/2396/testReport/junit/(root)/objc_macos_opt_native/objc_examples_build/\r\n```\r\nError Message\r\n\r\nFailure\r\nStacktrace\r\n\r\n Compiling RouteGuide-dummy.m\r\n Compiling RouteGuide.pbobjc.m\r\n Compiling RouteGuide.pbrpc.m\r\n Building library libRouteGuide.a\r\n Copying RouteGuide.pbrpc.h\r\n Copying RouteGuide.pbobjc.h\r\n Building Pods/Pods-RouteGuideClient [Debug]\r\n Check Dependencies\r\n Compiling Pods-RouteGuideClient-dummy.m\r\n Building library libPods-RouteGuideClient.a\r\n Building RouteGuideClient/RouteGuideClient [Debug]\r\n Check Dependencies\r\n Running script '[CP] Check Pods Manifest.lock'\r\n Compiling main.m\r\n Compiling AppDelegate.m\r\n Linking RouteGuideClient\r\n Copying route_guide_db.json\r\n Compiling Main.storyboard\r\n Processing Info.plist\r\n Generating 'RouteGuideClient.app.dSYM'\r\n Running script '[CP] Copy Pods Resources'\r\n Running script '[CP] Embed Pods Frameworks'\r\n Touching RouteGuideClient.app\r\n Build Succeeded\r\n\r\necho ""TIME:  $(date)""\r\ndate\r\nTIME:  Tue May 16 21:32:02 PDT 2017\r\nSCHEME=AuthSample                               \\\r\n  EXAMPLE_PATH=examples/objective-c/auth_sample \\\r\n  ./build_one_example.sh\r\n\r\n# Params:\r\n# EXAMPLE_PATH - directory of the example\r\n# SCHEME - scheme of the example, used by xcodebuild\r\n\r\n# CocoaPods requires the terminal to be using UTF-8 encoding.\r\nexport LANG=en_US.UTF-8\r\n\r\ncd `dirname $0`/../../..\r\ndirname $0\r\n\r\ncd $EXAMPLE_PATH\r\n\r\n# clean the directory\r\nrm -rf Pods\r\nrm -rf $SCHEME.xcworkspace\r\nrm -f Podfile.lock\r\n\r\npod install\r\nAnalyzing dependencies\r\nFetching podspec for `AuthTestService` from `.`\r\nDownloading dependencies\r\nInstalling !ProtoCompiler (3.2.0)\r\nInstalling !ProtoCompiler-gRPCPlugin (1.3.0)\r\nInstalling AuthTestService (0.0.1)\r\nInstalling BoringSSL (8.2)\r\n\r\n MARKDOWN TEMPLATE \r\n\r\n### Command\r\n\r\n```\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod install\r\n```\r\n\r\n### Report\r\n\r\n* What did you do?\r\n\r\n* What did you expect to happen?\r\n\r\n* What happened instead?\r\n\r\n\r\n### Stack\r\n\r\n```\r\n   CocoaPods : 1.2.0\r\n        Ruby : ruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-darwin14]\r\n    RubyGems : 2.6.10\r\n        Host : Mac OS X 10.10.5 (14F2315)\r\n       Xcode : 7.2.1 (7C1002)\r\n         Git : git version 2.5.4 (Apple Git-61)\r\nRuby lib dir : /usr/local/rvm/rubies/ruby-2.4.0/lib\r\nRepositories : master - https://github.com/CocoaPods/Specs.git @ f0acffea804ddc155099c1239659faa81738cb37\r\n```\r\n\r\n### Plugins\r\n\r\n```\r\ncocoapods-deintegrate : 1.0.1\r\ncocoapods-plugins     : 1.0.0\r\ncocoapods-search      : 1.0.0\r\ncocoapods-stats       : 1.0.0\r\ncocoapods-trunk       : 1.1.2\r\ncocoapods-try         : 1.1.0\r\n```\r\n\r\n### Podfile\r\n\r\n```ruby\r\nsource 'https://github.com/CocoaPods/Specs.git'\r\nplatform :ios, '8.0'\r\n\r\ninstall! 'cocoapods', :deterministic_uuids => false\r\n\r\ntarget 'AuthSample' do\r\n  # Depend on the generated AuthTestService library.\r\n  pod 'AuthTestService', :path => '.'\r\n\r\n  # Depend on Google's OAuth2 library\r\n  pod 'Google/SignIn'\r\nend\r\n```\r\n\r\n### Error\r\n\r\n```\r\nErrno::ENOENT - No such file or directory @ rb_file_s_lstat - /Users/jenkins/Library/Caches/CocoaPods/Pods/Release/BoringSSL/8.2-4135a/crypto/bytestring/internal.h\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1220:in `lstat'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1220:in `lstat'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1247:in `copy'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:409:in `block in copy_entry'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1381:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1384:in `block in wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `each'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1383:in `wrap_traverse'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:406:in `copy_entry'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:384:in `block in cp_r'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1454:in `block in fu_each_src_dest'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1470:in `fu_each_src_dest0'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:1452:in `fu_each_src_dest'\r\n/usr/local/rvm/rubies/ruby-2.4.0/lib/ruby/2.4.0/fileutils.rb:383:in `cp_r'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/downloader.rb:54:in `block in download'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:142:in `message'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/downloader.rb:52:in `download'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer/pod_source_installer.rb:120:in `download_source'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer/pod_source_installer.rb:60:in `install!'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:355:in `install_source_of_pod'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:322:in `block (2 levels) in install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:85:in `titled_section'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:321:in `block in install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:313:in `each'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:313:in `install_pod_sources'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:159:in `block in download_dependencies'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/user_interface.rb:64:in `section'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:157:in `download_dependencies'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/installer.rb:111:in `install!'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/command/install.rb:37:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/claide-1.0.1/lib/claide/command.rb:334:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/lib/cocoapods/command.rb:52:in `run'\r\n/usr/local/rvm/gems/ruby-2.4.0/gems/cocoapods-1.2.0/bin/pod:55:in `<top (required)>'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod:22:in `load'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/pod:22:in `<main>'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/ruby_executable_hooks:15:in `eval'\r\n/usr/local/rvm/gems/ruby-2.4.0/bin/ruby_executable_hooks:15:in `<main>'\r\n```\r\n\r\n TEMPLATE END \r\n\r\n[!] Oh no, an error occurred.\r\n\r\nSearch for existing GitHub issues similar to yours:\r\nhttps://github.com/CocoaPods/CocoaPods/search?q=No+such+file+or+directory+%40+rb_file_s_lstat+-+%2FUsers%2Fjenkins%2FLibrary%2FCaches%2FCocoaPods%2FPods%2FRelease%2FBoringSSL%2F8.2-4135a%2Fcrypto%2Fbytestring%2Finternal.h&type=Issues\r\n\r\nIf none exists, create a ticket, with the template displayed above, on:\r\nhttps://github.com/CocoaPods/CocoaPods/issues/new\r\n\r\nBe sure to first read the contributing guide for details on how to properly submit a ticket:\r\nhttps://github.com/CocoaPods/CocoaPods/blob/master/CONTRIBUTING.md\r\n\r\nDon't forget to anonymize any private data!\r\n\r\nLooking for related issues on cocoapods/cocoapods...\r\nFound no similar issues. To create a new issue, please visit:\r\nhttps://github.com/cocoapods/cocoapods/issues/new\r\necho ""EXIT TIME:  $(date)""\r\ndate\r\nEXIT TIME:  Tue May 16 21:32:29 PDT 2017\r\nreal       237.81\r\nuser       134.62\r\nsys         39.89\r\n```","ruby\r\nsource 'https://github.com/CocoaPods/Specs.git'\r\nplatform :ios, '8.0'\r\n\r\ninstall! 'cocoapods', :deterministic_uuids => false\r\n\r\ntarget 'AuthSample' do\r\n  # Depend on the generated AuthTestService library.\r\n  pod 'AuthTestService', :path => '.'\r\n\r\n  # Depend on Google's OAuth2 library\r\n  pod 'Google/SignIn'\r\nend\r\n"
11147,"Stuck ruby processesPlease answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n\r\n* grpc (1.2.5 x86_64-linux)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n* Ubuntu\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n* ruby 2.3.1p112 (2016-04-26) [x86_64-linux-gnu] (ubuntu 16.04)\r\n* ruby:2.3 (docker)\r\n* ruby:2.4 (docker)\r\n \r\n### What did you do?\r\n\r\n\r\n \r\n### Error output\r\n\r\n```\r\nE0516 06:29:57.386005114 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.385926227"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.385917322"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.387668045 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387616934"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387609641"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.387852032 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387810895"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387803647"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388028852 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387971675"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387965857"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388215359 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388168948"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388160101"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388393055 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388338727"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388331607"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388558738 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388517531"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388511351"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388894755 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388843011"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388836824"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389052448 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389003510"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388998284"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389137216 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389094855"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389085118"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389316436 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389261045"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389252521"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389409661 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389367280"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389358370"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.389976942 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389928537"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389920687"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390149715 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390106177"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390099924"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390313221 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390271070"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390265607"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390469541 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390430296"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390424923"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390658218 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390582389"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390576903"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390859674 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390806683"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390800757"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391153624 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391087522"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391081248"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391335247 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391291646"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391285292"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391416942 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391377028"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391370138"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391560874 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391521047"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391514323"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.391641193 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391604532"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391599198"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\n``` \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n",lang/ruby,apolcyn,"Please answer these questions before submitting your issue. \r\n \r\n### What version of gRPC and what language are you using?\r\n\r\n* grpc (1.2.5 x86_64-linux)\r\n \r\n### What operating system (Linux, Windows, \u2026) and version?\r\n \r\n* Ubuntu\r\n \r\n### What runtime / compiler are you using (e.g. python version or version of gcc)\r\n \r\n* ruby 2.3.1p112 (2016-04-26) [x86_64-linux-gnu] (ubuntu 16.04)\r\n* ruby:2.3 (docker)\r\n* ruby:2.4 (docker)\r\n \r\n### What did you do?\r\n\r\n```ruby\r\nloop do\r\n  stub = Etcdserverpb::KV::Stub.new(host, :this_channel_is_insecure)\r\n  res = stub.range(Etcdserverpb::RangeRequest.new(key: key)).kvs[0]\r\n  break if res and res.value == ""1""\r\n  sleep 1\r\nend\r\n```\r\n \r\n### Error output\r\n\r\n```\r\nE0516 06:29:57.386005114 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.385926227"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.385917322"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.387668045 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387616934"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387609641"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.387852032 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387810895"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387803647"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388028852 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.387971675"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.387965857"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388215359 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388168948"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388160101"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388393055 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388338727"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388331607"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388558738 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388517531"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388511351"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.388894755 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.388843011"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388836824"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389052448 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389003510"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.388998284"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389137216 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389094855"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389085118"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389316436 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389261045"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389252521"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.389409661 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389367280"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389358370"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.389976942 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.389928537"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.389920687"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390149715 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390106177"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390099924"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390313221 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390271070"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390265607"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390469541 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390430296"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390424923"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390658218 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390582389"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390576903"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.390859674 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.390806683"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.390800757"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391153624 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391087522"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391081248"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391335247 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391291646"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391285292"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391416942 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391377028"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391370138"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":601,""os_error"":""Too many open files"",""syscall"":""epoll_create1""}]}\r\nE0516 06:29:57.391560874 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391521047"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391514323"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\nE0516 06:29:57.391641193 3628179 ev_epoll_linux.c:1971]      add_poll_object: {""created"":""@1494908997.391604532"",""description"":""polling_island_create"",""file"":""src/core/lib/iomgr/ev_epoll_linux.c"",""file_line"":324,""referenced_errors"":[{""created"":""@1494908997.391599198"",""description"":""OS Error"",""errno"":24,""file"":""src/core/lib/iomgr/wakeup_fd_eventfd.c"",""file_line"":50,""os_error"":""Too many open files"",""syscall"":""eventfd""}]}\r\n``` \r\n \r\n### Anything else we should know about your project / environment?\r\n\r\n```bash\r\n~ # ulimit -n\r\n1024\r\n```","ruby\r\nloop do\r\n  stub = Etcdserverpb::KV::Stub.new(host, :this_channel_is_insecure)\r\n  res = stub.range(Etcdserverpb::RangeRequest.new(key: key)).kvs[0]\r\n  break if res and res.value == ""1""\r\n  sleep 1\r\nend\r\n"
11014,"change maximum rpc message sizeI'm trying to call python grpc stub from golang, passing in a 30mb file, and am getting an error ""Rendezvous of RPC that terminated with (StatusCode.INVALID_ARGUMENT, Received message larger than max.""\r\n\r\nWhat can I do to increase the maximum message size passed in to the python grpc stub?\r\n\r\nWe are trying: \r\n\r\nBut we still receive the same error message.\r\n\r\nThank you for your help,\r\nMichael",lang/Python|disposition/requires reporter action|lang/go,nathanielmanistaatgoogle,"I'm trying to call python grpc stub from golang, passing in a 30mb file, and am getting an error ""Rendezvous of RPC that terminated with (StatusCode.INVALID_ARGUMENT, Received message larger than max.""\r\n\r\nWhat can I do to increase the maximum message size passed in to the python grpc stub?\r\n\r\nWe are trying: \r\n```python\r\nchannel = grpc.insecure_channel(""localhost:50001"",options=[('grpc.max_message_length',1024*1024*1024),\r\n('grpc.max_send_message_length',1024*1024*1024),\r\n('grpc.max_receive_message_length',1024*1024*1024)])\r\n```\r\nBut we still receive the same error message.\r\n\r\nThank you for your help,\r\nMichael","python\r\nchannel = grpc.insecure_channel(""localhost:50001"",options=[('grpc.max_message_length',1024*1024*1024),\r\n('grpc.max_send_message_length',1024*1024*1024),\r\n('grpc.max_receive_message_length',1024*1024*1024)])\r\n"
10675,[c#] [test] Mocking ServerCallContextI am trying to unit test my server class and having trouble mocking  `ServerCallContext`. \r\n\r\nHere's the method I am trying to unit test.\r\n\r\n\r\n\r\nI tried creating the context myself but there is no public constructor. \r\n\r\nI also tried mocking it using `Moq` and `Mock<ServerCallContext>` but that fails for the same reason. \r\n\r\nAny ideas? \r\n\r\n,lang/C#,jtattermusch,"I am trying to unit test my server class and having trouble mocking  `ServerCallContext`. \r\n\r\nHere's the method I am trying to unit test.\r\n\r\n```cs\r\npublic override Task<KeyValueMessage> GetValue(KeyMessage request, ServerCallContext context)\r\n{\r\n    // some implementation\r\n}\r\n```\r\n\r\nI tried creating the context myself but there is no public constructor. \r\n\r\nI also tried mocking it using `Moq` and `Mock<ServerCallContext>` but that fails for the same reason. \r\n\r\nAny ideas? \r\n\r\n","cs\r\npublic override Task<KeyValueMessage> GetValue(KeyMessage request, ServerCallContext context)\r\n{\r\n    // some implementation\r\n}\r\n"
10615,"Always generate body for python code elementsSince\r\n\r\n\r\n\r\nwithout a body or `pass` is invalid Python, we ensure an empty documentation comment is always generated to make the generated code always valid.\r\n\r\nFixes https://github.com/grpc/grpc/issues/9451\r\n",cla: yes,mehrdada,"Since\r\n\r\n```python\r\nclass Service:\r\n   \r\n```\r\n\r\nwithout a body or `pass` is invalid Python, we ensure an empty documentation comment is always generated to make the generated code always valid.\r\n\r\nFixes https://github.com/grpc/grpc/issues/9451\r\n",python\r\nclass Service:\r\n   \r\n
10547,"Python does not complain if wrong message type is used when fields matchThe easiest example is to modify the helloworld example and add the following message:\r\n\r\n```proto\r\n// A clone of HelloRequest with an extra field\r\nmessage HelloRequestUnused {\r\n  string name = 1;\r\n  string extra_field = 2;\r\n}\r\n```\r\n\r\nThen modify `examples/python/helloworld/greeter_client.py` to send `HelloRequestUnused` instead of `HelloRequest`:\r\n\r\n\r\nRunning `greeter_client.py` will not result in a `ValueError`, but will instead just work as if a `HelloRequest` had been passed. This can be very confusing as unexpected behaviour will happen when passing a wrong type to the RPC by accident.",lang/Python,haberman|nathanielmanistaatgoogle,"The easiest example is to modify the helloworld example and add the following message:\r\n\r\n```proto\r\n// A clone of HelloRequest with an extra field\r\nmessage HelloRequestUnused {\r\n  string name = 1;\r\n  string extra_field = 2;\r\n}\r\n```\r\n\r\nThen modify `examples/python/helloworld/greeter_client.py` to send `HelloRequestUnused` instead of `HelloRequest`:\r\n```python\r\nresponse = stub.SayHello(helloworld_pb2.HelloRequestUnused(name='you'))\r\n```\r\n\r\nRunning `greeter_client.py` will not result in a `ValueError`, but will instead just work as if a `HelloRequest` had been passed. This can be very confusing as unexpected behaviour will happen when passing a wrong type to the RPC by accident.",python\r\nresponse = stub.SayHello(helloworld_pb2.HelloRequestUnused(name='you'))\r\n
10534,"How to expand Max ConcurrentStreams on server?In C#, gRPC 1.2.2, I've encounted freeze server program.\r\nI found which is caused by the MaxConcurrentStreams option on the server.\r\n\r\n\r\n\r\nThis program shows following result.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/46207/24830738/cb8e9b48-1cc6-11e7-9daf-a56c85498e92.png)\r\n\r\nThe server is limited to 8000 subscriptions.\r\nI try to use `ChannelOptions.MaxConcurrentStreams` to expand this limitation,\r\nIf value is under 8000 that works but over 8000 does not work, the limit remained at 8000.\r\nMy environment is 8 core CPU so maybe 8000 is processor count * 1000?\r\n\r\nIn my system, each client subscribes to nearly 20 ServerStreaming.\r\nThat Subscribe will be used for more than 10 minutes.\r\nIn this state, the limit of one server will be only 400 clients.\r\nThis is too little!\r\n\r\nHow to expand Max ConcurrentStreams on server?",lang/C#,jtattermusch,"In C#, gRPC 1.2.2, I've encounted freeze server program.\r\nI found which is caused by the MaxConcurrentStreams option on the server.\r\n\r\n```csharp\r\n// generated from ""hellostreamingworld.proto""\r\n\r\npublic class ServerImpl : MultiGreeter.MultiGreeterBase\r\n{\r\n    static int serverRequestCount;\r\n\r\n    public override async Task sayHello(HelloRequest request, IServerStreamWriter<HelloReply> responseStream, ServerCallContext context)\r\n    {\r\n        Console.WriteLine(""Server:"" + Interlocked.Increment(ref serverRequestCount));\r\n\r\n        await Task.Delay(TimeSpan.FromSeconds(60));\r\n\r\n        Console.WriteLine(""Server:"" + Interlocked.Decrement(ref serverRequestCount));\r\n    }\r\n}\r\n\r\nclass Program\r\n{\r\n    static void Main(string[] args)\r\n    {\r\n        if (args.Length == 0 || args[0] == ""server"")\r\n        {\r\n            // host server\r\n            var server = new global::Grpc.Core.Server(new ChannelOption[]\r\n            {\r\n                // new ChannelOption(ChannelOptions.MaxConcurrentStreams, 99999)\r\n            })\r\n            {\r\n                Services = { MultiGreeter.BindService(new ServerImpl()) },\r\n                Ports = { new ServerPort(""localhost"", 12345, ServerCredentials.Insecure) }\r\n            };\r\n\r\n            server.Start();\r\n\r\n            if (args.Length != 0)\r\n            {\r\n                Console.ReadLine();\r\n            }\r\n        }\r\n\r\n        if (args.Length == 0 || args[0] == ""client"")\r\n        {\r\n            // client\r\n            var channel = new Channel(""localhost"", 12345, ChannelCredentials.Insecure);\r\n\r\n            var client = new MultiGreeter.MultiGreeterClient(channel);\r\n\r\n            var tasks = Enumerable.Range(1, 10000).Select(async x =>\r\n            {\r\n                Console.WriteLine(""client call: "" + x);\r\n                var streaming = client.sayHello(new HelloRequest());\r\n                await streaming.ResponseStream.MoveNext();\r\n            });\r\n\r\n            Task.WaitAll(tasks.ToArray());\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis program shows following result.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/46207/24830738/cb8e9b48-1cc6-11e7-9daf-a56c85498e92.png)\r\n\r\nThe server is limited to 8000 subscriptions.\r\nI try to use `ChannelOptions.MaxConcurrentStreams` to expand this limitation,\r\nIf value is under 8000 that works but over 8000 does not work, the limit remained at 8000.\r\nMy environment is 8 core CPU so maybe 8000 is processor count * 1000?\r\n\r\nIn my system, each client subscribes to nearly 20 ServerStreaming.\r\nThat Subscribe will be used for more than 10 minutes.\r\nIn this state, the limit of one server will be only 400 clients.\r\nThis is too little!\r\n\r\nHow to expand Max ConcurrentStreams on server?","csharp\r\n// generated from ""hellostreamingworld.proto""\r\n\r\npublic class ServerImpl : MultiGreeter.MultiGreeterBase\r\n{\r\n    static int serverRequestCount;\r\n\r\n    public override async Task sayHello(HelloRequest request, IServerStreamWriter<HelloReply> responseStream, ServerCallContext context)\r\n    {\r\n        Console.WriteLine(""Server:"" + Interlocked.Increment(ref serverRequestCount));\r\n\r\n        await Task.Delay(TimeSpan.FromSeconds(60));\r\n\r\n        Console.WriteLine(""Server:"" + Interlocked.Decrement(ref serverRequestCount));\r\n    }\r\n}\r\n\r\nclass Program\r\n{\r\n    static void Main(string[] args)\r\n    {\r\n        if (args.Length == 0 || args[0] == ""server"")\r\n        {\r\n            // host server\r\n            var server = new global::Grpc.Core.Server(new ChannelOption[]\r\n            {\r\n                // new ChannelOption(ChannelOptions.MaxConcurrentStreams, 99999)\r\n            })\r\n            {\r\n                Services = { MultiGreeter.BindService(new ServerImpl()) },\r\n                Ports = { new ServerPort(""localhost"", 12345, ServerCredentials.Insecure) }\r\n            };\r\n\r\n            server.Start();\r\n\r\n            if (args.Length != 0)\r\n            {\r\n                Console.ReadLine();\r\n            }\r\n        }\r\n\r\n        if (args.Length == 0 || args[0] == ""client"")\r\n        {\r\n            // client\r\n            var channel = new Channel(""localhost"", 12345, ChannelCredentials.Insecure);\r\n\r\n            var client = new MultiGreeter.MultiGreeterClient(channel);\r\n\r\n            var tasks = Enumerable.Range(1, 10000).Select(async x =>\r\n            {\r\n                Console.WriteLine(""client call: "" + x);\r\n                var streaming = client.sayHello(new HelloRequest());\r\n                await streaming.ResponseStream.MoveNext();\r\n            });\r\n\r\n            Task.WaitAll(tasks.ToArray());\r\n        }\r\n    }\r\n}\r\n"
10458,"gRPC 1.1 + electron 1.4.15 = problemsThe compiled grpc_node.node installed via node-pre-gyp gets loaded correctly in electron 1.4.15 but it makes my electron crash just after a function call. The client gets loaded correctly and the server get to receive the message but it seems that the client (in electron) crashes when it gets the answer.\r\n\r\nSo, I tried to build the module 'from sources' using electron-rebuild but it didn't work. After a lot of review of the issue, I realized that the problem is the WINDOWS_BUILD_WARNING project. In my console I get:\r\n\r\n> '""echo""' is not recognized as an internal or external command,\r\n\r\ngyp adds quotes around the echo command and the win10 console doesn't seem to like it.\r\n\r\nThe solution I found is to change that action by a rule in binding.gyp which allowed me to build the module correctly.\r\n\r\n\r\n\r\nWith the module built locally, my electron app works as expected; no crashes no nothing. So it seems the version hosted here: https://storage.googleapis.com/grpc-precompiled-binaries/node/grpc/v1.1.0/electron-v1.4-win32-x64.tar.gz doesn't sympathize with electron 1.4.15.\r\n\r\nSo it would be nice if you can upload a good version of the module for electorn 1.4**.15** and fix the issue of the quotes in the warning project.\r\n\r\nPlatform: nodejs + electron\r\nGrpc version: 1.1.0\r\nOS: win10 (14393.969)",lang/node,murgatroid99,"The compiled grpc_node.node installed via node-pre-gyp gets loaded correctly in electron 1.4.15 but it makes my electron crash just after a function call. The client gets loaded correctly and the server get to receive the message but it seems that the client (in electron) crashes when it gets the answer.\r\n\r\nSo, I tried to build the module 'from sources' using electron-rebuild but it didn't work. After a lot of review of the issue, I realized that the problem is the WINDOWS_BUILD_WARNING project. In my console I get:\r\n\r\n> '""echo""' is not recognized as an internal or external command,\r\n\r\ngyp adds quotes around the echo command and the win10 console doesn't seem to like it.\r\n\r\nThe solution I found is to change that action by a rule in binding.gyp which allowed me to build the module correctly.\r\n\r\n```javascript\r\n{\r\n  # IMPORTANT WINDOWS BUILD INFORMATION\r\n  # This library does not build on Windows without modifying the Node\r\n  # development packages that node-gyp downloads in order to build.\r\n  # Due to https://github.com/nodejs/node/issues/4932, the headers for\r\n  # BoringSSL conflict with the OpenSSL headers included by default\r\n  # when including the Node headers. The remedy for this is to remove\r\n  # the OpenSSL headers, from the downloaded Node development package,\r\n  # which is typically located in `.node-gyp` in your home directory.\r\n  'target_name': 'WINDOWS_BUILD_WARNING',\r\n  'rules': [\r\n    {\r\n      'rule_name': 'WINDOWS_BUILD_WARNING',\r\n      'extension': 'S',\r\n      'inputs': [\r\n        'package.json'\r\n      ],\r\n      'outputs': [\r\n        'ignore_this_part'\r\n      ],\r\n      'action': ['echo', 'IMPORTANT: Due to https://github.com/nodejs/node/issues/4932, to build this library on Windows, you must first remove <(node_root_dir)/include/node/openssl/']\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nWith the module built locally, my electron app works as expected; no crashes no nothing. So it seems the version hosted here: https://storage.googleapis.com/grpc-precompiled-binaries/node/grpc/v1.1.0/electron-v1.4-win32-x64.tar.gz doesn't sympathize with electron 1.4.15.\r\n\r\nSo it would be nice if you can upload a good version of the module for electorn 1.4**.15** and fix the issue of the quotes in the warning project.\r\n\r\nPlatform: nodejs + electron\r\nGrpc version: 1.1.0\r\nOS: win10 (14393.969)","javascript\r\n{\r\n  # IMPORTANT WINDOWS BUILD INFORMATION\r\n  # This library does not build on Windows without modifying the Node\r\n  # development packages that node-gyp downloads in order to build.\r\n  # Due to https://github.com/nodejs/node/issues/4932, the headers for\r\n  # BoringSSL conflict with the OpenSSL headers included by default\r\n  # when including the Node headers. The remedy for this is to remove\r\n  # the OpenSSL headers, from the downloaded Node development package,\r\n  # which is typically located in `.node-gyp` in your home directory.\r\n  'target_name': 'WINDOWS_BUILD_WARNING',\r\n  'rules': [\r\n    {\r\n      'rule_name': 'WINDOWS_BUILD_WARNING',\r\n      'extension': 'S',\r\n      'inputs': [\r\n        'package.json'\r\n      ],\r\n      'outputs': [\r\n        'ignore_this_part'\r\n      ],\r\n      'action': ['echo', 'IMPORTANT: Due to https://github.com/nodejs/node/issues/4932, to build this library on Windows, you must first remove <(node_root_dir)/include/node/openssl/']\r\n    }\r\n  ]\r\n}\r\n"
10254,"C++ ServerBuilder::BuildAndStart hangs indefinitely So for the past couple of days I've been trying to implement a system that spawns a GRPC server in C++ that is very closely replicated from the example:\r\nhttps://github.com/grpc/grpc/blob/master/examples/cpp/route_guide/route_guide_server.cc#L184\r\n\r\nBuilding the server on localhost:50051\r\n\r\n\r\n\r\nConnecting a client to said server on localhost:50051\r\n\r\n\r\n\r\nIt seems as though previously I was able to successfully connect clients until over a certain amount of hours, `BuildAndStart` is hanging infinitely even after restarting the computer and verifying that the ports aren't being used.\r\n\r\nI should mention that I am calling RPC methods every millisecond as a way for a 'keep-alive' check for the client.\r\n\r\nI took liberty to see where exactly the code seems to be hanging and it actually seems to be hanging on `std::launch` with this call stack:\r\n\r\n```\r\nstd::_Launch\r\nThreadManager::WorkerThread(class grpc::ThreadManager*)\r\nThreadManager::Initialiize\r\nServer::Start\r\n```\r\n\r\nHope this is enough information to describe the issue.\r\n",kind/bug|lang/c++|priority/P1|platform/Windows|disposition/requires reporter action|transient/fixit,kpayson64,"So for the past couple of days I've been trying to implement a system that spawns a GRPC server in C++ that is very closely replicated from the example:\r\nhttps://github.com/grpc/grpc/blob/master/examples/cpp/route_guide/route_guide_server.cc#L184\r\n\r\nBuilding the server on localhost:50051\r\n\r\n```cpp\r\nClientCommunicatorImpl service;\r\nServerBuilder builder;\r\n\r\nbuilder.AddListeningPort(""localhost:50051"", grpc::InsecureServerCredentials());\r\nbuilder.RegisterService(&service);\r\nunique_ptr<Server> server = builder.BuildAndStart();\r\nserver->Wait();\r\n```\r\n\r\nConnecting a client to said server on localhost:50051\r\n\r\n```cpp\r\nshared_ptr<ChannelCredentials> creds = grpc::InsecureChannelCredentials();\r\n\r\nthis->m_pChannel = CreateChannel(""localhost:50051""), creds);\r\nthis->m_pStub = CommunicatorService::NewStub(this->m_pChannel);\r\n```\r\n\r\nIt seems as though previously I was able to successfully connect clients until over a certain amount of hours, `BuildAndStart` is hanging infinitely even after restarting the computer and verifying that the ports aren't being used.\r\n\r\nI should mention that I am calling RPC methods every millisecond as a way for a 'keep-alive' check for the client.\r\n\r\nI took liberty to see where exactly the code seems to be hanging and it actually seems to be hanging on `std::launch` with this call stack:\r\n\r\n```\r\nstd::_Launch\r\nThreadManager::WorkerThread(class grpc::ThreadManager*)\r\nThreadManager::Initialiize\r\nServer::Start\r\n```\r\n\r\nHope this is enough information to describe the issue.\r\n","cpp\r\nClientCommunicatorImpl service;\r\nServerBuilder builder;\r\n\r\nbuilder.AddListeningPort(""localhost:50051"", grpc::InsecureServerCredentials());\r\nbuilder.RegisterService(&service);\r\nunique_ptr<Server> server = builder.BuildAndStart();\r\nserver->Wait();\r\n"
10144,"Ruby server ActiveCall#cancelled? never returns true.Here is a simple demo to reproduce the problem: \r\n\r\n### say.proto:\r\n```protobuf\r\nsyntax = ""proto3"";\r\n\r\nservice Say {\r\n  rpc Hi(HiRequest) returns (HiResponse);\r\n}\r\n\r\nmessage HiRequest {}\r\nmessage HiResponse {\r\n  string message = 1;\r\n}\r\n```\r\nGenerate the standard ruby files using `protoc` + `grpc_ruby_plugin`... \r\n\r\n### say_client.rb\r\n\r\n\r\n\r\n### say.rb\r\n\r\n\r\nStart the server first by running `say.rb`, then run `say_client.rb`.  The client will cancel the request in 2 seconds.  After that, the server will keep on running and keep outputting ""running hi...""",kind/enhancement|lang/ruby|priority/P2|disposition/stale,apolcyn,"Here is a simple demo to reproduce the problem: \r\n\r\n### say.proto:\r\n```protobuf\r\nsyntax = ""proto3"";\r\n\r\nservice Say {\r\n  rpc Hi(HiRequest) returns (HiResponse);\r\n}\r\n\r\nmessage HiRequest {}\r\nmessage HiResponse {\r\n  string message = 1;\r\n}\r\n```\r\nGenerate the standard ruby files using `protoc` + `grpc_ruby_plugin`... \r\n\r\n### say_client.rb\r\n```ruby\r\nrequire 'grpc'\r\nrequire 'say_services_pb'\r\n\r\nstub = Say::Stub.new(""0.0.0.0:50051"", :this_channel_is_insecure)\r\nop = stub.hi(HiRequest.new, return_op: true)\r\nt = Thread.new { op.execute }  # async request\r\nsleep(2)  # sleeps for 2 seconds then cancels the operation\r\nop.cancel\r\nputs ""cancelled...""\r\nt.join  # throws a GRPC::Cancelled exception then exists\r\n```\r\n\r\n\r\n### say.rb\r\n```ruby\r\nrequire 'grpc'\r\nrequire 'say_services_pb'\r\n\r\nmodule Say\r\n  class Server < Service\r\n    def hi(request, call)\r\n      until call.cancelled? do # server will wait for the client to cancel and check the cancelled? status every second\r\n        puts ""running hi...""  # this will run forever even after client cancels the call\r\n        sleep(1)\r\n      end\r\n      puts ""cancelled...""\r\n      HiResponse.new(message: 'hi')\r\n    end\r\n  end\r\nend\r\n\r\nif __FILE__ == $0\r\n  s = GRPC::RpcServer.new\r\n  server_uri = ""0.0.0.0:50051""\r\n  s.add_http2_port(server_uri, :this_port_is_insecure)\r\n  s.handle(Say::Server)\r\n  puts server_uri\r\n  s.run_till_terminated\r\nend\r\n```\r\n\r\nStart the server first by running `say.rb`, then run `say_client.rb`.  The client will cancel the request in 2 seconds.  After that, the server will keep on running and keep outputting ""running hi...""","ruby\r\nrequire 'grpc'\r\nrequire 'say_services_pb'\r\n\r\nstub = Say::Stub.new(""0.0.0.0:50051"", :this_channel_is_insecure)\r\nop = stub.hi(HiRequest.new, return_op: true)\r\nt = Thread.new { op.execute }  # async request\r\nsleep(2)  # sleeps for 2 seconds then cancels the operation\r\nop.cancel\r\nputs ""cancelled...""\r\nt.join  # throws a GRPC::Cancelled exception then exists\r\n"
9988,"TLS Golang Server with Python client not workingHello,\r\n\r\nI've come across strange issue. I have Golang grpc server and python client. I have tested these together in insecure mode, which works perfectly. I'd like to add TLS, but for some reason python client can't connect to the server and server only returns these cryptic messages:\r\n\r\n```\r\n2017/03/06 14:56:14 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = ""transport: write tcp [::1]:8002->[::1]:47542: write: broken pipe""\r\n2017/03/06 14:56:14 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = ""transport: write tcp 127.0.0.1:8002->127.0.0.1:50096: write: broken pipe""\r\n2017/03/06 15:06:25 transport: http2Server.HandleStreams failed to receive the preface from client: EOF\r\n```\r\n\r\nServer:\r\n\r\n\r\nClient:\r\n\r\n\r\nThe certificates were generated with certstrap.\r\n\r\nThe strange thing is, when I create Golang client, it works perfectly (even with mutual TLS), but never when using Go server and Python client.\r\n\r\nAny ideas what should I try?\r\n\r\nThank you.",lang/Python|area/security|lang/go,kpayson64,"Hello,\r\n\r\nI've come across strange issue. I have Golang grpc server and python client. I have tested these together in insecure mode, which works perfectly. I'd like to add TLS, but for some reason python client can't connect to the server and server only returns these cryptic messages:\r\n\r\n```\r\n2017/03/06 14:56:14 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = ""transport: write tcp [::1]:8002->[::1]:47542: write: broken pipe""\r\n2017/03/06 14:56:14 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = ""transport: write tcp 127.0.0.1:8002->127.0.0.1:50096: write: broken pipe""\r\n2017/03/06 15:06:25 transport: http2Server.HandleStreams failed to receive the preface from client: EOF\r\n```\r\n\r\nServer:\r\n```go\r\nport := 8002\r\naddr := fmt.Sprintf(""0.0.0.0:%d"", port)\r\nlis, err := net.Listen(""tcp"", addr)\r\nif err != nil {\r\n\tgrpclog.Fatalf(""failed to listen: %v"", err)\r\n}\r\n\r\ncreds, err := credentials.NewServerTLSFromFile(""../cert/out/127.0.0.1.crt"", ""../cert/out/127.0.0.1.key"")\r\nif err != nil {\r\n\tgrpclog.Fatalf(""failed to create credentials: %v"", err)\r\n}\r\nopts := []grpc.ServerOption{\r\n\tgrpc.Creds(creds),\r\n}\r\ngrpcServer := grpc.NewServer(opts...)\r\n\r\n// regiser + Serve omitted\r\n```\r\n\r\nClient:\r\n```python\r\nca_cert = open('../cert/out/CA.crt').read()\r\ncreds = grpc.ssl_channel_credentials(root_certificates=ca_cert)\r\nchannel = grpc.secure_channel(""127.0.0.1:8002"", creds)\r\n```\r\n\r\nThe certificates were generated with certstrap.\r\n\r\nThe strange thing is, when I create Golang client, it works perfectly (even with mutual TLS), but never when using Go server and Python client.\r\n\r\nAny ideas what should I try?\r\n\r\nThank you.","go\r\nport := 8002\r\naddr := fmt.Sprintf(""0.0.0.0:%d"", port)\r\nlis, err := net.Listen(""tcp"", addr)\r\nif err != nil {\r\n\tgrpclog.Fatalf(""failed to listen: %v"", err)\r\n}\r\n\r\ncreds, err := credentials.NewServerTLSFromFile(""../cert/out/127.0.0.1.crt"", ""../cert/out/127.0.0.1.key"")\r\nif err != nil {\r\n\tgrpclog.Fatalf(""failed to create credentials: %v"", err)\r\n}\r\nopts := []grpc.ServerOption{\r\n\tgrpc.Creds(creds),\r\n}\r\ngrpcServer := grpc.NewServer(opts...)\r\n\r\n// regiser + Serve omitted\r\n"
9798,"PHP tests don't runCan't get the php unit tests to run from following the snippet in the readme. Looks like nothing is executing?\r\n\r\nCommands\r\n\r\n\r\nOutput\r\n```\r\nsh bin/run_tests.sh\r\n++ dirname bin/run_tests.sh\r\n+ cd bin/../../..\r\n++ pwd\r\n+ root=/Users/trent/grpc\r\n+ cd src/php/bin\r\n+ source ./determine_extension_dir.sh\r\n++ set -e\r\n+++ php-config --extension-dir\r\n++ default_extension_dir=/usr/local/Cellar/php55/5.5.35/lib/php/extensions/no-debug-non-zts-20121212\r\n++ '[' '!' -e /usr/local/Cellar/php55/5.5.35/lib/php/extensions/no-debug-non-zts-20121212/grpc.so ']'\r\n++ extension_dir='-d extension=grpc.so'\r\n+ export DYLD_LIBRARY_PATH=/Users/trent/grpc/libs/\r\n+ DYLD_LIBRARY_PATH=/Users/trent/grpc/libs/\r\n++ which phpunit\r\n+ php -d extension=grpc.so -d max_execution_time=300 -v --debug ../tests/unit_tests\r\nPHP 5.5.35 (cli) (built: Apr 29 2016 04:40:10)\r\nCopyright (c) 1997-2015 The PHP Group\r\nZend Engine v2.5.0, Copyright (c) 1998-2015 Zend Technologies\r\n```",lang/php|disposition/to close,stanley-cheung,"Can't get the php unit tests to run from following the snippet in the readme. Looks like nothing is executing?\r\n\r\nCommands\r\n```bash\r\ngit clone -b $(curl -L http://grpc.io/release) https://github.com/grpc/grpc\r\ncd grpc\r\ngit pull --recurse-submodules && git submodule update --init --recursive\r\ncd grpc/src/php\r\n./bin/run_tests.sh\r\n```\r\n\r\nOutput\r\n```\r\nsh bin/run_tests.sh\r\n++ dirname bin/run_tests.sh\r\n+ cd bin/../../..\r\n++ pwd\r\n+ root=/Users/trent/grpc\r\n+ cd src/php/bin\r\n+ source ./determine_extension_dir.sh\r\n++ set -e\r\n+++ php-config --extension-dir\r\n++ default_extension_dir=/usr/local/Cellar/php55/5.5.35/lib/php/extensions/no-debug-non-zts-20121212\r\n++ '[' '!' -e /usr/local/Cellar/php55/5.5.35/lib/php/extensions/no-debug-non-zts-20121212/grpc.so ']'\r\n++ extension_dir='-d extension=grpc.so'\r\n+ export DYLD_LIBRARY_PATH=/Users/trent/grpc/libs/\r\n+ DYLD_LIBRARY_PATH=/Users/trent/grpc/libs/\r\n++ which phpunit\r\n+ php -d extension=grpc.so -d max_execution_time=300 -v --debug ../tests/unit_tests\r\nPHP 5.5.35 (cli) (built: Apr 29 2016 04:40:10)\r\nCopyright (c) 1997-2015 The PHP Group\r\nZend Engine v2.5.0, Copyright (c) 1998-2015 Zend Technologies\r\n```",bash\r\ngit clone -b $(curl -L http://grpc.io/release) https://github.com/grpc/grpc\r\ncd grpc\r\ngit pull --recurse-submodules && git submodule update --init --recursive\r\ncd grpc/src/php\r\n./bin/run_tests.sh\r\n
9796,"Inconsistent rewriting of Empty to GPBEmpty with PHPUsing the recommended protoc invocation from [here](http://www.grpc.io/docs/quickstart/php.html#generate-grpc-code), a proto message of name `Empty` is generated as `GPBEmpty` under the `GPBMetadata` directory, but under the non-metadata directory it is still generated as `Empty` causing errors in PHP.\r\n\r\nYou can see in the generated non-metadata files that it knows about the `GPBEmpty` name from the metadata, but it still has the class name `Empty`.\r\n\r\n\r\n\r\nprotoc version: `libprotoc 3.2.0`",lang/php,stanley-cheung,"Using the recommended protoc invocation from [here](http://www.grpc.io/docs/quickstart/php.html#generate-grpc-code), a proto message of name `Empty` is generated as `GPBEmpty` under the `GPBMetadata` directory, but under the non-metadata directory it is still generated as `Empty` causing errors in PHP.\r\n\r\nYou can see in the generated non-metadata files that it knows about the `GPBEmpty` name from the metadata, but it still has the class name `Empty`.\r\n\r\n```php\r\nclass Empty extends \\Google\\Protobuf\\Internal\\Message\r\n{\r\n\r\n    public function __construct() {\r\n        \\GPBMetadata\\MyProject\\Rpc\\GPBEmpty::initOnce();\r\n        parent::__construct();\r\n    }\r\n\r\n}\r\n```\r\n\r\nprotoc version: `libprotoc 3.2.0`",php\r\nclass Empty extends \\Google\\Protobuf\\Internal\\Message\r\n{\r\n\r\n    public function __construct() {\r\n        \\GPBMetadata\\MyProject\\Rpc\\GPBEmpty::initOnce();\r\n        parent::__construct();\r\n    }\r\n\r\n}\r\n
9621,"gRPC can't recover itself after network changedI'm writing API server that talks to GoogleCloud and underlying calls over grpc protocol, the production deployment is in Compute Engine or App Engine but development is on local laptop; I found a problem of gRPC is it can't recover itself after network changed, like moved over different wifi hotspot, and suspend/resume; it always failed with `Error: Deadline Exceeded` and I have to restart this Server\r\n\r\n\r\n\r\nthis is grpc@1.1.1 from npm used by `@google-cloud/bigquery` `@google-cloud/monitoring` and many other `@google-cloud` Nodejs libraries\r\n\r\nWonder if gRPC has design to overcome network fail and recover",lang/node|disposition/to close,murgatroid99,"I'm writing API server that talks to GoogleCloud and underlying calls over grpc protocol, the production deployment is in Compute Engine or App Engine but development is on local laptop; I found a problem of gRPC is it can't recover itself after network changed, like moved over different wifi hotspot, and suspend/resume; it always failed with `Error: Deadline Exceeded` and I have to restart this Server\r\n\r\n```console\r\n[2017-02-07T17:45:30.690Z] ERROR: App/25905 on hostname: unable to format response (err.code=4)\r\n    Error: Deadline Exceeded\r\n        at /node_modules/grpc/src/node/src/client.js:434:17\r\n```\r\n\r\nthis is grpc@1.1.1 from npm used by `@google-cloud/bigquery` `@google-cloud/monitoring` and many other `@google-cloud` Nodejs libraries\r\n\r\nWonder if gRPC has design to overcome network fail and recover",console\r\n[2017-02-07T17:45:30.690Z] ERROR: App/25905 on hostname: unable to format response (err.code=4)\r\n    Error: Deadline Exceeded\r\n        at /node_modules/grpc/src/node/src/client.js:434:17\r\n
9559,PHP grpc client leaking memoryWhen running the example `route_guide_client.php` a bit of memory are leaking.\r\nIt becomes really problematic when a client is running in a worker (or any long running process).\r\nYou can see how obvious is the memory leak with that script\r\n\r\n\r\n\r\n\r\nThe environment:\r\n\r\n\r\n\r\n\r\nIt may be related to that issue #8074 ,lang/php|priority/P1,stanley-cheung,"When running the example `route_guide_client.php` a bit of memory are leaking.\r\nIt becomes really problematic when a client is running in a worker (or any long running process).\r\nYou can see how obvious is the memory leak with that script\r\n```php\r\n<?php\r\nrequire dirname(__FILE__).'/../vendor/autoload.php';\r\n\r\n@include_once dirname(__FILE__).'/route_guide.pb.php';\r\n@include_once dirname(__FILE__).'/route_guide_grpc_pb.php';\r\n\r\n$client = new Routeguide\\RouteGuideClient('localhost:50051', [\r\n    'credentials' => Grpc\\ChannelCredentials::createInsecure(),\r\n]);\r\n$point = new Routeguide\\Point();\r\n$point->setLatitude(409146138);\r\n$point->setLongitude(-746188906);\r\nfor($i=0;;$i++) {\r\n    $client->GetFeature($point)->wait();\r\n    usleep(30000);\r\n    if($i%1000) {\r\n        echo ""~"" . (memory_get_usage() * 10e-6). ""Mb\\n"";\r\n    }\r\n}\r\n```\r\n\r\n```shell\r\n> grpc/examples/php/route_guide$ php leak.php\r\n~13.90104Mb\r\n~13.90888Mb\r\n~13.9164Mb\r\n~13.92392Mb\r\n...\r\n~20.78888Mb\r\n~20.7964Mb\r\n~20.80392Mb\r\n```\r\n\r\nThe environment:\r\n```shell\r\n> grpc/examples/php/route_guide$ php -v\r\nPHP 7.0.15-1~dotdeb+8.1 (cli) ( NTS )\r\nCopyright (c) 1997-2017 The PHP Group\r\nZend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies\r\n    with Zend OPcache v7.0.15-1~dotdeb+8.1, Copyright (c) 1999-2017, by Zend Technologies\r\n```\r\n\r\n```shell\r\n> grpc/examples/php/route_guide$ pecl list\r\nINSTALLED PACKAGES, CHANNEL PECL.PHP.NET:\r\n=========================================\r\nPACKAGE VERSION STATE\r\ngrpc    1.1.0   stable\r\n```\r\n\r\nIt may be related to that issue #8074 ","php\r\n<?php\r\nrequire dirname(__FILE__).'/../vendor/autoload.php';\r\n\r\n@include_once dirname(__FILE__).'/route_guide.pb.php';\r\n@include_once dirname(__FILE__).'/route_guide_grpc_pb.php';\r\n\r\n$client = new Routeguide\\RouteGuideClient('localhost:50051', [\r\n    'credentials' => Grpc\\ChannelCredentials::createInsecure(),\r\n]);\r\n$point = new Routeguide\\Point();\r\n$point->setLatitude(409146138);\r\n$point->setLongitude(-746188906);\r\nfor($i=0;;$i++) {\r\n    $client->GetFeature($point)->wait();\r\n    usleep(30000);\r\n    if($i%1000) {\r\n        echo ""~"" . (memory_get_usage() * 10e-6). ""Mb\\n"";\r\n    }\r\n}\r\n"
9510,"Extremely high memory usage in 1.1.0-pre with moderately sized messagesI'm creating a benchmark of RPC libraries where a client and server are created in the same process and the performance of message passing is measured in each iteration. Creating the server and client happens once per benchmark, i.e. not per iteration.\r\n\r\nOne of the benchmarks uses cached large blobs of increasing sizes from 4k up to 32M. Using **grpc 1.1.0-pre** with the protobuf it references as a submodule, I consistently get several gigabytes of memory usage once the benchmark reaches the 2048k size (not sure if that's not from the previous 1024k step). Unfortunately I can't tell how much, because it eats all the memory I have in this machine (8 GB). \r\n\r\nI was able to reproduce this consistently on both my host machine (Arch x64, g++6.3) and a VM (Ubuntu 16.04 x64, g++5.4).\r\n\r\nI reduced my benchmark to only contain grpc code, this is the gist of it:\r\n\r\n\r\n\r\nThe benchmark calls `get_blob` with increasing sizes through a large number of iterations.\r\n\r\nYou can clone this branch for the full source: https://github.com/rpclib/benchmarks/tree/grpc_high_memory_repro\r\n\r\nTo build the above repo, [conan](https://www.conan.io/) is required:\r\n\r\n```\r\nmkdir build && cd build\r\nconan install .. --build missing\r\ncmake ..\r\nmake\r\n```\r\n\r\nThere is of course a chance that I made a mistake in my benchmarking code. Please let me know if I can provide any more details. \r\n\r\nAlso, I would be interested in you opinion about whether or not this code is a fair entry in this benchmark, as I'm not very knowledgeable about grpc. You can see the implementations using other libraries on the master branch of the same repo for comparison.",lang/c++,lyuxuan,"I'm creating a benchmark of RPC libraries where a client and server are created in the same process and the performance of message passing is measured in each iteration. Creating the server and client happens once per benchmark, i.e. not per iteration.\r\n\r\nOne of the benchmarks uses cached large blobs of increasing sizes from 4k up to 32M. Using **grpc 1.1.0-pre** with the protobuf it references as a submodule, I consistently get several gigabytes of memory usage once the benchmark reaches the 2048k size (not sure if that's not from the previous 1024k step). Unfortunately I can't tell how much, because it eats all the memory I have in this machine (8 GB). \r\n\r\nI was able to reproduce this consistently on both my host machine (Arch x64, g++6.3) and a VM (Ubuntu 16.04 x64, g++5.4).\r\n\r\nI reduced my benchmark to only contain grpc code, this is the gist of it:\r\n\r\n```cpp\r\nclass grpc_service : public GrpcServiceBenchmark::Service {\r\npublic:\r\n  ::grpc::Status get_blob(::grpc::ServerContext *context,\r\n                            const ::EmptyRequest *request,\r\n                            ::BlobResponse *response) override {\r\n      response->set_data(::get_blob(blob_size_));\r\n      return ::grpc::Status::OK;\r\n  }\r\n  int blob_size_;\r\n};\r\n\r\nclass grpc_bench : public benchmark::Fixture {\r\npublic:\r\n  grpc_bench()\r\n      : channel_(grpc::CreateChannel(server_addr_,\r\n                                     grpc::InsecureChannelCredentials())),\r\n        client_(channel_) {\r\n    grpc::ServerBuilder b;\r\n    b.AddListeningPort(server_addr_, grpc::InsecureServerCredentials());\r\n    b.RegisterService(&service_impl_);\r\n    server_ = b.BuildAndStart();\r\n  }\r\n  void get_blob(int param) {\r\n    service_impl_.blob_size_ = param;\r\n    grpc::ClientContext client_context;\r\n    EmptyRequest request;\r\n    BlobResponse response;\r\n    auto status = client_.get_blob(&client_context, request, &response);\r\n    std::string s;\r\n    benchmark::DoNotOptimize(s = response.data());\r\n    std::size_t size;\r\n    benchmark::DoNotOptimize(size = s.size());\r\n  }\r\n\r\n  ~grpc_bench() noexcept { server_->Shutdown(); }\r\n\r\n  std::shared_ptr<grpc::ChannelInterface> channel_;\r\n  GrpcServiceBenchmark::Stub client_;\r\n  std::unique_ptr<grpc::Server> server_;\r\n  grpc_service service_impl_;\r\n  static constexpr const char *server_addr_ = ""localhost:8082"";\r\n};\r\n```\r\n\r\nThe benchmark calls `get_blob` with increasing sizes through a large number of iterations.\r\n\r\nYou can clone this branch for the full source: https://github.com/rpclib/benchmarks/tree/grpc_high_memory_repro\r\n\r\nTo build the above repo, [conan](https://www.conan.io/) is required:\r\n\r\n```\r\nmkdir build && cd build\r\nconan install .. --build missing\r\ncmake ..\r\nmake\r\n```\r\n\r\nThere is of course a chance that I made a mistake in my benchmarking code. Please let me know if I can provide any more details. \r\n\r\nAlso, I would be interested in you opinion about whether or not this code is a fair entry in this benchmark, as I'm not very knowledgeable about grpc. You can see the implementations using other libraries on the master branch of the same repo for comparison.","cpp\r\nclass grpc_service : public GrpcServiceBenchmark::Service {\r\npublic:\r\n  ::grpc::Status get_blob(::grpc::ServerContext *context,\r\n                            const ::EmptyRequest *request,\r\n                            ::BlobResponse *response) override {\r\n      response->set_data(::get_blob(blob_size_));\r\n      return ::grpc::Status::OK;\r\n  }\r\n  int blob_size_;\r\n};\r\n\r\nclass grpc_bench : public benchmark::Fixture {\r\npublic:\r\n  grpc_bench()\r\n      : channel_(grpc::CreateChannel(server_addr_,\r\n                                     grpc::InsecureChannelCredentials())),\r\n        client_(channel_) {\r\n    grpc::ServerBuilder b;\r\n    b.AddListeningPort(server_addr_, grpc::InsecureServerCredentials());\r\n    b.RegisterService(&service_impl_);\r\n    server_ = b.BuildAndStart();\r\n  }\r\n  void get_blob(int param) {\r\n    service_impl_.blob_size_ = param;\r\n    grpc::ClientContext client_context;\r\n    EmptyRequest request;\r\n    BlobResponse response;\r\n    auto status = client_.get_blob(&client_context, request, &response);\r\n    std::string s;\r\n    benchmark::DoNotOptimize(s = response.data());\r\n    std::size_t size;\r\n    benchmark::DoNotOptimize(size = s.size());\r\n  }\r\n\r\n  ~grpc_bench() noexcept { server_->Shutdown(); }\r\n\r\n  std::shared_ptr<grpc::ChannelInterface> channel_;\r\n  GrpcServiceBenchmark::Stub client_;\r\n  std::unique_ptr<grpc::Server> server_;\r\n  grpc_service service_impl_;\r\n  static constexpr const char *server_addr_ = ""localhost:8082"";\r\n};\r\n"
9439,"Python Illegal wire type for field Message.Field .PeopleRequest.ages: 2 (0 expected))Hello,\r\nhere is my proto:\r\n```proto\r\nmessage PeopleRequest {\r\n  repeated string names = 1; // ex: ['jack', 'marie']\r\n  repeated int32 ages = 2; // ex: [1, 2]\r\n}\r\n```\r\nand here is my call:\r\n\r\nThis simple grpc call throws an error about the type of the field **ages**. I really can't see where is the problem...That's a simple list of integers isn't it? Why it seems to recognize it as a list of strings/bytes (wire type 2) ?\r\n\r\nMany thanks for you help!",lang/Python|disposition/requires reporter action,nathanielmanistaatgoogle|kpayson64,"Hello,\r\nhere is my proto:\r\n```proto\r\nmessage PeopleRequest {\r\n  repeated string names = 1; // ex: ['jack', 'marie']\r\n  repeated int32 ages = 2; // ex: [1, 2]\r\n}\r\n```\r\nand here is my call:\r\n```python\r\npeople_request = grpc.PeopleRequest(\r\n      names=['daniel'],\r\n      ages=[32],\r\n)\r\nprint(people_request) // Ok, no error when building the people_request object\r\nstub.GetPeoples(people_request) // ERROR Illegal wire type for field ages: 2 (0 expected))\r\n```\r\nThis simple grpc call throws an error about the type of the field **ages**. I really can't see where is the problem...That's a simple list of integers isn't it? Why it seems to recognize it as a list of strings/bytes (wire type 2) ?\r\n\r\nMany thanks for you help!","python\r\npeople_request = grpc.PeopleRequest(\r\n      names=['daniel'],\r\n      ages=[32],\r\n)\r\nprint(people_request) // Ok, no error when building the people_request object\r\nstub.GetPeoples(people_request) // ERROR Illegal wire type for field ages: 2 (0 expected))\r\n"
9377,"gRPC Server logs an error when the client is canceled in streaming on C#Grpc's logger will show an exception if the client cancels during streaming and the server successfully handles it.\r\n\r\nEnvironment:Windows, C#, Grpc.Core 1.0.1\r\nrepro code of use sample's route_guide.proto\r\n\r\n\r\n\r\nConsole log is here.\r\n\r\n```\r\nCancelled?False\r\nCancelled?True\r\nW0118 12:37:47.380258 Grpc.Core.Server Exception while handling RPC. System.InvalidOperationException\r\n   Grpc.Core.Internal.AsyncCallServer`2.SendStatusFromServerAsync(Status status, Metadata trailers, Tuple`2 optionalWrite)\r\n   Grpc.Core.Internal.ServerStreamingServerCallHandler`2.<HandleCall>d__4.MoveNext()\r\n   System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)\r\n   System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n   Grpc.Core.Server.<HandleCallAsync>d__34.MoveNext()\r\n```\r\n\r\nThis error is come from SendStatusFromServerAsync. https://github.com/grpc/grpc/blob/44e907af0/src/csharp/Grpc.Core/Internal/ServerCallHandler.cs#L156\r\nThis call always fail when canceled.\r\nHow about changes to this?\r\n\r\n",kind/enhancement|lang/C#|priority/P3|disposition/never stale,jtattermusch,"Grpc's logger will show an exception if the client cancels during streaming and the server successfully handles it.\r\n\r\nEnvironment:Windows, C#, Grpc.Core 1.0.1\r\nrepro code of use sample's route_guide.proto\r\n\r\n```csharp\r\n// Server Startup\r\nGrpcEnvironment.SetLogger(new ConsoleLogger()); // show inner log\r\n\r\nvar server = new global::Grpc.Core.Server\r\n{\r\n    Services = { Routeguide.RouteGuide.BindService(new RouteGuideImpl()) },\r\n    Ports = { new ServerPort(""localhost"", 12345, ServerCredentials.Insecure) }\r\n};\r\n\r\nserver.Start();\r\nConsole.ReadKey();\r\n\r\n// Server Impl\r\npublic class RouteGuideImpl : Routeguide.RouteGuide.RouteGuideBase\r\n{\r\n    public override async Task ListFeatures(Rectangle request, IServerStreamWriter<Feature> responseStream, ServerCallContext context)\r\n    {\r\n        Console.WriteLine(""Cancelled?"" + context.CancellationToken.IsCancellationRequested); // false\r\n\r\n        await responseStream.WriteAsync(new Feature());\r\n        await Task.Delay(TimeSpan.FromMilliseconds(500)); // wait for client calls cancel\r\n        Console.WriteLine(""Cancelled?"" + context.CancellationToken.IsCancellationRequested); // true\r\n\r\n        // complete...\r\n    }\r\n}\r\n\r\n// Client Impl\r\ntry\r\n{\r\n    var cts = new CancellationTokenSource();\r\n\r\n    var channel = new Channel(""127.0.0.1:12345"", ChannelCredentials.Insecure);\r\n    var client = new Routeguide.RouteGuide.RouteGuideClient(channel);\r\n    var feature = client.ListFeatures(new Routeguide.Rectangle(), cancellationToken: cts.Token);\r\n\r\n    while (await feature.ResponseStream.MoveNext())\r\n    {\r\n        cts.Cancel(); // cancel when first response.\r\n    }\r\n}\r\ncatch (RpcException ex)\r\n{\r\n    Console.WriteLine(ex.Status);// canceled\r\n}\r\n```\r\n\r\nConsole log is here.\r\n\r\n```\r\nCancelled?False\r\nCancelled?True\r\nW0118 12:37:47.380258 Grpc.Core.Server Exception while handling RPC. System.InvalidOperationException\r\n   Grpc.Core.Internal.AsyncCallServer`2.SendStatusFromServerAsync(Status status, Metadata trailers, Tuple`2 optionalWrite)\r\n   Grpc.Core.Internal.ServerStreamingServerCallHandler`2.<HandleCall>d__4.MoveNext()\r\n   System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task)\r\n   System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)\r\n   Grpc.Core.Server.<HandleCallAsync>d__34.MoveNext()\r\n```\r\n\r\nThis error is come from SendStatusFromServerAsync. https://github.com/grpc/grpc/blob/44e907af0/src/csharp/Grpc.Core/Internal/ServerCallHandler.cs#L156\r\nThis call always fail when canceled.\r\nHow about changes to this?\r\n\r\n```csharp\r\nif (!asyncCall.CancellationToken.IsCancellationRequested)\r\n{\r\n    await asyncCall.SendStatusFromServerAsync(status, context.ResponseTrailers, null).ConfigureAwait(false);\r\n}\r\n```","csharp\r\n// Server Startup\r\nGrpcEnvironment.SetLogger(new ConsoleLogger()); // show inner log\r\n\r\nvar server = new global::Grpc.Core.Server\r\n{\r\n    Services = { Routeguide.RouteGuide.BindService(new RouteGuideImpl()) },\r\n    Ports = { new ServerPort(""localhost"", 12345, ServerCredentials.Insecure) }\r\n};\r\n\r\nserver.Start();\r\nConsole.ReadKey();\r\n\r\n// Server Impl\r\npublic class RouteGuideImpl : Routeguide.RouteGuide.RouteGuideBase\r\n{\r\n    public override async Task ListFeatures(Rectangle request, IServerStreamWriter<Feature> responseStream, ServerCallContext context)\r\n    {\r\n        Console.WriteLine(""Cancelled?"" + context.CancellationToken.IsCancellationRequested); // false\r\n\r\n        await responseStream.WriteAsync(new Feature());\r\n        await Task.Delay(TimeSpan.FromMilliseconds(500)); // wait for client calls cancel\r\n        Console.WriteLine(""Cancelled?"" + context.CancellationToken.IsCancellationRequested); // true\r\n\r\n        // complete...\r\n    }\r\n}\r\n\r\n// Client Impl\r\ntry\r\n{\r\n    var cts = new CancellationTokenSource();\r\n\r\n    var channel = new Channel(""127.0.0.1:12345"", ChannelCredentials.Insecure);\r\n    var client = new Routeguide.RouteGuide.RouteGuideClient(channel);\r\n    var feature = client.ListFeatures(new Routeguide.Rectangle(), cancellationToken: cts.Token);\r\n\r\n    while (await feature.ResponseStream.MoveNext())\r\n    {\r\n        cts.Cancel(); // cancel when first response.\r\n    }\r\n}\r\ncatch (RpcException ex)\r\n{\r\n    Console.WriteLine(ex.Status);// canceled\r\n}\r\n"
9360,"Python thread throwing exception when passed a stream as argIf I create the stream object and then pass it to the thread, it will throw:\r\n\r\nthe error:\r\n```\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File ""~/.pyenv/versions/3.6.0/lib/python3.6/threading.py"", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File ""~/.pyenv/versions/3.6.0/lib/python3.6/threading.py"", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""~/myproject/.venv/lib/python3.6/site-packages/grpc/_channel.py"", line 341, in __next__\r\n    return self._next()\r\n  File ""~/myproject/.venv/lib/python3.6/site-packages/grpc/_channel.py"", line 335, in _next\r\n    raise self\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.CANCELLED, Cancelled)>\r\n```\r\nIf I create the stream in the thread, then no problem:\r\n\r\n@nathanielmanistaatgoogle Any idea why it's not possible to start the thread with the stream already created as an argument? \r\nthanks!",kind/question|area/core|lang/Python|priority/P2,dgquintas,"If I create the stream object and then pass it to the thread, it will throw:\r\n```python\r\nstream = stub.GetStream(md.Empty())\r\n\r\ndef stream_worker(thestream):\r\n    for elem in thestream:\r\n        logging.info(elem)\r\n\r\nmythread = Thread(\r\n    target=stream_worker,\r\n    args=(stream),\r\n )\r\nmythread.start()\r\n```\r\nthe error:\r\n```\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File ""~/.pyenv/versions/3.6.0/lib/python3.6/threading.py"", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File ""~/.pyenv/versions/3.6.0/lib/python3.6/threading.py"", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""~/myproject/.venv/lib/python3.6/site-packages/grpc/_channel.py"", line 341, in __next__\r\n    return self._next()\r\n  File ""~/myproject/.venv/lib/python3.6/site-packages/grpc/_channel.py"", line 335, in _next\r\n    raise self\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.CANCELLED, Cancelled)>\r\n```\r\nIf I create the stream in the thread, then no problem:\r\n```python\r\ndef stream_worker():\r\n    stream = stub.GetStream(md.Empty())\r\n    for elem in thestream:\r\n        logging.info(elem)\r\n\r\nmythread = Thread(\r\n    target=stream_worker,\r\n )\r\nmythread.start()\r\n```\r\n@nathanielmanistaatgoogle Any idea why it's not possible to start the thread with the stream already created as an argument? \r\nthanks!","python\r\nstream = stub.GetStream(md.Empty())\r\n\r\ndef stream_worker(thestream):\r\n    for elem in thestream:\r\n        logging.info(elem)\r\n\r\nmythread = Thread(\r\n    target=stream_worker,\r\n    args=(stream),\r\n )\r\nmythread.start()\r\n"
9289,"grpc core down while the request destroy and it was still in wait_concurrent list the sence is:\r\ni use concurrent limited in the server as:\r\nvoid UpdateArguments(::grpc::ChannelArguments* args)\r\n{\r\n    args->SetInt(GRPC_ARG_MAX_CONCURRENT_STREAMS, 1);\r\n}\r\nto limit the server link with max concurrent 1.\r\nmy service will not reply, so only one request will be accepted.\r\nand the second request will queue in GRPC_CHTTP2_LIST_WAITING_FOR_CONCURRENCY list .\r\n\r\nif the second request is timeout before the first request . while it was still stay in GRPC_CHTTP2_LIST_WAITING_FOR_CONCURRENCY list. i will try to destroy the request in function ~ClientContext.\r\nit will core down with errors ""client stream 0 still included in list 11""\r\n\r\ni try to write the gt case below,  it may not correct, but it will core down with errors info\u201cclient stream 0 still included in list 11\u201d .\r\n\r\n",lang/c++,dgquintas,"the sence is:\r\ni use concurrent limited in the server as:\r\nvoid UpdateArguments(::grpc::ChannelArguments* args)\r\n{\r\n    args->SetInt(GRPC_ARG_MAX_CONCURRENT_STREAMS, 1);\r\n}\r\nto limit the server link with max concurrent 1.\r\nmy service will not reply, so only one request will be accepted.\r\nand the second request will queue in GRPC_CHTTP2_LIST_WAITING_FOR_CONCURRENCY list .\r\n\r\nif the second request is timeout before the first request . while it was still stay in GRPC_CHTTP2_LIST_WAITING_FOR_CONCURRENCY list. i will try to destroy the request in function ~ClientContext.\r\nit will core down with errors ""client stream 0 still included in list 11""\r\n\r\ni try to write the gt case below,  it may not correct, but it will core down with errors info\u201cclient stream 0 still included in list 11\u201d .\r\n\r\n```cpp\r\nstatic void test_max_concurrent_streams_with_timeout_2(\r\n    grpc_end2end_test_config config) {\r\n  grpc_end2end_test_fixture f;\r\n  grpc_arg server_arg;\r\n  grpc_channel_args server_args;\r\n  grpc_call *c1;\r\n  grpc_call *c2;\r\n  grpc_call *s1;\r\n//  grpc_call *s2;\r\n  cq_verifier *cqv;\r\n  grpc_call_details call_details;\r\n  grpc_metadata_array request_metadata_recv;\r\n  grpc_metadata_array initial_metadata_recv1;\r\n  grpc_metadata_array trailing_metadata_recv1;\r\n  grpc_metadata_array initial_metadata_recv2;\r\n  grpc_metadata_array trailing_metadata_recv2;\r\n  grpc_status_code status1;\r\n  grpc_call_error error;\r\n  char *details1 = NULL;\r\n  size_t details_capacity1 = 0;\r\n  grpc_status_code status2;\r\n  char *details2 = NULL;\r\n  size_t details_capacity2 = 0;\r\n  grpc_op ops[6];\r\n  grpc_op *op;\r\n  int was_cancelled;\r\n\r\n  server_arg.key = GRPC_ARG_MAX_CONCURRENT_STREAMS;\r\n  server_arg.type = GRPC_ARG_INTEGER;\r\n  server_arg.value.integer = 1;\r\n\r\n  server_args.num_args = 1;\r\n  server_args.args = &server_arg;\r\n\r\n  f = begin_test(config, ""test_max_concurrent_streams_with_timeout"", NULL,\r\n                 &server_args);\r\n  cqv = cq_verifier_create(f.cq);\r\n\r\n  grpc_metadata_array_init(&request_metadata_recv);\r\n  grpc_metadata_array_init(&initial_metadata_recv1);\r\n  grpc_metadata_array_init(&trailing_metadata_recv1);\r\n  grpc_metadata_array_init(&initial_metadata_recv2);\r\n  grpc_metadata_array_init(&trailing_metadata_recv2);\r\n  grpc_call_details_init(&call_details);\r\n\r\n  /* perform a ping-pong to ensure that settings have had a chance to round\r\n     trip */\r\n  simple_request_body(f);\r\n  /* perform another one to make sure that the one stream case still works */\r\n  simple_request_body(f);\r\n\r\n  /* start two requests - ensuring that the second is not accepted until\r\n     the first completes , and the second request will timeout in the concurrent_list */\r\n  c1 = grpc_channel_create_call(f.client, NULL, GRPC_PROPAGATE_DEFAULTS, f.cq,\r\n                                ""/alpha"", ""foo.test.google.fr:1234"",\r\n                                n_seconds_time(1000), NULL);\r\n  GPR_ASSERT(c1);\r\n  c2 = grpc_channel_create_call(f.client, NULL, GRPC_PROPAGATE_DEFAULTS, f.cq,\r\n                                ""/beta"", ""foo.test.google.fr:1234"",\r\n                                n_seconds_time(3), NULL);\r\n  GPR_ASSERT(c2);\r\n\r\n  GPR_ASSERT(GRPC_CALL_OK == grpc_server_request_call(\r\n                                 f.server, &s1, &call_details,\r\n                                 &request_metadata_recv, f.cq, f.cq, tag(101)));\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_CLOSE_FROM_CLIENT;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c1, ops, (size_t)(op - ops), tag(301), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;\r\n  op->data.recv_status_on_client.trailing_metadata = &trailing_metadata_recv1;\r\n  op->data.recv_status_on_client.status = &status1;\r\n  op->data.recv_status_on_client.status_details = &details1;\r\n  op->data.recv_status_on_client.status_details_capacity = &details_capacity1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_INITIAL_METADATA;\r\n  op->data.recv_initial_metadata = &initial_metadata_recv1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c1, ops, (size_t)(op - ops), tag(302), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  cq_expect_completion(cqv, tag(101), 1);\r\n  cq_expect_completion(cqv, tag(301), 1);\r\n  cq_verify(cqv);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_CLOSE_FROM_CLIENT;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c2, ops, (size_t)(op - ops), tag(401), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;\r\n  op->data.recv_status_on_client.trailing_metadata = &trailing_metadata_recv2;\r\n  op->data.recv_status_on_client.status = &status2;\r\n  op->data.recv_status_on_client.status_details = &details2;\r\n  op->data.recv_status_on_client.status_details_capacity = &details_capacity2;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_INITIAL_METADATA;\r\n  op->data.recv_initial_metadata = &initial_metadata_recv1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c2, ops, (size_t)(op - ops), tag(402), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  /* the second request is time out*/\r\n  cq_expect_completion(cqv, tag(401), 0);\r\n  cq_expect_completion(cqv, tag(402), 1);\r\n  cq_verify(cqv);\r\n  \r\n  /* second request is finished because of time out, so destroy the second call */\r\n  grpc_call_destroy(c2);\r\n\r\n  /* now reply the first call */\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_CLOSE_ON_SERVER;\r\n  op->data.recv_close_on_server.cancelled = &was_cancelled;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_STATUS_FROM_SERVER;\r\n  op->data.send_status_from_server.trailing_metadata_count = 0;\r\n  op->data.send_status_from_server.status = GRPC_STATUS_UNIMPLEMENTED;\r\n  op->data.send_status_from_server.status_details = ""xyz"";\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(s1, ops, (size_t)(op - ops), tag(102), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  cq_expect_completion(cqv, tag(302), 1);\r\n  cq_expect_completion(cqv, tag(102), 1);\r\n  cq_verify(cqv);\r\n\r\n  cq_verifier_destroy(cqv);\r\n\r\n  grpc_call_destroy(c1);\r\n  grpc_call_destroy(s1);\r\n\r\n\r\n  gpr_free(details1);\r\n  gpr_free(details2);\r\n  grpc_metadata_array_destroy(&initial_metadata_recv1);\r\n  grpc_metadata_array_destroy(&trailing_metadata_recv1);\r\n  grpc_metadata_array_destroy(&initial_metadata_recv2);\r\n  grpc_metadata_array_destroy(&trailing_metadata_recv2);\r\n  grpc_metadata_array_destroy(&request_metadata_recv);\r\n  grpc_call_details_destroy(&call_details);\r\n\r\n  end_test(&f);\r\n  config.tear_down_data(&f);\r\n}\r\n```","cpp\r\nstatic void test_max_concurrent_streams_with_timeout_2(\r\n    grpc_end2end_test_config config) {\r\n  grpc_end2end_test_fixture f;\r\n  grpc_arg server_arg;\r\n  grpc_channel_args server_args;\r\n  grpc_call *c1;\r\n  grpc_call *c2;\r\n  grpc_call *s1;\r\n//  grpc_call *s2;\r\n  cq_verifier *cqv;\r\n  grpc_call_details call_details;\r\n  grpc_metadata_array request_metadata_recv;\r\n  grpc_metadata_array initial_metadata_recv1;\r\n  grpc_metadata_array trailing_metadata_recv1;\r\n  grpc_metadata_array initial_metadata_recv2;\r\n  grpc_metadata_array trailing_metadata_recv2;\r\n  grpc_status_code status1;\r\n  grpc_call_error error;\r\n  char *details1 = NULL;\r\n  size_t details_capacity1 = 0;\r\n  grpc_status_code status2;\r\n  char *details2 = NULL;\r\n  size_t details_capacity2 = 0;\r\n  grpc_op ops[6];\r\n  grpc_op *op;\r\n  int was_cancelled;\r\n\r\n  server_arg.key = GRPC_ARG_MAX_CONCURRENT_STREAMS;\r\n  server_arg.type = GRPC_ARG_INTEGER;\r\n  server_arg.value.integer = 1;\r\n\r\n  server_args.num_args = 1;\r\n  server_args.args = &server_arg;\r\n\r\n  f = begin_test(config, ""test_max_concurrent_streams_with_timeout"", NULL,\r\n                 &server_args);\r\n  cqv = cq_verifier_create(f.cq);\r\n\r\n  grpc_metadata_array_init(&request_metadata_recv);\r\n  grpc_metadata_array_init(&initial_metadata_recv1);\r\n  grpc_metadata_array_init(&trailing_metadata_recv1);\r\n  grpc_metadata_array_init(&initial_metadata_recv2);\r\n  grpc_metadata_array_init(&trailing_metadata_recv2);\r\n  grpc_call_details_init(&call_details);\r\n\r\n  /* perform a ping-pong to ensure that settings have had a chance to round\r\n     trip */\r\n  simple_request_body(f);\r\n  /* perform another one to make sure that the one stream case still works */\r\n  simple_request_body(f);\r\n\r\n  /* start two requests - ensuring that the second is not accepted until\r\n     the first completes , and the second request will timeout in the concurrent_list */\r\n  c1 = grpc_channel_create_call(f.client, NULL, GRPC_PROPAGATE_DEFAULTS, f.cq,\r\n                                ""/alpha"", ""foo.test.google.fr:1234"",\r\n                                n_seconds_time(1000), NULL);\r\n  GPR_ASSERT(c1);\r\n  c2 = grpc_channel_create_call(f.client, NULL, GRPC_PROPAGATE_DEFAULTS, f.cq,\r\n                                ""/beta"", ""foo.test.google.fr:1234"",\r\n                                n_seconds_time(3), NULL);\r\n  GPR_ASSERT(c2);\r\n\r\n  GPR_ASSERT(GRPC_CALL_OK == grpc_server_request_call(\r\n                                 f.server, &s1, &call_details,\r\n                                 &request_metadata_recv, f.cq, f.cq, tag(101)));\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_CLOSE_FROM_CLIENT;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c1, ops, (size_t)(op - ops), tag(301), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;\r\n  op->data.recv_status_on_client.trailing_metadata = &trailing_metadata_recv1;\r\n  op->data.recv_status_on_client.status = &status1;\r\n  op->data.recv_status_on_client.status_details = &details1;\r\n  op->data.recv_status_on_client.status_details_capacity = &details_capacity1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_INITIAL_METADATA;\r\n  op->data.recv_initial_metadata = &initial_metadata_recv1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c1, ops, (size_t)(op - ops), tag(302), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  cq_expect_completion(cqv, tag(101), 1);\r\n  cq_expect_completion(cqv, tag(301), 1);\r\n  cq_verify(cqv);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_CLOSE_FROM_CLIENT;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c2, ops, (size_t)(op - ops), tag(401), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;\r\n  op->data.recv_status_on_client.trailing_metadata = &trailing_metadata_recv2;\r\n  op->data.recv_status_on_client.status = &status2;\r\n  op->data.recv_status_on_client.status_details = &details2;\r\n  op->data.recv_status_on_client.status_details_capacity = &details_capacity2;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_INITIAL_METADATA;\r\n  op->data.recv_initial_metadata = &initial_metadata_recv1;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(c2, ops, (size_t)(op - ops), tag(402), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  /* the second request is time out*/\r\n  cq_expect_completion(cqv, tag(401), 0);\r\n  cq_expect_completion(cqv, tag(402), 1);\r\n  cq_verify(cqv);\r\n  \r\n  /* second request is finished because of time out, so destroy the second call */\r\n  grpc_call_destroy(c2);\r\n\r\n  /* now reply the first call */\r\n  memset(ops, 0, sizeof(ops));\r\n  op = ops;\r\n  op->op = GRPC_OP_SEND_INITIAL_METADATA;\r\n  op->data.send_initial_metadata.count = 0;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_RECV_CLOSE_ON_SERVER;\r\n  op->data.recv_close_on_server.cancelled = &was_cancelled;\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  op->op = GRPC_OP_SEND_STATUS_FROM_SERVER;\r\n  op->data.send_status_from_server.trailing_metadata_count = 0;\r\n  op->data.send_status_from_server.status = GRPC_STATUS_UNIMPLEMENTED;\r\n  op->data.send_status_from_server.status_details = ""xyz"";\r\n  op->flags = 0;\r\n  op->reserved = NULL;\r\n  op++;\r\n  error = grpc_call_start_batch(s1, ops, (size_t)(op - ops), tag(102), NULL);\r\n  GPR_ASSERT(GRPC_CALL_OK == error);\r\n\r\n  cq_expect_completion(cqv, tag(302), 1);\r\n  cq_expect_completion(cqv, tag(102), 1);\r\n  cq_verify(cqv);\r\n\r\n  cq_verifier_destroy(cqv);\r\n\r\n  grpc_call_destroy(c1);\r\n  grpc_call_destroy(s1);\r\n\r\n\r\n  gpr_free(details1);\r\n  gpr_free(details2);\r\n  grpc_metadata_array_destroy(&initial_metadata_recv1);\r\n  grpc_metadata_array_destroy(&trailing_metadata_recv1);\r\n  grpc_metadata_array_destroy(&initial_metadata_recv2);\r\n  grpc_metadata_array_destroy(&trailing_metadata_recv2);\r\n  grpc_metadata_array_destroy(&request_metadata_recv);\r\n  grpc_call_details_destroy(&call_details);\r\n\r\n  end_test(&f);\r\n  config.tear_down_data(&f);\r\n}\r\n"
9202,"Can't shutdown when there are active calls on client in C#The channel receiving StreamingCall can not shutdown.\r\nThis causes a deadlock when exiting the client from the outside, such as `GrpcEnvironment.ShutdownChannelsAsync`.\r\n\r\n\r\n\r\nIf you try to avoid it (Dispose CompletionQueueSafeHandle without Thread.Join and set the termination condition of RunHandlerLoop to `while (ev.type! = CompletionQueueEvent.CompletionType.Shutdown &&! Cq.IsClosed)`)\r\nBut `DebugStats.CheckOK` raises an exception(because it has some pending batch completions).",kind/bug|lang/C#|priority/P3|disposition/stale,jtattermusch,"The channel receiving StreamingCall can not shutdown.\r\nThis causes a deadlock when exiting the client from the outside, such as `GrpcEnvironment.ShutdownChannelsAsync`.\r\n\r\n```csharp\r\nvar client = new Routeguide.RouteGuide.RouteGuideClient(channel);\r\nvar streaming = client.ListFeatures(new Routeguide.Rectangle() { Hi = new Routeguide.Point { Latitude = 1, Longitude = 1 }, Lo = new Routeguide.Point { Latitude = 1, Longitude = 1 } });\r\n\r\n// true, streaming is yet connecting.\r\nawait streaming.ResponseStream.MoveNext();\r\n\r\n// Deadlock in thread.Join() in GrpcThreadPool.StopAsync, can't shutdown from user code.\r\nawait GrpcEnvironment.ShutdownChannelsAsync();\r\n\r\nConsole.WriteLine(""don't reach here."");\r\n```\r\n\r\nIf you try to avoid it (Dispose CompletionQueueSafeHandle without Thread.Join and set the termination condition of RunHandlerLoop to `while (ev.type! = CompletionQueueEvent.CompletionType.Shutdown &&! Cq.IsClosed)`)\r\nBut `DebugStats.CheckOK` raises an exception(because it has some pending batch completions).","csharp\r\nvar client = new Routeguide.RouteGuide.RouteGuideClient(channel);\r\nvar streaming = client.ListFeatures(new Routeguide.Rectangle() { Hi = new Routeguide.Point { Latitude = 1, Longitude = 1 }, Lo = new Routeguide.Point { Latitude = 1, Longitude = 1 } });\r\n\r\n// true, streaming is yet connecting.\r\nawait streaming.ResponseStream.MoveNext();\r\n\r\n// Deadlock in thread.Join() in GrpcThreadPool.StopAsync, can't shutdown from user code.\r\nawait GrpcEnvironment.ShutdownChannelsAsync();\r\n\r\nConsole.WriteLine(""don't reach here."");\r\n"
9142,"Mismatch between documentation and behavior w.r.t. RST_STREAM handlingAccording to the  [http2 transport mapping doc](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#http2-transport-mapping), a client should fail with INTERNAL (13) status when it encounters a RST_STREAM with error_code = 0. \r\n\r\nC++ (and all wrapped languages) currently fails with UNKOWN (2) status after getting a RST_STREAM. The behavior is demonstrated by running an interop client against our custom, faulty http2 server.\r\n\r\nStart the server:\r\n\r\n\r\n\r\nRun the client:\r\n\r\n```\r\nbins/opt/interop_client\r\n    --server_host=127.0.0.1\r\n    --server_host_override=localhost\r\n    --server_port=8080 \r\n    --test_case=large_unary\r\n```\r\n\r\nIt fails with:\r\n```Error status code: 2 (expected: 0), message:```\r\n\r\nThe Go client fails more accurately with:\r\n```rpc error: code = 13 desc = stream terminated by RST_STREAM with error code: 0```",area/core,ncteisen,"According to the  [http2 transport mapping doc](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#http2-transport-mapping), a client should fail with INTERNAL (13) status when it encounters a RST_STREAM with error_code = 0. \r\n\r\nC++ (and all wrapped languages) currently fails with UNKOWN (2) status after getting a RST_STREAM. The behavior is demonstrated by running an interop client against our custom, faulty http2 server.\r\n\r\nStart the server:\r\n\r\n```python test/http2_test/http2_test_server.py rst_after_data 8080```\r\n\r\nRun the client:\r\n\r\n```\r\nbins/opt/interop_client\r\n    --server_host=127.0.0.1\r\n    --server_host_override=localhost\r\n    --server_port=8080 \r\n    --test_case=large_unary\r\n```\r\n\r\nIt fails with:\r\n```Error status code: 2 (expected: 0), message:```\r\n\r\nThe Go client fails more accurately with:\r\n```rpc error: code = 13 desc = stream terminated by RST_STREAM with error code: 0```",python test/http2_test/http2_test_server.py rst_after_data 8080
8882,"after stream.write() throw once, following correct stream.write() get buffered.Hello,\r\n\r\nfrom **nodejs** grpc server, if I write to a stream with **BAD** data, it will throw an error:\r\n\r\nmy problem is that the next time I write to this stream, with **correct** data, the grpc client never receives the **correct** data written to the stream...\r\n\r\n@murgatroid99 Any idea on this one?\r\n\r\nMany thanks!",lang/node,murgatroid99,"Hello,\r\n\r\nfrom **nodejs** grpc server, if I write to a stream with **BAD** data, it will throw an error:\r\n```javascript\r\nstream.on('error', console.log('nothing catched here');\r\n\r\nfunction writeToStream(data) {\r\n  try {\r\n    stream.write(data)\r\n  } catch (error) {\r\n    console.log('catched the error here');\r\n  }\r\n}\r\nwriteToStream(correctData) // grpc client receive\r\nwriteToStream(badData); // grpc client NEVER receive\r\nwriteToStream(correctData); // grpc client NEVER receive\r\n```\r\nmy problem is that the next time I write to this stream, with **correct** data, the grpc client never receives the **correct** data written to the stream...\r\n\r\n@murgatroid99 Any idea on this one?\r\n\r\nMany thanks!","javascript\r\nstream.on('error', console.log('nothing catched here');\r\n\r\nfunction writeToStream(data) {\r\n  try {\r\n    stream.write(data)\r\n  } catch (error) {\r\n    console.log('catched the error here');\r\n  }\r\n}\r\nwriteToStream(correctData) // grpc client receive\r\nwriteToStream(badData); // grpc client NEVER receive\r\nwriteToStream(correctData); // grpc client NEVER receive\r\n"
8823,"Provide a way to get a notification on inability to start a node gRPC serverHello,\r\n\r\nPlease add an ability to be notified about errors on gRPC server start under node.js.\r\n\r\nI created a [small project](https://github.com/denis-zhdanov/samples/tree/master/node/grpc-port-conflict) which illustrates the problem:\r\n\r\n1. We can run _./src/grpc.js_ and ensure that gRPC is configured properly\r\n2. gRPC startup fails if target port is already occupied - run _grpc-port-conflict.js_ where target port is bound to a node http server, then try to start gRPC server at the same port\r\n\r\nThe following in printed to the console:\r\n\r\n\r\nI'd expect one of the following:\r\n\r\n- allow to provide a callback to _server.bind()_ and propagate the error there\r\n- throw an exception from _server.bind()_ in order for clients to be able to catch and report it",lang/node,murgatroid99,"Hello,\r\n\r\nPlease add an ability to be notified about errors on gRPC server start under node.js.\r\n\r\nI created a [small project](https://github.com/denis-zhdanov/samples/tree/master/node/grpc-port-conflict) which illustrates the problem:\r\n\r\n1. We can run _./src/grpc.js_ and ensure that gRPC is configured properly\r\n2. gRPC startup fails if target port is already occupied - run _grpc-port-conflict.js_ where target port is bound to a node http server, then try to start gRPC server at the same port\r\n\r\nThe following in printed to the console:\r\n```javascript\r\nE1122 08:53:29.221790000 140736378774464 server_chttp2.c:159] {""created"":""@1479794009.221737000"",""description"":""No address added out of total 1 resolved"",""file"":""../src/core/ext/transport/chttp2/server/insecure/server_chttp2.c"",""file_line"":125,""referenced_errors"":[{""created"":""@1479794009.221733000"",""description"":""Failed to add port to server"",""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":634,""referenced_errors"":[{""created"":""@1479794009.221723000"",""description"":""Unable to configure socket"",""fd"":22,""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":355,""referenced_errors"":[{""created"":""@1479794009.221703000"",""description"":""OS Error"",""errno"":48,""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":331,""os_error"":""Address already in use"",""syscall"":""bind""}]}],""target_address"":""ipv6:[::]:50051""}]}\r\n```\r\n\r\nI'd expect one of the following:\r\n\r\n- allow to provide a callback to _server.bind()_ and propagate the error there\r\n- throw an exception from _server.bind()_ in order for clients to be able to catch and report it","javascript\r\nE1122 08:53:29.221790000 140736378774464 server_chttp2.c:159] {""created"":""@1479794009.221737000"",""description"":""No address added out of total 1 resolved"",""file"":""../src/core/ext/transport/chttp2/server/insecure/server_chttp2.c"",""file_line"":125,""referenced_errors"":[{""created"":""@1479794009.221733000"",""description"":""Failed to add port to server"",""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":634,""referenced_errors"":[{""created"":""@1479794009.221723000"",""description"":""Unable to configure socket"",""fd"":22,""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":355,""referenced_errors"":[{""created"":""@1479794009.221703000"",""description"":""OS Error"",""errno"":48,""file"":""../src/core/lib/iomgr/tcp_server_posix.c"",""file_line"":331,""os_error"":""Address already in use"",""syscall"":""bind""}]}],""target_address"":""ipv6:[::]:50051""}]}\r\n"
8809,"PHP gRPC plugin not included on makeAccording to the docs found [here](https://github.com/grpc/grpc/tree/master/src/php#php-protoc-plugin), the PHP gRPC plugin should be created in `bins/opt` after running `make` in the root of `grpc`. From what I can tell this does not seem to be the case.\r\n\r\n\r\nI put together the following `Dockerfile` for the build process that I am using.\r\n\r\n```Dockerfile\r\nFROM ubuntu:trusty\r\n\r\nENV INSTALL wget \\\r\n  php5-cli \\\r\n  php5-dev \\\r\n  php-pear \\\r\n  git \\\r\n  python-pip \\\r\n  python-dev \\\r\n  build-essential \\\r\n  unzip \\\r\n  curl\r\nRUN apt-get -y update \\\r\n  && DEBIAN_FRONTEND=noninteractive apt-get install -yq --force-yes $INSTALL\r\n\r\n# gRPC & Protoc\r\nENV PROTOC_VER 3.1.0\r\nRUN wget https://github.com/google/protobuf/releases/download/v$PROTOC_VER/protoc-$PROTOC_VER-linux-x86_64.zip\r\nRUN unzip protoc-$PROTOC_VER-linux-x86_64.zip -d protoc\r\nRUN cp protoc/bin/protoc /usr/local/bin && rm -rf protoc\r\n\r\nENV BINPATH /usr/local/bin\r\nENV PROTO_PATH /var/lib/protobuf\r\nENV GRPC_WORK_DIR /grpc\r\nENV GRPC_VER v1.0.1\r\nRUN git clone -b $GRPC_VER https://github.com/grpc/grpc $GRPC_WORK_DIR\r\nRUN cd $GRPC_WORK_DIR && git pull origin $GRPC_VER --recurse-submodules && git submodule update --init --recursive\r\nRUN cd $GRPC_WORK_DIR && make && ls bins/opt\r\n```\r\n\r\nMaybe I am missing something? Any help would be appreciated, thanks!",lang/php|area/documentation,stanley-cheung,"According to the docs found [here](https://github.com/grpc/grpc/tree/master/src/php#php-protoc-plugin), the PHP gRPC plugin should be created in `bins/opt` after running `make` in the root of `grpc`. From what I can tell this does not seem to be the case.\r\n\r\n```bash\r\n$ ls bins/opt\r\ngrpc_cpp_plugin\r\ngrpc_csharp_plugin\r\ngrpc_node_plugin\r\ngrpc_objective_c_plugin\r\ngrpc_python_plugin\r\ngrpc_ruby_plugin\r\nprotobuf\r\n```\r\nI put together the following `Dockerfile` for the build process that I am using.\r\n\r\n```Dockerfile\r\nFROM ubuntu:trusty\r\n\r\nENV INSTALL wget \\\r\n  php5-cli \\\r\n  php5-dev \\\r\n  php-pear \\\r\n  git \\\r\n  python-pip \\\r\n  python-dev \\\r\n  build-essential \\\r\n  unzip \\\r\n  curl\r\nRUN apt-get -y update \\\r\n  && DEBIAN_FRONTEND=noninteractive apt-get install -yq --force-yes $INSTALL\r\n\r\n# gRPC & Protoc\r\nENV PROTOC_VER 3.1.0\r\nRUN wget https://github.com/google/protobuf/releases/download/v$PROTOC_VER/protoc-$PROTOC_VER-linux-x86_64.zip\r\nRUN unzip protoc-$PROTOC_VER-linux-x86_64.zip -d protoc\r\nRUN cp protoc/bin/protoc /usr/local/bin && rm -rf protoc\r\n\r\nENV BINPATH /usr/local/bin\r\nENV PROTO_PATH /var/lib/protobuf\r\nENV GRPC_WORK_DIR /grpc\r\nENV GRPC_VER v1.0.1\r\nRUN git clone -b $GRPC_VER https://github.com/grpc/grpc $GRPC_WORK_DIR\r\nRUN cd $GRPC_WORK_DIR && git pull origin $GRPC_VER --recurse-submodules && git submodule update --init --recursive\r\nRUN cd $GRPC_WORK_DIR && make && ls bins/opt\r\n```\r\n\r\nMaybe I am missing something? Any help would be appreciated, thanks!",bash\r\n$ ls bins/opt\r\ngrpc_cpp_plugin\r\ngrpc_csharp_plugin\r\ngrpc_node_plugin\r\ngrpc_objective_c_plugin\r\ngrpc_python_plugin\r\ngrpc_ruby_plugin\r\nprotobuf\r\n
8763,"Error by create secure_channelHi,\r\ngrpc was installed using pip. I tried to use it and I got an error. I looked for errors, but I could not find a solution.\r\nThe environment through uname is as follows.\r\n\r\n\r\ncode\r\n\r\nlog\r\n\r\n",kind/bug|lang/Python|area/security|priority/P3|disposition/stale,jiangtaoli2016,"Hi,\r\ngrpc was installed using pip. I tried to use it and I got an error. I looked for errors, but I could not find a solution.\r\nThe environment through uname is as follows.\r\n```python\r\nuname -s -> Linux\r\nuname -r -> 3.10.65\r\nuname -m -> aarch64\r\n```\r\n\r\ncode\r\n```python\r\nimport grpc\r\n\r\ncreds = grpc.ssl_channel_credentials(open('roots.pem').read())\r\nchannel = grpc.secure_channel('myservice.example.com:443', creds)\r\n```\r\nlog\r\n```python\r\n24061 ssl_transport_security.c:655] Could not set ephemeral ECDH key.\r\n24061 security_connector.c:774]   Handshaker factory creation failed with TSI_INTERNAL_ERROR.\r\n```\r\n",python\r\nuname -s -> Linux\r\nuname -r -> 3.10.65\r\nuname -m -> aarch64\r\n
8670,PHP error installing grpc from pecl -- BoringSSL does not build on Ubuntu 16.10Getting error trying to `sudo pecl install grpc` from http://www.grpc.io/docs/quickstart/php.html on Ubuntu 16.10.\r\n\r\n,priority/P1|priority/P0/RELEASE BLOCKER,stanley-cheung,"Getting error trying to `sudo pecl install grpc` from http://www.grpc.io/docs/quickstart/php.html on Ubuntu 16.10.\r\n\r\n```bash\r\n$ sudo pecl install grpc\r\ndownloading grpc-1.0.1.tgz ...\r\nStarting to download grpc-1.0.1.tgz (1,910,227 bytes)\r\n.....................................done: 1,910,227 bytes\r\n884 source files, building\r\nrunning: phpize\r\nConfiguring for:\r\nPHP Api Version:         20151012\r\nZend Module Api No:      20151012\r\nZend Extension Api No:   320151012\r\nbuilding in /tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1\r\nrunning: /tmp/pear/temp/grpc/configure --with-php-config=/usr/bin/php-config\r\nchecking for grep that handles long lines and -e... /bin/grep\r\nchecking for egrep... /bin/grep -E\r\nchecking for a sed that does not truncate output... /bin/sed\r\nchecking for cc... cc\r\nchecking whether the C compiler works... yes\r\nchecking for C compiler default output file name... a.out\r\nchecking for suffix of executables...\r\nchecking whether we are cross compiling... no\r\nchecking for suffix of object files... o\r\nchecking whether we are using the GNU C compiler... yes\r\nchecking whether cc accepts -g... yes\r\nchecking for cc option to accept ISO C89... none needed\r\nchecking how to run the C preprocessor... cc -E\r\nchecking for icc... no\r\nchecking for suncc... no\r\nchecking whether cc understands -c and -o together... yes\r\nchecking for system library directory... lib\r\nchecking if compiler supports -R... no\r\nchecking if compiler supports -Wl,-rpath,... yes\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... x86_64-pc-linux-gnu\r\nchecking target system type... x86_64-pc-linux-gnu\r\nchecking for PHP prefix... /usr\r\nchecking for PHP includes... -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib\r\nchecking for PHP extension directory... /usr/lib/php/20151012\r\nchecking for PHP installed headers prefix... /usr/include/php/20151012\r\nchecking if debug is enabled... no\r\nchecking if zts is enabled... no\r\nchecking for re2c... no\r\nconfigure: WARNING: You will need re2c 0.13.4 or later if you want to regenerate PHP parsers.\r\nchecking for gawk... no\r\nchecking for nawk... nawk\r\nchecking if nawk is broken... no\r\nchecking whether to enable grpc support... yes, shared\r\nchecking how to print strings... printf\r\nchecking for a sed that does not truncate output... (cached) /bin/sed\r\nchecking for fgrep... /bin/grep -F\r\nchecking for ld used by cc... /usr/bin/ld\r\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\r\nchecking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\r\nchecking the name lister (/usr/bin/nm -B) interface... BSD nm\r\nchecking whether ln -s works... yes\r\nchecking the maximum length of command line arguments... 1572864\r\nchecking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop\r\nchecking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop\r\nchecking for /usr/bin/ld option to reload object files... -r\r\nchecking for objdump... objdump\r\nchecking how to recognize dependent libraries... pass_all\r\nchecking for dlltool... no\r\nchecking how to associate runtime and link libraries... printf %s\\n\r\nchecking for ar... ar\r\nchecking for archiver @FILE support... @\r\nchecking for strip... strip\r\nchecking for ranlib... ranlib\r\nchecking for gawk... (cached) nawk\r\nchecking command to parse /usr/bin/nm -B output from cc object... ok\r\nchecking for sysroot... no\r\nchecking for a working dd... /bin/dd\r\nchecking how to truncate binary pipes... /bin/dd bs=4096 count=1\r\nchecking for mt... mt\r\nchecking if mt is a manifest tool... no\r\nchecking for ANSI C header files... yes\r\nchecking for sys/types.h... yes\r\nchecking for sys/stat.h... yes\r\nchecking for stdlib.h... yes\r\nchecking for string.h... yes\r\nchecking for memory.h... yes\r\nchecking for strings.h... yes\r\nchecking for inttypes.h... yes\r\nchecking for stdint.h... yes\r\nchecking for unistd.h... yes\r\nchecking for dlfcn.h... yes\r\nchecking for objdir... .libs\r\nchecking if cc supports -fno-rtti -fno-exceptions... no\r\nchecking for cc option to produce PIC... -fPIC -DPIC\r\nchecking if cc PIC flag -fPIC -DPIC works... yes\r\nchecking if cc static flag -static works... yes\r\nchecking if cc supports -c -o file.o... yes\r\nchecking if cc supports -c -o file.o... (cached) yes\r\nchecking whether the cc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\r\nchecking whether -lc should be explicitly linked in... no\r\nchecking dynamic linker characteristics... GNU/Linux ld.so\r\nchecking how to hardcode library paths into programs... immediate\r\nchecking whether stripping libraries is possible... yes\r\nchecking if libtool supports shared libraries... yes\r\nchecking whether to build shared libraries... yes\r\nchecking whether to build static libraries... no\r\nconfigure: creating ./config.status\r\nconfig.status: creating config.h\r\nconfig.status: executing libtool commands\r\nrunning: make\r\n........\r\n/bin/bash /tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/libtool --mode=compile cc -Wall -Werror     -Wno-parentheses-equality -Wno-unused-value -std=c11     -fvisibility=hidden -DOPENSSL_NO_ASM -D_GNU_SOURCE -DWIN32_LEAN_AND_MEAN     -D_HAS_EXCEPTIONS=0 -DNOMINMAX -I. -I/tmp/pear/temp/grpc -DPHP_ATOM_INC -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/include -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/main -I/tmp/pear/temp/grpc -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib -I/tmp/pear/temp/grpc/include -I/tmp/pear/temp/grpc/src/php/ext/grpc -I/tmp/pear/temp/grpc/third_party/boringssl/include  -DHAVE_CONFIG_H  -g -O2   -c /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c -o third_party/boringssl/crypto/directory_posix.lo\r\nlibtool: compile:  cc -Wall -Werror -Wno-parentheses-equality -Wno-unused-value -std=c11 -fvisibility=hidden -DOPENSSL_NO_ASM -D_GNU_SOURCE -DWIN32_LEAN_AND_MEAN -D_HAS_EXCEPTIONS=0 -DNOMINMAX -I. -I/tmp/pear/temp/grpc -DPHP_ATOM_INC -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/include -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/main -I/tmp/pear/temp/grpc -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib -I/tmp/pear/temp/grpc/include -I/tmp/pear/temp/grpc/src/php/ext/grpc -I/tmp/pear/temp/grpc/third_party/boringssl/include -DHAVE_CONFIG_H -g -O2 -c /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c  -fPIC -DPIC -o third_party/boringssl/crypto/.libs/directory_posix.o\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c: In function \u2018OPENSSL_DIR_read\u2019:\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c:88:3: error: \u2018readdir_r\u2019 is deprecated [-Werror=deprecated-declarations]\r\n   if (readdir_r((*ctx)->dir, &(*ctx)->dirent, &dirent) != 0 ||\r\n   ^~\r\nIn file included from /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c:36:0:\r\n/usr/include/dirent.h:183:12: note: declared here\r\n extern int readdir_r (DIR *__restrict __dirp,\r\n            ^~~~~~~~~\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c: At top level:\r\ncc1: error: unrecognized command line option \u2018-Wno-parentheses-equality\u2019 [-Werror]\r\ncc1: all warnings being treated as errors\r\nMakefile:883: recipe for target 'third_party/boringssl/crypto/directory_posix.lo' failed\r\nmake: *** [third_party/boringssl/crypto/directory_posix.lo] Error 1\r\nERROR: `make' failed\r\n```","bash\r\n$ sudo pecl install grpc\r\ndownloading grpc-1.0.1.tgz ...\r\nStarting to download grpc-1.0.1.tgz (1,910,227 bytes)\r\n.....................................done: 1,910,227 bytes\r\n884 source files, building\r\nrunning: phpize\r\nConfiguring for:\r\nPHP Api Version:         20151012\r\nZend Module Api No:      20151012\r\nZend Extension Api No:   320151012\r\nbuilding in /tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1\r\nrunning: /tmp/pear/temp/grpc/configure --with-php-config=/usr/bin/php-config\r\nchecking for grep that handles long lines and -e... /bin/grep\r\nchecking for egrep... /bin/grep -E\r\nchecking for a sed that does not truncate output... /bin/sed\r\nchecking for cc... cc\r\nchecking whether the C compiler works... yes\r\nchecking for C compiler default output file name... a.out\r\nchecking for suffix of executables...\r\nchecking whether we are cross compiling... no\r\nchecking for suffix of object files... o\r\nchecking whether we are using the GNU C compiler... yes\r\nchecking whether cc accepts -g... yes\r\nchecking for cc option to accept ISO C89... none needed\r\nchecking how to run the C preprocessor... cc -E\r\nchecking for icc... no\r\nchecking for suncc... no\r\nchecking whether cc understands -c and -o together... yes\r\nchecking for system library directory... lib\r\nchecking if compiler supports -R... no\r\nchecking if compiler supports -Wl,-rpath,... yes\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... x86_64-pc-linux-gnu\r\nchecking target system type... x86_64-pc-linux-gnu\r\nchecking for PHP prefix... /usr\r\nchecking for PHP includes... -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib\r\nchecking for PHP extension directory... /usr/lib/php/20151012\r\nchecking for PHP installed headers prefix... /usr/include/php/20151012\r\nchecking if debug is enabled... no\r\nchecking if zts is enabled... no\r\nchecking for re2c... no\r\nconfigure: WARNING: You will need re2c 0.13.4 or later if you want to regenerate PHP parsers.\r\nchecking for gawk... no\r\nchecking for nawk... nawk\r\nchecking if nawk is broken... no\r\nchecking whether to enable grpc support... yes, shared\r\nchecking how to print strings... printf\r\nchecking for a sed that does not truncate output... (cached) /bin/sed\r\nchecking for fgrep... /bin/grep -F\r\nchecking for ld used by cc... /usr/bin/ld\r\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\r\nchecking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\r\nchecking the name lister (/usr/bin/nm -B) interface... BSD nm\r\nchecking whether ln -s works... yes\r\nchecking the maximum length of command line arguments... 1572864\r\nchecking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop\r\nchecking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop\r\nchecking for /usr/bin/ld option to reload object files... -r\r\nchecking for objdump... objdump\r\nchecking how to recognize dependent libraries... pass_all\r\nchecking for dlltool... no\r\nchecking how to associate runtime and link libraries... printf %s\\n\r\nchecking for ar... ar\r\nchecking for archiver @FILE support... @\r\nchecking for strip... strip\r\nchecking for ranlib... ranlib\r\nchecking for gawk... (cached) nawk\r\nchecking command to parse /usr/bin/nm -B output from cc object... ok\r\nchecking for sysroot... no\r\nchecking for a working dd... /bin/dd\r\nchecking how to truncate binary pipes... /bin/dd bs=4096 count=1\r\nchecking for mt... mt\r\nchecking if mt is a manifest tool... no\r\nchecking for ANSI C header files... yes\r\nchecking for sys/types.h... yes\r\nchecking for sys/stat.h... yes\r\nchecking for stdlib.h... yes\r\nchecking for string.h... yes\r\nchecking for memory.h... yes\r\nchecking for strings.h... yes\r\nchecking for inttypes.h... yes\r\nchecking for stdint.h... yes\r\nchecking for unistd.h... yes\r\nchecking for dlfcn.h... yes\r\nchecking for objdir... .libs\r\nchecking if cc supports -fno-rtti -fno-exceptions... no\r\nchecking for cc option to produce PIC... -fPIC -DPIC\r\nchecking if cc PIC flag -fPIC -DPIC works... yes\r\nchecking if cc static flag -static works... yes\r\nchecking if cc supports -c -o file.o... yes\r\nchecking if cc supports -c -o file.o... (cached) yes\r\nchecking whether the cc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\r\nchecking whether -lc should be explicitly linked in... no\r\nchecking dynamic linker characteristics... GNU/Linux ld.so\r\nchecking how to hardcode library paths into programs... immediate\r\nchecking whether stripping libraries is possible... yes\r\nchecking if libtool supports shared libraries... yes\r\nchecking whether to build shared libraries... yes\r\nchecking whether to build static libraries... no\r\nconfigure: creating ./config.status\r\nconfig.status: creating config.h\r\nconfig.status: executing libtool commands\r\nrunning: make\r\n........\r\n/bin/bash /tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/libtool --mode=compile cc -Wall -Werror     -Wno-parentheses-equality -Wno-unused-value -std=c11     -fvisibility=hidden -DOPENSSL_NO_ASM -D_GNU_SOURCE -DWIN32_LEAN_AND_MEAN     -D_HAS_EXCEPTIONS=0 -DNOMINMAX -I. -I/tmp/pear/temp/grpc -DPHP_ATOM_INC -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/include -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/main -I/tmp/pear/temp/grpc -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib -I/tmp/pear/temp/grpc/include -I/tmp/pear/temp/grpc/src/php/ext/grpc -I/tmp/pear/temp/grpc/third_party/boringssl/include  -DHAVE_CONFIG_H  -g -O2   -c /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c -o third_party/boringssl/crypto/directory_posix.lo\r\nlibtool: compile:  cc -Wall -Werror -Wno-parentheses-equality -Wno-unused-value -std=c11 -fvisibility=hidden -DOPENSSL_NO_ASM -D_GNU_SOURCE -DWIN32_LEAN_AND_MEAN -D_HAS_EXCEPTIONS=0 -DNOMINMAX -I. -I/tmp/pear/temp/grpc -DPHP_ATOM_INC -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/include -I/tmp/pear/temp/pear-build-roothPkMAS/grpc-1.0.1/main -I/tmp/pear/temp/grpc -I/usr/include/php/20151012 -I/usr/include/php/20151012/main -I/usr/include/php/20151012/TSRM -I/usr/include/php/20151012/Zend -I/usr/include/php/20151012/ext -I/usr/include/php/20151012/ext/date/lib -I/tmp/pear/temp/grpc/include -I/tmp/pear/temp/grpc/src/php/ext/grpc -I/tmp/pear/temp/grpc/third_party/boringssl/include -DHAVE_CONFIG_H -g -O2 -c /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c  -fPIC -DPIC -o third_party/boringssl/crypto/.libs/directory_posix.o\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c: In function \u2018OPENSSL_DIR_read\u2019:\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c:88:3: error: \u2018readdir_r\u2019 is deprecated [-Werror=deprecated-declarations]\r\n   if (readdir_r((*ctx)->dir, &(*ctx)->dirent, &dirent) != 0 ||\r\n   ^~\r\nIn file included from /tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c:36:0:\r\n/usr/include/dirent.h:183:12: note: declared here\r\n extern int readdir_r (DIR *__restrict __dirp,\r\n            ^~~~~~~~~\r\n/tmp/pear/temp/grpc/third_party/boringssl/crypto/directory_posix.c: At top level:\r\ncc1: error: unrecognized command line option \u2018-Wno-parentheses-equality\u2019 [-Werror]\r\ncc1: all warnings being treated as errors\r\nMakefile:883: recipe for target 'third_party/boringssl/crypto/directory_posix.lo' failed\r\nmake: *** [third_party/boringssl/crypto/directory_posix.lo] Error 1\r\nERROR: `make' failed\r\n"
8594,"C# Hard crash when service account JSON has incorrect emailRepro:\r\n\r\nProgram.cs:\r\n\r\n\r\n\r\nproject.json:\r\n\r\n```json\r\n{\r\n  ""version"": ""1.0.0-*"",\r\n  ""buildOptions"": {\r\n    ""debugType"": ""portable"",\r\n    ""emitEntryPoint"": true\r\n  },\r\n  ""dependencies"": {\r\n    ""Google.Pubsub.V1"": ""1.0.0-beta03""\r\n  },\r\n  ""frameworks"": {\r\n    ""netcoreapp1.0"": {\r\n      ""dependencies"": {\r\n        ""Microsoft.NETCore.App"": {\r\n          ""type"": ""platform"",\r\n          ""version"": ""1.0.1""\r\n        }\r\n      },\r\n      ""imports"": ""dnxcore50""\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n(Google.Pubsub.V1 has a generated client for Pubsub, and uses Grpc.Core 1.0.1-pre1.)\r\n\r\n- Set `GOOGLE_APPLICATION_CREDENTIALS` environment variable to a service account JSON file\r\n- Edit the json file to modify `client_email` to an incorrect value (e.g. put ""foo"" on the end)\r\n- Run `dotnet run`\r\n\r\nThis crashes the app hard:\r\n\r\n```text\r\nUnhandled Exception: System.AccessViolationException: Attempted to read or write protected memory. This is often an indication that other memory is corrupt.\r\n   at Grpc.Core.Internal.NativeMetadataCredentialsPlugin.<StartGetMetadata>d__11.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Grpc.Auth.GoogleAuthInterceptors.<>c__DisplayClass2_0.<<FromCredential>b__0>d.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceAccountCredential.<GetAccessTokenForRequestAsync>d__21.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceCredential.<GetAccessTokenForRequestAsync>d__23.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceAccountCredential.<RequestAccessTokenAsync>d__20.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.Requests.TokenRequestExtenstions.<ExecuteAsync>d__0.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task`1.TrySetResult(TResult result)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetResult(TResult result)\r\n   at System.Net.Http.HttpClient.<FinishSendAsync>d__58.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.ThreadPoolWorkQueue.Dispatch()\r\n```",lang/C#|priority/P1,apolcyn|jtattermusch,"Repro:\r\n\r\nProgram.cs:\r\n\r\n```csharp\r\nusing System;\r\nusing Google.Apis.Auth.OAuth2;\r\nusing Google.Pubsub.V1;\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\n\r\nclass Program\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        var credentials = GoogleCredential\r\n            .GetApplicationDefaultAsync()\r\n            .Result\r\n            .CreateScoped(new[] { ""https://www.googleapis.com/auth/pubsub"" } );\r\n        var channelCredentials = credentials.ToChannelCredentials();\r\n        var channel = new Channel(""pubsub.googleapis.com"", 443, channelCredentials);\r\n        var client = new Publisher.PublisherClient(channel);\r\n        client.ListTopics(new ListTopicsRequest { Project = ""irrelevant"" });\r\n    }\r\n}\r\n```\r\n\r\nproject.json:\r\n\r\n```json\r\n{\r\n  ""version"": ""1.0.0-*"",\r\n  ""buildOptions"": {\r\n    ""debugType"": ""portable"",\r\n    ""emitEntryPoint"": true\r\n  },\r\n  ""dependencies"": {\r\n    ""Google.Pubsub.V1"": ""1.0.0-beta03""\r\n  },\r\n  ""frameworks"": {\r\n    ""netcoreapp1.0"": {\r\n      ""dependencies"": {\r\n        ""Microsoft.NETCore.App"": {\r\n          ""type"": ""platform"",\r\n          ""version"": ""1.0.1""\r\n        }\r\n      },\r\n      ""imports"": ""dnxcore50""\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n(Google.Pubsub.V1 has a generated client for Pubsub, and uses Grpc.Core 1.0.1-pre1.)\r\n\r\n- Set `GOOGLE_APPLICATION_CREDENTIALS` environment variable to a service account JSON file\r\n- Edit the json file to modify `client_email` to an incorrect value (e.g. put ""foo"" on the end)\r\n- Run `dotnet run`\r\n\r\nThis crashes the app hard:\r\n\r\n```text\r\nUnhandled Exception: System.AccessViolationException: Attempted to read or write protected memory. This is often an indication that other memory is corrupt.\r\n   at Grpc.Core.Internal.NativeMetadataCredentialsPlugin.<StartGetMetadata>d__11.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Grpc.Auth.GoogleAuthInterceptors.<>c__DisplayClass2_0.<<FromCredential>b__0>d.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceAccountCredential.<GetAccessTokenForRequestAsync>d__21.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceCredential.<GetAccessTokenForRequestAsync>d__23.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.ServiceAccountCredential.<RequestAccessTokenAsync>d__20.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task.Finish(Boolean bUserDelegateExecuted)\r\n   at System.Threading.Tasks.Task`1.TrySetException(Object exceptionObject)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetException(Exception exception)\r\n   at Google.Apis.Auth.OAuth2.Requests.TokenRequestExtenstions.<ExecuteAsync>d__0.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.Tasks.AwaitTaskContinuation.RunOrScheduleAction(Action action, Boolean allowInlining, Task& currentTask)\r\n   at System.Threading.Tasks.Task.FinishContinuations()\r\n   at System.Threading.Tasks.Task`1.TrySetResult(TResult result)\r\n   at System.Runtime.CompilerServices.AsyncTaskMethodBuilder`1.SetResult(TResult result)\r\n   at System.Net.Http.HttpClient.<FinishSendAsync>d__58.MoveNext()\r\n   at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)\r\n   at System.Runtime.CompilerServices.AsyncMethodBuilderCore.MoveNextRunner.RunWithDefaultContext()\r\n   at System.Threading.ThreadPoolWorkQueue.Dispatch()\r\n```","csharp\r\nusing System;\r\nusing Google.Apis.Auth.OAuth2;\r\nusing Google.Pubsub.V1;\r\nusing Grpc.Auth;\r\nusing Grpc.Core;\r\n\r\nclass Program\r\n{\r\n    public static void Main(string[] args)\r\n    {\r\n        var credentials = GoogleCredential\r\n            .GetApplicationDefaultAsync()\r\n            .Result\r\n            .CreateScoped(new[] { ""https://www.googleapis.com/auth/pubsub"" } );\r\n        var channelCredentials = credentials.ToChannelCredentials();\r\n        var channel = new Channel(""pubsub.googleapis.com"", 443, channelCredentials);\r\n        var client = new Publisher.PublisherClient(channel);\r\n        client.ListTopics(new ListTopicsRequest { Project = ""irrelevant"" });\r\n    }\r\n}\r\n"
