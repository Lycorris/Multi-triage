{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9544780-798f-4bf2-babe-5c804685346d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alion\\Desktop\\csweb\\Multi-triage\\Code\n",
      "c:\\Users\\Alion\\Desktop\\csweb\\Multi-triage\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import SGD, Adam\n",
    "from keras_preprocessing.text import Tokenizer \n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "#deepcopy\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "#get cwd\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "#get upper of cwd\n",
    "upper = os.path.dirname(cwd)\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab102b8-4e6c-47aa-8ecf-d24bc7afba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roslyn has been splited into 4 csv files\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:01<00:03,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspnet has been splited into 4 csv files\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "paths = {\n",
    "    'roslyn':f'{upper}\\Data\\\\roslyn\\IssueroslynWebScrap.csv', #roslyn\n",
    "    'aspnet':f'{upper}\\Data\\\\aspnet\\\\trainACU40.csv', #aspnet_train(80%)\n",
    "    'efcore':f'{upper}\\Data\\efcore\\IssueefcoreWebScrap.csv', #efcore\n",
    "    'powerShell':f'{upper}\\Data\\powershell\\C_uA_Train_without_doubleU.csv' ,#powershell_train (80%)\n",
    "    'elasticSearch':f'{upper}\\Data\\elasticSearch\\IssueelasticsearchWebScrap.csv', #elasticSearch\n",
    "    'mixedRealityToolUnity':f'{upper}\\Data\\mixedRealityToolUnity\\IssuemixedrealitytoolkitunityWebScrap.csv' ,#mixedRealityToolUnity\n",
    "    'monoGame':f'{upper}\\Data\\monoGame\\IssuemonogameWebScrap.csv', #monoGame\n",
    "    'realmJava':f'{upper}\\Data\\\\realmJava\\IssuerealmjavaWebScrap.csv',# realmjava\n",
    "    #nunit(to be processed from xml to csv)\n",
    "    #r'..\\Data\\nunit\\IssuenunitWebScrap.csv' #nunit\n",
    "    #rxjava(to be processed from xml to csv)\n",
    "    #r'..\\Data\\rxjava\\IssuerxjavaWebScrap.csv' #rxjava\n",
    "}\n",
    "\n",
    "#获取前5项数据的第二个tensor\n",
    "#如果AST为空，第二个tensor的值的只有一个10\n",
    "# print(train_dataset[0][0][1], train_dataset[1][0][1], train_dataset[2][0][1], train_dataset[3][0][1], train_dataset[4][0][1])\n",
    "# #遍历数据集，统计第二个tensor的值里面只有一个10的数量\n",
    "# count = 0\n",
    "# for i in range(len(train_dataset)):\n",
    "#     if train_dataset[i][0][1][0] == 10:\n",
    "#         count += 1\n",
    "# print(count, len(train_dataset))\n",
    "# has_AST = 1 - count/len(train_dataset)\n",
    "# print(has_AST)\n",
    "\n",
    "\n",
    "for datasetName,path in tqdm.tqdm(paths.items()):\n",
    "    print('-'*60)\n",
    "    data = pd.read_csv(path)\n",
    "    #保存为四个csv文件：\n",
    "    #1.含有unknown标签的数据集,含有AST为空的数据集\n",
    "    #2.含有unknown标签的数据集,不含有AST为空的数据集 \n",
    "    #3.不含有unknown标签的数据集,含有AST为空的数据集\n",
    "    #4.不含有unknown标签的数据集,不含有AST为空的数据集\n",
    "    #1\n",
    "    data.to_csv(f'{upper}\\Data\\{datasetName}\\{datasetName}_1.csv', index=False)\n",
    "    #2\n",
    "    # 筛选AST列不为空的数据\n",
    "    data_2 = data[data['AST'].notnull()]\n",
    "    data_2.to_csv(f'{upper}\\Data\\{datasetName}\\{datasetName}_2.csv', index=False)\n",
    "    #3 \n",
    "    data_3 = data[data['FixedByID'] != 'unknown']\n",
    "    data_3 = data_3[data_3['Name'] != 'unknown']\n",
    "    data_3.to_csv(f'{upper}\\Data\\{datasetName}\\{datasetName}_3.csv', index=False)\n",
    "    #4\n",
    "    data_4 = data_2[data_2['FixedByID'] != 'unknown']\n",
    "    data_4 = data_4[data_4['Name'] != 'unknown']\n",
    "    data_4.to_csv(f'{upper}\\Data\\{datasetName}\\{datasetName}_4.csv', index=False)\n",
    "\n",
    "    print(f'{datasetName} has been splited into 4 csv files')\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
