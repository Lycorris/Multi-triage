{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b392f11-4942-47e5-947e-ce499b721456",
   "metadata": {},
   "source": [
    "1. early-stop --> epoch\n",
    "2. hyper-prarm with list\n",
    "- datasets * 8\n",
    "- model * 3\n",
    "- loss * 3\n",
    "- use_ast\n",
    "  \n",
    "3. other hyper-prarm\n",
    "- lr = 3e-5\n",
    "- bsz = 8 (24g4090 + 2Bert)\n",
    "\n",
    "4. TBC\n",
    "- customized loss\n",
    "- ast ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.466477Z",
     "start_time": "2024-04-06T07:03:20.823265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from dataset import *\n",
    "from model import *\n",
    "from losses import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_metric(hist_metric, metric, l):\n",
    "    hist_metric[0] += loss.item()/l\n",
    "    hist_metric[1] += metric['acc'][0]/l\n",
    "    hist_metric[2] += metric['acc'][1]/l\n",
    "    hist_metric[3] += metric['F1'][0]/l\n",
    "    hist_metric[4] += metric['F1'][1]/l\n",
    "    \n",
    "def precess_data(x, y, device):\n",
    "    x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "    x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "    y = y.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f79139c423ae61"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710ecaf-fa9b-4e52-b973-b544e363f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    _code_format = 'None'  -> ignore Code Snippet \n",
    "                   'Front' -> add Code BEFORE Text\n",
    "                   'Back'  -> add Code BEHIND Text\n",
    "                   'Separate' -> consider Code as an independent input\n",
    "    _model_type =  'Multi-triage'\n",
    "                   'PreTrain'\n",
    "'''\n",
    "def train_imm(_path, _logname, _loss_fn, _code_format='None', _model_type = 'Multi-triage', \n",
    "              _num_epochs = 20, _bsz = 8, _lr = 3e-5, \n",
    "              _ckpt = 'bert-base-uncased', _code_ckpt = 'codebert-base', \n",
    "              device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \n",
    "    # DATASET Read Data: extract data\n",
    "    dataset = pd.read_csv(_path)\n",
    "    dataset = dataset.rename(columns={'Title_Description': 'Context', 'AST': 'AST', 'FixedByID': 'Dev', 'Name': 'Btype'})\n",
    "    dataset = dataset[['Context', 'AST', 'Dev', 'Btype']]\n",
    "    \n",
    "    # DATASET Output: convert label to tensor\n",
    "    dataset, D_ids2token, B_ids2token = label_vectorize(dataset)\n",
    "    n_classes = [len(D_ids2token), len(B_ids2token)]\n",
    "    # log\n",
    "    logname = '../res_log/' + _logname + '.txt'\n",
    "    logstr = _logname + '\\\\n' + '-'*60 + '\\\\n' + 'dataset shape:{}\\nn_classes: {}'.format(dataset.shape, n_classes) + '-'*60 + '\\n\\n'\n",
    "    print('dataset shape:{}\\nn_classes: {}'.format(dataset.shape, n_classes))\n",
    "    \n",
    "    # DATASET Input: convert text to tensor\n",
    "    dataset = text_tensorize(dataset, _ckpt, _code_format)\n",
    "\n",
    "    # DATASET Format: split train/val/test dataset and wrap into dataloader\n",
    "    train_dataloader, val_dataloader, test_dataloader = \\\n",
    "        split_and_wrap_dataset(dataset, _bsz)\n",
    "\n",
    "\n",
    "    # MODEL load model\n",
    "    if _model_type == 'Multi-triage':\n",
    "        model = MetaModel(n_classes = n_classes, use_AST=(_code_format=='Separate'))\n",
    "    else:\n",
    "        model = PretrainModel(text_ckpt=_ckpt, code_ckpt=_code_ckpt, n_classes=n_classes, use_AST=(_code_format=='Separate'))\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss\n",
    "    loss_fn = _loss_fn.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr)\n",
    "\n",
    "    # lr_scheduler\n",
    "    num_epochs = _num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps,)\n",
    "\n",
    "    # train process\n",
    "    for epoch in trange(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x_C, x_A, y = precess_data(x, y, device)\n",
    "            outputs = model(x_C, x_A)\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            train_loss += loss.item()/len(train_dataloader)\n",
    "            # back\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\n",
    "    \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_metric = [0.0] * 5\n",
    "        for x, y in val_dataloader:\n",
    "            x_C, x_A, y = precess_data(x, y, device)\n",
    "            outputs = model(x_C, x_A)\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "\n",
    "            metric = metrics(y, outputs, split_pos = n_classes)\n",
    "            val_metric = update_metric(val_metric, metric, len(val_dataloader))\n",
    "            \n",
    "        logstr += '-' * 60 + '{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(epoch, val_metric[0], val_metric[1:3], val_metric[3:])\n",
    "\n",
    "    # test\n",
    "    model.eval()\n",
    "    test_metric = [0.0] * 5\n",
    "    for x, y in tqdm(test_dataloader):\n",
    "        x_C, x_A, y = precess_data(x, y, device)\n",
    "        outputs = model(x_C, x_A)\n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        \n",
    "        metric = metrics(y, outputs, split_pos = n_classes)\n",
    "        test_metric = update_metric(test_metric, metric, len(test_dataloader))\n",
    "        \n",
    "    logstr += '-' * 60 + '\\ntest_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_metric[0], test_metric[1:3], test_metric[3:])\n",
    "    print('test_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_metric[0], test_metric[1:3], test_metric[3:]))\n",
    "\n",
    "    with open(logname, 'w') as f:\n",
    "        f.write(logstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06eddbeb-50fe-4beb-bcfc-07638df32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 datasets\n",
    "# path, name, dataset size\n",
    "pathlist = [\n",
    "    ('../Data/aspnet/aspnet_1.csv', 'aspnet', 7151),\n",
    "    ('../Data/efcore/efcore_1.csv', 'efcore', ),\n",
    "    ('../Data/elasticSearch/elasticSearch_1.csv', 'elasticSearch'),\n",
    "    ('../Data/mixedRealityToolUnity/mixedRealityToolUnity_1.csv', 'mixedRealityToolUnity'),\n",
    "    ('../Data/monoGame/monoGame_1.csv', 'monoGame'),\n",
    "    ('../Data/powershell/powerShell_1.csv', 'powerShell'),\n",
    "    ('../Data/realmJava/realmJava_1.csv', 'realmJava'),\n",
    "    ('../Data/roslyn/roslyn_1.csv', 'roslyn'),\n",
    "]\n",
    "losslist = [\n",
    "    (nn.BCEWithLogitsLoss(), 'BCE'),\n",
    "    (CustomizedBCELoss(), 'CBCE'),\n",
    "    (AsymmetricLossOptimized(), 'ASL'),\n",
    "]\n",
    "\n",
    "ckptlist = [\n",
    "    ('bert-base-uncased', 'Multi-triage'),  # just for tokenize\n",
    "    ('bert-base-uncased', ' Bert'),\n",
    "    ('roberta-base', 'Robert'),\n",
    "    ('albert-base-v2', 'albert'),\n",
    "]\n",
    "\n",
    "astlist = [\n",
    "    'no_AST',\n",
    "    # 'use_AST',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f5bcf-58f3-4100-b88f-f1f52b620be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 20/20 [00:34<00:00,  1.72s/it]\n",
      "100%|██████████| 178/178 [00:00<00:00, 548.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.08315895236191459\n",
      " test_acc:[0.9914924565995686, 0.9841036927164257]\n",
      " test_f1: [0.7464875521381351, 0.0029260144551772676]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:40<00:00,  2.04s/it]\n",
      "100%|██████████| 178/178 [00:00<00:00, 491.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.192807698517706\n",
      " test_acc:[0.9911816257439314, 0.9730167064104195]\n",
      " test_f1: [0.7443590021458304, 0.3252574691984619]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet  Bert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:27<00:00, 64.39s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.04116710515044041\n",
      " test_acc:[0.9921141156319828, 0.9872106035773665]\n",
      " test_f1: [0.6769651465298621, 0.4813009334402033]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet  Bert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:37<00:00, 64.90s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 28.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 17.13844633906075\n",
      " test_acc:[0.9901225024394777, 0.984295212150958]\n",
      " test_f1: [0.7419702737434072, 0.5681322851241607]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Robert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:44<00:00, 65.21s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.048497773204626664\n",
      " test_acc:[0.9919644580798205, 0.9841462541162281]\n",
      " test_f1: [0.690307830434486, 0.0]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Robert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [21:54<00:00, 65.71s/it]\n",
      "100%|██████████| 178/178 [00:06<00:00, 29.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 16.09969470742044\n",
      " test_acc:[0.9891324455148723, 0.9836461677979879]\n",
      " test_f1: [0.7334819687771934, 0.5725674298373816]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet albert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [23:25<00:00, 70.28s/it]\n",
      "100%|██████████| 178/178 [00:07<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.0445819696714955\n",
      " test_acc:[0.991998996627465, 0.9860880729857446]\n",
      " test_f1: [0.6804763791363463, 0.33973016652897636]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet albert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(7151, 7)\n",
      "n_classes:  [61, 132]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [23:34<00:00, 70.74s/it]\n",
      "100%|██████████| 178/178 [00:07<00:00, 23.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.162022064241139\n",
      " test_acc:[0.9897886417555016, 0.981911832027221]\n",
      " test_f1: [0.7233821751890152, 0.5343408129934141]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:30<00:00,  1.52s/it]\n",
      "100%|██████████| 165/165 [00:00<00:00, 545.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.11704160900730072\n",
      " test_acc:[0.9703938487804292, 0.9714080344546926]\n",
      " test_f1: [0.6113625533222877, 0.0708456471918589]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:37<00:00,  1.88s/it]\n",
      "100%|██████████| 165/165 [00:00<00:00, 485.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 10.80376569863522\n",
      " test_acc:[0.9619090091098434, 0.9374608177127265]\n",
      " test_f1: [0.6181393157525095, 0.3284657550165212]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore  Bert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 20/20 [19:46<00:00, 59.33s/it]\n",
      "100%|██████████| 165/165 [00:05<00:00, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.091461328778303\n",
      " test_acc:[0.9715756654739375, 0.9743991631450083]\n",
      " test_f1: [0.555063576446371, 0.3636511525008343]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore  Bert ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 80%|████████  | 16/20 [16:56<04:14, 63.55s/it]\n",
      "100%|██████████| 165/165 [00:05<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 18.672516912402536\n",
      " test_acc:[0.9661514285838961, 0.9654127500273972]\n",
      " test_f1: [0.6344624430134864, 0.4512340457911784]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Robert BCE no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(6612, 7)\n",
      "n_classes:  [25, 58]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 4/20 [03:59<16:00, 60.00s/it]"
     ]
    }
   ],
   "source": [
    "for path in pathlist:\n",
    "    for ckpt in ckptlist:\n",
    "        for loss in losslist:\n",
    "            for ast in astlist:\n",
    "                is_t, u_ast = (ckpt[1] == 'Multi-triage'), (ast == 'use_AST') \n",
    "                if loss[1] == 'CBCE':\n",
    "                    loss[0] = CustomizedBCELoss(p_N=path[2]/2)\n",
    "\n",
    "                logname = ' '.join([path[1], ckpt[1], loss[1], ast])\n",
    "                print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            \n",
    "                train_imm(_path = path[0], _logname = '../res_log/' + logname + '.txt', \n",
    "                      _loss_fn = loss[0], _use_ast = u_ast, _is_textcnn = is_t, _ckpt = ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f758c22-1da8-4bac-917e-a7309461873c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617388cb-09b9-46f4-8843-be99446f8fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
