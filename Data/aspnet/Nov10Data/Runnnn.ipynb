{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39ec2a8-3146-423a-90f9-2be73f1f93af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 17:25:09.606327: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9a61d9-24ab-4206-ba50-b8a36af6f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# (*func7*) one_hot_encode\n",
    "def one_hot_encode(tags, mapping):\n",
    "    # create empty vector\n",
    "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
    "    # mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190bfc35-1cc6-4cef-9a07-918ca6d2948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*func6*) create_tag_mapping -> 7\n",
    "# combineddata_dev = train_dev_y + test_dev_y, 'FixedByID'\n",
    "def create_tag_mapping(mapping_csv, tagname):\n",
    "    # tagname FixedByID\n",
    "    print('tagname', tagname)\n",
    "    # create a set of all known tags\n",
    "    labels = set()\n",
    "    IssueType_Tags = []\n",
    "    for i in range(len(mapping_csv)):\n",
    "        # convert spaced separated tags into an array of tags\n",
    "        tags = mapping_csv[i].split('|')\n",
    "        # add tags to the set of known labels\n",
    "        labels.update(tags)\n",
    "    # labels 存 tag(dev) \n",
    "    # convert set of labels to a list to list\n",
    "    labels = list(labels)\n",
    "    # order set alphabetically\n",
    "    labels.sort()\n",
    "    # label和数字的正反映射\n",
    "    # dict that maps labels to integers, and the reverse\n",
    "    labels_map = {labels[i]: i for i in range(len(labels))}\n",
    "    inv_labels_map = {i: labels[i] for i in range(len(labels))}\n",
    "    # mapping_csv转为onehot编码\n",
    "    for i in range(len(mapping_csv)):\n",
    "        # Create One Hot Encoding For Issue Type\n",
    "        IssueType_Tag = one_hot_encode(mapping_csv[i].split('|'), labels_map) # (*func7*) one_hot_encode\n",
    "        IssueType_Tags.append(IssueType_Tag)\n",
    "\n",
    "    result = IssueType_Tags\n",
    "    return labels_map, inv_labels_map, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e6adbb-919c-419a-9260-9b57d956c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (*func8*) RemoveTestRecordIfNotExistInTrainData\n",
    "def RemoveTestRecordIfNotExistInTrainData(traindata, testdata):\n",
    "    traingroup = traindata.groupby([\"Name\", \"FixedByID\"], as_index=True)[\"FixedByID\"].size().reset_index(name=\"count\")\n",
    "    testgroup = testdata.groupby([\"Name\", \"FixedByID\"], as_index=True)[\"FixedByID\"].size().reset_index(name=\"count\")\n",
    "    for ind in testgroup.index:\n",
    "        try:\n",
    "            record = traindata[\n",
    "                traindata['FixedByID'].str.match(testgroup['FixedByID'][ind]) & traindata['Name'].str.match(\n",
    "                    testgroup['Name'][ind])]\n",
    "            if len(record) < 1:\n",
    "                print('remove from testdata...')\n",
    "                testdata = testdata.drop(testdata[\n",
    "                                             testdata['FixedByID'].str.match(testgroup['FixedByID'][ind]) & testdata[\n",
    "                                                 'Name'].str.match(testgroup['Name'][ind])].index)\n",
    "        except:\n",
    "            print(\"An exception occurred index :\", ind)\n",
    "    return testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcdfee0f-11b1-4c08-b67a-3c5f66a6cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Project Parameters\n",
    "DataAugmentation, DataAugThreshold = True, 30000\n",
    "DataFilePath, DataFileName, FileType = \"Data/\", \"IssueaspnetcoreWebScrap\", \".csv\"\n",
    "MAX_SEQUENCE_LENGTH, EMBEDDING_DIM = 300, 100\n",
    "LoadDataAugFromFile = False\n",
    "LearningRate = 0.001\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a3e319-88fd-4b96-9ac0-f484f50d09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log file\n",
    "filename = 'Multimodel' + '_' + DataFileName + '_' + \"dataaug\" + '_' + str(LearningRate) + '_' + str(EMBEDDING_DIM) + '_' + str(MAX_SEQUENCE_LENGTH)\n",
    "filelog = open(filename + \".txt\", \"w\")\n",
    "filelog.write(\"StartTime:\" + str(datetime.datetime.now()))\n",
    "filelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9006227-3d89-43a4-b0e8-60f6f7d8caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get traindata & testdata\n",
    "import pandas as pd\n",
    "traindata = pd.read_csv(\"Data/IssueaspnetcoreWebScraptrainaugdata5.csv\",\n",
    "                                error_bad_lines=False, index_col=False, dtype='unicode', encoding='latin-1',\n",
    "                                low_memory=False).sample(frac=1)\n",
    "traindata = traindata.rename(columns={'ï»¿RepoID': 'RepoID'}, inplace=False)\n",
    "testdata = pd.read_csv(\"Data/IssueaspnetcoreWebScraptestdata5.csv\",\n",
    "                               error_bad_lines=False, index_col=False, dtype='unicode', encoding='latin-1',\n",
    "                               low_memory=False).sample(frac=1)\n",
    "testdata = testdata.rename(columns={'ï»¿RepoID': 'RepoID'}, inplace=False)\n",
    "testdata = RemoveTestRecordIfNotExistInTrainData(traindata, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "038ad7a2-0f73-4e09-a611-cf53738477dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /*****************Train Data***********************/\n",
    "train_dev_y = list(traindata['FixedByID'])  # Developer List\n",
    "train_btype_y = list(traindata['Name'])  # Bug Type List\n",
    "train_x_context = list(traindata['Title_Description'])\n",
    "traindata.AST = traindata.AST.astype(str)\n",
    "train_x_AST = list(traindata['AST'])\n",
    "\n",
    "x_train_context = train_x_context\n",
    "x_train_AST = train_x_AST\n",
    "\n",
    "# /****************Test Data************************/\n",
    "x_test_context = list(testdata['Title_Description'])\n",
    "x_test_AST = list(testdata['Title_Description'])\n",
    "test_dev_y = list(testdata['FixedByID'])  # Developer List\n",
    "test_btype_y = list(testdata['Name'])  # Bug Type List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "673683ae-d02e-4ab5-892f-678868a0819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagname FixedByID\n",
      "Developer Training:  31929 Testing : 1363 Combined DEV + TEST 33292\n",
      "tagname Name\n",
      "Bug Type Training:  31929 Testing : 1363 Combined DEV + TEST 33292\n"
     ]
    }
   ],
   "source": [
    "# /*******************Label Encoder***********************/\n",
    "### Developer Encoder \n",
    "combineddata_dev = train_dev_y + test_dev_y\n",
    "# (*func6*) create_tag_mapping 把train_dev_y 和 test_dev_y 由str进行onehot encoder\n",
    "dev_labels_map, dev_inv_labels_map, combineddata_dev_enc = create_tag_mapping(combineddata_dev, 'FixedByID')\n",
    "dev_y_train = combineddata_dev_enc[:len(train_dev_y)]\n",
    "dev_y_test = combineddata_dev_enc[len(train_dev_y):]\n",
    "# 31929 1430 33359\n",
    "print(\"Developer\", \"Training: \", len(train_dev_y), \"Testing :\", len(test_dev_y), \"Combined DEV + TEST\",\n",
    "        len(combineddata_dev_enc))\n",
    "\n",
    "### BugType Encoder\n",
    "combineddata_bugtype = train_btype_y + test_btype_y\n",
    "# 同理编码bugtype\n",
    "btype_labels_map, btype_inv_labels_map, combineddata_bugtype_enc = create_tag_mapping(combineddata_bugtype, 'Name')\n",
    "btype_y_train = combineddata_bugtype_enc[:len(train_btype_y)]\n",
    "btype_y_test = combineddata_bugtype_enc[len(train_btype_y):]\n",
    "print(\"Bug Type\", \"Training: \", len(btype_y_train), \"Testing :\", len(btype_y_test), \"Combined DEV + TEST\",\n",
    "        len(combineddata_bugtype_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a05ffe12-5df7-490c-9ac2-287319d568b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# /*******************Tokenizer****************************/\n",
    "\n",
    "## context 小写后 转为 toeknizer \n",
    "x_train_context = [str(row).lower() for row in x_train_context]\n",
    "x_test_context = [str(row).lower() for row in x_test_context]\n",
    "combineddata_context = x_train_context + x_test_context\n",
    "## 不限词汇表大小，以单词为单位，未知词标为Unknown，过滤掉'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\d+'\n",
    "tk_context = Tokenizer(num_words=None, char_level=None, oov_token='Unknown',\n",
    "                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tk_context.fit_on_texts(combineddata_context)\n",
    "\n",
    "## AST 小写后 转为 toeknizer \n",
    "x_train_AST = [str(row).lower() for row in x_train_AST]\n",
    "x_test_AST = [str(row).lower() for row in x_test_AST]\n",
    "combineddata_AST = x_train_AST + x_test_AST\n",
    "tk_AST = Tokenizer(num_words=None, char_level=None, oov_token='Unknown')\n",
    "tk_AST.fit_on_texts(combineddata_AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca06e6a7-b4da-4c50-be89-5c0a48d38fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to index \n",
    "x_train_context_sequences = tk_context.texts_to_sequences(x_train_context)\n",
    "x_train_AST_sequences = tk_AST.texts_to_sequences(x_train_AST)\n",
    "x_test_context_sequences = tk_context.texts_to_sequences(x_test_context)\n",
    "x_test_AST_sequences = tk_AST.texts_to_sequences(x_test_AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38ee9087-f99c-4c5b-a27a-49a6f2389de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Padding\n",
    "x_train_context = pad_sequences(x_train_context_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "x_train_AST = pad_sequences(x_train_AST_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "x_test_context = pad_sequences(x_test_context_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "x_test_AST = pad_sequences(x_test_AST_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "x_train_context = np.array(x_train_context)\n",
    "x_train_AST = np.array(x_train_AST)\n",
    "x_test_context = np.array(x_test_context)\n",
    "x_test_AST = np.array(x_test_AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc41ae1-fde2-421c-9c26-974977726c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "noofbugtype = len(btype_labels_map)\n",
    "noofdev = len(dev_labels_map)\n",
    "btype_y_train = np.array(btype_y_train)\n",
    "dev_y_train = np.array(dev_y_train)\n",
    "btype_y_test = np.array(btype_y_test)\n",
    "dev_y_test = np.array(dev_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed80709-683a-4294-bfbf-0acf5144f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time = 2023-11-10 17:45:32.508875\n",
      "Predict Developer\n"
     ]
    }
   ],
   "source": [
    "# Visualize Model\n",
    "logdir = \"logs/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "# A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patiencez\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "print(\"Start Time =\", starttime)\n",
    "print('Predict Developer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85ba3da-ac6f-44b3-96f3-14a34536a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "# inputs\n",
    "input_context = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32,\n",
    "                        name=\"Bug_TitleandDescription\")  # Bug Title and Description\n",
    "input_AST = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.float32, name=\"Bug_CodeSnippetAST\")  # Bug Code Snippet AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bccb112b-4e16-46f2-8977-ccb3b1f7c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 17:48:29.620237: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-11-10 17:48:29.620264: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2023-11-10 17:48:29.620995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Conv1D, GlobalMaxPool1D, Flatten\n",
    "# Context Enconder\n",
    "emb_Context = Embedding(input_dim=len(tk_context.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        output_dim=EMBEDDING_DIM, name=\"Context_Embedding\")(input_context)\n",
    "conv_Context = Conv1D(filters=64, kernel_size=2, padding='same', activation='relu',\n",
    "                        name=\"Context_Convolutional_Layer\")(emb_Context)\n",
    "maxpool_Context = GlobalMaxPool1D(name=\"Context_Maxpool_Layer\")(conv_Context)\n",
    "flatcon = Flatten(name=\"Context_Flatten_Layer\")(maxpool_Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16fa39d3-9dd8-4f96-b7eb-90d3f823c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Flatten\n",
    "# AST Enconder\n",
    "emb_AST = Embedding(input_dim=len(tk_AST.word_index) + 2, input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    output_dim=EMBEDDING_DIM, name=\"AST_Embedding\")(input_AST)\n",
    "bilstm_AST = Bidirectional(LSTM(25, return_sequences=True, name=\"AST_LSTM_Layer\"))(emb_AST)\n",
    "maxpool_AST = GlobalMaxPool1D(name=\"AST_Maxpool_Layer\")(bilstm_AST)\n",
    "flatAST = Flatten(name=\"AST_Flatten_Layer\")(maxpool_AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce8a5003-73a3-420c-9c53-a703827169b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, BatchNormalization, Dropout, Dense\n",
    "from keras.models import Model\n",
    "cat = concatenate([flatcon, flatAST], name=\"Concatenate_Flatten_Layer\")\n",
    "\n",
    "bn = BatchNormalization()(cat)\n",
    "drop = Dropout(0.5)(bn)\n",
    "dense = Dense(50, activation='relu')(drop)\n",
    "DevOutput = Dense(noofdev, activation='sigmoid', name=\"Developer\")(dense)\n",
    "BugTypeOutput = Dense(noofbugtype, activation='sigmoid', name=\"Bug_Type\")(dense)\n",
    "Bil_LSTM_MultiTask_model = Model(inputs=[input_context, input_AST], outputs=[DevOutput, BugTypeOutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccefb3-df73-4051-83a8-4bf4b8f1bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from evaluation import *\n",
    "Bil_LSTM_MultiTask_model.compile(loss='binary_crossentropy', optimizer=Adam(LearningRate),\n",
    "                                metrics=['accuracy', c_precision, c_recall, 'AUC', c_f1_macro, c_fbeta,\n",
    "                                              hammingloss])\n",
    "\n",
    "history = Bil_LSTM_MultiTask_model.fit([x_train_context, x_train_AST],\n",
    "                                        [dev_y_train, btype_y_train],\n",
    "                                        callbacks=[tensorboard_callback, earlystop],\n",
    "                                        epochs=50, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362c5ba0-5b83-4e91-84ce-0c8a5db486ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 17:57:34.620738: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CSVLogger\n\u001b[0;32m----> 2\u001b[0m csv_logger \u001b[38;5;241m=\u001b[39m CSVLogger(\u001b[43mfilename\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m Bil_LSTM_MultiTask_model\u001b[38;5;241m.\u001b[39mevaluate([x_test_context, x_test_AST], [dev_y_test, btype_y_test], callbacks\u001b[38;5;241m=\u001b[39m[csv_logger])\n\u001b[1;32m      5\u001b[0m filelog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "csv_logger = CSVLogger(filename + '.csv', append=True, separator=';')\n",
    "Bil_LSTM_MultiTask_model.evaluate([x_test_context, x_test_AST], [dev_y_test, btype_y_test], callbacks=[csv_logger])\n",
    "\n",
    "filelog = open(filename + \".txt\", \"a\")\n",
    "filelog.write(\"Endtime:\" + str(datetime.now()))\n",
    "filelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180920ab-be2e-48f6-9ead-6317a26ae723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
