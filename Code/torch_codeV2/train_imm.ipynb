{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.466477Z",
     "start_time": "2024-04-06T07:03:20.823265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6364a1518c0e7be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T07:03:27.472339Z",
     "start_time": "2024-04-06T07:03:27.466750Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset utils\n",
    "def get_map(labels):\n",
    "    l_set = set()\n",
    "    for label in labels:\n",
    "        l_set.update(label)\n",
    "    ids2token = list(l_set)\n",
    "    token2ids = {ids2token[i] : i for i in range(len(ids2token))}\n",
    "    return ids2token, token2ids\n",
    "\n",
    "def onehot(labels, token2ids):\n",
    "    vec = [0 for i in token2ids]\n",
    "    for label in labels.split('| '):\n",
    "        vec[token2ids[label]] = 1\n",
    "    return vec\n",
    "\n",
    "def lab(labels, token2ids):\n",
    "    return [token2ids[label] for label in labels.split('| ')]\n",
    "\n",
    "def label_vectorize(data):\n",
    "    data = data.rename(columns={'Title_Description' : 'Context', 'AST' : 'AST', 'FixedByID' : 'Dev', 'Name' : 'Btype'})\n",
    "    data = data[['Context', 'AST', 'Dev', 'Btype']]\n",
    "    # avoid NaN in dataset\n",
    "    data['Context'].fillna('[UNK]', inplace=True)\n",
    "    data['AST'].fillna('[UNK]', inplace=True)\n",
    "    data['Dev'].fillna('unknown', inplace=True)\n",
    "    data['Btype'].fillna('unknown', inplace=True)\n",
    "    \n",
    "    D_labels = [label.split('| ') for label in data['Dev']]\n",
    "    _D_ids2token, D_token2ids = get_map(D_labels)\n",
    "    data['Dev_l'] = data['Dev'].map(partial(lab, token2ids = D_token2ids))\n",
    "    data['Dev_vec'] = data['Dev'].map(partial(onehot, token2ids = D_token2ids))\n",
    "    \n",
    "    B_labels = [label.split('| ') for label in data['Btype']]\n",
    "    _B_ids2token, B_token2ids = get_map(B_labels)\n",
    "    data['Btype_l'] = data['Btype'].map(partial(lab, token2ids = B_token2ids))\n",
    "    data['Btype_vec'] = data['Btype'].map(partial(onehot, token2ids = B_token2ids))\n",
    "    \n",
    "    return data, _D_ids2token, _B_ids2token\n",
    "\n",
    "def tokenize_function(_tokenizer, example, max_seq_len = 512):\n",
    "    example = example if type(example) == str else _tokenizer.unk_token\n",
    "    return _tokenizer(example, padding='max_length',\n",
    "                                truncation=True, max_length=max_seq_len, return_tensors=\"pt\")\n",
    "\n",
    "def tensor_func(example):\n",
    "    return torch.tensor(example)\n",
    "\n",
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return (self.data['x_C'][item], self.data['x_A'][item]), self.data['y'][item]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe02fce-275e-4ca0-80f2-f25e72398747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T06:23:54.167637Z",
     "start_time": "2024-04-06T06:23:54.158896Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss & metrics\n",
    "class CustomizedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    a flexible version of BCE,\n",
    "    which enable the loss to focus more on the performance of positive samples' prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_pos=0.8, weight_neg=0.2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight_pos = weight_pos\n",
    "        self.weight_neg = weight_neg\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = nn.Sigmoid()(x)\n",
    "        loss_pos = y * torch.log(x)\n",
    "        loss_neg = (1 - y) * torch.log(1 - x)\n",
    "        # loss = 0.8*loss_pos + 0.2*loss_neg\n",
    "        loss = self.weight_pos * loss_pos + self.weight_neg * loss_neg\n",
    "        return -torch.sum(loss)\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    \"\"\"\n",
    "    AsymmetricLoss from https://github.com/Alibaba-MIIL/ASL/blob/main/src/loss_functions/losses.py\n",
    "\n",
    "    Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()\n",
    "\n",
    "def metrics(y: torch.Tensor, pred: torch.Tensor, split_pos: list, threshold: float = 0.5, from_logits=True):\n",
    "    if from_logits:\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "    pred = torch.where(pred > threshold, 1, 0)\n",
    "\n",
    "    y_d, y_b = torch.split(y, split_pos, dim=1)\n",
    "    pred_d, pred_b = torch.split(pred, split_pos, dim=1)\n",
    "\n",
    "    TPd, TPb = torch.sum(y_d * pred_d, dim=1), torch.sum(y_b * pred_b, dim=1)\n",
    "    TNd, TNb = torch.sum((1 - y_d) * (1 - pred_d), dim=1), torch.sum((1 - y_b) * (1 - pred_b), dim=1)\n",
    "    FPd, FPb = torch.sum((1 - y_d) * pred_d, dim=1), torch.sum((1 - y_b) * pred_b, dim=1)\n",
    "    FNd, FNb = torch.sum(y_d * (1 - pred_d), dim=1), torch.sum(y_b * (1 - pred_b), dim=1)\n",
    "\n",
    "    acc = torch.mean((TPd + TNd) / (TPd + TNd + FPd + FNd + 1e-6)).item(), torch.mean(\n",
    "        (TPb + TNb) / (TPb + TNb + FPb + FNb + 1e-6)).item()\n",
    "    recall = torch.mean(TPd / (TPd + FNd + 1e-6)).item(), torch.mean(TPb / (TPb + FNb + 1e-6)).item()\n",
    "    precision = torch.mean(TPd / (TPd + FPd + 1e-6)).item(), torch.mean(TPb / (TPb + FPb + 1e-6)).item()\n",
    "    F1 = 2 * recall[0] * precision[0] / (recall[0] + precision[0] + 1e-6), 2 * recall[1] * precision[1] / (\n",
    "            recall[1] + precision[1] + 1e-6)\n",
    "\n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': F1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8710ecaf-fa9b-4e52-b973-b544e363f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imm(_path, _logname, _loss_fn, _use_ast = True, _is_textcnn = False, _num_epochs = 100, _bsz = 8,\n",
    "              _lr = 3e-5, _ckpt = 'bert-base-uncased', device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    logname = '../res_log/' + _logname + '.txt'\n",
    "    logstr = _logname + '\\n' + '-'*60 + '\\n'\n",
    "    \n",
    "    # dataset label vectorize\n",
    "    dataset = pd.read_csv(_path)\n",
    "    logstr += 'dataset shape:{}\\n'.format(dataset.shape)\n",
    "    print('dataset shape:{}'.format(dataset.shape))\n",
    "    dataset, D_ids2token, B_ids2token = label_vectorize(dataset)\n",
    "    n_classes = [len(D_ids2token), len(B_ids2token)]\n",
    "    logstr += 'n_classes:{}\\n'.format(n_classes) + '-'*60 + '\\n'\n",
    "    print('n_classes: ', n_classes)\n",
    "\n",
    "    check_point = _ckpt\n",
    "    tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "    # datset tensorize\n",
    "    dataset['x_C'] = dataset['Context'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['x_A'] = dataset['AST'].map(partial(tokenize_function, tokenizer))\n",
    "    dataset['y'] = dataset['Dev_vec'] + dataset['Btype_vec']\n",
    "    dataset['y'] = dataset['y'].map(tensor_func)\n",
    "\n",
    "    # split datset\n",
    "    t_dataset = dataset[:int(0.8*len(dataset))].reset_index(drop=True)\n",
    "    train_dataset = t_dataset.sample(frac=0.8,random_state=0,axis=0).reset_index(drop=True)\n",
    "    val_dataset = t_dataset[~t_dataset.index.isin(train_dataset.index)].reset_index(drop=True)\n",
    "    test_dataset = dataset[int(0.8*len(dataset)):].reset_index(drop=True)\n",
    "\n",
    "    # wrap dataset & dataloader\n",
    "    train_dataset = TextCodeDataset(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "    val_dataset = TextCodeDataset(val_dataset)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "    test_dataset = TextCodeDataset(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=_bsz, drop_last=True)\n",
    "\n",
    "    # load model\n",
    "    if _is_textcnn:\n",
    "        model = MetaModel(n_classes = n_classes, use_AST=_use_ast)\n",
    "    else:\n",
    "        # TODO: seperate ckpt\n",
    "        model = PretrainModel(text_ckpt=_ckpt, code_ckpt=_ckpt, n_classes=n_classes, use_AST=_use_ast)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # loss\n",
    "    loss_fn = _loss_fn.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=_lr)\n",
    "\n",
    "    # lr_scheduler\n",
    "    num_epochs = _num_epochs\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # train process\n",
    "    val_loss_min, val_down = 100000.0, [1, 1, 1, 1, 1]\n",
    "    for epoch in trange(num_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "            x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(x_C, x_A)\n",
    "\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            train_loss += loss.item()/len(train_dataloader)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        logstr += '{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss)\n",
    "        # print('{}th epoch\\n train_loss: {}\\n'.format(epoch, train_loss))\n",
    "    \n",
    "        # val\n",
    "        model.eval()\n",
    "        val_loss, val_acc, val_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "        for x, y in val_dataloader:\n",
    "            x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "            x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x_C, x_A)\n",
    "        \n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            val_loss += loss.item()/len(val_dataloader)\n",
    "            metric = metrics(y, outputs, split_pos = n_classes)\n",
    "            val_acc[0] += metric['acc'][0]/len(val_dataloader)\n",
    "            val_acc[1] += metric['acc'][1]/len(val_dataloader)\n",
    "            val_f1[0] += metric['F1'][0]/len(val_dataloader)\n",
    "            val_f1[1] += metric['F1'][1]/len(val_dataloader)\n",
    "        logstr += '{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}\\n'.format(epoch, val_loss, val_acc, val_f1)\n",
    "        # print('{}th epoch\\n val_loss: {}\\n val_acc:{}\\n val_f1: {}'.format(epoch, val_loss, val_acc, val_f1))\n",
    "\n",
    "        val_down.append(1 if val_loss_min - val_loss > 1e-10 else 0)\n",
    "        val_loss_min = min(val_loss_min, val_loss)\n",
    "        if val_down[-1] + val_down[-2] + val_down[-3] + val_down[-4] + val_down[-5] == 0:\n",
    "            break\n",
    "\n",
    "    # test\n",
    "    model.eval()\n",
    "    test_loss, test_acc, test_f1 = 0.0, [0.0, 0.0], [0.0, 0.0]\n",
    "    for x, y in tqdm(test_dataloader):\n",
    "        x_C = {k: v.to(device) for k, v in x[0].items()}\n",
    "        x_A = {k: v.to(device) for k, v in x[1].items()}\n",
    "        y = y.to(device)\n",
    "            \n",
    "        outputs = model(x_C, x_A)\n",
    "                \n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        test_loss += loss.item()/len(test_dataloader)\n",
    "        metric = metrics(y, outputs, split_pos = n_classes)\n",
    "        test_acc[0] += metric['acc'][0]/len(test_dataloader)\n",
    "        test_acc[1] += metric['acc'][1]/len(test_dataloader)\n",
    "        test_f1[0] += metric['F1'][0]/len(test_dataloader)\n",
    "        test_f1[1] += metric['F1'][1]/len(test_dataloader)\n",
    "    logstr += '-' * 60 + '\\ntest_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1)\n",
    "    print('test_loss: {}\\n test_acc:{}\\n test_f1: {}'.format(test_loss, test_acc, test_f1))\n",
    "\n",
    "    with open(logname, 'w') as f:\n",
    "        f.write(logstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06eddbeb-50fe-4beb-bcfc-07638df32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 datasets\n",
    "pathlist = [\n",
    "    ('../Data/aspnet/aspnet_2.csv', 'aspnet'),\n",
    "    ('../Data/efcore/efcore_2.csv', 'efcore'),\n",
    "    ('../Data/elasticSearch/elasticSearch_2.csv', 'elasticSearch'),\n",
    "    # ('../Data/mixedRealityToolUnity/mixedRealityToolUnity_2.csv', 'mixedRealityToolUnity'),\n",
    "    # ('../Data/monoGame/monoGame_2.csv', 'monoGame'),\n",
    "    ('../Data/powershell/powerShell_2.csv', 'powerShell'),\n",
    "    ('../Data/realmJava/realmJava_2.csv', 'realmJava'),\n",
    "    ('../Data/roslyn/roslyn_2.csv', 'roslyn'),\n",
    "]\n",
    "losslist = [\n",
    "    # (nn.BCEWithLogitsLoss(), 'BCE'),\n",
    "    # (CustomizedBCELoss(), 'CBCE'),\n",
    "    (AsymmetricLossOptimized(), 'ASL'),\n",
    "]\n",
    "\n",
    "ckptlist = [\n",
    "    ('bert-base-uncased', 'Multi-triage'),  # just for tokenize\n",
    "    # ('bert-base-uncased', ' Bert'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9f5bcf-58f3-4100-b88f-f1f52b620be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1006, 7)\n",
      "n_classes:  [32, 88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1008.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.24it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 393.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.834178695678714\n",
      " test_acc:[0.9817187500000001, 0.9507386350631711]\n",
      " test_f1: [0.7364400908384727, 0.2955426034800301]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "aspnet Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1006, 7)\n",
      "n_classes:  [32, 88]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [00:51<00:01,  1.89it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 345.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 14.474770202636716\n",
      " test_acc:[0.9801562500000001, 0.9521590852737424]\n",
      " test_f1: [0.7231755649094918, 0.30623835983731007]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1740, 7)\n",
      "n_classes:  [15, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:31<00:21,  1.89it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 478.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 10.691873993984489\n",
      " test_acc:[0.9281007190083348, 0.9319530462109764]\n",
      " test_f1: [0.6238718662398647, 0.38134451645918155]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "efcore Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1740, 7)\n",
      "n_classes:  [15, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:09<00:17,  1.16it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 346.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 10.386687378550684\n",
      " test_acc:[0.9255813429521959, 0.9287494379420611]\n",
      " test_f1: [0.6177459288151886, 0.3993598219173758]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "elasticSearch Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1634, 7)\n",
      "n_classes:  [86, 188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:42<00:09,  1.91it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 399.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 17.80180823802948\n",
      " test_acc:[0.9868822619318962, 0.9843915998935694]\n",
      " test_f1: [0.44955986896692823, 0.3866683318388676]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "elasticSearch Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1634, 7)\n",
      "n_classes:  [86, 188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:13<00:17,  1.10it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 322.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 18.001556611061098\n",
      " test_acc:[0.9870276063680653, 0.9843749806284899]\n",
      " test_f1: [0.4381392580493889, 0.3586912095279595]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "powerShell Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(312, 7)\n",
      "n_classes:  [144, 90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:08<00:01, 10.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 357.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 36.66150883265904\n",
      " test_acc:[0.865823405129569, 0.8331349066325597]\n",
      " test_f1: [0.019181429644476063, 0.11382812210539411]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "powerShell Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(312, 7)\n",
      "n_classes:  [144, 90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:01<00:15,  5.89it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 347.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 62.55499485560826\n",
      " test_acc:[0.5307539531162806, 0.5418650848524912]\n",
      " test_f1: [0.021118453230275367, 0.04941333283908968]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "realmJava Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(340, 7)\n",
      "n_classes:  [11, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:09<00:02,  8.87it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 381.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 8.333059787750244\n",
      " test_acc:[0.9176135584712029, 0.760416567325592]\n",
      " test_f1: [0.7455773750184688, 0.2459116555090667]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "realmJava Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(340, 7)\n",
      "n_classes:  [11, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:13<00:05,  5.31it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 317.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 8.750559389591217\n",
      " test_acc:[0.8778408244252205, 0.762152686715126]\n",
      " test_f1: [0.6599005121643212, 0.2394443346822738]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "roslyn Multi-triage ASL no_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1316, 7)\n",
      "n_classes:  [58, 97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [00:38<00:02,  2.44it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 460.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 18.020436460321598\n",
      " test_acc:[0.9799503720167914, 0.9527881849895822]\n",
      " test_f1: [0.4839983090578555, 0.39450672330491543]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "roslyn Multi-triage ASL use_AST\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dataset shape:(1316, 7)\n",
      "n_classes:  [58, 97]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:50<00:16,  1.48it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 335.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 18.356628475767195\n",
      " test_acc:[0.9793625835216407, 0.9517728866952839]\n",
      " test_f1: [0.4942357072722464, 0.37503145194570287]\n"
     ]
    }
   ],
   "source": [
    "for path in pathlist:\n",
    "    for ckpt in ckptlist:\n",
    "        for loss in losslist:\n",
    "            is_t = (ckpt[1] == 'Multi-triage')       \n",
    "            logname = ' '.join([path[1], ckpt[1], loss[1], 'no_AST'])\n",
    "            print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            train_imm(_path = path[0], _logname = logname, _loss_fn = loss[0], \n",
    "                      _use_ast = False, _is_textcnn = is_t, _ckpt = ckpt[0])\n",
    "            \n",
    "            logname = ' '.join([path[1], ckpt[1], loss[1], 'use_AST'])\n",
    "            print('-'*100, logname, '-'*100, sep='\\n')\n",
    "            train_imm(_path = path[0], _logname = logname, _loss_fn = loss[0], _is_textcnn = is_t, _ckpt = ckpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f758c22-1da8-4bac-917e-a7309461873c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617388cb-09b9-46f4-8843-be99446f8fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
